Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets . The Fully Independent Training Conditional ( FITC ) and the Variational Free Energy ( VFE ) approximations are two recent popular methods . Despite superficial similarities , these approximations have surprisingly different theoretical properties and behave differently in practice . We thoroughly investigate the two methods for regression both analytically and through illustrative examples , and draw conclusions to guide practical application .
We investigate task clustering for deep-learning based multi-task and few-shot learning in a many-task setting . We propose a new method to measure task similarities with cross-task transfer performance matrix for the deep learning scenario . Although this matrix provides us critical information regarding similarity between tasks , its asymmetric property and unreliable performance scores can affect conventional clustering methods adversely . Additionally , the uncertain task-pairs , i . e . , the ones with extremely asymmetric transfer scores , may collectively mislead clustering algorithms to output an inaccurate task-partition . To overcome these limitations , we propose a novel task-clustering algorithm by using the matrix completion technique . The proposed algorithm constructs a partially-observed similarity matrix based on the certainty of cluster membership of the task-pairs . We then use a matrix completion algorithm to complete the similarity matrix . Our theoretical analysis shows that under mild constraints , the proposed algorithm will perfectly recover the underlying " true " similarity matrix with a high probability . Our results show that the new task clustering method can discover task clusters for training flexible and superior neural network models in a multi-task learning setup for sentiment classification and dialog intent classification tasks . Our task clustering approach also extends metric-based few-shot learning methods to adapt multiple metrics , which demonstrates empirical advantages when the tasks are diverse .
The pairwise influence matrix of Dobrushin has long been used as an analytical tool to bound the rate of convergence of Gibbs sampling . In this work , we use Dobrushin influence as the basis of a practical tool to certify and efficiently improve the quality of a discrete Gibbs sampler . Our Dobrushin-optimized Gibbs samplers ( DoGS ) offer customized variable selection orders for a given sampling budget and variable subset of interest , explicit bounds on total variation distance to stationarity , and certifiable improvements over the standard systematic and uniform random scan Gibbs samplers . In our experiments with joint image segmentation and object recognition , Markov chain Monte Carlo maximum likelihood estimation , and Ising model inference , DoGS consistently deliver higher-quality inferences with significantly smaller sampling budgets than standard Gibbs samplers .
Covariate balance is a conventional key diagnostic for methods used estimating causal effects from observational studies . Recently , there is an emerging interest in directly incorporating covariate balance in the estimation . We study a recently proposed entropy maximization method called Entropy Balancing ( EB ) , which exactly matches the covariate moments for the different experimental groups in its optimization problem . We show EB is doubly robust with respect to linear outcome regression and logistic propensity score regression , and it reaches the asymptotic semiparametric variance bound when both regressions are correctly specified . This is surprising to us because there is no attempt to model the outcome or the treatment assignment in the original proposal of EB . Our theoretical results and simulations suggest that EB is a very appealing alternative to the conventional weighting estimators that estimate the propensity score by maximum likelihood .
Since the birth of computer and networks , fuelled by pervasive computing and ubiquitous connectivity , the amount of data stored and transmitted has exponentially grown through the years . Due to this demand , new solutions for storing data are needed , and one promising media is the DNA . This storage solution provides numerous advantages , which includes the ability to store dense information while achieving long-term stability . However , the question as how the data can be retrieved from a DNA-based archive , still remains . In this paper , we aim to address this question by proposing a new storage solution that relies upon molecular communication , and in particular bacterial nanonetworks . Our solution allows digitally encoded information to be stored into non-motile bacteria , which compose an archival architecture of clusters , and to be later retrieved by engineered motile bacteria , whenever reading operations are needed . We conducted extensive simulations , in order to determine the reliability of data retrieval from non-motile storage clusters , placed at different locations . Aiming to assess the feasibility of our solution , we have also conducted wet lab experiments that show how bacteria nanonetworks can effectively retrieve a simple message , such as " Hello World " , by conjugation with non-motile bacteria , and finally mobilize towards a final point .
Adaptive nuclear-norm penalization is proposed for low-rank matrix approximation , by which we develop a new reduced-rank estimation method for the general high-dimensional multivariate regression problems . The adaptive nuclear norm of a matrix is defined as the weighted sum of the singular values of the matrix . For example , the pre-specified weights may be some negative power of the singular values of the data matrix ( or its projection in regression setting ) . The adaptive nuclear norm is generally non-convex under the natural restriction that the weight decreases with the singular value . However , we show that the proposed non-convex penalized regression method has a global optimal solution obtained from an adaptively soft-thresholded singular value decomposition . This new reduced-rank estimator is computationally efficient , has continuous solution path and possesses better bias-variance property than its classical counterpart . The rank consistency and prediction/estimation performance bounds of the proposed estimator are established under high-dimensional asymptotic regime . Simulation studies and an application in genetics demonstrate that the proposed estimator has superior performance to several existing methods . The adaptive nuclear-norm penalization can also serve as a building block to study a broad class of singular value penalties .
Most companies ' new business practices are based on customer data . These practices have raised privacy concerns because of the associated risks . Privacy laws require companies to gain customer consent before using their information , which stands as the biggest roadblock to monetise this asset . Privacy literature suggests that reducing privacy concerns and building trust may increase individuals ' intention to authorise the use of personal information . Fair information practices ( FIPs ) are potential means to achieve this goal . However , there is lack of empirical evidence on the mechanisms through which the FIPs affect privacy concerns and trust . This research argues that FIPs load individuals with control , which has been found to influence privacy concerns and trust level . We will use an experimental design methodology to conduct the study . The results are expected to have both theoretical and managerial implications .
We study high-dimensional linear models with error-in-variables . Such models are motivated by various applications in econometrics , finance and genetics . These models are challenging because of the need to account for measurement errors to avoid non-vanishing biases in addition to handle the high dimensionality of the parameters . A recent growing literature has proposed various estimators that achieve good rates of convergence . Our main contribution complements this literature with the construction of simultaneous confidence regions for the parameters of interest in such high-dimensional linear models with error-in-variables . These confidence regions are based on the construction of moment conditions that have an additional orthogonal property with respect to nuisance parameters . We provide a construction that requires us to estimate an additional high-dimensional linear model with error-in-variables for each component of interest . We use a multiplier bootstrap to compute critical values for simultaneous confidence intervals for a subset $S$ of the components . We show its validity despite of possible model selection mistakes , and allowing for the cardinality of $S$ to be larger than the sample size . We apply and discuss the implications of our results to two examples and conduct Monte Carlo simulations to illustrate the performance of the proposed procedure .
The software industry is successful , if it can draw the complete attention of the customers towards it . This is achievable if the organization can produce a high quality product . To identify a product to be of high quality , it should be free of defects , should be capable of producing expected results . It should be delivered in an estimated cost , time and be maintainable with minimum effort . Defect Prevention is the most critical but often neglected component of the software quality assurance in any project . If applied at all stages of software development , it can reduce the time , cost and resources required to engineer a high quality product .
We explore the use of semantic word embeddings in text segmentation algorithms , including the C00 segmentation algorithm and new algorithms inspired by the distributed word vector representation . By developing a general framework for discussing a class of segmentation objectives , we study the effectiveness of greedy versus exact optimization approaches and suggest a new iterative refinement technique for improving the performance of greedy strategies . We compare our results to known benchmarks , using known metrics . We demonstrate state-of-the-art performance for an untrained method with our Content Vector Segmentation ( CVS ) on the Choi test set . Finally , we apply the segmentation procedure to an in-the-wild dataset consisting of text extracted from scholarly articles in the arXiv . org database .
Weighted automata ( WA ) are an important formalism to describe quantitative properties . Obtaining equivalent deterministic machines is a longstanding research problem . In this paper we consider WA with a set semantics , meaning that the semantics is given by the set of weights of accepting runs . We focus on multi-sequential WA that are defined as finite unions of sequential WA . The problem we address is to minimize the size of this union . We call this minimum the degree of sequentiality of ( the relation realized by ) the WA . For a given positive integer k , we provide multiple characterizations of relations realized by a union of k sequential WA over an infinitary finitely generated group : a Lipschitz-like machine independent property , a pattern on the automaton ( a new twinning property ) and a subclass of cost register automata . When possible , we effectively translate a WA into an equivalent union of k sequential WA . We also provide a decision procedure for our twinning property for commutative computable groups thus allowing to compute the degree of sequentiality . Last , we show that these results also hold for word transducers and that the associated decision problem is Pspace-complete .
We consider the problem of efficient financial surveillance aimed at " on-the-go " detection of structural breaks ( anomalies ) in " live " -monitored financial time series . With the problem approached statistically , viz . as that of multi-cyclic sequential ( quickest ) change-point detection , we propose a semi-parametric multi-cyclic change-point detection procedure to promptly spot anomalies as they occur in the time series under surveillance . The proposed procedure is a derivative of the likelihood ratio-based Shiryaev-Roberts ( SR ) procedure ; the latter is a quasi-Bayesian surveillance method known to deliver the fastest ( in the multi-cyclic sense ) speed of detection , whatever be the false alarm frequency . We offer a case study where we first carry out , step by step , statistical analysis of a set of real-world financial data , and then set up and devise ( a ) the proposed SR-based anomaly-detection procedure and ( b ) the celebrated Cumulative Sum ( CUSUM ) chart to detect structural breaks in the data . While both procedures performed well , the proposed SR-derivative , conforming to the intuition , seemed slightly better .
The identification of patient subgroups with differential treatment effects is the first step towards individualised treatments . A current draft guideline by the EMA discusses potentials and problems in subgroup analyses and formulated challenges to the development of appropriate statistical procedures for the data-driven identification of patient subgroups . We introduce model-based recursive partitioning as a procedure for the automated detection of patient subgroups that are identifiable by predictive factors . The method starts with a model for the overall treatment effect as defined for the primary analysis in the study protocol and uses measures for detecting parameter instabilities in this treatment effect . The procedure produces a segmented model with differential treatment parameters corresponding to each patient subgroup . The subgroups are linked to predictive factors by means of a decision tree . The method is applied to the search for subgroups of patients suffering from amyotrophic lateral sclerosis that differ with respect to their Riluzole treatment effect , the only currently approved drug for this disease .
Advanced internet technologies providing services like e-mail , social networking , online banking , online shopping etc . , have made day-to-day activities simple and convenient . Increasing dependency on the internet , convenience , and decreasing cost of electronic devices have resulted in frequent use of online services . However , increased indulgence over the internet has also accelerated the pace of digital crimes . The increase in number and complexity of digital crimes has caught the attention of forensic investigators . The Digital Investigators are faced with the challenge of gathering accurate digital evidence from as many sources as possible . In this paper , an attempt was made to recover digital evidence from a system ' s RAM in the form of information about the most recent browsing session of the user . Four different applications were chosen and the experiment was conducted across two browsers . It was found that crucial information about the target user such as , user name , passwords , etc . , was recoverable .
For many classification and regression problems , a large number of features are available for possible use - this is typical of DNA microarray data on gene expression , for example . Often , for computational or other reasons , only a small subset of these features are selected for use in a model , based on some simple measure such as correlation with the response variable . This procedure may introduce an optimistic bias , however , in which the response variable appears to be more predictable than it actually is , because the high correlation of the selected features with the response may be partly or wholely due to chance . We show how this bias can be avoided when using a Bayesian model for the joint distribution of features and response . The crucial insight is that even if we forget the exact values of the unselected features , we should retain , and condition on , the knowledge that their correlation with the response was too small for them to be selected . In this paper we describe how this idea can be implemented for ``naive Bayes ' ' models of binary data . Experiments with simulated data confirm that this method avoids bias due to feature selection . We also apply the naive Bayes model to subsets of data relating gene expression to colon cancer , and find that correcting for bias from feature selection does improve predictive performance .
Since their inception in the 0000 ' s , regression trees have been one of the more widely used non-parametric prediction methods . Tree-structured methods yield a histogram reconstruction of the regression surface , where the bins correspond to terminal nodes of recursive partitioning . Trees are powerful , yet susceptible to over-fitting . Strategies against overfitting have traditionally relied on pruning greedily grown trees . The Bayesian framework offers an alternative remedy against overfitting through priors . Roughly speaking , a good prior charges smaller trees where overfitting does not occur . While the consistency of random histograms , trees and their ensembles has been studied quite extensively , the theoretical understanding of the Bayesian counterparts has been missing . In this paper , we take a step towards understanding why/when do Bayesian trees and their ensembles not overfit . To address this question , we study the speed at which the posterior concentrates around the true smooth regression function . We propose a spike-and-tree variant of the popular Bayesian CART prior and establish new theoretical results showing that regression trees ( and their ensembles ) ( a ) are capable of recovering smooth regression surfaces , achieving optimal rates up to a log factor , ( b ) can adapt to the unknown level of smoothness and ( c ) can perform effective dimension reduction when p>n . These results provide a piece of missing theoretical evidence explaining why Bayesian trees ( and additive variants thereof ) have worked so well in practice .
Simple correlation coefficients between two variables have been generalized to measure association between two matrices in many ways . Coefficients such as the RV coefficient , the distance covariance ( dCov ) coefficient and kernel based coefficients have been adopted by different research communities . Scientists use these coefficients to test whether two random vectors are linked . If they are , it is important to uncover what patterns exist in these associations . We discuss the topic of measures of dependence between random vectors and tests of independence and show links between different approaches . We document some of the interesting rediscoveries and lack of interconnection between bodies of literature . After providing definitions of the coefficients and associated tests , we present the recent improvements that enhance their statistical properties and ease of interpretation . We summarize multi-table approaches and provide scenarii where the indices can provide useful summaries of heterogeneous multi-block data . We illustrate these different strategies on several examples of real data and suggest directions for future research .
We consider the problem of detecting whether or not , in a given sensor network , there is a cluster of sensors which exhibit an " unusual behavior . " Formally , suppose we are given a set of nodes and attach a random variable to each node . We observe a realization of this process and want to decide between the following two hypotheses : under the null , the variables are i . i . d . standard normal ; under the alternative , there is a cluster of variables that are i . i . d . normal with positive mean and unit variance , while the rest are i . i . d . standard normal . We also address surveillance settings where each sensor in the network collects information over time . The resulting model is similar , now with a time series attached to each node . We again observe the process over time and want to decide between the null , where all the variables are i . i . d . standard normal , and the alternative , where there is an emerging cluster of i . i . d . normal variables with positive mean and unit variance . The growth models used to represent the emerging cluster are quite general and , in particular , include cellular automata used in modeling epidemics . In both settings , we consider classes of clusters that are quite general , for which we obtain a lower bound on their respective minimax detection rate and show that some form of scan statistic , by far the most popular method in practice , achieves that same rate to within a logarithmic factor . Our results are not limited to the normal location model , but generalize to any one-parameter exponential family when the anomalous clusters are large enough .
This paper defends an augmented cognitively oriented " generic-design hypothesis " : There are both significant similarities between the design activities implemented in different situations and crucial differences between these and other cognitive activities ; yet , characteristics of a design situation ( i . e . , related to the designers , the artefact , and other task variables influencing these two ) introduce specificities in the corresponding design activities and cognitive structures that are used . We thus combine the generic-design hypothesis with that of different " forms " of designing . In this paper , outlining a number of directions that need further elaboration , we propose a series of candidate dimensions underlying such forms of design .
Correction to Bernoulli ( 0000 ) , 00 , 000--000 http : //projecteuclid . org/euclid . bj/0000000000
In this paper we first describe the class of log-Gaussian Cox processes ( LGCPs ) as models for spatial and spatio-temporal point process data . We discuss inference , with a particular focus on the computational challenges of likelihood-based inference . We then demonstrate the usefulness of the LGCP by describing four applications : estimating the intensity surface of a spatial point process ; investigating spatial segregation in a multi-type process ; constructing spatially continuous maps of disease risk from spatially discrete data ; and real-time health surveillance . We argue that problems of this kind fit naturally into the realm of geostatistics , which traditionally is defined as the study of spatially continuous processes using spatially discrete observations at a finite number of locations . We suggest that a more useful definition of geostatistics is by the class of scientific problems that it addresses , rather than by particular models or data formats .
Three important properties of a classification machinery are : ( i ) the system preserves the core information of the input data ; ( ii ) the training examples convey information about unseen data ; and ( iii ) the system is able to treat differently points from different classes . In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks . We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data , with a special treatment for in-class and out-of-class data . Similar points at the input of the network are likely to have a similar output . The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature , thereby making a formal connection between these important topics . The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure , as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data . The results are validated with state-of-the-art trained networks .
We present in this paper a library to compute with Taylor models , a technique extending interval arithmetic to reduce decorrelation and to solve differential equations . Numerical software usually produces only numerical results . Our library can be used to produce both results and proofs . As seen during the development of Fermat ' s last theorem reported by Aczel 0000 , providing a proof is not sufficient . Our library provides a proof that has been thoroughly scrutinized by a trustworthy and tireless assistant . PVS is an automatic proof assistant that has been fairly developed and used and that has no internal connection with interval arithmetic or Taylor models . We built our library so that PVS validates each result as it is produced . As producing and validating a proof , is and will certainly remain a bigger task than just producing a numerical result our library will never be a replacement to imperative implementations of Taylor models such as Cosy Infinity . Our library should mainly be used to validate small to medium size results that are involved in safety or life critical applications .
We propose a new , generic and flexible methodology for nonparametric function estimation , in which we first estimate the number and locations of any features that may be present in the function , and then estimate the function parametrically between each pair of neighbouring detected features . Examples of features handled by our methodology include change-points in the piecewise-constant signal model , kinks in the piecewise-linear signal model , and other similar irregularities , which we also refer to as generalised change-points . Our methodology works with only minor modifications across a range of generalised change-point scenarios , and we achieve such a high degree of generality by proposing and using a new multiple generalised change-point detection device , termed Narrowest-Over-Threshold ( NOT ) . The key ingredient of NOT is its focus on the smallest local sections of the data on which the existence of a feature is suspected . Crucially , this adaptive localisation technique prevents NOT from considering subsamples containing two or more features , a key factor that ensures the general applicability of NOT . For selected scenarios , we show the consistency and near-optimality of NOT in detecting the number and locations of generalised change-points , and discuss how to extend the proof to other settings . The NOT estimators are easy to implement and rapid to compute : the entire threshold-indexed solution path can be computed in close-to-linear time . Importantly , the NOT approach is easy to extend by the user to tailor to their own needs . There is no single competitor , but we show that the performance of NOT matches or surpasses the state of the art in the scenarios tested . Our methodology is implemented in the R package \textbf{not} .
If a game has a Nash equilibrium with probability values that are either zero or Omega ( 0 ) then this equilibrium can be found exhaustively in polynomial time . Somewhat surprisingly , we show that there is a PTAS for the games whose equilibria are guaranteed to have small-O ( 0/n ) -values , and therefore large-Omega ( n ) -supports . We also point out that there is a PTAS for games with sparse payoff matrices , which are known to be PPAD-complete to solve exactly . Both algorithms are of a special kind that we call oblivious : The algorithm just samples a fixed distribution on pairs of mixed strategies , and the game is only used to determine whether the sampled strategies comprise an eps-Nash equilibrium ; the answer is yes with inverse polynomial probability . These results bring about the question : Is there an oblivious PTAS for Nash equilibrium in general games ? We answer this question in the negative ; our lower bound comes close to the quasi-polynomial upper bound of [Lipton , Markakis , Mehta 0000] . Another recent PTAS for anonymous games is also oblivious in a weaker sense appropriate for this class of games ( it samples from a fixed distribution on unordered collections of mixed strategies ) , but its runtime is exponential in 0/eps . We prove that any oblivious PTAS for anonymous games with two strategies and three player types must have 0/eps^c in the exponent of the running time for some c>0/0 , rendering the algorithm in [Daskalakis 0000] essentially optimal within oblivious algorithms . In contrast , we devise a poly ( n ) ( 0/eps ) ^O ( log^0 ( 0/eps ) ) non-oblivious PTAS for anonymous games with 0 strategies and any bounded number of player types . Our algorithm is based on the construction of a sparse ( and efficiently computable ) eps-cover of the set of all possible sums of n independent indicators , under the total variation distance . The size of the cover is poly ( n ) ( 0/ eps^{O ( log^0 ( 0/eps ) ) } .
Crowd sensing is a new paradigm that leverages pervasive sensor-equipped mobile devices to provide sensing services like forensic analysis , documenting public spaces , and collaboratively constructing statistical models . Extensive user participation is indispensable for achieving good service quality . Nowadays , most of existing mechanisms focus on guaranteeing good service quality based on instantaneous extensive user participation for crowd sensing applications . Little attention has been dedicated to maximizing long-term service quality for crowd sensing applications due to their asymmetric interests , preferences , selfish behaviors , etc . To fill these gaps , in this paper , we derive the closed expression of the marginal sensing data quality based on the monopoly aggregation in economics . Furthermore , we design marginalquality based incentive mechanisms for long-term crowd sensing applications , not only to enhance extensive user participation by maximizing the expected total profits of mobile users , but also to stimulate mobile users to produce high-quality contents by applying the marginal quality . Finally , simulation results show that our mechanisms outperform the existing solutions .
Today AUVs operation still remains restricted to very particular tasks with low real autonomy due to battery restrictions . Efficient motion planning and mission scheduling are principle requirement toward advance autonomy and facilitate the vehicle to handle long-range operations . A single vehicle cannot carry out all tasks in a large scale terrain ; hence , it needs a certain degree of autonomy in performing robust decision making and awareness of the mission/environment to trade-off between tasks to be completed , managing the available time , and ensuring safe deployment at all stages of the mission . In this respect , this research introduces a modular control architecture including higher/lower level planners , in which the higher level module is responsible for increasing mission productivity by assigning prioritized tasks while guiding the vehicle toward its final destination in a terrain covered by several waypoints ; and the lower level is responsible for vehicle ' s safe deployment in a smaller scale encountering time-varying ocean current and different uncertain static/moving obstacles similar to actual ocean environment . Synchronization between higher and lower level modules is efficiently configured to manage the mission time and to guarantee on-time termination of the mission . The performance and accuracy of two higher and lower level modules are tested and validated using ant colony and firefly optimization algorithm , respectively . After all , the overall performance of the architecture is investigated in 00 different mission scenarios . The analyze of the captured results from different simulated missions confirm the efficiency and inherent robustness of the introduced architecture in efficient time management , safe deployment , and providing beneficial operation by proper prioritizing the tasks in accordance with mission time .
Parametric conditional copula models allow the copula parameters to vary with a set of covariates according to an unknown calibration function . Flexible Bayesian inference for the calibration function of a bivariate conditional copula is proposed via a sparse Gaussian process ( GP ) prior distribution over the set of smooth calibration functions for the single index model ( SIM ) . The estimation of parameters from the marginal distributions and the calibration function is done jointly via Markov Chain Monte Carlo sampling from the full posterior distribution . A new Conditional Cross Validated Pseudo-Marginal ( CCVML ) criterion is introduced in order to perform copula selection and is modified using a permutation-based procedure to assess data support for the simplifying assumption . The performance of the estimation method and model selection criteria is studied via a series of simulations using correct and misspecified models with Clayton , Frank and Gaussian copulas and a numerical application involving red wine features .
Phase II dose finding studies in clinical drug development are typically conducted to adequately characterize the dose response relationship of a new drug . An important decision is then on the choice of a suitable dose response function to support dose selection for the subsequent Phase III studies . In this paper we compare different approaches for model selection and model averaging using mathematical properties as well as simulations . Accordingly , we review and illustrate asymptotic properties of model selection criteria and investigate their behavior when changing the sample size but keeping the effect size constant . In a large scale simulation study we investigate how the various approaches perform in realistically chosen settings . Finally , the different methods are illustrated with a recently conducted Phase II dosefinding study in patients with chronic obstructive pulmonary disease .
The paper by Mayo claims to provide a new clarification and critique of Birnbaum ' s argument for showing that sufficiency and conditionality principles imply the likelihood principle . However , much of the arguments go back to arguments made thirty to forty years ago . Also , the main contention in the paper , that Birnbaum ' s arguments are not valid , seems to rest on a misunderstanding . [arXiv : 0000 . 0000]
This paper deals with multivariate Gaussian models for which the covariance matrix is a Kronecker product of two matrices . We consider maximum likelihood estimation of the model parameters , in particular of the covariance matrix . There is no explicit expression for the maximum likelihood estimator of a Kronecker product covariance matrix . The main question in this paper is whether the maximum likelihood estimator of the covariance matrix exists and if it is unique . The answers are different for different models that we consider .
Given a set of $n$ terminals , which are points in $d$-dimensional Euclidean space , the minimum Manhattan network problem ( MMN ) asks for a minimum-length rectilinear network that connects each pair of terminals by a Manhattan path , that is , a path consisting of axis-parallel segments whose total length equals the pair ' s Manhattan distance . Even for $d=0$ , the problem is NP-hard , but constant-factor approximations are known . For $d \ge 0$ , the problem is APX-hard ; it is known to admit , for any $\eps > 0$ , an $O ( n^\eps ) $-approximation . In the generalized minimum Manhattan network problem ( GMMN ) , we are given a set $R$ of $n$ terminal pairs , and the goal is to find a minimum-length rectilinear network such that each pair in $R$ is connected by a Manhattan path . GMMN is a generalization of both MMN and the well-known rectilinear Steiner arborescence problem ( RSA ) . So far , only special cases of GMMN have been considered . We present an $O ( \log^{d+0} n ) $-approximation algorithm for GMMN ( and , hence , MMN ) in $d \ge 0$ dimensions and an $O ( \log n ) $-approximation algorithm for 0D . We show that an existing $O ( \log n ) $-approximation algorithm for RSA in 0D generalizes easily to $d>0$ dimensions .
Fruit of the relationship of our research group with the team coordinated by the biologist Miguel Morales ( http : //spineup . es ) , we have applied different topo-geometric techniques for neuronal image processing . The images , captured with a powerful confocal microscope , allow to study the evolution of synaptic density under the influence of various substances , with the aim of studying neurodegenerative diseases like Alzheimer . In the paper we make a brief review of the techniques that appear in our bioinformatic problems , including the calculation of ordinary and persistent homology ( for which one can use the program Kenzo for symbolic computation in algebraic topology ) and classical problems of digital topology as skeleton location and path tracking . We focus on some particular cases of recent application , with which we will illustrate the previous techniques .
A typical random effects meta-analysis of odds-ratios assumes binomially distributed numbers of events in a treatment and control group and requires the proportion of deaths to be extracted from published papers . This data is often not available in the publications due to loss to follow-up . When the Kaplan Meier survival plot is available , it is common practice to manually measure the needed information from the plot and infer the probability of survival and then to infer a best-guess of the number of deaths . Uncertainty introduced from theses guesses is not accounted for in current models . This naive approach leads to over-certain results and potentially inaccurate conclusions . We propose the Uncertain Reading-Estimated Events model to construct each study ' s contribution to the meta-analysis separately using the data available for extraction in the publications . We use real and simulated data to illustrate our methods . Meta-analysis based on the observed number of deaths lead to biased estimates while our proposed model does not . Our results show increases in the standard deviation of the log-odds as compared to a naive meta-analysis that assumes ideal extracted data , equivalent to a reduction of the overall sample size of 00% in our example .
In this paper , we study a novel approach for the estimation of quantiles when facing potential right censoring of the responses . Contrary to the existing literature on the subject , the adopted strategy of this paper is to tackle censoring at the very level of the loss function usually employed for the computation of quantiles , the so-called " check " function . For interpretation purposes , a simple comparison with the latter reveals how censoring is accounted for in the newly proposed loss function . Subsequently , when considering the inclusion of covariates for conditional quantile estimation , by defining a new general loss function , the proposed methodology opens the gate to numerous parametric , semiparametric and nonparametric modelling techniques . In order to illustrate this statement , we consider the well-studied linear regression under the usual assumption of conditional independence between the true response and the censoring variable . For practical minimization of the studied loss function , we also provide a simple algorithmic procedure shown to yield satisfactory results for the proposed estimator with respect to the existing literature in an extensive simulation study . From a more theoretical prospect , consistency of the estimator for linear regression is obtained using very recent results on non-smooth semiparametric estimation equations with an infinite-dimensional nuisance parameter , while numerical examples illustrate the adequateness of a simple bootstrap procedure for inferential purposes . Lastly , an application to a real dataset is used to further illustrate the validity and finite sample performance of the proposed estimator .
Micro- and nanorobots are often controlled by global input signals , such as an electromagnetic or gravitational field . These fields move each robot maximally until it hits a stationary obstacle or another stationary robot . This paper investigates 0D motion-planning complexity for large swarms of simple mobile robots ( such as bacteria , sensors , or smart building material ) . In previous work we proved it is NP-hard to decide whether a given initial configuration can be transformed into a desired target configuration ; in this paper we prove a stronger result : the problem of finding an optimal control sequence is PSPACE-complete . On the positive side , we show we can build useful systems by designing obstacles . We present a reconfigurable hardware platform and demonstrate how to form arbitrary permutations and build a compact absolute encoder . We then take the same platform and use dual-rail logic to build a universal logic gate that concurrently evaluates AND , NAND , NOR and OR operations . Using many of these gates and appropriate interconnects we can evaluate any logical expression .
Distributed consensus with data rate constraint is an important research topic of multi-agent systems . Some results have been obtained for consensus of multi-agent systems with integrator dynamics , but it remains challenging for general high-order systems , especially in the presence of unmeasurable states . In this paper , we study the quantized consensus problem for a special kind of high-order systems and investigate the corresponding data rate required for achieving consensus . The state matrix of each agent is a 0m-th order real Jordan block admitting m identical pairs of conjugate poles on the unit circle ; each agent has a single input , and only the first state variable can be measured . The case of harmonic oscillators corresponding to m=0 is first investigated under a directed communication topology which contains a spanning tree , while the general case of m >= 0 is considered for a connected and undirected network . In both cases it is concluded that the sufficient number of communication bits to guarantee the consensus at an exponential convergence rate is an integer between $m$ and $0m$ , depending on the location of the poles .
Mixture modeling is a general technique for making any simple model more expressive through weighted combination . This generality and simplicity in part explains the success of the Expectation Maximization ( EM ) algorithm , in which updates are easy to derive for a wide class of mixture models . However , the likelihood of a mixture model is non-convex , so EM has no known global convergence guarantees . Recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist . In this work , we present Polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in EM . Polymom is applicable when the moments of a single mixture component are polynomials of the parameters . Our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a Generalized Moment Problem . We solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra . This framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .
The present paper introduces new adaptive multiple tests which rely on the estimation of the number of true null hypotheses and which control the false discovery rate ( FDR ) at level alpha for finite sample size . We derive exact formulas for the FDR for a large class of adaptive multiple tests which apply to a new class of testing procedures . In the following , generalized Storey estimators and weighted versions are introduced and it turns out that the corresponding adaptive step up and step down tests control the FDR . The present results also include particular dynamic adaptive step wise tests which use a data dependent weighting of the new generalized Storey estimators . In addition , a converse of the Benjamini Hochberg ( 0000 ) theorem is given . The Benjamini Hochberg ( 0000 ) test is the only " distribution free " step up test with FDR independent of the distribution of the p-values of false null hypotheses .
During the last decades particular effort has been directed towards understanding and predicting the relevant state of the business cycle with the objective of decomposing permanent shocks from those having only a transitory impact on real output . This trend--cycle decomposition has a relevant impact on several economic and fiscal variables and constitutes by itself an important indicator for policy purposes . This paper deals with trend--cycle decomposition for the Italian economy having some interesting peculiarities which makes it attractive to analyse from both a statistic and an historical perspective . We propose an univariate model for the quarterly real GDP , subsequently extended to include the price dynamics through a Phillips curve . This study considers a series of the Italian quarterly real GDP recently released by OECD which includes both the 0000s and the recent global financial crisis of 0000--0000 . Parameters estimate as well as the signal extraction are performed within the Bayesian paradigm which effectively handles complex models where the parameters enter the log--likelihood function in a strongly nonlinear way . A new Adaptive Independent Metropolis--within--Gibbs sampler is then developed to efficiently simulate the parameters of the unobserved cycle . Our results suggest that inflation influences the Output Gap estimate , making the extracted Italian OG an important indicator of inflation pressures on the real side of the economy , as stated by the Phillips theory . Moreover , our estimate of the sequence of peaks and troughs of the Output Gap is in line with the OECD official dating of the Italian business cycle .
We propose a novel , efficient approach for distributed sparse learning in high-dimensions , where observations are randomly partitioned across machines . Computationally , at each round our method only requires the master machine to solve a shifted ell_0 regularized M-estimation problem , and other workers to compute the gradient . In respect of communication , the proposed approach provably matches the estimation error bound of centralized methods within constant rounds of communications ( ignoring logarithmic factors ) . We conduct extensive experiments on both simulated and real world datasets , and demonstrate encouraging performances on high-dimensional regression and classification tasks .
In this paper , first we present a new explanation for the relation between logical circuits and artificial neural networks , logical circuits and fuzzy logic , and artificial neural networks and fuzzy inference systems . Then , based on these results , we propose a new neuro-fuzzy computing system which can effectively be implemented on the memristor-crossbar structure . One important feature of the proposed system is that its hardware can directly be trained using the Hebbian learning rule and without the need to any optimization . The system also has a very good capability to deal with huge number of input-out training data without facing problems like overtraining .
Brain networks has attracted the interests of many neuroscientists . From functional MRI ( fMRI ) data , statistical tools have been developed to recover brain networks . However , the dimensionality of whole-brain fMRI , usually in hundreds of thousands , challenges the applicability of these methods . We develop a hierarchical graphical model ( HGM ) to remediate this difficulty . This model introduces a hidden layer of networks based on sparse Gaussian graphical models , and the observed data are sampled from individual network nodes . In fMRI , the network layer models the underlying signals of different brain functional units , and how these units directly interact with each other . The introduction of this hierarchical structure not only provides a formal and interpretable approach , but also enables efficient computation for inferring big networks with hundreds of thousands of nodes . Based on the conditional convexity of our formulation , we develop an alternating update algorithm to compute the HGM model parameters simultaneously . The effectiveness of this approach is demonstrated on simulated data and a real dataset from a stop/go fMRI experiment .
Tian Ji ' s horse racing strategy , a famous Chinese legend , constitutes a promising concept to be applied to important issues in today ' s competitive environment ; this strategy is elaborated on and analyzed by examining the general case . The mathematical formulation concerning the calculation of winning , drawing or losing combinations and probabilities is presented to illustrate the interesting insights on how ancient philosophies could promote thinking in business competitiveness , in particular , the wisdom behind sacrificing the part for the benefit of the whole or sacrificing the short-term objectives in order to gain the long-term goal .
Migration to the cloud has been a popular topic in industry and academia in recent years . Despite many benefits that the cloud presents , such as high availability and scalability , most of the on-premise application architectures are not ready to fully exploit the benefits of this environment , and adapting them to this environment is a non-trivial task . Microservices have appeared recently as novel architectural styles that are native to the cloud . These cloud-native architectures can facilitate migrating on-premise architectures to fully benefit from the cloud environments because non-functional attributes , like scalability , are inherent in this style . The existing approaches on cloud migration does not mostly consider cloud-native architectures as their first-class citizens . As a result , the final product may not meet its primary drivers for migration . In this paper , we intend to report our experience and lessons learned in an ongoing project on migrating a monolithic on-premise software architecture to microservices . We concluded that microservices is not a one-fit-all solution as it introduces new complexities to the system , and many factors , such as distribution complexities , should be considered before adopting this style . However , if adopted in a context that needs high flexibility in terms of scalability and availability , it can deliver its promised benefits .
This paper concerns the use of the expectation-maximisation ( EM ) algorithm for inference in partially observed diffusion processes . In this context , a well known problem is that all except a few diffusion processes lack closed-form expressions of the transition densities . Thus , in order to estimate efficiently the EM intermediate quantity we construct , using novel techniques for unbiased estimation of diffusion transition densities , a random weight fixed-lag auxiliary particle smoother , which avoids the well known problem of particle trajectory degeneracy in the smoothing mode . The estimator is justified theoretically and demonstrated on a simulated example .
This is the proceedings of the third workshop on Robots and Sensors integration in future rescue INformation system ( ROSIN 0000 )
The advent of sensor networks presents untapped opportunities for synthesis . We examine the problem of synthesis of behavioral specifications into networks of programmable sensor blocks . The particular behavioral specification we consider is an intuitive user-created network diagram of sensor blocks , each block having a pre-defined combinational or sequential behavior . We synthesize this specification to a new network that utilizes a minimum number of programmable blocks in place of the pre-defined blocks , thus reducing network size and hence network cost and power . We focus on the main task of this synthesis problem , namely partitioning pre-defined blocks onto a minimum number of programmable blocks , introducing the efficient but effective PareDown decomposition algorithm for the task . We describe the synthesis and simulation tools we developed . We provide results showing excellent network size reductions through such synthesis , and significant speedups of our algorithm over exhaustive search while obtaining near-optimal results for 00 real network designs as well as nearly 00 , 000 randomly generated designs .
Efficient service composition in real time while providing necessary Quality of Service ( QoS ) guarantees has been a challenging research problem with ever growing complexity . Several heuristic based approaches with diverse proposals for taming the scale and complexity of web service composition , have been proposed in literature . In this paper , we present a new approach for efficient service composition based on abstraction refinement . Instead of considering individual services during composition , we propose several abstractions to form service groups and the composition is done on these abstract services . Abstraction reduces the search space significantly and thereby can be done reasonably fast . While this can expedite solution construction to a great extent , this also entails a possibility that it may fail to generate any solution satisfying the QoS constraints , though the individual services construct a valid solution . Hence , we propose to refine an abstraction to generate the composite solution with desired QoS values . A QoS satisfying solution , if one exists , can be constructed with multiple iterations of abstraction refinement . While in the worst case , this approach may end up exploring the complete composition graph constructed on individual services , on an average , the solution can be achieved on the abstract graph . The abstraction refinement techniques give a significant speed-up compared to the traditional composition techniques . Experimental results on real benchmarks show the efficiency of our proposed mechanism in terms of time and the number of services considered for composition .
The advent of new special-purpose hardware such as FPGA or ASIC-based annealers and quantum processors has shown potential in solving certain families of complex combinatorial optimization problems more efficiently than conventional CPUs . We show that to address an industrial optimization problem , a hybrid architecture of CPUs and non-CPU devices is inevitable . In this paper , we propose problem decomposition as an effective method for designing a hybrid CPU--non-CPU optimization solver . We introduce the required algorithmic elements for making problem decomposition a viable approach in meeting the real-world constraints such as communication time and the potential higher cost of using non-CPU hardware . We then turn to the well-known maximum clique problem , and propose a new method of decomposition for this problem . Our method enables us to solve the maximum clique problem on very large graphs using non-CPU hardware that is considerably smaller than the size of the graph . As an example , we show that the maximum clique problem on the com-Amazon graph , with 000 , 000 vertices and 000 , 000 edges , can be solved with a single call to a device that can embed a fully connected graph of size at least 00 nodes , such as the D-Wave 0000Q . We also show that our proposed problem decomposition approach can improve the runtime of two of the best-known classical algorithms for large , sparse graphs , namely PMC and BBMCSP , by orders of magnitude . In the light of our study , we believe that new non-CPU hardware that is small in size could become competitive with CPUs if it could be either mass produced and highly parallelized , or able to provide high-quality solutions to specific , small-sized problems significantly faster than CPUs .
In this paper we study a broad class of structured nonlinear programming ( SNLP ) problems . In particular , we first establish the first-order optimality conditions for them . Then we propose sequential convex programming ( SCP ) methods for solving them in which each iteration is obtained by solving a convex programming problem exactly or inexactly . Under some suitable assumptions , we establish that any accumulation point of the sequence generated by the methods is a KKT point of the SNLP problems . In addition , we propose a variant of the exact SCP method for SNLP in which nonmonotone scheme and " local " Lipschitz constants of the associated functions are used . And a similar convergence result as mentioned above is established .
Two major bottlenecks to the solution of large-scale Bayesian inverse problems are the scaling of posterior sampling algorithms to high-dimensional parameter spaces and the computational cost of forward model evaluations . Yet incomplete or noisy data , the state variation and parameter dependence of the forward model , and correlations in the prior collectively provide useful structure that can be exploited for dimension reduction in this setting--both in the parameter space of the inverse problem and in the state space of the forward model . To this end , we show how to jointly construct low-dimensional subspaces of the parameter space and the state space in order to accelerate the Bayesian solution of the inverse problem . As a byproduct of state dimension reduction , we also show how to identify low-dimensional subspaces of the data in problems with high-dimensional observations . These subspaces enable approximation of the posterior as a product of two factors : ( i ) a projection of the posterior onto a low-dimensional parameter subspace , wherein the original likelihood is replaced by an approximation involving a reduced model ; and ( ii ) the marginal prior distribution on the high-dimensional complement of the parameter subspace . We present and compare several strategies for constructing these subspaces using only a limited number of forward and adjoint model simulations . The resulting posterior approximations can rapidly be characterized using standard sampling techniques , e . g . , Markov chain Monte Carlo . Two numerical examples demonstrate the accuracy and efficiency of our approach : inversion of an integral equation in atmospheric remote sensing , where the data dimension is very high ; and the inference of a heterogeneous transmissivity field in a groundwater system , which involves a partial differential equation forward model with high dimensional state and parameters .
We calculate the $k$-point generating function of the correlated Jacobi ensemble using supersymmetric methods . We use the result for complex matrices for $k=0$ to derive a closed-form expression for eigenvalue density . For real matrices we obtain the density in terms of a twofold integral that we evaluate numerically . For both expressions we find agreement when comparing with Monte Carlo simulations . Relations between these quantities for the Jacobi and the Cauchy-Lorentz ensemble are derived .
In this paper we present an application of a simple technique of local recompression , previously developed by the author in the context of compressed membership problems and compressed pattern matching , to word equations . The technique is based on local modification of variables ( replacing X by aX or Xa ) and iterative replacement of pairs of letters appearing in the equation by a `fresh ' letter , which can be seen as a bottom-up compression of the solution of the given word equation , to be more specific , building an SLP ( Straight-Line Programme ) for the solution of the word equation . Using this technique we give a new , independent and self-contained proofs of most of the known results for word equations . To be more specific , the presented ( nondeterministic ) algorithm runs in O ( n log n ) space and in time polynomial in log N , where N is the size of the length-minimal solution of the word equation . The presented algorithm can be easily generalised to a generator of all solutions of the given word equation ( without increasing the space usage ) . Furthermore , a further analysis of the algorithm yields a doubly exponential upper bound on the size of the length-minimal solution . The presented algorithm does not use exponential bound on the exponent of periodicity . Conversely , the analysis of the algorithm yields an independent proof of the exponential bound on exponent of periodicity . We believe that the presented algorithm , its idea and analysis are far simpler than all previously applied . Furthermore , thanks to it we can obtain a unified and simple approach to most of known results for word equations . As a small additional result we show that for O ( 0 ) variables ( with arbitrary many appearances in the equation ) word equations can be solved in linear space , i . e . they are context-sensitive .
We consider varying coefficient Cox models with high-dimensional covariates . We apply the group Lasso method to these models and propose a variable selection procedure . Our procedure copes with variable selection and structure identification from a high dimensional varying coefficient model to a semivarying coefficient model simultaneously . We derive an oracle inequality and closely examine restrictive eigenvalue conditions , too . In this paper , we give the details for Cox models with time-varying coefficients . The theoretical results on variable selection can be easily extended to some other important models and we briefly mention those models since those models can be treated in the same way . The models considered in this paper are the most popular models among structured nonparametric regression models . The results of a small numerical study are also given .
We consider a sequential learning problem with Gaussian payoffs and side information : after selecting an action $i$ , the learner receives information about the payoff of every action $j$ in the form of Gaussian observations whose mean is the same as the mean payoff , but the variance depends on the pair $ ( i , j ) $ ( and may be infinite ) . The setup allows a more refined information transfer from one action to another than previous partial monitoring setups , including the recently introduced graph-structured feedback case . For the first time in the literature , we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm , which recover existing asymptotic problem-dependent lower bounds and finite-time minimax lower bounds available in the literature . We also provide algorithms that achieve the problem-dependent lower bound ( up to some universal constant factor ) or the minimax lower bounds ( up to logarithmic factors ) .
Based on periodogram-ratios of two univariate time series at different frequency points , two tests are proposed for comparing their spectra . One is an Anderson-Darling-like statistic for testing the equality of two time-invariant spectra . The other is the maximum of Anderson-Darling-like statistics for testing the equality of two spectra no matter that they are time-invariant and time-varying . Both of two tests are applicable for independent or dependent time series . Several simulation examples show that the proposed statistics outperform those that are also based on periodogram-ratios but constructed by the Pearson-like statistics .
We introduce the multiresolution recurrent neural network , which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes : a sequence of high-level coarse tokens , and a sequence of natural language tokens . There are many ways to estimate or learn the high-level coarse tokens , but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics . Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences . In contrast to the standard log- likelihood objective w . r . t . natural language tokens ( word perplexity ) , optimizing the joint log-likelihood biases the model towards modeling high-level abstractions . We apply the proposed model to the task of dialogue response generation in two challenging domains : the Ubuntu technical support domain , and Twitter conversations . On Ubuntu , the model outperforms competing approaches by a substantial margin , achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study . On Twitter , the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics . Finally , our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure .
Bivariate linear mixed models are useful when analyzing longitudinal data of two associated markers . In this paper , we present a bivariate linear mixed model including random effects or first-order auto-regressive process and independent measurement error for both markers . Codes and tricks to fit these models using SAS Proc MIXED are provided . Limitations of this program are discussed and an example in the field of HIV infection is shown . Despite some limitations , SAS Proc MIXED is a useful tool that may be easily extendable to multivariate response in longitudinal studies .
The following conversation is based in part on a transcript of a 0000 interview funded by Pfizer Global Research-Connecticut , the American Statistical Association and the Department of Statistics at the University of Connecticut-Storrs as part of the " Conversations with Distinguished Statisticians in Memory of Professor Harry O . Posten " .
Unlike telephone operators , which pay termination fees to reach the users of another network , Internet Content Providers ( CPs ) do not pay the Internet Service Providers ( ISPs ) of users they reach . While the consequent cross subsidization to CPs has nurtured content innovations at the edge of the Internet , it reduces the investment incentives for the access ISPs to expand capacity . As potential charges for terminating CPs ' traffic are criticized under the net neutrality debate , we propose to allow CPs to voluntarily subsidize the usagebased fees induced by their content traffic for end-users . We model the regulated subsidization competition among CPs under a neutral network and show how deregulation of subsidization could increase an access ISP ' s utilization and revenue , strengthening its investment incentives . Although the competition might harm certain CPs , we find that the main cause comes from high access prices rather than the existence of subsidization . Our results suggest that subsidization competition will increase the competitiveness and welfare of the Internet content market ; however , regulators might need to regulate access prices if the access ISP market is not competitive enough . We envision that subsidization competition could become a viable model for the future Internet .
A novel family of twelve mixture models with random covariates , nested in the linear $t$ cluster-weighted model ( CWM ) , is introduced for model-based clustering . The linear $t$ CWM was recently presented as a robust alternative to the better known linear Gaussian CWM . The proposed family of models provides a unified framework that also includes the linear Gaussian CWM as a special case . Maximum likelihood parameter estimation is carried out within the EM framework , and both the BIC and the ICL are used for model selection . A simple and effective hierarchical random initialization is also proposed for the EM algorithm . The novel model-based clustering technique is illustrated in some applications to real data . Finally , a simulation study for evaluating the performance of the BIC and the ICL is presented .
The proliferation of public Wi-Fi hotspots has brought new business potentials for Wi-Fi networks , which carry a significant amount of global mobile data traffic today . In this paper , we propose a novel Wi-Fi monetization model for venue owners ( VOs ) deploying public Wi-Fi hotspots , where the VOs can generate revenue by providing two different Wi-Fi access schemes for mobile users ( MUs ) : ( i ) the premium access , in which MUs directly pay VOs for their Wi-Fi usage , and ( ii ) the advertising sponsored access , in which MUs watch advertisements in exchange of the free usage of Wi-Fi . VOs sell their ad spaces to advertisers ( ADs ) via an ad platform , and share the ADs ' payments with the ad platform . We formulate the economic interactions among the ad platform , VOs , MUs , and ADs as a three-stage Stackelberg game . In Stage I , the ad platform announces its advertising revenue sharing policy . In Stage II , VOs determine the Wi-Fi prices ( for MUs ) and advertising prices ( for ADs ) . In Stage III , MUs make access choices and ADs purchase advertising spaces . We analyze the sub-game perfect equilibrium ( SPE ) of the proposed game systematically , and our analysis shows the following useful observations . First , the ad platform ' s advertising revenue sharing policy in Stage I will affect only the VOs ' Wi-Fi prices but not the VOs ' advertising prices in Stage II . Second , both the VOs ' Wi-Fi prices and advertising prices are non-decreasing in the advertising concentration level and non-increasing in the MU visiting frequency . Numerical results further show that the VOs are capable of generating large revenues through mainly providing one type of Wi-Fi access ( the premium access or advertising sponsored access ) , depending on their advertising concentration levels and MU visiting frequencies .
We present a distributed ( non-Bayesian ) learning algorithm for the problem of parameter estimation with Gaussian noise . The algorithm is expressed as explicit updates on the parameters of the Gaussian beliefs ( i . e . means and precision ) . We show a convergence rate of $O ( 0/k ) $ with the constant term depending on the number of agents and the topology of the network . Moreover , we show almost sure convergence to the optimal solution of the estimation problem for the general case of time-varying directed graphs .
This paper provides a semiparametric model of estimating states of the volatility defined as the squared diffusion coefficient of a stochastic differential equation . Without assuming any functional form of the volatility function , we consider the volatility state as an unobservable state in a state space model and estimate it by filtering . By using the estimates , we can draw the information on what kind of functions the volatility has behind the observable state of the process . We also prove the consistency of the model in the sense that estimated states converges to the true ones as the observation time interval goes to zero . In addition to that , from a numerical point of views , we carry out numerical experiments by examples of stochastic differential equations with linear/nonlinear volatility functions in order to check whether or not the model can actually estimate the volatility and capture the information on its functional form .
The mark-recapture method was devised by Petersen in 0000 to estimate the number of fish migrating into the Limfjord , and independently by Lincoln in 0000 to estimate waterfowl abundance . The technique applies to any search for a finite number of items by two or more people or agents , allowing the number of searched-for items to be estimated . This ubiquitous problem appears in fields from ecology and epidemiology , through to mathematics , social sciences , and computing . Here we exactly calculate the moments of the hypergeometric distribution associated with this long-standing problem , confirming that widely used estimates conjectured in 0000 are often too small . Our Bayesian approach highlights how different search strategies will modify the estimates . As an example , we assess the accuracy of a systematic literature review , an application we recommend .
The challenge of taking many variables into account in optimization problems may be overcome under the hypothesis of low effective dimensionality . Then , the search of solutions can be reduced to the random embedding of a low dimensional space into the original one , resulting in a more manageable optimization problem . Specifically , in the case of time consuming black-box functions and when the budget of evaluations is severely limited , global optimization with random embeddings appears as a sound alternative to random search . Yet , in the case of box constraints on the native variables , defining suitable bounds on a low dimensional domain appears to be complex . Indeed , a small search domain does not guarantee to find a solution even under restrictive hypotheses about the function , while a larger one may slow down convergence dramatically . Here we tackle the issue of low-dimensional domain selection based on a detailed study of the properties of the random embedding , giving insight on the aforementioned difficulties . In particular , we describe a minimal low-dimensional set in correspondence with the embedded search space . We additionally show that an alternative equivalent embedding procedure yields simultaneously a simpler definition of the low-dimensional minimal set and better properties in practice . Finally , the performance and robustness gains of the proposed enhancements for Bayesian optimization are illustrated on three examples .
The exact maximum likelihood estimate ( MLE ) provides a test statistic for the unit root test that is more powerful \citep[p . 000]{Fuller00} than the usual least squares approach . In this paper a new derivation is given for the asymptotic distribution of this test statistic that is simpler and more direct than the previous method . The response surface regression method is used to obtain a fast algorithm that computes accurate finite-sample critical values . This algorithm is available in the R package {\tt mleur} that is available on CRAN . The empirical power of the new test is shown to be much better than the usual test not only in the normal case but also for innovations generated from an infinite variance stable distribution as well as for innovations generated from a GARCH$ ( 0 , 0 ) $ process .
This paper presents a practical approach for identifying unknown mechanical parameters , such as mass and friction models of manipulated rigid objects or actuated robotic links , in a succinct manner that aims to improve the performance of policy search algorithms . Key features of this approach are the use of off-the-shelf physics engines and the adaptation of a black-box Bayesian optimization framework for this purpose . The physics engine is used to reproduce in simulation experiments that are performed on a real robot , and the mechanical parameters of the simulated system are automatically fine-tuned so that the simulated trajectories match with the real ones . The optimized model is then used for learning a policy in simulation , before safely deploying it on the real robot . Given the well-known limitations of physics engines in modeling real-world objects , it is generally not possible to find a mechanical model that reproduces in simulation the real trajectories exactly . Moreover , there are many scenarios where a near-optimal policy can be found without having a perfect knowledge of the system . Therefore , searching for a perfect model may not be worth the computational effort in practice . The proposed approach aims then to identify a model that is good enough to approximate the value of a locally optimal policy with a certain confidence , instead of spending all the computational resources on searching for the most accurate model . Empirical evaluations , performed in simulation and on a real robotic manipulation task , show that model identification via physics engines can significantly boost the performance of policy search algorithms that are popular in robotics , such as TRPO , PoWER and PILCO , with no additional real-world data .
The last decade has seen the advent and consolidation of ontology based tools for the identification and biological interpretation of classes of genes , such as the Gene Ontology . The information accumulated time-by-time and included in the GO is encoded in the definition of terms and in the setting up of semantic relations amongst terms . This approach might be usefully complemented by a bottom-up approach based on the knowledge of relationships amongst genes . To this end , we investigate the Gene Ontology from a complex network perspective . We consider the semantic network of terms naturally associated with the semantic relationships provided by the Gene Ontology consortium and a gene-based weighted network in which the nodes are the terms and a link between any two terms is set up whenever genes are annotated in both terms . One aim of the present paper is to understand whether the semantic and the gene-based network share the same structural properties or not . We then consider network communities . The identification of communities in the SVNs network can therefore be the basis of a simple protocol aiming at fully exploiting the possible relationships amongst terms , thus improving the knowledge of the semantic structure of GO . This is also important from a biomedical point of view , as it might reveal how genes over-expressed in a certain term also affect other biological functions not directly linked by the GO semantics . As a by-product , we present a simple methodology that allows to have a first glance insight about the biological characterization of groups of GO terms .
Effective and accurate model selection is an important problem in modern data analysis . One of the major challenges is the computational burden required to handle large data sets that cannot be stored or processed on one machine . Another challenge one may encounter is the presence of outliers and contaminations that damage the inference quality . In this paper , we extend the recently studied " divide and conquer " strategy in Bayesian parametric inference to the model selection context , in which we divide the observations of the full data set into roughly equal subsets and perform inference and model selection independently on each subset . After local subset inference , we aggregate the posterior model probabilities or other model/variable selection criteria to obtain a final model , by using the notion of geometric median . We show how this approach leads to improved concentration in finding the " correct " model and also parameters , and how it is robust to outliers and data contamination .
A number of tasks in classification , information retrieval , recommendation systems , and record linkage reduce to the core problem of inner product similarity join ( IPS join ) : identifying pairs of vectors in a collection that have a sufficiently large inner product . IPS join is well understood when vectors are normalized and some approximation of inner products is allowed . However , the general case where vectors may have any length appears much more challenging . Recently , new upper bounds based on asymmetric locality-sensitive hashing ( ALSH ) and asymmetric embeddings have emerged , but little has been known on the lower bound side . In this paper we initiate a systematic study of inner product similarity join , showing new lower and upper bounds . Our main results are : * Approximation hardness of IPS join in subquadratic time , assuming the strong exponential time hypothesis . * New upper and lower bounds for ( A ) LSH-based algorithms . In particular , we show that asymmetry can be avoided by relaxing the LSH definition to only consider the collision probability of distinct elements . * A new indexing method for IPS based on linear sketches , implying that our hardness results are not far from being tight . Our technical contributions include new asymmetric embeddings that may be of independent interest . At the conceptual level we strive to provide greater clarity , for example by distinguishing among signed and unsigned variants of IPS join and shedding new light on the effect of asymmetry .
With the aim to contribute to humanitarian response to disasters and violent events , scientists have proposed the development of analytical tools that could identify emergency events in real-time , using mobile phone data . The assumption is that dramatic and discrete changes in behavior , measured with mobile phone data , will indicate extreme events . In this study , we propose an efficient system for spatiotemporal detection of behavioral anomalies from mobile phone data and compare sites with behavioral anomalies to an extensive database of emergency and non-emergency events in Rwanda . Our methodology successfully captures anomalous behavioral patterns associated with a broad range of events , from religious and official holidays to earthquakes , floods , violence against civilians and protests . Our results suggest that human behavioral responses to extreme events are complex and multi-dimensional , including extreme increases and decreases in both calling and movement behaviors . We also find significant temporal and spatial variance in responses to extreme events . Our behavioral anomaly detection system and extensive discussion of results are a significant contribution to the long-term project of creating an effective real-time event detection system with mobile phone data and we discuss the implications of our findings for future research to this end . KEYWORDS : Big data , call detail record , emergency events , human mobility
We provide a characterization of revenue-optimal dynamic mechanisms in settings where a monopolist sells k items over k periods to a buyer who realizes his value for item i in the beginning of period i . We require that the mechanism satisfies a strong individual rationality constraint , requiring that the stage utility of each agent be positive during each period . We show that the optimum mechanism can be computed by solving a nested sequence of static ( single-period ) mechanisms that optimize a tradeoff between the surplus of the allocation and the buyer ' s utility . We also provide a simple dynamic mechanism that obtains at least half of the optimal revenue . The mechanism either ignores history and posts the optimal monopoly price in each period , or allocates with a probability that is independent of the current report of the agent and is based only on previous reports . Our characterization extends to multi-agent auctions . We also formulate a discounted infinite horizon version of the problem , where we study the performance of " Markov mechanisms . "
A terrain T is an x-monotone polygonal chain in the plane ; T is orthogonal if each edge of T is either horizontal or vertical . In this paper , we give an exact algorithm for the problem of guarding the convex vertices of an orthogonal terrain with the minimum number of reflex vertices .
The Shatters relation and the VC dimension have been investigated since the early seventies . These concepts have found numerous applications in statistics , combinatorics , learning theory and computational geometry . Shattering extremal systems are set-systems with a very rich structure and many different characterizations . The goal of this thesis is to elaborate on the structure of these systems .
Dwelling is an essential task to be performed to select keys from an on-screen keyboard present in the eye typing interface . This selection task can be performed by fixing eye gaze on a key for a prolonged time . Spending sufficient amount of time on each key effectively decreases the overall eye typing rate . To address the problem , researchers proposed mechanisms , which diminish the dwell time . We conducted a within-subject usability evaluation of four dwell-free eye typing techniques . The results of first-time usability study , longitudinal study and subjective evaluation conducted with 00 participants confirm the superiority of controlled eye movement based advanced eye typing method ( Adv-EyeK ) than the other three techniques .
The estimation of the correct number of dimensions is a long-standing problem in psychometrics . Several methods have been proposed , such as parallel analysis ( PA ) , Kaiser-Guttman ' s eigenvalue-greaterthan-one rule , multiple average partial procedure ( MAP ) , the maximum-likelihood approaches that use fit indexes as BIC and EBIC and the less used and studied approach called very simple structure ( VSS ) . In the present paper a new approach to estimate the number of dimensions will be introduced and compared via simulation to the traditional techniques pointed above . The approach proposed in the current paper is called exploratory graph analysis ( EGA ) , since it is based on the graphical lasso with the regularization parameter specified using EBIC . The number of dimensions is verified using the walktrap , a random walk algorithm used to identify communities in networks . In total , 00 , 000 data sets were simulated to fit known factor structures , with the data sets varying across different criteria : number of factors ( 0 and 0 ) , number of items ( 0 and 00 ) , sample size ( 000 , 000 , 0000 and 0000 ) and correlation between factors ( orthogonal , . 00 , . 00 and . 00 ) , resulting in 00 different conditions . For each condition , 000 data sets were simulated using lavaan . The result shows that the EGA performs comparable to parallel analysis , EBIC , eBIC and to KaiserGuttman rule in a number of situations , especially when the number of factors was two . However , EGA was the only technique able to correctly estimate the number of dimensions in the four-factor structure when the correlation between factors were . 0 , showing an accuracy of 000% for a sample size of 0 , 000 observations . Finally , the EGA was used to estimate the number of factors in a real dataset , in order to compare its performance with the other six techniques tested in the simulation study .
We propose a new class of models specifically tailored for spatio-temporal data analysis . To this end , we generalize the spatial autoregressive model with autoregressive and heteroskedastic disturbances , i . e . SARAR ( 0 , 0 ) , by exploiting the recent advancements in Score Driven ( SD ) models typically used in time series econometrics . In particular , we allow for time-varying spatial autoregressive coefficients as well as time-varying regressor coefficients and cross-sectional standard deviations . We report an extensive Monte Carlo simulation study in order to investigate the finite sample properties of the Maximum Likelihood estimator for the new class of models as well as its flexibility in explaining several dynamic spatial dependence processes . The new proposed class of models are found to be economically preferred by rational investors through an application in portfolio optimization .
Robust and semiparametric statistics are of the same historical origin and largely employ the same locally asymptotically normal framework . In our talk , we consider he following more intrinsic connections of both fields : 0 ) Robust influence curves for semiparametric models with infinite dimensional nuisance parameter ; for example , for semiparametric regression ( Cox ) , and mixture models ( Neyman--Scott ) . 0 ) Adaptiveness in the sense of Stein ' s necessary condition of robust neighborhood models and estimators with respect to a finite dimensional nuisance parameter ; for example , location , linear regression , and ARMA . 0 ) Semiparametric treatment of gross error deviations from an ideal model as an infinite dimensional nuisance parameter , by projection on balls ; for testing , an asymptotic version of the Huber--Strassen maximin result is thus obtained . 0 ) Uniform and nonuniform asymptotic normality of robust and adaptive estimators , respectively , in regression and time series models . 0 ) Fragility of optimal one-sided tests and confidence limits obtained for convex tangent cones , by projection on cones , as opposed to stability of corresponding procedures , even two-sided , for linear tangent spaces . 0 ) The unknown neighborhood radius as a nuisance parameter in robustness . The investigation relies on asymptotic techniques and on numerical evaluations of robust estimates . The construction under 0 ) of robust estimates which are adaptive , an appropriate LAM estimation bound for balls under 0 ) , and the minimization of the norm under 0 ) on a certain restricted set of differences of tangents from a cone are examples of challenging open problems .
This letter summarizes some known properties and also presents several new properties of the Numerical Integration ( NI ) method for time-optimal trajectory planning along a specified path . The contribution is that rigorous mathematical proofs of these properties are presented , most of which cannot be found in existing literatures . We first give some properties regarding switch points and accelerating/decelerating curves of the NI method . Then , for the fact that when kinematic constraints are considered , the original version of NI which only considers torque constraints may result in failure of trajectory planning , we give the concrete failure conditions with rigorous mathematical proof . Accordingly , a failure detection algorithm is given in a run-and-test manner . Some simulation results on a unicycle vehicle are provided to verify those presented properties . Note that though those known properties are not discovered first , their mathematical proofs are given first in this letter . The detailed proofs make the theory of NI more complete and help interested readers to gain a thorough understanding of the method .
Gaussian Mixture Models ( GMM ) have found many applications in density estimation and data clustering . However , the model does not adapt well to curved and strongly nonlinear data . Recently there appeared an improvement called AcaGMM ( Active curve axis Gaussian Mixture Model ) , which fits Gaussians along curves using an EM-like ( Expectation Maximization ) approach . Using the ideas standing behind AcaGMM , we build an alternative active function model of clustering , which has some advantages over AcaGMM . In particular it is naturally defined in arbitrary dimensions and enables an easy adaptation to clustering of complicated datasets along the predefined family of functions . Moreover , it does not need external methods to determine the number of clusters as it automatically reduces the number of groups on-line .
In this paper , we present a novel approach to fitting mixture models based on estimating first the posterior distribution of the auxiliary variables that assign each observation to a group in the mixture . The posterior distributions of the remainder of the parameters in the mixture is obtained by averaging over their conditional posterior marginals on the auxiliary variables using Bayesian model averaging . A new algorithm based on Gibbs sampling is used to approximate the posterior distribution of the auxiliary variables without sampling any other parameter in the model . In particular , the modes of the full conditionals of the parameters of the densities in the mixture are computed and these are plugged-in to the full conditional of the auxiliary variables to draw samples . This approximation , that we have called ' modal ' Gibbs sampling , reduces the computational burden in the Gibbs sampling algorithm and still provides very good estimates of the posterior distribution of the auxiliary variables . Conditional models on the auxiliary variables are fitted using the Integrated Nested Laplace Approximation ( INLA ) to obtain the conditional posterior distributions , including modes , of the distributional parameters in the mixtures . This approach is general enough to consider mixture models with discrete or continuous outcomes from a wide range of distributions and latent models as conditional model fitting is done with INLA . This presents several other advantages , such as fast fitting of the conditional models , not being restricted to the use of conjugate priors on the model parameters and being less prone to label switching . Within this framework , computing the marginal likelihood of the mixture model when the number of groups in the mixture is known is easy and it can be used to tackle selection of the number of components .
Matrix decomposition is a popular and fundamental approach in machine learning and data mining . It has been successfully applied into various fields . Most matrix decomposition methods focus on decomposing a data matrix from one single source . However , it is common that data are from different sources with heterogeneous noise . A few of matrix decomposition methods have been extended for such multi-view data integration and pattern discovery . While only few methods were designed to consider the heterogeneity of noise in such multi-view data for data integration explicitly . To this end , we propose a joint matrix decomposition framework ( BJMD ) , which models the heterogeneity of noise by Gaussian distribution in a Bayesian framework . We develop two algorithms to solve this model : one is a variational Bayesian inference algorithm , which makes full use of the posterior distribution ; and another is a maximum a posterior algorithm , which is more scalable and can be easily paralleled . Extensive experiments on synthetic and real-world datasets demonstrate that BJMD considering the heterogeneity of noise is superior or competitive to the state-of-the-art methods .
This paper shows how to construct locally robust semiparametric GMM estimators , meaning equivalently moment conditions have zero derivative with respect to the first step and the first step does not affect the asymptotic variance . They are constructed by adding to the moment functions the adjustment term for first step estimation . Locally robust estimators have several advantages . They are vital for valid inference with machine learning in the first step , see Belloni et . al . ( 0000 , 0000 ) , and are less sensitive to the specification of the first step . They are doubly robust for affine moment functions , so moment conditions continue to hold when one first step component is incorrect . Locally robust moment conditions also have smaller bias that is flatter as a function of first step smoothing leading to improved small sample properties . Series first step estimators confer local robustness on any moment conditions and are doubly robust for affine moments , in the direction of the series approximation . Many new locally and doubly robust estimators are given here , including for economic structural models . We give simple asymptotic theory for estimators that use cross-fitting in the first step , including machine learning .
Modularisation , repetition , and symmetry are structural features shared by almost all biological neural networks . These features are very unlikely to be found by the means of structural evolution of artificial neural networks . This paper introduces NMODE , which is specifically designed to operate on neuro-modules . NMODE addresses a second problem in the context of evolutionary robotics , which is incremental evolution of complex behaviours for complex machines , by offering a way to interface neuro-modules . The scenario in mind is a complex walking machine , for which a locomotion module is evolved first , that is then extended by other modules in later stages . We show that NMODE is able to evolve a locomotion behaviour for a standard six-legged walking machine in approximately 00 generations and show how it can be used for incremental evolution of a complex walking machine . The entire source code used in this paper is publicly available through GitHub .
In recent years , a variety of digital repository and archival systems have been developed and adopted . All of these systems aim at hosting a variety of compound digital assets and at providing tools for storing , managing and accessing those assets . This paper will focus on the definition of common and standardized access interfaces that could be deployed across such diverse digital respository and archival systems . The proposed interfaces are based on the two formal specifications that have recently emerged from the Digital Library community : The Open Archive Initiative Protocol for Metadata Harvesting ( OAI-PMH ) and the NISO OpenURL Framework for Context-Sensitive Services ( OpenURL Standard ) . As will be described , the former allows for the retrieval of batches of XML-based representations of digital assets , while the latter facilitates the retrieval of disseminations of a specific digital asset or of one or more of its constituents . The core properties of the proposed interfaces are explained in terms of the Reference Model for an Open Archival Information System ( OAIS ) .
The ability of sensing breathing is becoming an increasingly important function for technology that aims at supporting both psychological and physical wellbeing . We demonstrate ThermSense , a new breathing sensing platform based on smartphone technology and low-cost thermal camera , which allows a user to measure his/her breathing pattern in a contact-free manner . With the designed key functions of Thermal Voxel Integration-based breathing estimation and respiration variability spectrogram ( RVS , bi-dimensional representation of breathing dynamics ) , the developed platform provides scalability and flexibility for gathering respiratory physiological measurements ubiquitously . The functionality could be used for a variety of applications from stress monitoring to respiration training .
Approximate Bayesian Computation ( ABC ) methods are used to approximate posterior distributions in models with unknown or computationally intractable likelihoods . Both the accuracy and computational efficiency of ABC depend on the choice of summary statistic , but outside of special cases where the optimal summary statistics are known , it is unclear which guiding principles can be used to construct effective summary statistics . In this paper we explore the possibility of automating the process of constructing summary statistics by training deep neural networks to predict the parameters from artificially generated data : the resulting summary statistics are approximately posterior means of the parameters . With minimal model-specific tuning , our method constructs summary statistics for the Ising model and the moving-average model , which match or exceed theoretically-motivated summary statistics in terms of the accuracies of the resulting posteriors .
A random coefficient autoregressive process is deeply investigated in which the coefficients are correlated . First we look at the existence of a strictly stationary causal solution , then we give the second-order stationarity conditions and the autocorrelation function of the process . Then we study some asymptotic properties of the empirical mean and the usual least squares estimators of the process , such as convergence , asymptotic normality and rates of convergence , supplied with the appropriate assumptions on the driving perturbations . Our objective is to get an overview of the influence of correlated coefficients in the estimation step , through a simple model . In particular , the lack of consistency is shown for the estimation of the autoregressive parameter . Finally , a consistent estimation is given together with a testing procedure for the existence of correlation in the random coefficients . While convergence properties rely on the ergodicity , we use a martingale approach to reach most of the results .
Internet telephony and multimedia communication protocols have matured over the last fifteen years . Recently , the web is evolving as a popular platform for everything we do on the Internet including email , text chat , voice calls , discussions , enterprise apps and multi-party collaboration . Unfortunately , there is a disconnect between web and traditional Internet telephony protocols as they have ignored the constraints and requirements of each other . Consequently , the Flash Player is being used as a web browser plugin by many developers for web-based voice and video calls . We describe the challenges of video communication using a web browser , present a simple API using a Flash Player application , show how it supports wide range of web communication scenarios in the cloud , and describe how it can interoperate with Session Initiation Protocol ( SIP ) -based systems . We describe both the advantages and challenges of Flash Player based communication applications . The presented API could guide future work on communication-related web protocol extensions .
We study a new class of games which generalizes congestion games and its bottleneck variant . We introduce congestion games with mixed objectives to model network scenarios in which players seek to optimize for latency and bandwidths alike . We characterize the existence of pure Nash equilibria ( PNE ) and the convergence of improvement dynamics . For games that do not possess PNE we give bounds on the approximation ratio of approximate pure Nash equilibria .
In this paper the extended model of Minority game ( MG ) , incorporating variable number of agents and therefore called Grand Canonical , is used for prediction . We proved that the best MG-based predictor is constituted by a tremendously degenerated system , when only one agent is involved . The prediction is the most efficient if the agent is equipped with all strategies from the Full Strategy Space . Each of these filters is evaluated and , in each step , the best one is chosen . Despite the casual simplicity of the method its usefulness is invaluable in many cases including real problems . The significant power of the method lies in its ability to fast adaptation if \lambda-GCMG modification is used . The success rate of prediction is sensitive to the properly set memory length . We considered the feasibility of prediction for the Minority and Majority games . These two games are driven by different dynamics when self-generated time series are considered . Both dynamics tend to be the same when a feedback effect is removed and an exogenous signal is applied .
Sequential Monte Carlo methods , also known as particle methods , are a widely used set of computational tools for inference in non-linear non-Gaussian state-space models . In many applications it may be necessary to compute the sensitivity , or derivative , of the optimal filter with respect to the static parameters of the state-space model ; for instance , in order to obtain maximum likelihood model parameters of interest , or to compute the optimal controller in an optimal control problem . In Poyiadjis et al . [0000] an original particle algorithm to compute the filter derivative was proposed and it was shown using numerical examples that the particle estimate was numerically stable in the sense that it did not deteriorate over time . In this paper we substantiate this claim with a detailed theoretical study . Lp bounds and a central limit theorem for this particle approximation of the filter derivative are presented . It is further shown that under mixing conditions these Lp bounds and the asymptotic variance characterized by the central limit theorem are uniformly bounded with respect to the time index . We demon- strate the performance predicted by theory with several numerical examples . We also use the particle approximation of the filter derivative to perform online maximum likelihood parameter estimation for a stochastic volatility model .
Tucker decomposition is the cornerstone of modern machine learning on tensorial data analysis , which have attracted considerable attention for multiway feature extraction , compressive sensing , and tensor completion . The most challenging problem is related to determination of model complexity ( i . e . , multilinear rank ) , especially when noise and missing data are present . In addition , existing methods cannot take into account uncertainty information of latent factors , resulting in low generalization performance . To address these issues , we present a class of probabilistic generative Tucker models for tensor decomposition and completion with structural sparsity over multilinear latent space . To exploit structural sparse modeling , we introduce two group sparsity inducing priors by hierarchial representation of Laplace and Student-t distributions , which facilitates fully posterior inference . For model learning , we derived variational Bayesian inferences over all model ( hyper ) parameters , and developed efficient and scalable algorithms based on multilinear operations . Our methods can automatically adapt model complexity and infer an optimal multilinear rank by the principle of maximum lower bound of model evidence . Experimental results and comparisons on synthetic , chemometrics and neuroimaging data demonstrate remarkable performance of our models for recovering ground-truth of multilinear rank and missing entries .
We present a new platform , " Regulus Lite " , which supports rapid development and web deployment of several types of phrasal speech translation systems using a minimal formalism . A distinguishing feature is that most development work can be performed directly by domain experts . We motivate the need for platforms of this type and discuss three specific cases : medical speech translation , speech-to-sign-language translation and voice questionnaires . We briefly describe initial experiences in developing practical systems .
Over the past couple of years , clicking and posting selfies has become a popular trend . However , since March 0000 , 000 people have died and many have been injured while trying to click a selfie . Researchers have studied selfies for understanding the psychology of the authors , and understanding its effect on social media platforms . In this work , we perform a comprehensive analysis of the selfie-related casualties and infer various reasons behind these deaths . We use inferences from incidents and from our understanding of the features , we create a system to make people more aware of the dangerous situations in which these selfies are taken . We use a combination of text-based , image-based and location-based features to classify a particular selfie as dangerous or not . Our method ran on 0 , 000 annotated selfies collected on Twitter gave 00% accuracy . Individually the image-based features were the most informative for the prediction task . The combination of image-based and location-based features resulted in the best accuracy . We have made our code and dataset available at http : //labs . precog . iiitd . edu . in/killfie .
The most important factors which contribute to the efficiency of game-theoretical algorithms are time and game complexity . In this study , we have offered an elegant method to deal with high complexity of game theoretic multi-objective clustering methods in large-sized data sets . Here , we have developed a method which selects a subset of strategies from strategies profile for each player . In this case , the size of payoff matrices reduces significantly which has a remarkable impact on time complexity . Therefore , practical problems with more data are tractable with less computational complexity . Although strategies set may grow with increasing the number of data points , the presented model of strategy selection reduces the strategy space , considerably , where clusters are subdivided into several sub-clusters in each local game . The remarkable results demonstrate the efficiency of the presented approach in reducing computational complexity of the problem of concern .
Existing research in crowdsourcing has investigated how to recommend tasks to workers based on which task the workers have already completed , referred to as {\em implicit feedback} . We , on the other hand , investigate the task recommendation problem , where we leverage both implicit feedback and explicit features of the task . We assume that we are given a set of workers , a set of tasks , interactions ( such as the number of times a worker has completed a particular task ) , and the presence of explicit features of each task ( such as , task location ) . We intend to recommend tasks to the workers by exploiting the implicit interactions , and the presence or absence of explicit features in the tasks . We formalize the problem as an optimization problem , propose two alternative problem formulations and respective solutions that exploit implicit feedback , explicit features , as well as similarity between the tasks . We compare the efficacy of our proposed solutions against multiple state-of-the-art techniques using two large scale real world datasets .
In this paper we present the study of the mathematical model of a real life joint used in an underwater robotic fish . Fluid-structure interaction is utterly simplified and the motion of the joint is approximated by D\ " uffing ' s equation . We compare the quality of analytical harmonic solutions previously reported , with the input-output relation obtained via truncated Volterra series expansion . Comparisons show a trade-off between accuracy and flexibility of the methods . The methods are discussed in detail in order to facilitate reproduction of our results . The approach presented herein can be used to verify results in nonlinear resonance applications and in the design of bio-inspired compliant robots that exploit passive properties of their dynamics . We focus on the potential use of this type of joint for energy extraction from environmental sources , in this case a K\ ' arm\ ' an vortex street shed by an obstacle in a flow . Open challenges and questions are mentioned throughout the document .
The EM algorithm is a widely used methodology for penalized likelihood estimation . Provable monotonicity and convergence are the hallmarks of the EM algorithm and these properties are well established for smooth likelihood and smooth penalty functions . However , many relaxed versions of variable selection penalties are not smooth . The goal of this paper is to introduce a new class of Space Alternating Penalized Kullback Proximal extensions of the EM algorithm for nonsmooth likelihood inference . We show that the cluster points of the new method are stationary points even when on the boundary of the parameter set . Special attention has been paid to the construction of component-wise version of the method in order to ease the implementation for complicated models . Illustration for the problems of model selection for finite mixtures of regression and to sparse image reconstruction is presented .
We introduce pyndri , a Python interface to the Indri search engine . Pyndri allows to access Indri indexes from Python at two levels : ( 0 ) dictionary and tokenized document collection , ( 0 ) evaluating queries on the index . We hope that with the release of pyndri , we will stimulate reproducible , open and fast-paced IR research .
Community-based Question Answering ( CQA ) sites play an important role in addressing health information needs . However , a significant number of posted questions remain unanswered . Automatically answering the posted questions can provide a useful source of information for online health communities . In this study , we developed an algorithm to automatically answer health-related questions based on past questions and answers ( QA ) . We also aimed to understand information embedded within online health content that are good features in identifying valid answers . Our proposed algorithm uses information retrieval techniques to identify candidate answers from resolved QA . In order to rank these candidates , we implemented a semi-supervised leaning algorithm that extracts the best answer to a question . We assessed this approach on a curated corpus from Yahoo ! Answers and compared against a rule-based string similarity baseline . On our dataset , the semi-supervised learning algorithm has an accuracy of 00 . 0% . UMLS-based ( health-related ) features used in the model enhance the algorithm ' s performance by proximately 0 % . A reasonably high rate of accuracy is obtained given that the data is considerably noisy . Important features distinguishing a valid answer from an invalid answer include text length , number of stop words contained in a test question , a distance between the test question and other questions in the corpus as well as a number of overlapping health-related terms between questions . Overall , our automated QA system based on historical QA pairs is shown to be effective according to the data set in this case study . It is developed for general use in the health care domain which can also be applied to other CQA sites .
In this paper we review the notion of direct causal effect as introduced by Pearl ( 0000 ) . We show how it can be formulated without counterfactuals , using intervention indicators instead . This allows to consider the natural direct effect as a special case of sequential treatments discussed by Dawid and Didelez ( 0000 ) which immediately yields conditions for identifiability as well as a graphical way of checking identifiability . The results are contrasted with the criteria given by Pearl ( 0000 ) and Robins ( 0000 ) .
Sparse alpha-norm regularization has many data-rich applications in marketing and economics . In contrast to traditional lasso and ridge regularization , the alpha-norm penalty has the property of jumping to a sparse solution . This is an attractive feature for ultra high-dimensional problems that occur in market demand estimation and forecasting . The underlying nonconvex regularization problem is solved via coordinate descent , and a proximal operator . To illustrate our methodology , we study a classic demand forecasting problem of Bajari , Nekipelov , Ryan , and Yang ( 0000a ) . On the empirical side , we find many strong sparse predictors , including price , equivalized volume , promotion , flavor scent , and brand effects . Benchmark methods including linear regression , ridge , lasso and elastic net , are used in an out-of-sample forecasting study . In particular , alpha-norm regularization provides accurate estimates for the promotion effects . Finally , we conclude with directions for future research .
This work focuses on the topic of melodic characterization and similarity in a specific musical repertoire : a cappella flamenco singing , more specifically in debla and martinete styles . We propose the combination of manual and automatic description . First , we use a state-of-the-art automatic transcription method to account for general melodic similarity from music recordings . Second , we define a specific set of representative mid-level melodic features , which are manually labeled by flamenco experts . Both approaches are then contrasted and combined into a global similarity measure . This similarity measure is assessed by inspecting the clusters obtained through phylogenetic algorithms algorithms and by relating similarity to categorization in terms of style . Finally , we discuss the advantage of combining automatic and expert annotations as well as the need to include repertoire-specific descriptions for meaningful melodic characterization in traditional music collections .
In real settings , natural body movements can be erroneously recognized by whole-body input systems as explicit input actions . We call body activity not intended as input actions " background activity . " We argue that understanding background activity is crucial to the success of always-available whole-body input in the real world . To operationalize this argument , we contribute a reusable study methodology and software tools to generate standardized background activity datasets composed of data from multiple Kinect cameras , a Vicon tracker , and two high-definition video cameras . Using our methodology , we create an example background activity dataset for a television-oriented living room setting . We use this dataset to demonstrate how it can be used to redesign a gestural interaction vocabulary to minimize conflicts with the real world . The software tools and initial living room dataset are publicly available ( http : //www . dgp . toronto . edu/~dustin/backgroundactivity/ ) .
Discussion of ``The Dantzig selector : Statistical estimation when $p$ is much larger than $n$ ' ' [math/0000000]
Lehmann ' s ideas on concepts of dependence have had a profound effect on mathematical theory of reliability . The aim of this paper is two-fold . The first is to show how the notion of a ``hazard potential ' ' can provide an explanation for the cause of dependence between life-times . The second is to propose a general framework under which two currently discussed issues in reliability and in survival analysis involving interdependent stochastic processes , can be meaningfully addressed via the notion of a hazard potential . The first issue pertains to the failure of an item in a dynamic setting under multiple interdependent risks . The second pertains to assessing an item ' s life length in the presence of observable surrogates or markers . Here again the setting is dynamic and the role of the marker is akin to that of a leading indicator in multiple time series .
Vehicular networks are used to coordinate actions among vehicles in traffic by the use of wireless transceivers ( pairs of transmitters and receivers ) . Unfortunately , the wireless communication among vehicles is vulnerable to security threats that may lead to very serious safety hazards . In this work , we propose a viable solution for coping with Man-in-the-Middle attacks . Conventionally , Public Key Infrastructure ( PKI ) is utilized for a secure communication with the pre-certified public key . However , a secure vehicle-to-vehicle communication requires additional means of verification in order to avoid impersonation attacks . To the best of our knowledge , this is the first work that proposes to certify both the public key and out-of-band sense-able static attributes to enable mutual authentication of the communicating vehicles . Vehicle owners are bound to preprocess ( periodically ) a certificate for both a public key and a list of fixed unchangeable attributes of the vehicle . Furthermore , the proposed approach is shown to be adaptable with regards to the existing authentication protocols . We illustrate the security verification of the proposed protocol using a detailed proof in Spi calculus .
As the social coding is becoming increasingly popular , understanding the influence of developers can benefit various applications , such as advertisement for new projects and innovations . However , most existing works have focused only on ranking influential nodes in non-weighted and homogeneous networks , which are not able to transfer proper importance scores to the real important node . To rank developers in Github , we define developer ' s influence on the capacity of attracting attention which can be measured by the number of followers obtained in the future . We further defined a new method , DevRank , which ranks the developers by influence propagation through heterogeneous network constructed according to user behaviors , including " commit " and " follow " . Our experiment compares the performance between DevRank and some other link analysis algorithms , the results have shown that DevRank can improve the ranking accuracy .
Fully depleted ( FD ) Silicon on Insulator ( SOI ) metal oxide Field Effect Transistor ( MOSFET ) Is the Leading Contender for Sun 00nm Regime . This paper presents a study of effects of work functions of metal gate on the performance of FD-SOI MOSFET . Sentaurus TCAD simulation tool is used to investigate the effect of work function of gates ont he performance FDSOI MOSFET . Specific channel length of the device that had been concentrated is 00nm . From simulation we observed that by changing the work function of the metal gates of FD-SOI MOSFET we can change the threshold voltage . Hence by using this technique we can set the appropriate threshold voltage of FD-SOI MOSFET at same voltage and we can decrease the leakage current , gate tunneling current and short channel effects and increase drive current .
Random effects are implemented for aster models using two approximations taken from Breslow and Clayton [J . Amer . Statist . Assoc . 00 ( 0000 ) 0-00] . Random effects are analytically integrated out of the Laplace approximation to the complete data log likelihood , giving a closed-form expression for an approximate missing data log likelihood . Third and higher derivatives of the complete data log likelihood with respect to the random effects are ignored , giving a closed-form expression for second derivatives of the approximate missing data log likelihood , hence approximate observed Fisher information . This method is applicable to any exponential family random effects model . It is implemented in the CRAN package aster ( R Core Team [R : A Language and Environment for Statistical Computing ( 0000 ) R Foundation for Statistical Computing] , Geyer [R package aster ( 0000 ) http : //cran . r-project . org/package=aster] ) . Applications are analyses of local adaptation in the invasive California wild radish ( Raphanus sativus ) and the slender wild oat ( Avena barbata ) and of additive genetic variance for fitness in the partridge pea ( Chamaecrista fasciculata ) .
We show how systems of sessions types can enforce interactions to be bounded for all typable processes . The type system we propose is based on Lafont ' s soft linear logic and is strongly inspired by recent works about session types as intuitionistic linear logic formulas . Our main result is the existence , for every typable process , of a polynomial bound on the length of any reduction sequence starting from it and on the size of any of its reducts .
We consider a Bayesian approach to model selection in Gaussian linear regression , where the number of predictors might be much larger than the number of observations . From a frequentist view , the proposed procedure results in the penalized least squares estimation with a complexity penalty associated with a prior on the model size . We investigate the optimality properties of the resulting estimator . We establish the oracle inequality and specify conditions on the prior that imply its asymptotic minimaxity within a wide range of sparse and dense settings for " nearly-orthogonal " and " multicollinear " designs .
Outlier detection is a fundamental data science task with applications ranging from data cleaning to network security . Given the fundamental nature of the task , this has been the subject of much research . Recently , a new class of outlier detection algorithms has emerged , called {\it contextual outlier detection} , and has shown improved performance when studying anomalous behavior in a specific context . However , as we point out in this article , such approaches have limited applicability in situations where the context is sparse ( i . e . lacking a suitable frame of reference ) . Moreover , approaches developed to date do not scale to large datasets . To address these problems , here we propose a novel and robust approach alternative to the state-of-the-art called RObust Contextual Outlier Detection ( ROCOD ) . We utilize a local and global behavioral model based on the relevant contexts , which is then integrated in a natural and robust fashion . We also present several optimizations to improve the scalability of the approach . We run ROCOD on both synthetic and real-world datasets and demonstrate that it outperforms other competitive baselines on the axes of efficacy and efficiency ( 00X speedup compared to modern contextual outlier detection methods ) . We also drill down and perform a fine-grained analysis to shed light on the rationale for the performance gains of ROCOD and reveal its effectiveness when handling objects with sparse contexts .
This paper compares two models for delivering broadband wireless services : best effort vs . QoS guaranteed services . The ' best effort ' services we refer to in this paper are more commonly known as unlicensed wireless services , while the ' Quality of Service guaranteed ' services are more commonly referred to as traditional landline telephony , as well as cellular telephone services of either the second or third generation . This paper highlights the differing ' market ' versus ' engineering ' philosophies implicit in alternative wireless service architectures .
This paper presents a generalization of the random dot product model for networks whose edge weights are drawn from a parametrized probability distribution . We focus on the case of integer weight edges and show that many previously studied models can be recovered as special cases of this generalization . Our model also determines a dimension--reducing embedding process that gives geometric interpretations of community structure and centrality . The dimension of the embedding has consequences for the derived community structure and we exhibit a stress function for determining appropriate dimensions . We use this approach to analyze a coauthorship network and voting data from the U . S . Senate .
The problem of choosing spatial sampling designs for investigating unobserved spatial phenomenon S arises in many contexts , for example in identifying households to select for a prevalence survey to study disease burden and heterogeneity in a study region D . We studied randomised inhibitory spatial sampling designs to address the problem of spatial prediction whilst taking account of the need to estimate covariance structure . Two specific classes of design are inhibitory designs and inhibitory designs plus close pairs . In an inhibitory design , any pair of sample locations must be separated by at least an inhibition distance {$\delta$} . In an inhibitory plus close pairs design , n - k sample locations in an inhibitory design with inhibition distance {$\delta$} are augmented by k locations each positioned close to one of the randomly selected n - k locations in the inhibitory design , uniformly distributed within a disc of radius {$\zeta$} . We present simulation results for the Matern class of covariance structures . When the nugget variance is non-negligible , inhibitory plus close pairs designs demonstrate improved predictive efficiency over designs without close pairs . We illustrate how these findings can be applied to the design of a rolling Malaria Indicator Survey that forms part of an ongoing large-scale , five-year malaria transmission reduction project in Malawi .
Recently the generalisation error of deep neural networks has been analysed through the PAC-Bayesian framework , for the case of fully connected layers . We adapt this approach to the convolutional setting .
In many domains , scientists build complex simulators of natural phenomena that encode their hypotheses about the underlying processes . These simulators can be deterministic or stochastic , fast or slow , constrained or unconstrained , and so on . Optimizing the simulators with respect to a set of parameter values is common practice , resulting in a single parameter setting that minimizes an objective subject to constraints . We propose a post optimization posterior analysis that computes and visualizes all the models that can generate equally good or better simulation results , subject to constraints . These optimization posteriors are desirable for a number of reasons among which easy interpretability , automatic parameter sensitivity and correlation analysis and posterior predictive analysis . We develop a new sampling framework based on approximate Bayesian computation ( ABC ) with one-sided kernels . In collaboration with two groups of scientists we applied POPE to two important biological simulators : a fast and stochastic simulator of stem-cell cycling and a slow and deterministic simulator of tumor growth patterns .
Consistency , defined as the requirement that a series of measurements of the same project carried out by different raters using the same method should produce similar results , is one of the most important aspects to be taken into account in the measurement methods of the software . In spite of this , there is a widespread view that many measurement methods introduce an undesirable amount of subjectivity in the measurement process . This perception has made several organizations develop revisions of the standard methods whose main aim is to improve their consistency by introducing some suitable modifications of those aspects which are believed to introduce a greater degree of subjectivity . Each revision of a method must be empirically evaluated to determine to what extent is the aim of improving its consistency achieved . In this article we will define an homogeneous statistic intended to describe the consistency level of a method , and we will develop the statistical analysis which should be carried out in order to conclude whether or not a measurement method is more consistent than other one .
We show that for constraint satisfaction problems ( CSPs ) , sub-exponential size linear programming relaxations are as powerful as $n^{\Omega ( 0 ) }$-rounds of the Sherali-Adams linear programming hierarchy . As a corollary , we obtain sub-exponential size lower bounds for linear programming relaxations that beat random guessing for many CSPs such as MAX-CUT and MAX-0SAT . This is a nearly-exponential improvement over previous results , previously , it was only known that linear programs of size $n^{o ( \log n ) }$ cannot beat random guessing for any CSP ( Chan-Lee-Raghavendra-Steurer 0000 ) . Our bounds are obtained by exploiting and extending the recent progress in communication complexity for " lifting " query lower bounds to communication problems . The main ingredient in our results is a new structural result on " high-entropy rectangles " that may of independent interest in communication complexity .
Computational models pervade all branches of the exact sciences and have in recent times also started to prove to be of immense utility in some of the traditionally ' soft ' sciences like ecology , sociology and politics . This volume is a collection of a few cutting-edge research papers on the application of variety of computational models and tools in the analysis , interpretation and solution of vexing real-world problems and issues in economics , management , ecology and global politics by some prolific researchers in the field .
Syntax-Guided Synthesis ( SyGuS ) is the computational problem of finding an implementation f that meets both a semantic constraint given by a logical formula $\varphi$ in a background theory T , and a syntactic constraint given by a grammar G , which specifies the allowed set of candidate implementations . Such a synthesis problem can be formally defined in SyGuS-IF , a language that is built on top of SMT-LIB . The Syntax-Guided Synthesis Competition ( SyGuS-comp ) is an effort to facilitate , bring together and accelerate research and development of efficient solvers for SyGuS by providing a platform for evaluating different synthesis techniques on a comprehensive set of benchmarks . In this year ' s competition we added two specialized tracks : a track for conditional linear arithmetic , where the grammar need not be specified and is implicitly assumed to be that of the LIA logic of SMT-LIB , and a track for invariant synthesis problems , with special constructs conforming to the structure of an invariant synthesis problem . This paper presents and analyzes the results of SyGuS-comp ' 00 .
Simulators , as tools that can clearly bring out the effect of impairment , are invaluable in the design and development process of an assistive device . Simulators are vital in meeting high standards of accessibility . Described is our work on a smartphone-based vision simulator for diabetic retinopathy that is economic , portable , flexible and easy-to-use .
We present a novel view that unifies two frameworks that aim to solve sequential prediction problems : learning to search ( L0S ) and recurrent neural networks ( RNN ) . We point out equivalences between elements of the two frameworks . By complementing what is missing from one framework comparing to the other , we introduce a more advanced imitation learning framework that , on one hand , augments L0S s notion of search space and , on the other hand , enhances RNNs training procedure to be more robust to compounding errors arising from training on highly correlated examples .
This paper investigates , from information theoretic grounds , a learning problem based on the principle that any regularity in a given dataset can be exploited to extract compact features from data , i . e . , using fewer bits than needed to fully describe the data itself , in order to build meaningful representations of a relevant content ( multiple labels ) . We begin by introducing the noisy lossy source coding paradigm with the log-loss fidelity criterion which provides the fundamental tradeoffs between the \emph{cross-entropy loss} ( average risk ) and the information rate of the features ( model complexity ) . Our approach allows an information theoretic formulation of the \emph{multi-task learning} ( MTL ) problem which is a supervised learning framework in which the prediction models for several related tasks are learned jointly from common representations to achieve better generalization performance . Then , we present an iterative algorithm for computing the optimal tradeoffs and its global convergence is proven provided that some conditions hold . An important property of this algorithm is that it provides a natural safeguard against overfitting , because it minimizes the average risk taking into account a penalization induced by the model complexity . Remarkably , empirical results illustrate that there exists an optimal information rate minimizing the \emph{excess risk} which depends on the nature and the amount of available training data . An application to hierarchical text categorization is also investigated , extending previous works .
We present a methodology for clustering N objects which are described by multivariate time series , i . e . several sequences of real-valued random variables . This clustering methodology leverages copulas which are distributions encoding the dependence structure between several random variables . To take fully into account the dependence information while clustering , we need a distance between copulas . In this work , we compare renowned distances between distributions : the Fisher-Rao geodesic distance , related divergences and optimal transport , and discuss their advantages and disadvantages . Applications of such methodology can be found in the clustering of financial assets . A tutorial , experiments and implementation for reproducible research can be found at www . datagrapple . com/Tech .
The problem of ranking can be described as follows . We have a set of combinatorial objects $S$ , such as , say , the k-subsets of n things , and we can imagine that they have been arranged in some list , say lexicographically , and we want to have a fast method for obtaining the rank of a given object in the list . This problem is widely known in Combinatorial Analysis , Computer Science and Information Theory . Ranking is closely connected with the hashing problem , especially with perfect hashing and with generating of random combinatorial objects . In Information Theory the ranking problem is closely connected with so-called enumerative encoding , which may be described as follows : there is a set of words $S$ and an enumerative code has to one-to-one encode every $s \in S$ by a binary word $code ( s ) $ . The length of the $code ( s ) $ must be the same for all $s \in S$ . Clearly , $|code ( s ) |\geq \log |S|$ . ( Here and below $\log x=\log_{0}x ) $ . ) The suggested method allows the exponential growth of the speed of encoding and decoding for all combinatorial problems of enumeration which are considered , including the enumeration of permutations , compositions and others .
This paper presents a novel method for structural data recognition using a large number of graph models . In general , prevalent methods for structural data recognition have two shortcomings : 0 ) Only a single model is used to capture structural variation . 0 ) Naive recognition methods are used , such as the nearest neighbor method . In this paper , we propose strengthening the recognition performance of these models as well as their ability to capture structural variation . The proposed method constructs a large number of graph models and trains decision trees using the models . This paper makes two main contributions . The first is a novel graph model that can quickly perform calculations , which allows us to construct several models in a feasible amount of time . The second contribution is a novel approach to structural data recognition : graph model boosting . Comprehensive structural variations can be captured with a large number of graph models constructed in a boosting framework , and a sophisticated classifier can be formed by aggregating the decision trees . Consequently , we can carry out structural data recognition with powerful recognition capability in the face of comprehensive structural variation . The experiments shows that the proposed method achieves impressive results and outperforms existing methods on datasets of IAM graph database repository .
As advances in technology allow for the collection , storage , and analysis of vast amounts of data , the task of screening and assessing the significance of discovered patterns is becoming a major challenge in data mining applications . In this work , we address significance in the context of frequent itemset mining . Specifically , we develop a novel methodology to identify a meaningful support threshold s* for a dataset , such that the number of itemsets with support at least s* represents a substantial deviation from what would be expected in a random dataset with the same number of transactions and the same individual item frequencies . These itemsets can then be flagged as statistically significant with a small false discovery rate . We present extensive experimental results to substantiate the effectiveness of our methodology .
Tensor methods have emerged as a powerful paradigm for consistent learning of many latent variable models such as topic models , independent component analysis and dictionary learning . Model parameters are estimated via CP decomposition of the observed higher order input moments . However , in many domains , additional invariances such as shift invariances exist , enforced via models such as convolutional dictionary learning . In this paper , we develop novel tensor decomposition algorithms for parameter estimation of convolutional models . Our algorithm is based on the popular alternating least squares method , but with efficient projections onto the space of stacked circulant matrices . Our method is embarrassingly parallel and consists of simple operations such as fast Fourier transforms and matrix multiplications . Our algorithm converges to the dictionary much faster and more accurately compared to the alternating minimization over filters and activation maps .
Context as the dynamic information describing the situation of items and users and affecting the users decision process is essential to be used by recommender systems in mobile commerce to guarantee the quality of recommendation . This paper proposes a novel multidimensional approach for context aware recommendation in mobile commerce . The approach represents users , items , context information and the relationship between them in a multidimensional space . It then determines the usage patterns of each user under different contextual situations and creates a new 0 dimensional recommendation space and does the final recommendation in that space . This paper also represents an evaluation process by implementing the proposed approach in a restaurant food recommendation system considering day , time , weather and companion as the contextual information and comparing the approach with the traditional 0 dimensional one . The results of comparison illustrates that the multidimensional approach increases the recommendation quality .
We report in this paper the design , fabrication and experimental characterization of a piezoelectric MEMS microgenerator . This device scavenges the energy of ambient mechanical vibrations characterized by frequencies in the range of 0 kHz . This component is made with Aluminum Nitride thin film deposited with a CMOS compatible process . Moreover we analyze two possible solutions for the signal rectification : a discrete doubler-rectifier and a full custom power management circuit . The ASIC developed for this application takes advantage of diodes with very low threshold voltage and therefore allows the conversion of extremely low input voltages corresponding to very weak input accelerations . The volume of the proposed generator is inferior to 0mm0 and the generated powers are in the range of 0$\mu$W . This system is intended to supply power to autonomous wireless sensor nodes .
This paper explores a new framework for reinforcement learning based on online convex optimization , in particular mirror descent and related algorithms . Mirror descent can be viewed as an enhanced gradient method , particularly suited to minimization of convex functions in highdimensional spaces . Unlike traditional gradient methods , mirror descent undertakes gradient updates of weights in both the dual space and primal space , which are linked together using a Legendre transform . Mirror descent can be viewed as a proximal algorithm where the distance generating function used is a Bregman divergence . A new class of proximal-gradient based temporal-difference ( TD ) methods are presented based on different Bregman divergences , which are more powerful than regular TD learning . Examples of Bregman divergences that are studied include p-norm functions , and Mahalanobis distance based on the covariance of sample gradients . A new family of sparse mirror-descent reinforcement learning methods are proposed , which are able to find sparse fixed points of an l0-regularized Bellman equation at significantly less computational cost than previous methods based on second-order matrix methods . An experimental study of mirror-descent reinforcement learning is presented using discrete and continuous Markov decision processes .
Gaussian processes ( GPs ) are powerful non-parametric function estimators . However , their applications are largely limited by the expensive computational cost of the inference procedures . Existing stochastic or distributed synchronous variational inferences , although have alleviated this issue by scaling up GPs to millions of samples , are still far from satisfactory for real-world large applications , where the data sizes are often orders of magnitudes larger , say , billions . To solve this problem , we propose ADVGP , the first Asynchronous Distributed Variational Gaussian Process inference for regression , on the recent large-scale machine learning platform , PARAMETERSERVER . ADVGP uses a novel , flexible variational framework based on a weight space augmentation , and implements the highly efficient , asynchronous proximal gradient optimization . While maintaining comparable or better predictive performance , ADVGP greatly improves upon the efficiency of the existing variational methods . With ADVGP , we effortlessly scale up GP regression to a real-world application with billions of samples and demonstrate an excellent , superior prediction accuracy to the popular linear models .
The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance , but it has proved difficult to directly implement . Instead , most vision tasks are approached via complex bottom-up processing pipelines . Here we show that it is possible to write short , simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images . Generative probabilistic graphics programs consist of a stochastic scene generator , a renderer based on graphics software , a stochastic likelihood model linking the renderer ' s output and the data , and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model . Representations and algorithms from computer graphics , originally designed to produce high-quality images , are instead used as the deterministic backbone for highly approximate and stochastic generative models . This formulation combines probabilistic programming , computer graphics , and approximate Bayesian computation , and depends only on general-purpose , automatic inference techniques . We describe two applications : reading sequences of degraded and adversarially obscured alphanumeric characters , and inferring 0D road models from vehicle-mounted camera images . Each of the probabilistic graphics programs we present relies on under 00 lines of probabilistic code , and supports accurate , approximately Bayesian inferences about ambiguous real-world images .
We consider the general problem of resource sharing in societal networks , consisting of interconnected communication , transportation , energy and other networks important to the functioning of society . Participants in such network need to take decisions daily , both on the quantity of resources to use as well as the periods of usage . With this in mind , we discuss the problem of incentivizing users to behave in such a way that society as a whole benefits . In order to perceive societal level impact , such incentives may take the form of rewarding users with lottery tickets based on good behavior , and periodically conducting a lottery to translate these tickets into real rewards . We will pose the user decision problem as a mean field game ( MFG ) , and the incentives question as one of trying to select a good mean field equilibrium ( MFE ) . In such a framework , each agent ( a participant in the societal network ) takes a decision based on an assumed distribution of actions of his/her competitors , and the incentives provided by the social planner . The system is said to be at MFE if the agent ' s action is a sample drawn from the assumed distribution . We will show the existence of such an MFE under different settings , and also illustrate how to choose an attractive equilibrium using as an example demand-response in energy networks .
This paper describes a learning environment for image-guided prostate biopsies in cancer diagnosis ; it is based on an ultrasound probe simulator virtually exploring real datasets obtained from patients . The aim is to make the training of young physicians easier and faster with a tool that combines lectures , biopsy simulations and recommended exercises to master this medical gesture . It will particularly help acquiring the three-dimensional representation of the prostate needed for practicing biopsy sequences . The simulator uses a haptic feedback to compute the position of the virtual probe from three-dimensional ( 0D ) ultrasound recorded data . This paper presents the current version of this learning environment .
In the classical best arm identification ( Best-$0$-Arm ) problem , we are given $n$ stochastic bandit arms , each associated with a reward distribution with an unknown mean . We would like to identify the arm with the largest mean with probability at least $0-\delta$ , using as few samples as possible . Understanding the sample complexity of Best-$0$-Arm has attracted significant attention since the last decade . However , the exact sample complexity of the problem is still unknown . Recently , Chen and Li made the gap-entropy conjecture concerning the instance sample complexity of Best-$0$-Arm . Given an instance $I$ , let $\mu_{[i]}$ be the $i$th largest mean and $\Delta_{[i]}=\mu_{[0]}-\mu_{[i]}$ be the corresponding gap . $H ( I ) =\sum_{i=0}^n\Delta_{[i]}^{-0}$ is the complexity of the instance . The gap-entropy conjecture states that $\Omega\left ( H ( I ) \cdot\left ( \ln\delta^{-0}+\mathsf{Ent} ( I ) \right ) \right ) $ is an instance lower bound , where $\mathsf{Ent} ( I ) $ is an entropy-like term determined by the gaps , and there is a $\delta$-correct algorithm for Best-$0$-Arm with sample complexity $O\left ( H ( I ) \cdot\left ( \ln\delta^{-0}+\mathsf{Ent} ( I ) \right ) +\Delta_{[0]}^{-0}\ln\ln\Delta_{[0]}^{-0}\right ) $ . If the conjecture is true , we would have a complete understanding of the instance-wise sample complexity of Best-$0$-Arm . We make significant progress towards the resolution of the gap-entropy conjecture . For the upper bound , we provide a highly nontrivial algorithm which requires \[O\left ( H ( I ) \cdot\left ( \ln\delta^{-0} +\mathsf{Ent} ( I ) \right ) +\Delta_{[0]}^{-0}\ln\ln\Delta_{[0]}^{-0}\mathrm{polylog} ( n , \delta^{-0} ) \right ) \] samples in expectation . For the lower bound , we show that for any Gaussian Best-$0$-Arm instance with gaps of the form $0^{-k}$ , any $\delta$-correct monotone algorithm requires $\Omega\left ( H ( I ) \cdot\left ( \ln\delta^{-0} + \mathsf{Ent} ( I ) \right ) \right ) $ samples in expectation .
The busy beaver problem is a well-known example of a non-computable function . In order to determine a particular value of this function , it is necessary to generate and classify a large number of Turing machines . Previous work on this problem has described the processes used for the generation and classification of these machines , but unfortunately has generally not provided details of the machines considered . While there is no reason to doubt the veracity of the results known so far , it is difficult to accept such results as scientifically proven without being able to inspect the appropriate evidence . In addition , a list of machines and their classifications can be used for other results , such as variations on the busy beaver problem and related problems such as the placid platypus problem . In this paper we investigate how to generate classes of machines to be considered for the busy beaver problem . We discuss the relationship between quadruple and quintuple variants of Turing machines , and show that the latter are more general than the former . We give some formal results to justify our strategy for minimising the number of machines generated , and define a process reflecting this strategy for generating machines . We describe our implementation , and the results of generating various classes of machines with up to 0 states or up to 0 symbols , all of which ( together with our code ) are available on the author ' s website .
This paper presents our approach to the quantitative modeling and analysis of highly ( re ) configurable systems , such as software product lines . Different combinations of the optional features of such a system give rise to combinatorially many individual system variants . We use a formal modeling language that allows us to model systems with probabilistic behavior , possibly subject to quantitative feature constraints , and able to dynamically install , remove or replace features . More precisely , our models are defined in the probabilistic feature-oriented language QFLAN , a rich domain specific language ( DSL ) for systems with variability defined in terms of features . QFLAN specifications are automatically encoded in terms of a process algebra whose operational behavior interacts with a store of constraints , and hence allows to separate system configuration from system behavior . The resulting probabilistic configurations and behavior converge seamlessly in a semantics based on discrete-time Markov chains , thus enabling quantitative analysis . Our analysis is based on statistical model checking techniques , which allow us to scale to larger models with respect to precise probabilistic analysis techniques . The analyses we can conduct range from the likelihood of specific behavior to the expected average cost , in terms of feature attributes , of specific system variants . Our approach is supported by a novel Eclipse-based tool which includes state-of-the-art DSL utilities for QFLAN based on the Xtext framework as well as analysis plug-ins to seamlessly run statistical model checking analyses . We provide a number of case studies that have driven and validated the development of our framework .
Technological advancement in Wireless Sensor Networks ( WSN ) has made it become an invaluable component of a reliable environmental monitoring system ; they form the digital skin ' through which to ' sense ' and collect the context of the surroundings and provides information on the process leading to complex events such as drought . However , these environmental properties are measured by various heterogeneous sensors of different modalities in distributed locations making up the WSN , using different abstruse terms and vocabulary in most cases to denote the same observed property , causing data heterogeneity . Adding semantics and understanding the relationships that exist between the observed properties , and augmenting it with local indigenous knowledge is necessary for an accurate drought forecasting system . In this paper , we propose the framework for the semantic representation of sensor data and integration with indigenous knowledge on drought using a middleware for an efficient drought forecasting system .
We study full Bayesian procedures for high-dimensional linear regression under sparsity constraints . The prior is a mixture of point masses at zero and continuous distributions . Under compatibility conditions on the design matrix , the posterior distribution is shown to contract at the optimal rate for recovery of the unknown sparse vector , and to give optimal prediction of the response vector . It is also shown to select the correct sparse model , or at least the coefficients that are significantly different from zero . The asymptotic shape of the posterior distribution is characterized and employed to the construction and study of credible sets for uncertainty quantification .
A Cloud Services Brokerage ( CSB ) acts as an intermediary between cloud service providers ( e . g . , Amazon and Google ) and cloud service end users , providing a number of value adding services . CSBs as a research topic are in there infancy . The goal of this paper is to provide a concise survey of existing CSB technologies in a variety of areas and highlight a roadmap , which details five future opportunities for research .
A simple and computationally efficient scheme for tree-structured vector quantization is presented . Unlike previous methods , its quantization error depends only on the intrinsic dimension of the data distribution , rather than the apparent dimension of the space in which the data happen to lie .
In this paper we develop a theory of matrix completion for the extreme case of noisy 0-bit observations . Instead of observing a subset of the real-valued entries of a matrix M , we obtain a small number of binary ( 0-bit ) measurements generated according to a probability distribution determined by the real-valued entries of M . The central question we ask is whether or not it is possible to obtain an accurate estimate of M from this data . In general this would seem impossible , but we show that the maximum likelihood estimate under a suitable constraint returns an accurate estimate of M when ||M||_{\infty} <= \alpha , and rank ( M ) <= r . If the log-likelihood is a concave function ( e . g . , the logistic or probit observation models ) , then we can obtain this maximum likelihood estimate by optimizing a convex program . In addition , we also show that if instead of recovering M we simply wish to obtain an estimate of the distribution generating the 0-bit measurements , then we can eliminate the requirement that ||M||_{\infty} <= \alpha . For both cases , we provide lower bounds showing that these estimates are near-optimal . We conclude with a suite of experiments that both verify the implications of our theorems as well as illustrate some of the practical applications of 0-bit matrix completion . In particular , we compare our program to standard matrix completion methods on movie rating data in which users submit ratings from 0 to 0 . In order to use our program , we quantize this data to a single bit , but we allow the standard matrix completion program to have access to the original ratings ( from 0 to 0 ) . Surprisingly , the approach based on binary data performs significantly better .
In this paper , we study the approximation and estimation of $s$-concave densities via R\ ' enyi divergence . We first show that the approximation of a probability measure $Q$ by an $s$-concave densities exists and is unique via the procedure of minimizing a divergence functional proposed by Koenker and Mizera ( 0000 ) if and only if $Q$ admits full-dimensional support and a first moment . We also show continuity of the divergence functional in $Q$ : if $Q_n \to Q$ in the Wasserstein metric , then the projected densities converge in weighted $L_0$ metrics and uniformly on closed subsets of the continuity set of the limit . Moreover , directional derivatives of the projected densities also enjoy local uniform convergence . This contains both on-the-model and off-the-model situations , and entails strong consistency of the divergence estimator of an $s$-concave density under mild conditions . One interesting and important feature for the R\ ' enyi divergence estimator of an $s$-concave density is that the estimator is intrinsically related with the estimation of log-concave densities via maximum likelihood methods . In fact , we show that for $d=0$ at least , the R\ ' enyi divergence estimators for $s$-concave densities converge to the maximum likelihood estimator of a log-concave density as $s \nearrow 0$ . The R\ ' enyi divergence estimator shares similar characterizations as the MLE for log-concave distributions , which allows us to develop pointwise asymptotic distribution theory assuming that the underlying density is $s$-concave .
When targeting a distribution that is artificially invariant under some permutations , Markov chain Monte Carlo ( MCMC ) algorithms face the label-switching problem , rendering marginal inference particularly cumbersome . Such a situation arises , for example , in the Bayesian analysis of finite mixture models . Adaptive MCMC algorithms such as adaptive Metropolis ( AM ) , which self-calibrates its proposal distribution using an online estimate of the covariance matrix of the target , are no exception . To address the label-switching issue , relabeling algorithms associate a permutation to each MCMC sample , trying to obtain reasonable marginals . In the case of adaptive Metropolis ( Bernoulli 0 ( 0000 ) 000-000 ) , an online relabeling strategy is required . This paper is devoted to the AMOR algorithm , a provably consistent variant of AM that can cope with the label-switching problem . The idea is to nest relabeling steps within the MCMC algorithm based on the estimation of a single covariance matrix that is used both for adapting the covariance of the proposal distribution in the Metropolis algorithm step and for online relabeling . We compare the behavior of AMOR to similar relabeling methods . In the case of compactly supported target distributions , we prove a strong law of large numbers for AMOR and its ergodicity . These are the first results on the consistency of an online relabeling algorithm to our knowledge . The proof underlines latent relations between relabeling and vector quantization .
In language recognition , the task of rejecting/differentiating closely spaced versus acoustically far spaced languages remains a major challenge . For confusable closely spaced languages , the system needs longer input test duration material to obtain sufficient information to distinguish between languages . Alternatively , if languages are distinct and not acoustically/linguistically similar to others , duration is not a sufficient remedy . The solution proposed here is to explore duration distribution analysis for near/far languages based on the Language Recognition i-Vector Machine Learning Challenge 0000 ( LRiMLC00 ) database . Using this knowledge , we propose a likelihood ratio based fusion approach that leveraged both score and duration information . The experimental results show that the use of duration and score fusion improves language recognition performance by 0% relative in LRiMLC00 cost .
A classical approach for dealing with the multiple testing problem is to restrict attention to procedures that control the familywise error rate ( FWER ) , the probability of at least one false rejection . In many applications , one might be willing to tolerate more than one false rejection provided the number of such cases is controlled , thereby increasing the ability of the procedure to detect false null hypotheses . This suggests replacing control of the FWER by controlling the probability of $k$ or more false rejections , which is called the $k$-FWER . In this article , a unified approach is presented for deriving the $k$-FWER controlling procedures . We first generalize the well-known closure principle in the context of the FWER to the case of controlling the $k$-FWER . Then , we discuss how to derive the $k$-FWER controlling stepwise ( stepdown or stepup ) procedures based on marginal $p$-values using this principle . We show that , under certain conditions , generalized closed testing procedures can be reduced to stepwise procedures , and any stepwise procedure is equivalent to a generalized closed testing procedure . Finally , we generalize the well-known Hommel procedure in two directions , and show that any generalized Hommel procedure is equivalent to a generalized closed testing procedure with the same critical values .
We investigate adversarial attacks for autoencoders . We propose a procedure that distorts the input image to mislead the autoencoder in reconstructing a completely different target image . We attack the internal latent representations , attempting to make the adversarial input produce an internal representation as similar as possible as the target ' s . We find that autoencoders are much more robust to the attack than classifiers : while some examples have tolerably small input distortion , and reasonable similarity to the target image , there is a quasi-linear trade-off between those aims . We report results on MNIST and SVHN datasets , and also test regular deterministic autoencoders , reaching similar conclusions in all cases . Finally , we show that the usual adversarial attack for classifiers , while being much easier , also presents a direct proportion between distortion on the input , and misdirection on the output . That proportionality however is hidden by the normalization of the output , which maps a linear layer into non-linear probabilities .
We study the problem of generating , ranking and unranking of unlabeled ordered trees whose nodes have maximum degree of $\Delta$ . This class of trees represents a generalization of chemical trees . A chemical tree is an unlabeled tree in which no node has degree greater than 0 . By allowing up to $\Delta$ children for each node of chemical tree instead of 0 , we will have a generalization of chemical trees . Here , we introduce a new encoding over an alphabet of size 0 for representing unlabeled ordered trees with maximum degree of $\Delta$ . We use this encoding for generating these trees in A-order with constant average time and O ( n ) worst case time . Due to the given encoding , with a precomputation of size and time O ( n^0 ) ( assuming $\Delta$ is constant ) , both ranking and unranking algorithms are also designed taking O ( n ) and O ( nlogn ) time complexities .
Bio-medical ontologies can contain a large number of concepts . Often many of these concepts are very similar to each other , and similar or identical to concepts found in other bio-medical databases . This presents both a challenge and opportunity : maintaining many similar concepts is tedious and fastidious work , which could be substantially reduced if the data could be derived from pre-existing knowledge sources . In this paper , we describe how we have achieved this for an ontology of the mitochondria using our novel ontology development environment , the Tawny-OWL library .
This book chapter ( written in French ) is a review of the foundations of the Bayesian approach to statistical inference , relating to its historical roots and some philosophical arguments , as well as a short presentation of its practical implementation .
PP ( top x% ) is the proportion of papers of a unit ( e . g . an institution or a group of researchers ) , which belongs to the x% most frequently cited papers in the corresponding fields and publication years . It has been proposed that x% of papers can be expected which belongs to the x% most frequently cited papers . In this Letter to the Editor we will present the results of an empirical test whether we can really have this expectation and how strong the deviations from the expected values are when many random samples are drawn from the database .
Multivariate regression model is a natural generalization of the classical univari- ate regression model for fitting multiple responses . In this paper , we propose a high- dimensional multivariate conditional regression model for constructing sparse estimates of the multivariate regression coefficient matrix that accounts for the dependency struc- ture among the multiple responses . The proposed method decomposes the multivariate regression problem into a series of penalized conditional log-likelihood of each response conditioned on the covariates and other responses . It allows simultaneous estimation of the sparse regression coefficient matrix and the sparse inverse covariance matrix . The asymptotic selection consistency and normality are established for the diverging dimension of the covariates and number of responses . The effectiveness of the pro- posed method is also demonstrated in a variety of simulated examples as well as an application to the Glioblastoma multiforme cancer data .
The minmax regret problem for combinatorial optimization under uncertainty can be viewed as a zero-sum game played between an optimizing player and an adversary , where the optimizing player selects a solution and the adversary selects costs with the intention of maximizing the regret of the player . The existing minmax regret model considers only deterministic solutions/strategies , and minmax regret versions of most polynomial solvable problems are NP-hard . In this paper , we consider a randomized model where the optimizing player selects a probability distribution ( corresponding to a mixed strategy ) over solutions and the adversary selects costs with knowledge of the player ' s distribution , but not its realization . We show that under this randomized model , the minmax regret version of any polynomial solvable combinatorial problem becomes polynomial solvable . This holds true for both the interval and discrete scenario representations of uncertainty . Using the randomized model , we show new proofs of existing approximation algorithms for the deterministic model based on primal-dual approaches . Finally , we prove that minmax regret problems are NP-hard under general convex uncertainty .
A flexible spatio-temporal model is implemented to analyse extreme extra-tropical cyclones objectively identified over the Atlantic and Europe in 0-hourly re-analyses from 0000-0000 . Spatial variation in the extremal properties of the cyclones is captured using a 000 cell spatial regularisation , latitude as a covariate , and spatial random effects . The North Atlantic Oscillation ( NAO ) is also used as a covariate and is found to have a significant effect on intensifying extremal storm behaviour , especially over Northern Europe and the Iberian peninsula . Estimates of lower bounds on minimum sea-level pressure are typically 00-00 hPa below the minimum values observed for historical storms with largest differences occurring when the NAO index is positive .
Markov chain Monte Calro methods ( MCMC ) are commonly used in Bayesian statistics . In the last twenty years , many results have been established for the calculation of the exact convergence rate of MCMC methods . We introduce another rate of convergence for MCMC methods by approximation techniques . This rate can be obtained by the convergence of the Markov chain to a diffusion process . We apply it to a simple mixture model and obtain its convergence rate . Numerical simulations are performed to illustrate the effect of the rate .
This paper introduces a robust point-to-point transmission scheme : Tetrys , that relies on a novel on-the-fly erasure coding concept which reduces the delay for recovering lost data at the receiver side . In current erasure coding schemes , the packets that are not rebuilt at the receiver side are either lost or delayed by at least one RTT before transmission to the application . The present contribution aims at demonstrating that Tetrys coding scheme can fill the gap between real-time applications requirements and full reliability . Indeed , we show that in several cases , Tetrys can recover lost packets below one RTT over lossy and best-effort networks . We also show that Tetrys allows to enable full reliability without delay compromise and as a result : significantly improves the performance of time constrained applications . For instance , our evaluations present that video-conferencing applications obtain a PSNR gain up to 0dB compared to classic block-based erasure codes .
0D video coding is one of the most popular research area in multimedia . This paper reviews the recent progress of the coding technologies for multiview video ( MVV ) and free view-point video ( FVV ) which is represented by MVV and depth maps . We first discuss the traditional multiview video coding ( MVC ) framework with different prediction structures . The rate-distortion performance and the view switching delay of the three main coding prediction structures are analyzed . We further introduce the joint coding technologies for MVV and depth maps and evaluate the rate-distortion performance of them . The scalable 0D video coding technologies are reviewed by the quality and view scalability , respectively . Finally , we summarize the bit allocation work of 0D video coding . This paper also points out some future research problems in high efficiency 0D video coding such as the view switching latency optimization in coding structure and bit allocation .
Bayesian max-margin models have shown superiority in various practical applications , such as text categorization , collaborative prediction , social network link prediction and crowdsourcing , and they conjoin the flexibility of Bayesian modeling and predictive strengths of max-margin learning . However , Monte Carlo sampling for these models still remains challenging , especially for applications that involve large-scale datasets . In this paper , we present the stochastic subgradient Hamiltonian Monte Carlo ( HMC ) methods , which are easy to implement and computationally efficient . We show the approximate detailed balance property of subgradient HMC which reveals a natural and validated generalization of the ordinary HMC . Furthermore , we investigate the variants that use stochastic subsampling and thermostats for better scalability and mixing . Using stochastic subgradient Markov Chain Monte Carlo ( MCMC ) , we efficiently solve the posterior inference task of various Bayesian max-margin models and extensive experimental results demonstrate the effectiveness of our approach .
f0mma program can be used to translate programs written in some subset of the FORTRAN language into {\sl Mathematica} system ' s programming language . This subset have been enough to translate GAPP ( Global Analysis of Particle Properties ) programm into {\sl Mathematica} language automatically . Observables table calculated with GAPP ( {\sl Mathematica} ) is presented .
Assume a ( semi ) parametrically efficient estimator is given of the Euclidean parameter in a ( semi ) parametric model . A submodel is obtained by constraining this model in that a continuously differentiable function of the Euclidean parameter vanishes . We present an explicit method to construct ( semi ) parametrically efficient estimators of the Euclidean parameter in such equality constrained submodels and prove their efficiency . Our construction is based solely on the original efficient estimator and the constraining function . Only the parametric case of this estimation problem and a nonparametric version of it have been considered in literature .
Priebe et al . ( 0000 ) introduced the class cover catch digraphs and computed the distribution of the domination number of such digraphs for one dimensional data . In higher dimensions these calculations are extremely difficult due to the geometry of the proximity regions ; and only upper-bounds are available . In this article , we introduce a new type of data-random proximity map and the associated ( di ) graph in $\mathbb R^d$ . We find the asymptotic distribution of the domination number and use it for testing spatial point patterns of segregation and association .
Customer retention campaigns increasingly rely on predictive models to detect potential churners in a vast customer base . From the perspective of machine learning , the task of predicting customer churn can be presented as a binary classification problem . Using data on historic behavior , classification algorithms are built with the purpose of accurately predicting the probability of a customer defecting . The predictive churn models are then commonly selected based on accuracy related performance measures such as the area under the ROC curve ( AUC ) . However , these models are often not well aligned with the core business requirement of profit maximization , in the sense that , the models fail to take into account not only misclassification costs , but also the benefits originating from a correct classification . Therefore , the aim is to construct churn prediction models that are profitable and preferably interpretable too . The recently developed expected maximum profit measure for customer churn ( EMPC ) has been proposed in order to select the most profitable churn model . We present a new classifier that integrates the EMPC metric directly into the model construction . Our technique , called ProfTree , uses an evolutionary algorithm for learning profit driven decision trees . In a benchmark study with real-life data sets from various telecommunication service providers , we show that ProfTree achieves significant profit improvements compared to classic accuracy driven tree-based methods .
As a popular tool for producing meaningful and interpretable models , large-scale sparse learning works efficiently when the underlying structures are indeed or close to sparse . However , naively applying the existing regularization methods can result in misleading outcomes due to model misspecification . In particular , the direct sparsity assumption on coefficient vectors has been questioned in real applications . Therefore , we consider nonsparse learning with the conditional sparsity structure that the coefficient vector becomes sparse after taking out the impacts of certain unobservable latent variables . A new methodology of nonsparse learning with latent variables ( NSL ) is proposed to simultaneously recover the significant observable predictors and latent factors as well as their effects . We explore a common latent family incorporating population principal components and derive the convergence rates of both sample principal components and their score vectors that hold for a wide class of distributions . With the properly estimated latent variables , properties including model selection consistency and oracle inequalities under various prediction and estimation losses are established for the proposed methodology . Our new methodology and results are evidenced by simulation and real data examples .
This volume contains the proceedings of the Seventh International Symposium on Games , Automata , Logic and Formal Verification ( GandALF 0000 ) . The symposium took place in Catania , Italy , from the 00th to the 00th of September 0000 . The proceedings of the symposium contain abstracts of the 0 invited talks and 00 full papers that were accepted after a careful evaluation for presentation at the conference . The topics of the accepted papers cover algorithmic game theory , automata theory , synthesis , formal verification , and dynamic , modal and temporal logics .
Recently , there has been focus on penalized log-likelihood covariance estimation for sparse inverse covariance ( precision ) matrices . The penalty is responsible for inducing sparsity , and a very common choice is the convex $l_0$ norm . However , the best estimator performance is not always achieved with this penalty . The most natural sparsity promoting " norm " is the non-convex $l_0$ penalty but its lack of convexity has deterred its use in sparse maximum likelihood estimation . In this paper we consider non-convex $l_0$ penalized log-likelihood inverse covariance estimation and present a novel cyclic descent algorithm for its optimization . Convergence to a local minimizer is proved , which is highly non-trivial , and we demonstrate via simulations the reduced bias and superior quality of the $l_0$ penalty as compared to the $l_0$ penalty .
Entity retrieval is the task of finding entities such as people or products in response to a query , based solely on the textual documents they are associated with . Recent semantic entity retrieval algorithms represent queries and experts in finite-dimensional vector spaces , where both are constructed from text sequences . We investigate entity vector spaces and the degree to which they capture structural regularities . Such vector spaces are constructed in an unsupervised manner without explicit information about structural aspects . For concreteness , we address these questions for a specific type of entity : experts in the context of expert finding . We discover how clusterings of experts correspond to committees in organizations , the ability of expert representations to encode the co-author graph , and the degree to which they encode academic rank . We compare latent , continuous representations created using methods based on distributional semantics ( LSI ) , topic models ( LDA ) and neural networks ( word0vec , doc0vec , SERT ) . Vector spaces created using neural methods , such as doc0vec and SERT , systematically perform better at clustering than LSI , LDA and word0vec . When it comes to encoding entity relations , SERT performs best .
This paper provides a theoretical justification of the superior classification performance of deep rectifier networks over shallow rectifier networks from the geometrical perspective of piecewise linear ( PWL ) classifier boundaries . We show that , for a given threshold on the approximation error , the required number of boundary facets to approximate a general smooth boundary grows exponentially with the dimension of the data , and thus the number of boundary facets , referred to as boundary resolution , of a PWL classifier is an important quality measure that can be used to estimate a lower bound on the classification errors . However , learning naively an exponentially large number of boundary facets requires the determination of an exponentially large number of parameters and also requires an exponentially large number of training patterns . To overcome this issue of " curse of dimensionality " , compressive representations of high resolution classifier boundaries are required . To show the superior compressive power of deep rectifier networks over shallow rectifier networks , we prove that the maximum boundary resolution of a single hidden layer rectifier network classifier grows exponentially with the number of units when this number is smaller than the dimension of the patterns . When the number of units is larger than the dimension of the patterns , the growth rate is reduced to a polynomial order . Consequently , the capacity of generating a high resolution boundary will increase if the same large number of units are arranged in multiple layers instead of a single hidden layer . Taking high dimensional spherical boundaries as examples , we show how deep rectifier networks can utilize geometric symmetries to approximate a boundary with the same accuracy but with a significantly fewer number of parameters than single hidden layer nets .
In recent years multilayer perceptrons ( MLPs ) with many hid- den layers Deep Neural Network ( DNN ) has performed sur- prisingly well in many speech tasks , i . e . speech recognition , speaker verification , speech synthesis etc . Although in the context of F0 modeling these techniques has not been ex- ploited properly . In this paper , Deep Belief Network ( DBN ) , a class of DNN family has been employed and applied to model the F0 contour of synthesized speech which was generated by HMM-based speech synthesis system . The experiment was done on Bengali language . Several DBN-DNN architectures ranging from four to seven hidden layers and up to 000 hid- den units per hidden layer was presented and evaluated . The results were compared against clustering tree techniques pop- ularly found in statistical parametric speech synthesis . We show that from textual inputs DBN-DNN learns a high level structure which in turn improves F0 contour in terms of ob- jective and subjective tests .
We address the problem of extracting an automaton from a trained recurrent neural network ( RNN ) . We present a novel algorithm that uses exact learning and abstract interpretation to perform efficient extraction of a minimal automaton describing the state dynamics of a given RNN . We use Angluin ' s L* algorithm as a learner and the given RNN as an oracle , employing abstract interpretation of the RNN for answering equivalence queries . Our technique allows automaton-extraction from the RNN while avoiding state explosion , even when the state vectors are large and fine differentiation is required between RNN states . We experiment with automata extraction from multi-layer GRU and LSTM based RNNs , with state-vector dimensions and underlying automata and alphabet sizes which are significantly larger than in previous automata-extraction work . In some cases , the underlying target language can be described with a succinct automata , yet the extracted automata is large and complex . These are cases in which the RNN failed to learn the intended generalization , and our extraction procedure highlights words which are misclassified by the seemingly " perfect " RNN .
One of the most valuable assets of an organization is its organizational data . The analysis and mining of this potential hidden treasure can lead to much added-value for the organization . Process mining is an emerging area that can be useful in helping organizations understand the status quo , check for compliance and plan for improving their processes . The aim of process mining is to extract knowledge from event logs of today ' s organizational information systems . Process mining includes three main types : discovering process models from event logs , conformance checking and organizational mining . In this paper , we briefly introduce process mining and review some of its most important techniques . Also , we investigate some of the applications of process mining in industry and present some of the most important challenges that are faced in this area .
Causal inference from observational data is an ambitious but highly relevant task , with diverse applications ranging from natural to social sciences . Within the scope of nonparametric time series , causal inference defined through interventions ( cf . Pearl ( 0000 ) ) is largely unexplored , although time order simplifies the problem substantially . We consider a marginal integration scheme for inferring causal effects from observational time series data , MINT-T ( marginal integration in time series ) , which is an adaptation for time series of a method proposed by Ernest and B\ " {u}hlmann ( Electron . J . Statist , pp . 0000-0000 , vol . 0 , 0000 ) for the case of independent data . Our approach for stationary stochastic processes is fully nonparametric and , assuming no instantaneous effects consistently recovers the total causal effect of a single intervention with optimal one-dimensional nonparametric convergence rate $n^{-0/0}$ assuming regularity conditions and twice differentiability of a certain corresponding regression function . Therefore , MINT-T remains largely unaffected by the curse of dimensionality as long as smoothness conditions hold in higher dimensions and it is feasible for a large class of stationary time series , including nonlinear and multivariate processes . For the case with instantaneous effects , we provide a procedure which guards against false positive causal statements .
Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features . In this work , we use Kurihara & Welling ( 0000 ) ' s maximization-expectation framework to perform approximate MAP inference for linear-Gaussian latent feature models with an Indian Buffet Process ( IBP ) prior . This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence . By adding a constant to this function , we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a one-third approximation to the optimal solution . Our inference method scales linearly with the size of the input data , and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model .
The recently developed variational autoencoders ( VAEs ) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods . However , most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution , thereby restricting its applications to relatively simple phenomena . In this work , we propose hierarchical nonparametric variational autoencoders , which combines tree-structured Bayesian nonparametric priors with VAEs , to enable infinite flexibility of the latent representation space . Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference . The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus , and infers accurate representations of data instances . We apply our model in video representation learning . Our method is able to discover highly interpretable activity hierarchies , and obtain improved clustering accuracy and generalization capacity based on the learned rich representations .
This paper establishes the consistency of spectral approaches to data clustering . We consider clustering of point clouds obtained as samples of a ground-truth measure . A graph representing the point cloud is obtained by assigning weights to edges based on the distance between the points they connect . We investigate the spectral convergence of both unnormalized and normalized graph Laplacians towards the appropriate operators in the continuum domain . We obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the spectral convergence to hold . We also show that the discrete clusters obtained via spectral clustering converge towards a continuum partition of the ground truth measure . Such continuum partition minimizes a functional describing the continuum analogue of the graph-based spectral partitioning . Our approach , based on variational convergence , is general and flexible .
We consider a classical model related to an empirical distribution function $ F_n ( t ) =\frac{0}{n}\sum_{k=0}^nI_{\{\xi_k\le t\}}$ of $ ( \xi_k ) _{i\ge 0}$ -- i . i . d . sequence of random variables , supported on the interval $[0 , 0]$ , with continuous distribution function $F ( t ) =\mathsf{P} ( \xi_0\le t ) $ . Applying ``Stopping Time Techniques ' ' , we give a proof of Kolmogorov ' s exponential bound $$ \mathsf{P}\big ( \sup_{t\in[0 , 0]}|F_n ( t ) -F ( t ) |\ge \varepsilon\big ) \le \text{const . }e^{-n\delta_\varepsilon} $$ conjectured by Kolmogorov in 0000 . Using this bound we establish a best possible logarithmic asymptotic of $$ \mathsf{P}\big ( \sup_{t\in[0 , 0]}n^\alpha|F_n ( t ) -F ( t ) |\ge \varepsilon\big ) $$ with rate $ \frac{0}{n^{0-0\alpha}} $ slower than $\frac{0}{n}$ for any $\alpha\in\big ( 0 , {0/0}\big ) $ .
Energy efficient watermarking preserves the watermark energy after linear attack as much as possible . We consider in this letter non-stationary signal models and derive conditions for energy efficient watermarking under random vector model without WSS assumption . We find that the covariance matrix of the energy efficient watermark should be proportional to host covariance matrix to best resist the optimal linear removal attacks . In WSS process our result reduces to the well known power spectrum condition . Intuitive geometric interpretation of the results are also discussed which in turn also provide more simpler proof of the main results .
We review the class of species sampling models ( SSM ) . In particular , we investigate the relation between the exchangeable partition probability function ( EPPF ) and the predictive probability function ( PPF ) . It is straightforward to define a PPF from an EPPF , but the converse is not necessarily true . In this paper we introduce the notion of putative PPFs and show novel conditions for a putative PPF to define an EPPF . We show that all possible PPFs in a certain class have to define ( unnormalized ) probabilities for cluster membership that are linear in cluster size . We give a new necessary and sufficient condition for arbitrary putative PPFs to define an EPPF . Finally , we show posterior inference for a large class of SSMs with a PPF that is not linear in cluster size and discuss a numerical method to derive its PPF .
Sampling theory encompasses all aspects related to the conversion of continuous-time signals to discrete streams of numbers . The famous Shannon-Nyquist theorem has become a landmark in the development of digital signal processing . In modern applications , an increasingly number of functions is being pushed forward to sophisticated software algorithms , leaving only those delicate finely-tuned tasks for the circuit level . In this paper , we review sampling strategies which target reduction of the ADC rate below Nyquist . Our survey covers classic works from the early 00 ' s of the previous century through recent publications from the past several years . The prime focus is bridging theory and practice , that is to pinpoint the potential of sub-Nyquist strategies to emerge from the math to the hardware . In that spirit , we integrate contemporary theoretical viewpoints , which study signal modeling in a union of subspaces , together with a taste of practical aspects , namely how the avant-garde modalities boil down to concrete signal processing systems . Our hope is that this presentation style will attract the interest of both researchers and engineers in the hope of promoting the sub-Nyquist premise into practical applications , and encouraging further research into this exciting new frontier .
This paper provides asymptotic theory for Inverse Probability Weighing ( IPW ) and Locally Robust Estimator ( LRE ) of Best Linear Predictor where the response missing at random ( MAR ) , but not completely at random ( MCAR ) . We relax previous assumptions in the literature about the first-step nonparametric components , requiring only their mean square convergence . This relaxation allows to use a wider class of machine leaning methods for the first-step , such as lasso . For a generic first-step , IPW incurs a first-order bias unless the model it approximates is truly linear in the predictors . In contrast , LRE remains first-order unbiased provided one can estimate the conditional expectation of the response with sufficient accuracy . An additional novelty is allowing the dimension of Best Linear Predictor to grow with sample size . These relaxations are important for estimation of best linear predictor of teacher-specific and hospital-specific effects with large number of individuals .
Interactive multi-view video streaming ( IMVS ) services permit to remotely immerse within a 0D scene . This is possible by transmitting a set of reference camera views ( anchor views ) , which are used by the clients to freely navigate in the scene and possibly synthesize additional viewpoints of interest . From a networking perspective , the big challenge in IMVS systems is to deliver to each client the best set of anchor views that maximizes the navigation quality , minimizes the view-switching delay and yet satisfies the network constraints . Integrating adaptive streaming solutions in free-viewpoint systems offers a promising solution to deploy IMVS in large and heterogeneous scenarios , as long as the multi-view video representations on the server are properly selected . We therefore propose to optimize the multi-view data at the server by minimizing the overall resource requirements , yet offering a good navigation quality to the different users . We propose a video representation set optimization for multiview adaptive streaming systems and we show that it is NP-hard . We therefore introduce the concept of multi-view navigation segment that permits to cast the video representation set selection as an integer linear programming problem with a bounded computational complexity . We then show that the proposed solution reduces the computational complexity while preserving optimality in most of the 0D scenes . We then provide simulation results for different classes of users and show the gain offered by an optimal multi-view video representation selection compared to recommended representation sets ( e . g . , Netflix and Apple ones ) or to a baseline representation selection algorithm where the encoding parameters are decided a priori for all the views .
So-called sparse estimators arise in the context of model fitting , when one a priori assumes that only a few ( unknown ) model parameters deviate from zero . Sparsity constraints can be useful when the estimation problem is under-determined , i . e . when number of model parameters is much higher than the number of data points . Typically , such constraints are enforced by minimizing the L0 norm , which yields the so-called LASSO estimator . In this work , we propose a simple parameter transform that emulates sparse priors without sacrificing the simplicity and robustness of L0-norm regularization schemes . We show how L0 regularization can be obtained with a " sparsify " remapping of parameters under normal Bayesian priors , and we demonstrate the ensuing variational Laplace approach using Monte-Carlo simulations .
In computational biology , numerous recent studies have been dedicated to the analysis of the chromatin structure within the cell by two-dimensional segmentation methods . Motivated by this application , we consider the problem of retrieving the diagonal blocks in a matrix of observations . The theoretical properties of the least-squares estimators of both the boundaries and the number of blocks proposed by L\ ' evy-Leduc et al . [0000] are investigated . More precisely , the contribution of the paper is to establish the consistency of these estimators . A surprising consequence of our results is that , contrary to the onedimensional case , a penalty is not needed for retrieving the true number of diagonal blocks . Finally , the results are illustrated on synthetic data .
In this paper , we propose optimal tests for circular reflective symmetry about a fixed median direction . The distributions against which optimality is achieved are the so-called k-sine-skewed distributions of Umbach and Jammalamadaka ( 0000 ) . We first show that sequences of k-sine-skewed models are locally and asymptotically normal in the vicinity of reflective symmetry . Following the Le Cam methodology , we then construct optimal ( in the maximin sense ) parametric tests for reflective symmetry , which we render semi-parametric by a studentization argument . These asymptotically distribution-free tests happen to be uniformly optimal ( under any reference density ) and are moreover of a very simple and intuitive form . They furthermore exhibit nice small sample properties , as we show through a Monte Carlo simulation study . Our new tests also allow us to re-visit the famous red wood ants data set of Jander ( 0000 ) . We further show that one of the proposed parametric tests can as well serve as a test for uniformity against cardioid alternatives ; this test coincides with the famous circular Rayleigh ( 0000 ) test for uniformity which is thus proved to be ( also ) optimal against cardioid alternatives . Moreover , our choice of k-sine-skewed alternatives , which are the circular analogues of the classical linear skew-symmetric distributions , permits us a Fisher singularity analysis \`a la Hallin and Ley ( 0000 ) with the result that only the prominent sine-skewed von Mises distribution suffers from these inferential drawbacks . Finally , we conclude the paper by discussing the unspecified location case .
We propose a new framework for manifold denoising based on processing in the graph Fourier frequency domain , derived from the spectral decomposition of the discrete graph Laplacian . Our approach uses the Spectral Graph Wavelet transform in order to per- form non-iterative denoising directly in the graph frequency domain , an approach inspired by conventional wavelet-based signal denoising methods . We theoretically justify our approach , based on the fact that for smooth manifolds the coordinate information energy is localized in the low spectral graph wavelet sub-bands , while the noise affects all frequency bands in a similar way . Experimental results show that our proposed manifold frequency denoising ( MFD ) approach significantly outperforms the state of the art denoising meth- ods , and is robust to a wide range of parameter selections , e . g . , the choice of k nearest neighbor connectivity of the graph .
We analyse bill cosponsorship networks in the Italian Chamber of Deputies . In comparison with other parliaments , a distinguishing feature of the Chamber is the large number of political groups . Our analysis aims to infer the pattern of collaborations between these groups from data on bill cosponsorships . We propose an extension of stochastic block models for edge-valued graphs and derive measures of group productivity and of collaboration between political parties . As the model proposed encloses a large number of parameters , we pursue a penalized likelihood approach that enables us to infer a sparse reduced graph displaying collaborations between political parties .
The {\em spectrum} of a first-order logic sentence is the set of natural numbers that are cardinalities of its finite models . In this paper we show that when restricted to using only two variables , but allowing counting quantifiers , the spectra of first-order logic sentences are semilinear and hence , closed under complement . At the heart of our proof are semilinear characterisations for the existence of regular and biregular graphs , the class of graphs in which there are a priori bounds on the degrees of the vertices . Our proof also provides a simple characterisation of models of two-variable logic with counting -- that is , up to renaming and extending the relation names , they are simply a collection of regular and biregular graphs .
As datasets capturing human choices grow in richness and scale---particularly in online domains---there is an increasing need for choice models that escape traditional choice-theoretic axioms such as regularity , stochastic transitivity , and Luce ' s choice axiom . In this work we introduce the Pairwise Choice Markov Chain ( PCMC ) model of discrete choice , an inferentially tractable model that does not assume any of the above axioms while still satisfying the foundational axiom of uniform expansion , a considerably weaker assumption than Luce ' s choice axiom . We show that the PCMC model significantly outperforms the Multinomial Logit ( MNL ) model in prediction tasks on both synthetic and empirical datasets known to exhibit violations of Luce ' s axiom . Our analysis also synthesizes several recent observations connecting the Multinomial Logit model and Markov chains ; the PCMC model retains the Multinomial Logit model as a special case .
While there has been an extended discussion concerning city population distribution , little has been said about administrative units . Even though there might be a correspondence between cities and administrative divisions , they are conceptually different entities and the correspondence breaks as artificial divisions form and evolve . In this work we investigate the population distribution of second level administrative units for 000 countries and propose the Discrete Generalized Beta Distribution ( DGBD ) rank-size function to describe the data . After testing the goodness of fit of this two parameter function against power law , which is the most common model for city population , DGBD is a good statistical model for 00% of our data sets and better than power law in almost every case . Particularly , DGBD is better than power law for fitting country population data . The fitted parameters of this function allow us to construct a phenomenological characterization of countries according to the way in which people are distributed inside them . We present a computational model to simulate the formation of administrative divisions and give numerical evidence that DGBD arises from it . This model along with the DGBD function prove adequate to reproduce and describe local unit evolution and its effect on population distribution .
Time series modeling and forecasting has fundamental importance to various practical domains . Thus a lot of active research works is going on in this subject during several years . Many important models have been proposed in literature for improving the accuracy and effectiveness of time series forecasting . The aim of this dissertation work is to present a concise description of some popular time series forecasting models used in practice , with their salient features . In this thesis , we have described three important classes of time series models , viz . the stochastic , neural networks and SVM based models , together with their inherent forecasting strengths and weaknesses . We have also discussed about the basic issues related to time series modeling , such as stationarity , parsimony , overfitting , etc . Our discussion about different time series models is supported by giving the experimental forecast results , performed on six real time series datasets . While fitting a model to a dataset , special care is taken to select the most parsimonious one . To evaluate forecast accuracy as well as to compare among different models fitted to a time series , we have used the five performance measures , viz . MSE , MAD , RMSE , MAPE and Theil ' s U-statistics . For each of the six datasets , we have shown the obtained forecast diagram which graphically depicts the closeness between the original and forecasted observations . To have authenticity as well as clarity in our discussion about time series modeling and forecasting , we have taken the help of various published research works from reputed journals and some standard books .
We present Tweet0Vec , a novel method for generating general-purpose vector representation of tweets . The model learns tweet embeddings using character-level CNN-LSTM encoder-decoder . We trained our model on 0 million , randomly selected English-language tweets . The model was evaluated using two methods : tweet semantic similarity and tweet sentiment categorization , outperforming the previous state-of-the-art in both tasks . The evaluations demonstrate the power of the tweet embeddings generated by our model for various tweet categorization tasks . The vector representations generated by our model are generic , and hence can be applied to a variety of tasks . Though the model presented in this paper is trained on English-language tweets , the method presented can be used to learn tweet embeddings for different languages .
Networks or graphs can easily represent a diverse set of data sources that are characterized by interacting units or actors . Social networks , representing people who communicate with each other , are one example . Communities or clusters of highly connected actors form an essential feature in the structure of several empirical networks . Spectral clustering is a popular and computationally feasible method to discover these communities . The stochastic blockmodel [Social Networks 0 ( 0000 ) 000--000] is a social network model with well-defined communities ; each node is a member of one community . For a network generated from the Stochastic Blockmodel , we bound the number of nodes " misclustered " by spectral clustering . The asymptotic results in this paper are the first clustering results that allow the number of clusters in the model to grow with the number of nodes , hence the name high-dimensional . In order to study spectral clustering under the stochastic blockmodel , we first show that under the more general latent space model , the eigenvectors of the normalized graph Laplacian asymptotically converge to the eigenvectors of a " population " normalized graph Laplacian . Aside from the implication for spectral clustering , this provides insight into a graph visualization technique . Our method of studying the eigenvectors of random matrices is original .
We investigate the use of alternative divergences to Kullback-Leibler ( KL ) in variational inference ( VI ) , based on the Variational Dropout \cite{kingma0000} . Stochastic gradient variational Bayes ( SGVB ) \cite{aevb} is a general framework for estimating the evidence lower bound ( ELBO ) in Variational Bayes . In this work , we extend the SGVB estimator with using Alpha-Divergences , which are alternative to divergences to VI ' KL objective . The Gaussian dropout can be seen as a local reparametrization trick of the SGVB objective . We extend the Variational Dropout to use alpha divergences for variational inference . Our results compare $\alpha$-divergence variational dropout with standard variational dropout with correlated and uncorrelated weight noise . We show that the $\alpha$-divergence with $\alpha \rightarrow 0$ ( or KL divergence ) is still a good measure for use in variational inference , in spite of the efficient use of Alpha-divergences for Dropout VI \cite{Li00} . $\alpha \rightarrow 0$ can yield the lowest training error , and optimizes a good lower bound for the evidence lower bound ( ELBO ) among all values of the parameter $\alpha \in [0 , \infty ) $ .
Traditional molecular communications via diffusion ( MCvD ) systems have used baseband modulation techniques by varying properties of molecular pulses such as the amplitude , the frequency of the transversal wave of the pulse , and the time delay between subsequent pulses . In this letter , we propose and implement passband modulation with molecules that exhibit longitudinal carrier wave properties . This is achieved through the oscillation of the transmitter . Frequency division multiplexing is employed to allow different molecular information streams to co-exist in the same space and time channel , creating an effective bandwidth for MCvD .
The discriminative approach to classification using deep neural networks has become the de-facto standard in various fields . Complementing recent reservations about safety against adversarial examples , we show that conventional discriminative methods can easily be fooled to provide incorrect labels with very high confidence to out of distribution examples . We posit that a generative approach is the natural remedy for this problem , and propose a method for classification using generative models . At training time , we learn a generative model for each class , while at test time , given an example to classify , we query each generator for its most similar generation , and select the class corresponding to the most similar one . Our approach is general and can be used with expressive models such as GANs and VAEs . At test time , our method accurately " knows when it does not know , " and provides resilience to out of distribution examples while maintaining competitive performance for standard examples .
0D object proposals , quickly detected regions in a 0D scene that likely contain an object of interest , are an effective approach to improve the computational efficiency and accuracy of the object detection framework . In this work , we propose a novel online method that uses our previously developed 0D object proposals , in a RGB-D video sequence , to match and track static objects in the scene using shape matching . Our main observation is that depth images provide important information about the geometry of the scene that is often ignored in object matching techniques . Our method takes less than a second in MATLAB on the UW-RGBD scene dataset on a single thread CPU and thus , has potential to be used in low-power chips in Unmanned Aerial Vehicles ( UAVs ) , quadcopters , and drones .
Modern Internet services are increasingly leveraging on cloud computing for flexible , elastic and on-demand provision . Typically , Quality of Service ( QoS ) of cloud-based services can be tuned using different underlying cloud configurations and resources , e . g . , number of threads , CPU and memory etc . , which are shared , leased and priced as utilities . This benefit is fundamentally grounded by autoscaling : an automatic and elastic process that adapts cloud configurations on-demand according to time-varying workloads . This thesis proposes a holistic cloud autoscaling framework to effectively and seamlessly address existing challenges related to different logical aspects of autoscaling , including architecting autoscaling system , modelling the QoS of cloud-based service , determining the granularity of control and deciding trade-off autoscaling decisions . The framework takes advantages of the principles of self-awareness and the related algorithms to adaptively handle the dynamics , uncertainties , QoS interference and trade-offs on objectives that are exhibited in the cloud . The major benefit is that , by leveraging the framework , cloud autoscaling can be effectively achieved without heavy human analysis and design time knowledge . Through conducting various experiments using RUBiS benchmark and realistic workload on real cloud setting , this thesis evaluates the effectiveness of the framework based on various quality indicators and compared with other state-of-the-art approaches .
In this work we design an iterative receiver that incorporate sparse channel estimation . State-of-the-art sparse channel estimators simplify the estimation problem to be a finite basis selection problem by restricting the multipath delays to a grid . Our main contribution is a receiver that is released from such a restriction ; the delays are " off-the-grid " , i . e . , they are estimated and tracked directly as continuous values . As a result our receiver does not suffer from the leakage effect , which destroys sparsity when the delays are restricted to a grid . We use the unifying framework of combined belief-propagation and mean-field . All parameters in the receiver are inherently estimated . The receiver outperforms iterative receivers embedding state-of-the-art sparse channel estimators in terms of both mean- squared error of the channel estimate and bit error rate . We also demonstrate that our receiver design allows for a significant reduction in the number of pilot signals , without incurring any increase in bit error rate . The receiver also adapts well to situations where the sparse channel assumption is violated ; in this case its bit error rate is comparable to that of an iterative receiver that uses minimum mean-squared error channel estimation .
Database schema elements such as tables , views , triggers and functions are typically defined with many interrelationships . In order to support database users in understanding a given schema , a rule-based approach for analyzing the respective dependencies is proposed using Datalog expressions . We show that many interesting properties of schema elements can be systematically determined this way . The expressiveness of the proposed analysis is exemplarily shown with the problem of computing induced functional dependencies for derived relations . The propagation of functional dependencies plays an important role in data integration and query optimization but represents an undecidable problem in general . And yet , our rule-based analysis covers all relational operators as well as linear recursive expressions in a systematic way showing the depth of analysis possible by our proposal . The analysis of functional dependencies is well-integrated in a uniform approach to analyzing dependencies between schema elements in general .
Supralinear and sublinear pre-synaptic and dendritic integration is considered to be responsible for nonlinear computation power of biological neurons , emphasizing the role of nonlinear integration as opposed to nonlinear output thresholding . How , why , and to what degree the transfer function nonlinearity helps biologically inspired neural network models is not fully understood . Here , we study these questions in the context of echo state networks ( ESN ) . ESN is a simple neural network architecture in which a fixed recurrent network is driven with an input signal , and the output is generated by a readout layer from the measurements of the network states . ESN architecture enjoys efficient training and good performance on certain signal-processing tasks , such as system identification and time series prediction . ESN performance has been analyzed with respect to the connectivity pattern in the network structure and the input bias . However , the effects of the transfer function in the network have not been studied systematically . Here , we use an approach tanh on the Taylor expansion of a frequently used transfer function , the hyperbolic tangent function , to systematically study the effect of increasing nonlinearity of the transfer function on the memory , nonlinear capacity , and signal processing performance of ESN . Interestingly , we find that a quadratic approximation is enough to capture the computational power of ESN with tanh function . The results of this study apply to both software and hardware implementation of ESN .
Effect modification means the magnitude or stability of a treatment effect varies as a function of an observed covariate . Generally , larger and more stable treatment effects are insensitive to larger biases from unmeasured covariates , so a causal conclusion may be considerably firmer if this pattern is noted if it occurs . We propose a new strategy , called the submax-method , that combines exploratory and confirmatory efforts to determine whether there is stronger evidence of causality - that is , greater insensitivity to unmeasured confounding - in some subgroups of individuals . It uses the joint distribution of test statistics that split the data in various ways based on certain observed covariates . For $L$ binary covariates , the method splits the population $L$ times into two subpopulations , perhaps first men and women , perhaps then smokers and nonsmokers , computing a test statistic from each subpopulation , and appends the test statistic for the whole population , making $0L+0$ test statistics in total . Although $L$ binary covariates define $0^{L}$ interaction groups , only $0L+0$ tests are performed , and at least $L+0$ of these tests use at least half of the data . The submax-method achieves the highest design sensitivity and the highest Bahadur efficiency of its component tests . Moreover , the form of the test is sufficiently tractable that its large sample power may be studied analytically . The simulation suggests that the submax method exhibits superior performance , in comparison with an approach using CART , when there is effect modification of moderate size . Using data from the NHANES I Epidemiologic Follow-Up Survey , an observational study of the effects of physical activity on survival is used to illustrate the method . The method is implemented in the $\texttt{R}$ package $\texttt{submax}$ which contains the NHANES example .
Manufacturing enterprises are facing a competitive challenge . This paper proposes the use of a value chain based approach to support the modelling and simulation of manufacturing enterprise processes . The aim is to help experts to make relevant decisions on product design and/or product manufacturing process planning . This decision tool is based on the value chain modelling , by considering the product requirements . In order to evaluate several performance indicators , a simulation of various potential value chains adapted to market demand was conducted through a Value Chains Simulator ( VCS ) . A discrete event simulator is used to perform the simulation of these scenarios and to evaluate the value as a global performance criterion ( balancing cost , quality , delivery time , services , etc . ) . An Analytical Hierarchy Process module supports the analysis process . The value chain model is based on activities and uses the concepts of resource consumption , while integrating the benefiting entities view point . A case study in the microelectronic field is carried out to corroborate the validity of the proposed VCS .
Sample size determination for a data set is an important statistical process for analyzing the data to an optimum level of accuracy and using minimum computational work . The applications of this process are credible in every domain which deals with large data sets and high computational work . This study uses Bayesian analysis for determination of minimum sample size of vibration signals to be considered for fault diagnosis of a bearing using pre-defined parameters such as the inverse standard probability and the acceptable margin of error . Thus an analytical formula for sample size determination is introduced . The fault diagnosis of the bearing is done using a machine learning approach using an entropy-based J00 algorithm . The following method will help researchers involved in fault diagnosis to determine minimum sample size of data for analysis for a good statistical stability and precision .
In this paper , we propose a methodology for partitioning and mapping computational intensive applications in reconfigurable hardware blocks of different granularity . A generic hybrid reconfigurable architecture is considered so as the methodology can be applicable to a large number of heterogeneous reconfigurable platforms . The methodology mainly consists of two stages , the analysis and the mapping of the application onto fine and coarse-grain hardware resources . A prototype framework consisting of analysis , partitioning and mapping tools has been also developed . For the coarse-grain reconfigurable hardware , we use our previous-developed high-performance coarse-grain data-path . In this work , the methodology is validated using two real-world applications , an OFDM transmitter and a JPEG encoder . In the case of the OFDM transmitter , a maximum clock cycles decrease of 00% relative to the ones in an all fine-grain mapping solution is achieved . The corresponding performance improvement for the JPEG is 00% .
The asymptotic behavior of estimates and information criteria in linear models are studied in the context of hierarchically correlated sampling units . The work is motivated by biological data collected on species where autocorrelation is based on the species ' genealogical tree . Hierarchical autocorrelation is also found in many other kinds of data , such as from microarray experiments or human languages . Similar correlation also arises in ANOVA models with nested effects . I show that the best linear unbiased estimators are almost surely convergent but may not be consistent for some parameters such as the intercept and lineage effects , in the context of Brownian motion evolution on the genealogical tree . For the purpose of model selection I show that the usual BIC does not provide an appropriate approximation to the posterior probability of a model . To correct for this , an effective sample size is introduced for parameters that are inconsistently estimated . For biological studies , this work implies that tree-aware sampling design is desirable ; adding more sampling units may not help ancestral reconstruction and only strong lineage effects may be detected with high power .
We construct an explicit family of linear rank-metric codes over any field ${\mathbb F}_h$ that enables efficient list decoding up to a fraction $\rho$ of errors in the rank metric with a rate of $0-\rho-\epsilon$ , for any desired $\rho \in ( 0 , 0 ) $ and $\epsilon > 0$ . Previously , a Monte Carlo construction of such codes was known , but this is in fact the first explicit construction of positive rate rank-metric codes for list decoding beyond the unique decoding radius . Our codes are subcodes of the well-known Gabidulin codes , which encode linearized polynomials of low degree via their values at a collection of linearly independent points . The subcode is picked by restricting the message polynomials to an ${\mathbb F}_h$-subspace that evades the structured subspaces over an extension field ${\mathbb F}_{h^t}$ that arise in the linear-algebraic list decoder for Gabidulin codes due to Guruswami and Xing ( STOC ' 00 ) . This subspace is obtained by combining subspace designs contructed by Guruswami and Kopparty ( FOCS ' 00 ) with subspace evasive varieties due to Dvir and Lovett ( STOC ' 00 ) . We establish a similar result for subspace codes , which are a collection of subspaces , every pair of which have low-dimensional intersection , and which have received much attention recently in the context of network coding . We also give explicit subcodes of folded Reed-Solomon ( RS ) codes with small folding order that are list-decodable ( in the Hamming metric ) with optimal redundancy , motivated by the fact that list decoding RS codes reduces to list decoding such folded RS codes . However , as we only list decode a subcode of these codes , the Johnson radius continues to be the best known error fraction for list decoding RS codes .
This paper presents Gom , a language for describing abstract syntax trees and generating a Java implementation for those trees . Gom includes features allowing the user to specify and modify the interface of the data structure . These features provide in particular the capability to maintain the internal representation of data in canonical form with respect to a rewrite system . This explicitly guarantees that the client program only manipulates normal forms for this rewrite system , a feature which is only implicitly used in many implementations .
Partially identified models occur commonly in economic applications . A common problem in this literature is a regression problem with bracketed ( interval-censored ) outcome variable Y , which creates a set-identified parameter of interest . The recent studies have only considered finite-dimensional linear regression in such context . To incorporate more complex controls into the problem , we consider a partially linear projection of Y on the set functions that are linear in treatment/policy variables and nonlinear in the controls . We characterize the identified set for the linear component of this projection and propose an estimator of its support function . Our estimator converges at parametric rate and has asymptotic normality properties . It may be useful for labor economics applications that involve bracketed salaries and rich , high-dimensional demographic data about the subjects of the study .
We consider two-class linear classification in a high-dimensional , low-sample size setting . Only a small fraction of the features are useful , the useful features are unknown to us , and each useful feature contributes weakly to the classification decision -- this setting was called the rare/weak model ( RW Model ) . We select features by thresholding feature $z$-scores . The threshold is set by {\it higher criticism} ( HC ) . Let $\pee_i$ denote the $P$-value associated to the $i$-th $z$-score and $\pee_{ ( i ) }$ denote the $i$-th order statistic of the collection of $P$-values . The HC threshold ( HCT ) is the order statistic of the $z$-score corresponding to index $i$ maximizing $ ( i/n - \pee_{ ( i ) } ) /\sqrt{\pee_{ ( i ) } ( 0-\pee_{ ( i ) } ) }$ . The ideal threshold optimizes the classification error . In \cite{PNAS} we showed that HCT was numerically close to the ideal threshold . We formalize an asymptotic framework for studying the RW model , considering a sequence of problems with increasingly many features and relatively fewer observations . We show that along this sequence , the limiting performance of ideal HCT is essentially just as good as the limiting performance of ideal thresholding . Our results describe two-dimensional {\it phase space} , a two-dimensional diagram with coordinates quantifying " rare " and " weak " in the RW model . Phase space can be partitioned into two regions -- one where ideal threshold classification is successful , and one where the features are so weak and so rare that it must fail . Surprisingly , the regions where ideal HCT succeeds and fails make the exact same partition of the phase diagram . Other threshold methods , such as FDR threshold selection , are successful in a substantially smaller region of the phase space than either HCT or Ideal thresholding .
This work presents a novel framework based on feed-forward neural network for text-independent speaker classification and verification , two related systems of speaker recognition . With optimized features and model training , it achieves 000% classification rate in classification and less than 0% Equal Error Rate ( ERR ) , using merely about 0 second and 0 seconds of data respectively . Features with stricter Voice Active Detection ( VAD ) than the regular one for speech recognition ensure extracting stronger voiced portion for speaker recognition , speaker-level mean and variance normalization helps to eliminate the discrepancy between samples from the same speaker . Both are proven to improve the system performance . In building the neural network speaker classifier , the network structure parameters are optimized with grid search and dynamically reduced regularization parameters are used to avoid training terminated in local minimum . It enables the training goes further with lower cost . In speaker verification , performance is improved with prediction score normalization , which rewards the speaker identity indices with distinct peaks and penalizes the weak ones with high scores but more competitors , and speaker-specific thresholding , which significantly reduces ERR in the ROC curve . TIMIT corpus with 0K sampling rate is used here . First 000 male speakers are used to train and test the classification performance . The testing files of them are used as in-domain registered speakers , while data from the remaining 000 male speakers are used as out-of-domain speakers , i . e . imposters in speaker verification .
For the additive white Gaussian noise channel with average codeword power constraint , sparse superposition codes are developed . These codes are based on the statistical high-dimensional regression framework . The paper [IEEE Trans . Inform . Theory 00 ( 0000 ) , 0000 - 0000] investigated decoding using the optimal maximum-likelihood decoding scheme . Here a fast decoding algorithm , called adaptive successive decoder , is developed . For any rate R less than the capacity C communication is shown to be reliable with exponentially small error probability .
An important application of interactive machine learning is extending or amplifying the cognitive and physical capabilities of a human . To accomplish this , machines need to learn about their human users ' intentions and adapt to their preferences . In most current research , a user has conveyed preferences to a machine using explicit corrective or instructive feedback ; explicit feedback imposes a cognitive load on the user and is expensive in terms of human effort . The primary objective of the current work is to demonstrate that a learning agent can reduce the amount of explicit feedback required for adapting to the user ' s preferences pertaining to a task by learning to perceive a value of its behavior from the human user , particularly from the user ' s facial expressions---we call this face valuing . We empirically evaluate face valuing on a grip selection task . Our preliminary results suggest that an agent can quickly adapt to a user ' s changing preferences with minimal explicit feedback by learning a value function that maps facial features extracted from a camera image to expected future reward . We believe that an agent learning to perceive a value from the body language of its human user is complementary to existing interactive machine learning approaches and will help in creating successful human-machine interactive applications .
We study the joint limit distribution of the $k$ largest eigenvalues of a $p\times p$ sample covariance matrix $XX^\T$ based on a large $p\times n$ matrix $X$ . The rows of $X$ are given by independent copies of a linear process , $X_{it}=\sum_j c_j Z_{i , t-j}$ , with regularly varying noise $ ( Z_{it} ) $ with tail index $\alpha\in ( 0 , 0 ) $ . It is shown that a point process based on the eigenvalues of $XX^\T$ converges , as $n\to\infty$ and $p\to\infty$ at a suitable rate , in distribution to a Poisson point process with an intensity measure depending on $\alpha$ and $\sum c_j^0$ . This result is extended to random coefficient models where the coefficients of the linear processes $ ( X_{it} ) $ are given by $c_j ( \theta_i ) $ , for some ergodic sequence $ ( \theta_i ) $ , and thus vary in each row of $X$ . As a by-product of our techniques we obtain a proof of the corresponding result for matrices with iid entries in cases where $p/n$ goes to zero or infinity and $\alpha\in ( 0 , 0 ) $ .
Streaming of 00 de-interlaced fields per second digital uncompressed video with 000x000 resolution without a loss of video fields is one of the desired technologies by scientists in biomechanics . If it is possible to stream digital uncompressed video without dropped video fields , then a sophisticated computer analysis of the transmitted via IEEE 0000a connection video is possible . Such process is used in biomechanics when it is important to analyze athletes performance via streaming digital uncompressed video to a computer and then analyzing it using specific software such as Arial Performance Analysis Systems .
Given a set of leaf-labeled trees with identical leaf sets , the well-known " Maximum Agreement SubTree " problem ( MAST ) consists of finding a subtree homeomorphically included in all input trees and with the largest number of leaves . Its variant called " Maximum Compatible Tree " ( MCT ) is less stringent , as it allows the input trees to be refined . Both problems are of particular interest in computational biology , where trees encountered have often small degrees . In this paper , we study the parameterized complexity of MAST and MCT with respect to the maximum degree , denoted by D , of the input trees . It is known that MAST is polynomial for bounded D . As a counterpart , we show that the problem is W[0]-hard with respect to parameter D . Moreover , relying on recent advances in parameterized complexity we obtain a tight lower bound : while MAST can be solved in O ( N^{O ( D ) } ) time where N denotes the input length , we show that an O ( N^{o ( D ) } ) bound is not achievable , unless SNP is contained in SE . We also show that MCT is W[0]-hard with respect to D , and that MCT cannot be solved in O ( N^{o ( 0^{D/0} ) } ) time , SNP is contained in SE .
Graph representations offer powerful and intuitive ways to describe data in a multitude of application domains . Here , we consider stochastic processes generating graphs and propose a methodology for detecting changes in stationarity of such processes . The methodology is general and considers a process generating attributed graphs with a variable number of vertices/edges , without the need to assume one-to-one correspondence between vertices at different time steps . The methodology acts by embedding every graph of the stream into a vector domain , where a conventional multivariate change detection procedure can be easily applied . We ground the soundness of our proposal by proving several theoretical results . In addition , we provide a specific implementation of the methodology and evaluate its effectiveness on several detection problems involving attributed graphs representing biological molecules and drawings . Experimental results are contrasted with respect to suitable baseline methods , demonstrating the competitiveness of our approach .
This note makes an observation that significantly simplifies a number of previous parallel , two-way merge algorithms based on binary search and sequential merge in parallel . First , it is shown that the additional merge step of distinguished elements as found in previous algorithms is not necessary , thus simplifying the implementation and reducing constant factors . Second , by fixating the requirements to the binary search , the merge algorithm becomes stable , provided that the sequential merge subroutine is stable . The stable , parallel merge algorithm can easily be used to implement a stable , parallel merge sort . For ordered sequences with $n$ and $m$ elements , $m\leq n$ , the simplified merge algorithm runs in $O ( n/p+\log n ) $ operations using $p$ processing elements . It can be implemented on an EREW PRAM , but since it requires only a single synchronization step , it is also a candidate for implementation on other parallel , shared-memory computers .
The Mann-Whitney effect is an intuitive measure for discriminating two survival distributions . Here we analyze various inference techniques for this parameter in a two-sample survival setting with independent right-censoring , where the survival times are even allowed to be discretely distributed . This allows for ties in the data and requires the introduction of normalized versions of Kaplan-Meier estimators from which adequate point estimates are deduced . From an asymptotic analysis of the latter , asymptotically exact inference procedures based on standard normal , bootstrap- and permutation-quantiles are developed and compared in simulations . Here , the asymptotically robust and , in case of equal survival and censoring distributions , even finitely exact permutation procedure turned out to be the best . Finally , all procedures are illustrated using a real data set .
-Complex manufacturing systems are subject to high levels of variability that decrease productivity , increase cycle times and severely impact the systems tractability . As accurate modelling of the sources of variability is a cornerstone to intelligent decision making , we investigate the consequences of the assumption of independent and identically distributed variables that is often made when modelling sources of variability such as down-times , arrivals , or process-times . We first explain the experiment setting that allows , through simulations and statistical tests , to measure the variability potential stored in a specific sequence of data . We show from industrial data that dependent behaviors might actually be the rule with potentially considerable consequences in terms of cycle time . As complex industries require strong levers to allow their tractability , this work underlines the need for a richer and more accurate modelling of real systems . Keywords-variability ; cycle time ; dependent events ; simulation ; complex manufacturing ; industry 0 . 0 I . Accurate modelling of variability and the independence assumption Industry 0 . 0 is said to be the next industrial revolution . The proper use of real-time information in complex manufacturing systems is expected to allow more customization of products in highly flexible production factories . Semiconductor High Mix Low Volume ( HMLV ) manufacturing facilities ( called fabs ) are one example of candidates for this transition towards " smart industries " . However , because of the high levels of variability , the environment of a HMLV fab is highly stochastic and difficult to manage . The uncontrolled variability limits the predictability of the system and thus the ability to meet delivery requirements in terms of volumes , cycle times and due dates . Typically , the HMLV STMicroelectronics Crolles 000 fab regularly experiences significant mix changes that result in unanticipated bottlenecks , leading to firefighting to meet commitment to customers . The overarching goal of our strategy is to improve the forecasting of future occurrences of bottlenecks and cycle time issues in order to anticipate them through allocation of the correct attention and resources . Our current finite capacity projection engine can effectively forecast bottlenecks , but it does not include reliable cycle time estimates . In order to enhance our projections , better forecast cycle time losses ( queuing times ) , improve the tractability of our system and reduce our cycle times , we now need accurate dynamic cycle time predictions . As increased cycle-time is the main reason workflow variability is studied ( both by the scientific community and practitioners , see e . g . [0] and [0] ) , what follows concentrates on cycle times . Moreover , the " variability " we account for should be understood as the potential to create higher cycle times , even though " variability " may be understood in a broader meaning . This choice is made for the sake of clarity , but the methodology we propose and the discussion we lead can be applied to any other measurable indicator . Sources of variability have been intensely investigated in both the literature and the industry , and tool down-times , arrivals variability as well as process-time variability are recognized as the major sources of variability in that sense that they create higher cycle times ( see [0] for a review and discussion ) . As a consequence , these factors are widely integrated into queuing formulas and simulation models with the objective to better model the complex reality of manufacturing facilities . One commonly accepted assumption in the development of these models is that the variables ( MTBF , MTTR , processing times , time between arrivals , etc . ) are independent and identically distributed ( i . i . d . ) random variables . However , these assumptions might be the reason for models inaccuracies as [0] points out in a literature review on queuing theory . Several authors have studied the potential effects of dependencies , such as [0] who studied the potential effects of dependencies between arrivals and process-times or [0] who investigated dependent process times , [0] also gives further references for studies on dependencies effects . In a previous work [0] , we pinpointed a few elements from industrial data that questioned the viability of this assumption in complex manufacturing systems . Figure 0 : Number of arrivals per week from real data ( A ) and generated by removing dependencies ( B )
We propose a new paradigm for telecommunications , and develop a framework drawing on concepts from information ( i . e . , different metrics of complexity ) and computational ( i . e . , agent based modeling ) theory , adapted from complex system science . We proceed in a systematic fashion by dividing network complexity understanding and analysis into different layers . Modelling layer forms the foundation of the proposed framework , supporting analysis and tuning layers . The modelling layer aims at capturing the significant attributes of networks and the interactions that shape them , through the application of tools such as agent-based modelling and graph theoretical abstractions , to derive new metrics that holistically describe a network . The analysis phase completes the core functionality of the framework by linking our new metrics to the overall network performance . The tuning layer augments this core with algorithms that aim at automatically guiding networks toward desired conditions . In order to maximize the impact of our ideas , the proposed approach is rooted in relevant , near-future architectures and use cases in 0G networks , i . e . , Internet of Things ( IoT ) and self-organizing cellular networks .
Regression for spatially dependent outcomes poses many challenges , for inference and for computation . Non-spatial models and traditional spatial mixed-effects models each have their advantages and disadvantages , making it difficult for practitioners to determine how to carry out a spatial regression analysis . We discuss the data-generating mechanisms implicitly assumed by various popular spatial regression models , and discuss the implications of these assumptions . We propose Bayesian spatial filtering as an approximate middle way between non-spatial models and traditional spatial mixed models . We show by simulation that our Bayesian spatial filtering model has several desirable properties and hence may be a useful addition to a spatial statistician ' s toolkit .
Let ( RU_0 , R U_0 ) be a given bivariate scale mixture random vector , with R>0 being independent of the bivariate random vector ( U_0 , U_0 ) . In this paper we derive exact asymptotic expansions of the tail probability P{RU_0> x , RU_0> ax} , a \in ( 0 , 0] as x tends infintiy assuming that R has distribution function in the Gumbel max-domain of attraction and ( U_0 , U_0 ) has a specific tail behaviour around some absorbing point . As a special case of our results we retrieve the exact asymptotic behaviour of bivariate polar random vectors . We apply our results to investigate the asymptotic independence and the asymptotic behaviour of conditional excess for bivariate scale mixture distributions .
We introduce capital flow constraints , loss of good will and loan to the lot sizing problem . Capital flow constraint is different from traditional capacity constraints : when a manufacturer launches production , its present capital should not be less than its present total production cost ; otherwise , it must decrease production quantity or suspend production . Unsatisfied demand in one period may cause customer ' s demand to shrink in the next period considering loss of goodwill . Fixed loan can be adopted in the starting period for production . A mixed integer model for a deterministic single-item problem is constructed . Based on the analysis about the structure of optimal solutions , we approximate it to a traveling salesman problem , and divide it into sub-linear programming problems without integer variables . A forward recursive algorithm with heuristic adjustments is proposed to solve it . When unit variable production costs are equal and goodwill loss rate is zero , the algorithm can obtain optimal solutions . Under other situations , numerical comparisons with CPLEX 00 . 0 . 0 show our algorithm can reach optimal in most cases and has computation time advantage for large-size problems . Numerical tests also demonstrate that initial capital availability as well as loan interest rate can substantially affect the manufacturer ' s optimal lot sizing decisions .
Automatic quality evaluation of Web information is a task with many fields of applications and of great relevance , especially in critical domains like the medical one . We move from the intuition that the quality of content of medical Web documents is affected by features related with the specific domain . First , the usage of a specific vocabulary ( Domain Informativeness ) ; then , the adoption of specific codes ( like those used in the infoboxes of Wikipedia articles ) and the type of document ( e . g . , historical and technical ones ) . In this paper , we propose to leverage specific domain features to improve the results of the evaluation of Wikipedia medical articles . In particular , we evaluate the articles adopting an " actionable " model , whose features are related to the content of the articles , so that the model can also directly suggest strategies for improving a given article quality . We rely on Natural Language Processing ( NLP ) and dictionaries-based techniques in order to extract the bio-medical concepts in a text . We prove the effectiveness of our approach by classifying the medical articles of the Wikipedia Medicine Portal , which have been previously manually labeled by the Wiki Project team . The results of our experiments confirm that , by considering domain-oriented features , it is possible to obtain sensible improvements with respect to existing solutions , mainly for those articles that other approaches have less correctly classified . Other than being interesting by their own , the results call for further research in the area of domain specific features suitable for Web data quality assessment .
Given data sampled from a number of variables , one is often interested in the underlying causal relationships in the form of a directed acyclic graph . In the general case , without interventions on some of the variables it is only possible to identify the graph up to its Markov equivalence class . However , in some situations one can find the true causal graph just from observational data , for example in structural equation models with additive noise and nonlinear edge functions . Most current methods for achieving this rely on nonparametric independence tests . One of the problems there is that the null hypothesis is independence , which is what one would like to get evidence for . We take a different approach in our work by using a penalized likelihood as a score for model selection . This is practically feasible in many settings and has the advantage of yielding a natural ranking of the candidate models . When making smoothness assumptions on the probability density space , we prove consistency of the penalized maximum likelihood estimator . We also present empirical results for simulated scenarios and real two-dimensional data sets ( cause-effect pairs ) where we obtain similar results as other state-of-the-art methods .
Stepwise refinement and Design-by-Contract are two formal approaches for modelling systems . These approaches are widely used in the development of systems . Both approaches have ( dis- ) advantages . This thesis aims to answer , is it possible to combine both approaches in the development of systems , providing the user with the benefits of both ? We answer this question by translating the stepwise refinement method with Event-B to Design-by-Contract with Java and JML , so users can take full advantage of both formal approaches without losing their benefits . This thesis presents a set of syntactic rules that translates Event-B to JML-annotated Java code . It also presents the implementation of the syntactic rules as the EventB0Java tool . We used the tool to translate several Event-B models . It generated JML-annotated Java code for all the considered models that serve as initial implementation . We also used EventB0Java for the development of two software applications . Additionally , we compared EventB0Java against two other tools for Event-B code generation . EventB0Java enables users to start the software development process in Event-B , where users can model the system and prove its consistency , to then transition to JML-annotated Java code , where users can continue the development process .
We demonstrate that the Faithfulness property that is assumed in much causal analysis is robustly violated for a large class of systems of a type that occurs throughout the life and social sciences : control systems . These systems exhibit correlations indistinguishable from zero between variables that are strongly causally connected , and can show very high correlations between variables that have no direct causal connection , only a connection via causal links between uncorrelated variables . Their patterns of correlation are robust , in that they remain unchanged when their parameters are varied . The violation of Faithfulness is fundamental to what a control system does : hold some variable constant despite the disturbing influences on it . No method of causal analysis that requires Faithfulness is applicable to such systems .
A network of agents attempt to learn some unknown state of the world drawn by nature from a finite set . Agents observe private signals conditioned on the true state , and form beliefs about the unknown state accordingly . Each agent may face an identification problem in the sense that she cannot distinguish the truth in isolation . However , by communicating with each other , agents are able to benefit from side observations to learn the truth collectively . Unlike many distributed algorithms which rely on all-time communication protocols , we propose an efficient method by switching between Bayesian and non-Bayesian regimes . In this model , agents exchange information only when their private signals are not informative enough ; thence , by switching between the two regimes , agents efficiently learn the truth using only a few rounds of communications . The proposed algorithm preserves learnability while incurring a lower communication cost . We also verify our theoretical findings by simulation examples .
Simulated tempering ( ST ) is an established Markov chain Monte Carlo ( MCMC ) method for sampling from a multimodal density $\pi ( \theta ) $ . Typically , ST involves introducing an auxiliary variable $k$ taking values in a finite subset of $[0 , 0]$ and indexing a set of tempered distributions , say $\pi_k ( \theta ) \propto \pi ( \theta ) ^k$ . In this case , small values of $k$ encourage better mixing , but samples from $\pi$ are only obtained when the joint chain for $ ( \theta , k ) $ reaches $k=0$ . However , the entire chain can be used to estimate expectations under $\pi$ of functions of interest , provided that importance sampling ( IS ) weights are calculated . Unfortunately this method , which we call importance tempering ( IT ) , can disappoint . This is partly because the most immediately obvious implementation is na\ " ive and can lead to high variance estimators . We derive a new optimal method for combining multiple IS estimators and prove that the resulting estimator has a highly desirable property related to the notion of effective sample size . We briefly report on the success of the optimal combination in two modelling scenarios requiring reversible-jump MCMC , where the na\ " ive approach fails .
Ridge estimator is an alternative to ordinary least square estimator when there is multicollinearity problem . There are many proposed estimators in literature . In this paper , we propose new estimators which are modifications of the estimator suggested by Lawless and Wang ( 0000 ) . A Monte Carlo experiment has been conducted for the comparison of the performances of the estimators . Mean squared error ( MSE ) is used as a performance criterion . The benefits of new estimators are illustrated using two real datasets . According to both simulation results and applications , our new estimators have better performances in the sense of MSE in most of the situations .
A method using multiple imputation and bootstrap for dealing with missing data in mediation analysis is introduced and implemented in SAS . Through simulation studies , it is shown that the method performs well for both MCAR and MAR data without and with auxiliary variables . It is also shown that the method works equally well for MNAR data if auxiliary variables related to missingness are included . The application of the method is demonstrated through the analysis of a subset of data from the National Longitudinal Survey of Youth .
We consider settings where data are available on a nonparametric function and various partial derivatives . Such circumstances arise in practice , for example in the joint estimation of cost and input functions in economics . We show that when derivative data are available , local averages can be replaced in certain dimensions by nonlocal averages , thus reducing the nonparametric dimension of the problem . We derive optimal rates of convergence and conditions under which dimension reduction is achieved . Kernel estimators and their properties are analyzed , although other estimators , such as local polynomial , spline and nonparametric least squares , may also be used . Simulations and an application to the estimation of electricity distribution costs are included .
An extension to the classical notion of core is the notion of $k$-additive core , that is , the set of $k$-additive games which dominate a given game , where a $k$-additive game has its M\ " obius transform ( or Harsanyi dividends ) vanishing for subsets of more than $k$ elements . Therefore , the 0-additive core coincides with the classical core . The advantages of the $k$-additive core is that it is never empty once $k\geq 0$ , and that it preserves the idea of coalitional rationality . However , it produces $k$-imputations , that is , imputations on individuals and coalitions of at most $k$ inidividuals , instead of a classical imputation . Therefore one needs to derive a classical imputation from a $k$-order imputation by a so-called sharing rule . The paper investigates what set of imputations the $k$-additive core can produce from a given sharing rule .
Quantum finite automata were introduced by C . Moore , J . P . Crutchfield , and by A . Kondacs and J . Watrous . This notion is not a generalization of the deterministic finite automata . Moreover , it was proved that not all regular languages can be recognized by quantum finite automata . A . Ambainis and R . Freivalds proved that for some languages quantum finite automata may be exponentially more concise rather than both deterministic and probabilistic finite automata . In this paper we introduce the notion of quantum finite multitape automata and prove that there is a language recognized by a quantum finite automaton but not by a deterministic or probabilistic finite automata . This is the first result on a problem which can be solved by a quantum computer but not by a deterministic or probabilistic computer . Additionally we discover unexpected probabilistic automata recognizing complicated languages .
With the growth of digital networks such as the Internet , digital media have been explosively developed in e-commerce and online services . This causes problems such as illegal copy and fake ownership . Watermarking is proposed as one of the solutions to such cases . Among different watermarking techniques , the wavelet transform has been used more because of its good ability in modeling the human visual system . Recently , Shearlet transform as an extension of Wavelet transform which is based on multi-resolution and multi-directional analysis is introduced . The most important feature of this transform is the appropriate representation of image edges . In this paper a hybrid scheme using Discrete Wavelet Transform ( DWT ) and Discrete Shearlet Transform ( DST ) is presented . In this way , the host image is decomposed using DWT , and then its low frequency sub-band is decomposed by DST . After that , the bidiagonal singular value decomposition ( BSVD ) is applied on the selected sub-band from Shearlet transform and the gray-scale watermark image is embedded into its bidiagonal singular values . The proposed method is examined on the images with different textures and resistance is evaluated against various attacks like image processing and geometric attacks . The results show good transparency and high robustness in proposed method .
In video coding , it is expected that the encoder could adaptively select the encoding parameters ( e . g . , quantization parameter ) to optimize the bit allocation to different sources under the given constraint . However , in hybrid video coding , the dependency between sources brings high complexity for the bit allocation optimization , especially in the block-level , and existing optimization methods mostly focus on frame-level bit allocation . In this paper , we propose a macroblock ( MB ) level bit allocation method based on the minimum maximum ( MINMAX ) criterion , which has acceptable encoding complexity for offline applications . An iterative-based algorithm , namely maximum distortion descend ( MDD ) , is developed to reduce quality fluctuation among MBs within a frame , where the Structure SIMilarity ( SSIM ) index is used to measure the perceptual distortion of MBs . Our extensive experimental results on benchmark video sequences show that the proposed method can greatly enhance the encoding performance in terms of both bits saving and perceptual quality improvement .
0D sensing and content capture have made significant progress in recent years and the MPEG standardization organization is launching a new project on immersive media with point cloud compression ( PCC ) as one key corner stone . In this work , we introduce a new binary tree based point cloud content partition and explore the graph signal processing tools , especially the graph transform with optimized Laplacian sparsity , to achieve better energy compaction and compression efficiency . The resulting rate-distortion operating points are convex-hull optimized over the existing Lagrangian solutions . Simulation results with the latest high quality point cloud content captured from the MPEG PCC demonstrated the transform efficiency and rate-distortion ( R-D ) optimal potential of the proposed solutions .
Recurrent neural networks ( RNNs ) have shown outstanding performance on processing sequence data . However , they suffer from long training time , which demands parallel implementations of the training procedure . Parallelization of the training algorithms for RNNs are very challenging because internal recurrent paths form dependencies between two different time frames . In this paper , we first propose a generalized graph-based RNN structure that covers the most popular long short-term memory ( LSTM ) network . Then , we present a parallelization approach that automatically explores parallelisms of arbitrary RNNs by analyzing the graph structure . The experimental results show that the proposed approach shows great speed-up even with a single training stream , and further accelerates the training when combined with multiple parallel training streams .
Conventional Web archives are created by periodically crawling a web site and archiving the responses from the Web server . Although easy to implement and common deployed , this form of archiving typically misses updates and may not be suitable for all preservation scenarios , for example a site that is required ( perhaps for records compliance ) to keep a copy of all pages it has served . In contrast , transactional archives work in conjunction with a Web server to record all pages that have been served . Los Alamos National Laboratory has developed SiteSory , an open-source transactional archive written in Java solution that runs on Apache Web servers , provides a Memento compatible access interface , and WARC file export features . We used the ApacheBench utility on a pre-release version of to measure response time and content delivery time in different environments and on different machines . The performance tests were designed to determine the feasibility of SiteStory as a production-level solution for high fidelity automatic Web archiving . We found that SiteStory does not significantly affect content server performance when it is performing transactional archiving . Content server performance slows from 0 . 000 seconds to 0 . 000 seconds per Web page access when the content server is under load , and from 0 . 00 seconds to 0 . 00 seconds when the resource has many embedded and changing resources .
It takes skill to build a meaningful predictive model even with the abundance of implementations of modern machine learning algorithms and readily available computing resources . Building a model becomes challenging if hundreds of terabytes of data need to be processed to produce the training data set . In a digital advertising technology setting , we are faced with the need to build thousands of such models that predict user behavior and power advertising campaigns in a 00/0 chaotic real-time production environment . As data scientists , we also have to convince other internal departments critical to implementation success , our management , and our customers that our machine learning system works . In this paper , we present the details of the design and implementation of an automated , robust machine learning platform that impacts billions of advertising impressions monthly . This platform enables us to continuously optimize thousands of campaigns over hundreds of millions of users , on multiple continents , against varying performance objectives .
Hawkes ( 0000 ) introduced a powerful multivariate point process model of mutually exciting processes to explain causal structure in data . In this paper it is shown that the Granger causality structure of such processes is fully encoded in the corresponding link functions of the model . A new nonparametric estimator of the link functions based on a time-discretized version of the point process is introduced by using an infinite order autoregression . Consistency of the new estimator is derived . The estimator is applied to simulated data and to neural spike train data from the spinal dorsal horn of a rat .
In the context of strategic games , we provide an axiomatic proof of the statement Common knowledge of rationality implies that the players will choose only strategies that survive the iterated elimination of strictly dominated strategies . Rationality here means playing only strategies one believes to be best responses . This involves looking at two formal languages . One is first-order , and is used to formalise optimality conditions , like avoiding strictly dominated strategies , or playing a best response . The other is a modal fixpoint language with expressions for optimality , rationality and belief . Fixpoints are used to form expressions for common belief and for iterated elimination of non-optimal strategies .
Deep learning became the method of choice in recent year for solving a wide variety of predictive analytics tasks . For sequence prediction , recurrent neural networks ( RNN ) are often the go-to architecture for exploiting sequential information where the output is dependent on previous computation . However , the dependencies of the computation lie in the latent domain which may not be suitable for certain applications involving the prediction of a step-wise transformation sequence that is dependent on the previous computation only in the visible domain . We propose that a hybrid architecture of convolution neural networks ( CNN ) and stacked autoencoders ( SAE ) is sufficient to learn a sequence of actions that nonlinearly transforms an input shape or distribution into a target shape or distribution with the same support . While such a framework can be useful in a variety of problems such as robotic path planning , sequential decision-making in games , and identifying material processing pathways to achieve desired microstructures , the application of the framework is exemplified by the control of fluid deformations in a microfluidic channel by deliberately placing a sequence of pillars . Learning of a multistep topological transform has significant implications for rapid advances in material science and biomedical applications .
The globalization of the software market leads to crucial problems for software companies . More competition between software companies arises and leads to the force on companies to develop ever newer software products in ever shortened time interval . Therefor the time to market for software systems is shortened and obviously the product life cycle is shortened too[ . . . ]The approach introduced here presents the novel technique together with a supportive environment that enables developers to cope with the adaptability of black-box software components . A supported environment will be designed that checks the compatibility of black-box software components with the assistance of their specifications .
We compare and contrast the statistical physics and quantum physics inspired approaches for unsupervised generative modeling of classical data . The two approaches represent probabilities of observed data using energy-based models and quantum states respectively . Classical and quantum information patterns of the target datasets therefore provide principled guidelines for structural design and learning in these two approaches . Taking the restricted Boltzmann machines ( RBM ) as an example , we analyze the information theoretical bounds of the two approaches . We verify our reasonings by comparing the performance of RBMs of various architectures on the standard MNIST datasets .
Refactoring is a widespread practice that helps developers to improve the maintainability and readability of their code . However , there is a limited number of studies empirically investigating the actual motivations behind specific refactoring operations applied by developers . To fill this gap , we monitored Java projects hosted on GitHub to detect recently applied refactorings , and asked the developers to ex- plain the reasons behind their decision to refactor the code . By applying thematic analysis on the collected responses , we compiled a catalogue of 00 distinct motivations for 00 well-known refactoring types . We found that refactoring activity is mainly driven by changes in the requirements and much less by code smells . Extract Method is the most versatile refactoring operation serving 00 different purposes . Finally , we found evidence that the IDE used by the developers affects the adoption of automated refactoring tools .
A Web service is a software system designed to support interoperable machine-to-machine interaction over a network . Web services provide a standard means of interoperating between different software applications , running on a variety of platforms and/or frameworks . One of the main advantages of the usage of web services is its ability to integrate with the other services through web service composition and realize the required functionality . This paper presents a new paradigm of dynamic web services composition using network analysis paired with backtracking . An algorithm called " Zeittafel " for the selection and scheduling of services that are to be composed is also presented . With the proposed system better percentage of job success rate is obtained compared to the existing methodology .
We derive an asymptotic theory of nonparametric estimation for a time series regression model $Z_t=f ( X_t ) +W_t$ , where \ensuremath\{X_t\} and \ensuremath\{Z_t\} are observed nonstationary processes and $\{W_t\}$ is an unobserved stationary process . In econometrics , this can be interpreted as a nonlinear cointegration type relationship , but we believe that our results are of wider interest . The class of nonstationary processes allowed for $\{X_t\}$ is a subclass of the class of null recurrent Markov chains . This subclass contains random walk , unit root processes and nonlinear processes . We derive the asymptotics of a nonparametric estimate of f ( x ) under the assumption that $\{W_t\}$ is a Markov chain satisfying some mixing conditions . The finite-sample properties of $\hat{f} ( x ) $ are studied by means of simulation experiments .
We consider the problem of broadcasting data streams over a wireless network for multiple receivers with reliability and timely delivery guarantees . In our framework , we consider packets that need to be delivered within a given time interval , after which the packet is no longer useful at the application layer . We set the notion of critical packet and , based on periodic feedback from the receivers , we propose a retransmission scheme that will guarantee timely delivery of such packets , as well as packets that are innovative for other receivers . Our solution provides a trade-off between packet delivery ratio and bandwidth use , which contrasts with existing approaches such as FEC and ARQ , where the focus is on ensuring reliability first , offering no guarantees of timely delivery of data . We evaluate the performance of our proposal in a 000 . 00 wireless network testbed .
In retailer management , the Newsvendor problem has widely attracted attention as one of basic inventory models . In the traditional approach to solving this problem , it relies on the probability distribution of the demand . In theory , if the probability distribution is known , the problem can be considered as fully solved . However , in any real world scenario , it is almost impossible to even approximate or estimate a better probability distribution for the demand . In recent years , researchers start adopting machine learning approach to learn a demand prediction model by using other feature information . In this paper , we propose a supervised learning that optimizes the demand quantities for products based on feature information . We demonstrate that the original Newsvendor loss function as the training objective outperforms the recently suggested quadratic loss function . The new algorithm has been assessed on both the synthetic data and real-world data , demonstrating better performance .
In this paper , we aim at recovering an unknown signal x0 from noisy L0measurements y=Phi*x0+w , where Phi is an ill-conditioned or singular linear operator and w accounts for some noise . To regularize such an ill-posed inverse problem , we impose an analysis sparsity prior . More precisely , the recovery is cast as a convex optimization program where the objective is the sum of a quadratic data fidelity term and a regularization term formed of the L0-norm of the correlations between the sought after signal and atoms in a given ( generally overcomplete ) dictionary . The L0-sparsity analysis prior is weighted by a regularization parameter lambda>0 . In this paper , we prove that any minimizers of this problem is a piecewise-affine function of the observations y and the regularization parameter lambda . As a byproduct , we exploit these properties to get an objectively guided choice of lambda . In particular , we develop an extension of the Generalized Stein Unbiased Risk Estimator ( GSURE ) and show that it is an unbiased and reliable estimator of an appropriately defined risk . The latter encompasses special cases such as the prediction risk , the projection risk and the estimation risk . We apply these risk estimators to the special case of L0-sparsity analysis regularization . We also discuss implementation issues and propose fast algorithms to solve the L0 analysis minimization problem and to compute the associated GSURE . We finally illustrate the applicability of our framework to parameter ( s ) selection on several imaging problems .
A definition of qualitative robustness for point estimators in general statistical models is proposed . Some criteria for robustness are established and applied to estimators in parametric , semiparametric , and nonparametric models . In specific nonparametric models , the proposed definition boils down to Hampel robustness . It is also explained how plug-in estimators in certain nonparametric models can be reasonably classified w . r . t . their degrees of robustness .
One of the major problems for maximum likelihood estimation in the well-established directional models is that the normalising constants can be difficult to evaluate . A new general method of " score matching estimation " is presented here on a compact oriented Riemannian manifold . Important applications include von Mises-Fisher , Bingham and joint models on the sphere and related spaces . The estimator is consistent and asymptotically normally distributed under mild regularity conditions . Further , it is easy to compute as a solution of a linear set of equations and requires no knowledge of the normalizing constant . Several examples are given , both analytic and numerical , to demonstrate its good performance .
We provide compact algebraic expressions that replace the lengthy symbolic-algebra-generated integrals I0 and I0 in Part I of this series of papers [0] . The MRSE entries of Part I , Table 0 . 0 are thus updated to simpler algebraic expressions . We use MINOS [0-0] to tabulate several IMSPE-optimal designs with one factor and one or two design points , for the exponential- , two of the Mat\ ' ern- , and the Gaussian-correlation functions , i . e . for the class of problems considered in Part I . The tabulated results can be used as standards for optimal-design-software developers and users .
In this paper , we present a deterministic algorithm for the closest vector problem for all l_p-norms , 0 < p < \infty , and all polyhedral norms , especially for the l_0-norm and the l_{\infty}-norm . We achieve our results by introducing a new lattice problem , the lattice membership problem . We describe a deterministic algorithm for the lattice membership problem , which is a generalization of Lenstra ' s algorithm for integer programming . We also describe a polynomial time reduction from the closest vector problem to the lattice membership problem . This approach leads to a deterministic algorithm that solves the closest vector problem for all l_p-norms , 0 < p < \infty , in time p log_0 ( r ) ^{O ( 0 ) } n^{ ( 0/0+o ( 0 ) ) n} and for all polyhedral norms in time ( s log_0 ( r ) ) ^{O ( 0 ) } n^{ ( 0+o ( 0 ) ) n} , where s is the number of constraints defining the polytope and r is an upper bound on the coefficients used to describe the convex body .
We present a new , deadlock-free , routing scheme for toroidal interconnection networks , called OutFlank Routing ( OFR ) . OFR is an adaptive strategy which exploits non-minimal links , both in the source and in the destination nodes . When minimal links are congested , OFR deroutes packets to carefully chosen intermediate destinations , in order to obtain travel paths which are only an additive constant longer than the shortest ones . Since routing performance is very sensitive to changes in the traffic model or in the router parameters , an accurate discrete-event simulator of the toroidal network has been developed to empirically validate OFR , by comparing it against other relevant routing strategies , over a range of typical real-world traffic patterns . On the 00x00x00 ( 0000 nodes ) simulated network OFR exhibits improvements of the maximum sustained throughput between 00% and 000% , with respect to Adaptive Bubble Routing .
Software cost estimation is one of the prerequisite managerial activities carried out at the software development initiation stages and also repeated throughout the whole software life-cycle so that amendments to the total cost are made . In software cost estimation typically , a selection of project attributes is employed to produce effort estimations of the expected human resources to deliver a software product . However , choosing the appropriate project cost drivers in each case requires a lot of experience and knowledge on behalf of the project manager which can only be obtained through years of software engineering practice . A number of studies indicate that popular methods applied in the literature for software cost estimation , such as linear regression , are not robust enough and do not yield accurate predictions . Recently the dual variables Ridge Regression ( RR ) technique has been used for effort estimation yielding promising results . In this work we show that results may be further improved if an AI method is used to automatically select appropriate project cost drivers ( inputs ) for the technique . We propose a hybrid approach combining RR with a Genetic Algorithm , the latter evolving the subset of attributes for approximating effort more accurately . The proposed hybrid cost model has been applied on a widely known high-dimensional dataset of software project samples and the results obtained show that accuracy may be increased if redundant attributes are eliminated .
Subset selection in multiple linear regression is to choose a subset of candidate explanatory variables that tradeoff error and the number of variables selected . We built mathematical programming models for subset selection and compare the performance of an LP-based branch-and-bound algorithm with tailored valid inequalities to known heuristics . We found that our models quickly find a quality solution while the rest of the time is spent to prove optimality . Our models are also applicable with slight modifications to the case with more candidate explanatory variables than observations . For this case , we provide mathematical programming models , propose new criteria , and develop heuristic algorithms based on mathematical programming .
Given $n$ line segments in the plane , do they form the edge set of a \emph{weakly simple polygon} ; that is , can the segment endpoints be perturbed by at most $\varepsilon$ , for any $\varepsilon>0$ , to obtain a simple polygon ? While the analogous question for \emph{simple polygons} can easily be answered in $O ( n\log n ) $ time , we show that it is NP-complete for weakly simple polygons . We give $O ( n ) $-time algorithms in two special cases : when all segments are collinear , or the segment endpoints are in general position . These results extend to the variant in which the segments are \emph{directed} , and the counterclockwise traversal of a polygon should follow the orientation . We study related problems for the case that the union of the $n$ input segments is connected . ( i ) If each segment can be subdivided into several segments , find the minimum number of subdivision points to form a weakly simple polygon . ( ii ) If new line segments can be added , find the minimum total length of new segments that creates a weakly simple polygon . We give worst-case upper and lower bounds for both problems .
We introduce the heat method for computing the shortest geodesic distance to a specified subset ( e . g . , point or curve ) of a given domain . The heat method is robust , efficient , and simple to implement since it is based on solving a pair of standard linear elliptic problems . The method represents a significant breakthrough in the practical computation of distance on a wide variety of geometric domains , since the resulting linear systems can be prefactored once and subsequently solved in near-linear time . In practice , distance can be updated via the heat method an order of magnitude faster than with state-of-the-art methods while maintaining a comparable level of accuracy . We provide numerical evidence that the method converges to the exact geodesic distance in the limit of refinement ; we also explore smoothed approximations of distance suitable for applications where more regularity is required .
In this paper , a real-time quasi-optimal trajectory planning scheme is employed to guide an autonomous underwater vehicle ( AUV ) safely into a funnel-shape stationary docking station . By taking advantage of the direct method of calculus of variation and inverse dynamics optimization , the proposed trajectory planner provides a computationally efficient framework for autonomous underwater docking in a 0D cluttered undersea environment . Vehicular constraints , such as constraints on AUV states and actuators ; boundary conditions , including initial and final vehicle poses ; and environmental constraints , for instance no-fly zones and current disturbances , are all modelled and considered in the problem formulation . The performance of the proposed planner algorithm is analyzed through simulation studies . To show the reliability and robustness of the method in dealing with uncertainty , Monte Carlo runs and statistical analysis are carried out . The results of the simulations indicate that the proposed planner is well suited for real-time implementation in a dynamic and uncertain environment .
Recent distributed computing trends advocate the use of Representational State Transfer ( REST ) to alleviate the inherent complexity of the Web services standards in building service-oriented web applications . In this paper we focus on the particular case of geospatial services interfaced by the OGC Web Processing Service ( WPS ) specification in order to assess whether WPS-based geospatial services can be viewed from the architectural principles exposed in REST . Our concluding remarks suggest that the adoption of REST principles , to specially harness the built-in mechanisms of the HTTP application protocol , may be beneficial in scenarios where ad hoc composition of geoprocessing services are required , common for most non-expert users of geospatial information infrastructures .
During our earlier research , it was recognised that in order to be successful with an indirect genetic algorithm approach using a decoder , the decoder has to strike a balance between being an optimiser in its own right and finding feasible solutions . Previously this balance was achieved manually . Here we extend this by presenting an automated approach where the genetic algorithm itself , simultaneously to solving the problem , sets weights to balance the components out . Subsequently we were able to solve a complex and non-linear scheduling problem better than with a standard direct genetic algorithm implementation .
The article presents a general concept of the organization of pseudo three dimension visualization of graphics and video content for three dimension visualization systems . The steps of algorithms for solving the problem of synthesis of three dimension stereo images based on two dimension images are introduced . The features of synthesis organization of standard format of three dimension stereo frame are presented . Moreover , the performed experimental simulation for generating complete stereoframes and the results of its time complexity are shown . Keywords : Three dimension visualization , pseudo three dimension stereo , a stereo pair , three dimension stereo format , algorithm , modeling , time complexity .
We consider the first serial correlation coefficient under an AR ( 0 ) model where errors are not assumed to be Gaussian . In this case it is necessary to consider bootstrap approximations for tests based on the statistic since the distribution of errors is unknown . We obtain saddle-point approximations for tail probabilities of the statistic and its bootstrap version and use these to show that the bootstrap tail probabilities approximate the true values with given relative errors , thus extending the classical results of Daniels [Biometrika 00 ( 0000 ) 000-000] for the Gaussian case . The methods require conditioning on the set of odd numbered observations and suggest a conditional bootstrap which we show has similar relative error properties .
Discussion on " Sparse graphs using exchangeable random measures " by F . Caron and E . B . Fox . In this discussion we contribute to the analysis of the GGP model as compared to the Erdos-Renyi ( ER ) and the preferential attachment ( AB ) models , using different measures such as number of connected components , global clustering coefficient , assortativity coefficient and share of nodes in the core .
The goal of this paper is to extend independent subspace analysis ( ISA ) to the case of ( i ) nonparametric , not strictly stationary source dynamics and ( ii ) unknown source component dimensions . We make use of functional autoregressive ( fAR ) processes to model the temporal evolution of the hidden sources . An extension of the ISA separation principle--which states that the ISA problem can be solved by traditional independent component analysis ( ICA ) and clustering of the ICA elements--is derived for the solution of the defined fAR independent process analysis task ( fAR-IPA ) : applying fAR identification we reduce the problem to ISA . A local averaging approach , the Nadaraya-Watson kernel regression technique is adapted to obtain strongly consistent fAR estimation . We extend the Amari-index to different dimensional components and illustrate the efficiency of the fAR-IPA approach by numerical examples .
In this paper we propose a game-theoretic model to analyze events similar to the 0000 \emph{DARPA Network Challenge} , which was organized by the Defense Advanced Research Projects Agency ( DARPA ) for exploring the roles that the Internet and social networks play in incentivizing wide-area collaborations . The challenge was to form a group that would be the first to find the locations of ten moored weather balloons across the United States . We consider a model in which $N$ people ( who can form groups ) are located in some topology with a fixed coverage volume around each person ' s geographical location . We consider various topologies where the players can be located such as the Euclidean $d$-dimension space and the vertices of a graph . A balloon is placed in the space and a group wins if it is the first one to report the location of the balloon . A larger team has a higher probability of finding the balloon , but we assume that the prize money is divided equally among the team members . Hence there is a competing tension to keep teams as small as possible . \emph{Risk aversion} is the reluctance of a person to accept a bargain with an uncertain payoff rather than another bargain with a more certain , but possibly lower , expected payoff . In our model we consider the \emph{isoelastic} utility function derived from the Arrow-Pratt measure of relative risk aversion . The main aim is to analyze the structures of the groups in Nash equilibria for our model . For the $d$-dimensional Euclidean space ( $d\geq 0$ ) and the class of bounded degree regular graphs we show that in any Nash Equilibrium the \emph{richest} group ( having maximum expected utility per person ) covers a constant fraction of the total volume .
We investigate a semiparametric regression model where one gets noisy non linear non invertible functions of the observations . We focus on the application to bearings-only tracking . We first investigate the least squares estimator and prove its consistency and asymptotic normality under mild assumptions . We study the semiparametric likelihood process and prove local asymptotic normality of the model . This allows to define the efficient Fisher information as a lower bound for the asymptotic variance of regular estimators , and to prove that the parametric likelihood estimator is regular and asymptotically efficient . Simulations are presented to illustrate our results .
The paper is dealing with semi-classical asymptotics of a characteristic function for a stochastic process . The main technical tool is provided by the stationary phase method . The extremal range for a stochastic process is defined by limit values of the complex logarithm of the characteristic function . The paper also outlines a numerical method for calculating stochastic extrema .
Bayesian optimization has recently attracted the attention of the automatic machine learning community for its excellent results in hyperparameter tuning . BO is characterized by the sample efficiency with which it can optimize expensive black-box functions . The efficiency is achieved in a similar fashion to the learning to learn methods : surrogate models ( typically in the form of Gaussian processes ) learn the target function and perform intelligent sampling . This surrogate model can be applied even in the presence of noise ; however , as with most regression methods , it is very sensitive to outlier data . This can result in erroneous predictions and , in the case of BO , biased and inefficient exploration . In this work , we present a GP model that is robust to outliers which uses a Student-t likelihood to segregate outliers and robustly conduct Bayesian optimization . We present numerical results evaluating the proposed method in both artificial functions and real problems .
We consider the setting of linear regression in high dimension . We focus on the problem of constructing adaptive and honest confidence sets for the sparse parameter \theta , i . e . we want to construct a confidence set for theta that contains theta with high probability , and that is as small as possible . The l_0 diameter of a such confidence set should depend on the sparsity S of \theta - the larger S , the wider the confidence set . However , in practice , S is unknown . This paper focuses on constructing a confidence set for \theta which contains \theta with high probability , whose diameter is adaptive to the unknown sparsity S , and which is implementable in practice .
Interfaces that support multi-lingual content can reach a broader community . We wish to extend the reach of CITIDEL , a digital library for computing education materials , to support multiple languages . By doing so , we hope that it will increase the number of users , and in turn the number of resources . This paper discusses three approaches to translation ( automated translation , developer-based , and community-based ) , and a brief evaluation of these approaches . It proposes a design for an online community translation center where volunteers help translate interface components and educational materials available in CITIDEL .
Discrete-time hidden Markov models are a broadly useful class of latent-variable models with applications in areas such as speech recognition , bioinformatics , and climate data analysis . It is common in practice to introduce temporal non-homogeneity into such models by making the transition probabilities dependent on time-varying exogenous input variables via a multinomial logistic parametrization . We extend such models to introduce additional non-homogeneity into the emission distribution using a generalized linear model ( GLM ) , with data augmentation for sampling-based inference . However , the presence of the logistic function in the state transition model significantly complicates parameter inference for the overall model , particularly in a Bayesian context . To address this we extend the recently-proposed Polya-Gamma data augmentation approach to handle non-homogeneous hidden Markov models ( NHMMs ) , allowing the development of an efficient Markov chain Monte Carlo ( MCMC ) sampling scheme . We apply our model and inference scheme to 00 years of daily rainfall in India , leading to a number of insights into rainfall-related phenomena in the region . Our proposed approach allows for fully Bayesian analysis of relatively complex NHMMs on a scale that was not possible with previous methods . Software implementing the methods described in the paper is available via the R package NHMM .
We present an efficient feature selection method that can find all multiplicative combinations of continuous features that are statistically significantly associated with the class variable , while rigorously correcting for multiple testing . The key to overcome the combinatorial explosion in the number of candidates is to derive a lower bound on the $p$-value for each feature combination , which enables us to massively prune combinations that can never be significant and gain more statistical power . While this problem has been addressed for binary features in the past , we here present the first solution for continuous features . In our experiments , our novel approach detects true feature combinations with higher precision and recall than competing methods that require a prior binarization of the data .
We consider the problem of online nonparametric regression with arbitrary deterministic sequences . Using ideas from the chaining technique , we design an algorithm that achieves a Dudley-type regret bound similar to the one obtained in a non-constructive fashion by Rakhlin and Sridharan ( 0000 ) . Our regret bound is expressed in terms of the metric entropy in the sup norm , which yields optimal guarantees when the metric and sequential entropies are of the same order of magnitude . In particular our algorithm is the first one that achieves optimal rates for online regression over H{\ " o}lder balls . In addition we show for this example how to adapt our chaining algorithm to get a reasonable computational efficiency with similar regret guarantees ( up to a log factor ) .
Looking into the growth of information in the web it is a very tedious process of getting the exact information the user is looking for . Many search engines generate user profile related data listing . This paper involves one such process where the rating is given to the link that the user is clicking on . Rather than avoiding the uninterested links both interested links and the uninterested links are listed . But sorted according to the weightings given to each link by the number of visit made by the particular user and the amount of time spent on the particular link .
Recent literature on deep neural networks for tagging of highly energetic jets resulting from top quark decays has focused on image based techniques or multivariate approaches using high-level jet substructure variables . Here , a sequential approach to this task is taken by using an ordered sequence of jet constituents as training inputs . Unlike the majority of previous approaches , this strategy does not result in a loss of information during pixelisation or the calculation of high level features . The jet classification method achieves a background rejection of 00 at a 00% efficiency operating point for reconstruction level jets with transverse momentum range of 000 to 0000 GeV and is insensitive to multiple proton-proton interactions at the levels expected throughout Run 0 of the LHC .
This paper is Part II of a two-paper set which develops a finest-grain per-molecule timing treatment of molecular communication . We first consider a simple one-molecule timing channel with a molecule launch deadline , similar to but different from previous work ( " Bits Through Queues " ) where the constraint was mean launch time . We also derive a number of results related to the {\em ordering entropy} , a key quantity which undergirds the capacity bounds for the molecular timing channel , both with and without token data payloads . We then conclude with an upper bound on molecular timing-channel capacity .
The paper builds upon a recent approach to find the approximate bounds of a real function using Polynomial Chaos expansions . Given a function of random variables with compact support probability distributions , the intuition is to quantify the uncertainty in the response using Polynomial Chaos expansion and discard all the information provided about the randomness of the output and extract only the bounds of its compact support . To solve for the bounding range of polynomials , we transform the Polynomial Chaos expansion in the Bernstein form , and use the range enclosure property of Bernstein polynomials to find the minimum and maximum value of the response . This procedure is used to propagate Dempster-Shafer structures on closed intervals through nonlinear functions and it is applied on an algebraic challenge problem .
We consider how to generate chemical reaction networks ( CRNs ) from functional specifications . We propose a two-stage approach that combines synthesis by satisfiability modulo theories and Markov chain Monte Carlo based optimisation . First , we identify candidate CRNs that have the possibility to produce correct computations for a given finite set of inputs . We then optimise the reaction rates of each CRN using a combination of stochastic search techniques applied to the chemical master equation , simultaneously improving the of correct behaviour and ruling out spurious solutions . In addition , we use techniques from continuous time Markov chain theory to study the expected termination time for each CRN . We illustrate our approach by identifying CRNs for majority decision-making and division computation , which includes the identification of both known and unknown networks .
We introduce a class of dimension reduction estimators based on an ensemble of the minimum average variance estimates of functions that characterize the central subspace , such as the characteristic functions , the Box--Cox transformations and wavelet basis . The ensemble estimators exhaustively estimate the central subspace without imposing restrictive conditions on the predictors , and have the same convergence rate as the minimum average variance estimates . They are flexible and easy to implement , and allow repeated use of the available sample , which enhances accuracy . They are applicable to both univariate and multivariate responses in a unified form . We establish the consistency and convergence rate of these estimators , and the consistency of a cross validation criterion for order determination . We compare the ensemble estimators with other estimators in a wide variety of models , and establish their competent performance .
We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules , based on nuclear charges and atomic positions only . The problem of solving the molecular Schr\ " odinger equation is mapped onto a non-linear statistical regression problem of reduced complexity . Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory . Cross-validation over more than seven thousand small organic molecules yields a mean absolute error of ~00 kcal/mol . Applicability is demonstrated for the prediction of molecular atomization potential energy curves .
The paper presents a solution for the Petri-Net to Statecharts case study of the Transformation Tool Contest 0000 , using EMF-IncQuery and Xtend for implementing the model transformation .
This paper proposes a new Bayesian multiple change-point model which is based on the hidden Markov approach . The Dirichlet process hidden Markov model does not require the specification of the number of change-points a priori . Hence our model is robust to model specification in contrast to the fully parametric Bayesian model . We propose a general Markov chain Monte Carlo algorithm which only needs to sample the states around change-points . Simulations for a normal mean-shift model with known and unknown variance demonstrate advantages of our approach . Two applications , namely the coal-mining disaster data and the real United States Gross Domestic Product growth , are provided . We detect a single change-point for both the disaster data and US GDP growth . All the change-point locations and posterior inferences of the two applications are in line with existing methods .
Flow cytometry is often used to characterize the malignant cells in leukemia and lymphoma patients , traced to the level of the individual cell . Typically , flow cytometric data analysis is performed through a series of 0-dimensional projections onto the axes of the data set . Through the years , clinicians have determined combinations of different fluorescent markers which generate relatively known expression patterns for specific subtypes of leukemia and lymphoma -- cancers of the hematopoietic system . By only viewing a series of 0-dimensional projections , the high-dimensional nature of the data is rarely exploited . In this paper we present a means of determining a low-dimensional projection which maintains the high-dimensional relationships ( i . e . information ) between differing oncological data sets . By using machine learning techniques , we allow clinicians to visualize data in a low dimension defined by a linear combination of all of the available markers , rather than just 0 at a time . This provides an aid in diagnosing similar forms of cancer , as well as a means for variable selection in exploratory flow cytometric research . We refer to our method as Information Preserving Component Analysis ( IPCA ) .
In this paper , we investigate the testing problem that the spectral density matrices of several , not necessarily independent , stationary processes are equal . Based on an $L_0$-type test statistic , we propose a new nonparametric approach , where the critical values of the tests are calculated with the help of randomization methods . We analyze asymptotic exactness and consistency of these randomization tests and show in simulation studies that the new procedures posses very good size and power characteristics .
We consider the recovery of regression coefficients , denoted by $\boldsymbol{\beta}_0$ , for a single index model ( SIM ) relating a binary outcome $Y$ to a set of possibly high dimensional covariates $\boldsymbol{X}$ , based on a large but ' unlabeled ' dataset $\mathcal{U}$ , with $Y$ never observed . On $\mathcal{U}$ , we fully observe $\boldsymbol{X}$ and additionally , a surrogate $S$ which , while not being strongly predictive of $Y$ throughout the entirety of its support , can forecast it with high accuracy when it assumes extreme values . Such datasets arise naturally in modern studies involving large databases such as electronic medical records ( EMR ) where $Y$ , unlike $ ( \boldsymbol{X} , S ) $ , is difficult and/or expensive to obtain . In EMR studies , an example of $Y$ and $S$ would be the true disease phenotype and the count of the associated diagnostic codes respectively . Assuming another SIM for $S$ given $\boldsymbol{X}$ , we show that under sparsity assumptions , we can recover $\boldsymbol{\beta}_0$ proportionally by simply fitting a least squares LASSO estimator to the subset of the observed data on $ ( \boldsymbol{X} , S ) $ restricted to the extreme sets of $S$ , with $Y$ imputed using the surrogacy of $S$ . We obtain sharp finite sample performance bounds for our estimator , including deterministic deviation bounds and probabilistic guarantees . We demonstrate the effectiveness of our approach through multiple simulation studies , as well as by application to real data from an EMR study conducted at the Partners HealthCare Systems .
In this work , we present the Grounded Recurrent Neural Network ( GRNN ) , a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state ( we call this process " grounding " ) . The approach is particularly well-suited for extracting large numbers of concepts from text . We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text . Using a publicly available dataset derived from Intensive Care Units , we learn to label a patient ' s diagnoses and procedures from their discharge summary . Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines .
The increasing number of robots in home environments leads to an emerging coexistence between humans and robots . Robots undertake common tasks and support the residents in their everyday life . People appreciate the presence of robots in their environment as long as they keep the control over them . One important aspect is the control of a robot ' s workspace . Therefore , we introduce virtual borders to precisely and flexibly define the workspace of mobile robots . First , we propose a novel framework that allows a person to interactively restrict a mobile robot ' s workspace . To show the validity of this framework , a concrete implementation based on visual markers is implemented . Afterwards , the mobile robot is capable of performing its tasks while respecting the new virtual borders . The approach is accurate , flexible and less time consuming than explicit robot programming . Hence , even non-experts are able to teach virtual borders to their robots which is especially interesting in domains like vacuuming or service robots in home environments .
Given a document D in the form of an unordered node-labeled tree , we study the expressiveness on D of various basic fragments of XPath , the core navigational language on XML documents . Working from the perspective of these languages as fragments of Tarski ' s relation algebra , we give characterizations , in terms of the structure of D , for when a binary relation on its nodes is definable by an expression in these algebras . Since each pair of nodes in such a relation represents a unique path in D , our results therefore capture the sets of paths in D definable in each of the fragments . We refer to this perspective on language semantics as the " global view . " In contrast with this global view , there is also a " local view " where one is interested in the nodes to which one can navigate starting from a particular node in the document . In this view , we characterize when a set of nodes in D can be defined as the result of applying an expression to a given node of D . All these definability results , both in the global and the local view , are obtained by using a robust two-step methodology , which consists of first characterizing when two nodes cannot be distinguished by an expression in the respective fragments of XPath , and then bootstrapping these characterizations to the desired results .
Biomarker data is often subject to limits of quantification and/or limits of detection . Statistically , this corresponds to left- or interval-censoring . To be able to associate a censored time-to-event endpoint to a biomarker covariate , the R package SurvRegCensCov provides software for Weibull regression for a right-censored endpoint , one interval-censored , and an arbitrary number of non-censored covariates . Furthermore , the package provides functions to estimate canonical parameters from censored samples based on several distributional assumptions , and a function to switch between different parametrizations used in R for Weibull regression . We illustrate the new software by applying it to assess Prentice ' s criteria for surrogacy in data simulated from a randomized clinical registration trial .
We propose a functional version of the Hodrick-Prescott filter for functional data which take values in an infinite dimensional separable Hilbert space . We further characterize the associated optimal smoothing parameter when the associated linear operator is compact and the underlying distribution of the data is Gaussian .
Recently Convolutional Neural Networks ( CNNs ) have been shown to achieve state-of-the-art performance on various classification tasks . In this paper , we present for the first time a place recognition technique based on CNN models , by combining the powerful features learnt by CNNs with a spatial and sequential filter . Applying the system to a 00 km benchmark place recognition dataset we achieve a 00% increase in recall at 000% precision , significantly outperforming all previous state of the art techniques . We also conduct a comprehensive performance comparison of the utility of features from all 00 layers for place recognition , both for the benchmark dataset and for a second dataset with more significant viewpoint changes .
This paper shows the complementary roles of mathematical and engineering points of view when dealing with truss analysis problems involving systems of linear equations and inequalities . After the compatibility condition and the mathematical structure of the general solution of a system of linear equations is discussed , the truss analysis problem is used to illustrate its mathematical and engineering multiple aspects , including an analysis of the compatibility conditions and a physical interpretation of the general solution , and the generators of the resulting affine space . Next , the compatibility and the mathematical structure of the general solution of linear systems of inequalities are analyzed and the truss analysis problem revisited adding some inequality constraints , and discussing how they affect the resulting general solution and many other aspects of it . Finally , some conclusions are drawn .
Timing and power consumption play an important role in the design of embedded systems . Furthermore , both properties are directly related to the safety requirements of many embedded systems . With regard to availability requirements , power considerations are of uttermost importance for battery operated systems . Validation of timing and power requires observability of these properties . In many cases this is difficult , because the observability is either not possible or requires big extra effort in the system validation process . In this paper , we present a measurement-based approach for the joint timing and power analysis of Synchronous Dataflow ( SDF ) applications running on a shared memory multiprocessor systems-on-chip ( MPSoC ) architecture . As a proof-of-concept , we implement an MPSoC system with configurable power and timing measurement interfaces inside a Field Programmable Gate Array ( FPGA ) . Our experiments demonstrate the viability of our approach being able of accurately analyzing different mappings of image processing applications ( Sobel filter and JPEG encoder ) on an FPGA-based MPSoC implementation .
Classifiers for the semi-supervised setting often combine strong supervised models with additional learning objectives to make use of unlabeled data . This results in powerful though very complex models that are hard to train and that demand additional labels for optimal parameter tuning , which are often not given when labeled data is very sparse . We here study a minimalistic multi-layer generative neural network for semi-supervised learning in a form and setting as similar to standard discriminative networks as possible . Based on normalized Poisson mixtures , we derive compact and local learning and neural activation rules . Learning and inference in the network can be scaled using standard deep learning tools for parallelized GPU implementation . With the single objective of likelihood optimization , both labeled and unlabeled data are naturally incorporated into learning . Empirical evaluations on standard benchmarks show , that for datasets with few labels the derived minimalistic network improves on all classical deep learning approaches and is competitive with their recent variants without the need of additional labels for parameter tuning . Furthermore , we find that the studied network is the best performing monolithic ( `non-hybrid ' ) system for few labels , and that it can be applied in the limit of very few labels , where no other system has been reported to operate so far .
In this paper we address cardinality estimation problem which is an important subproblem in query optimization . Query optimization is a part of every relational DBMS responsible for finding the best way of the execution for the given query . These ways are called plans . The execution time of different plans may differ by several orders , so query optimizer has a great influence on the whole DBMS performance . We consider cost-based query optimization approach as the most popular one . It was observed that cost-based optimization quality depends much on cardinality estimation quality . Cardinality of the plan node is the number of tuples returned by it . In the paper we propose a novel cardinality estimation approach with the use of machine learning methods . The main point of the approach is using query execution statistics of the previously executed queries to improve cardinality estimations . We called this approach adaptive cardinality estimation to reflect this point . The approach is general , flexible , and easy to implement . The experimental evaluation shows that this approach significantly increases the quality of cardinality estimation , and therefore increases the DBMS performance for some queries by several times or even by several dozens of times .
Lu and Boutilier proposed a novel approach based on " minimax regret " to use classical score based voting rules in the setting where preferences can be any partial ( instead of complete ) orders over the set of alternatives . We show here that such an approach is vulnerable to a new kind of manipulation which was not present in the classical ( where preferences are complete orders ) world of voting . We call this attack " manipulative elicitation . " More specifically , it may be possible to ( partially ) elicit the preferences of the agents in a way that makes some distinguished alternative win the election who may not be a winner if we elicit every preference completely . More alarmingly , we show that the related computational task is polynomial time solvable for a large class of voting rules which includes all scoring rules , maximin , Copeland$^\alpha$ for every $\alpha\in[0 , 0]$ , simplified Bucklin voting rules , etc . We then show that introducing a parameter per pair of alternatives which specifies the minimum number of partial preferences where this pair of alternatives must be comparable makes the related computational task of manipulative elicitation \NPC for all common voting rules including a class of scoring rules which includes the plurality , $k$-approval , $k$-veto , veto , and Borda voting rules , maximin , Copeland$^\alpha$ for every $\alpha\in[0 , 0]$ , and simplified Bucklin voting rules . Hence , in this work , we discover a fundamental vulnerability in using minimax regret based approach in partial preferential setting and propose a novel way to tackle it .
We describe a content based video retrieval ( CBVR ) software system for identifying specific locations of a human action within a full length film , and retrieving similar video shots from a query . For this , we introduce the concept of a trajectory point cloud for classifying unique actions , encoded in a spatio-temporal covariant eigenspace , where each point is characterized by its spatial location , local Frenet-Serret vector basis , time averaged curvature and torsion and the mean osculating hyperplane . Since each action can be distinguished by their unique trajectories within this space , the trajectory point cloud is used to define an adaptive distance metric for classifying queries against stored actions . Depending upon the distance to other trajectories , the distance metric uses either large scale structure of the trajectory point cloud , such as the mean distance between cloud centroids or the difference in hyperplane orientation , or small structure such as the time averaged curvature and torsion , to classify individual points in a fuzzy-KNN . Our system can function in real-time and has an accuracy greater than 00% for multiple action recognition within video repositories . We demonstrate the use of our CBVR system in two situations : by locating specific frame positions of trained actions in two full featured films , and video shot retrieval from a database with a web search application .
In human-computer conversation systems , the context of a user-issued utterance is particularly important because it provides useful background information of the conversation . However , it is unwise to track all previous utterances in the current session as not all of them are equally important . In this paper , we address the problem of session segmentation . We propose an embedding-enhanced TextTiling approach , inspired by the observation that conversation utterances are highly noisy , and that word embeddings provide a robust way of capturing semantics . Experimental results show that our approach achieves better performance than the TextTiling , MMD approaches .
In this paper , an attempt has been made to analyze the Activeness of an Object Oriented Component Library and develop a special type of software metric called Component Activeness Quotient which is defined as the degree of readiness of an OOCL . The advantages of the CAQ include a possible comparison between various OOCLs leading to selection of the best OOCL for use during the development task , and Stability of the software can be gauged as indicated by the value of the CAQ . The disadvantage of the CAQ is that it may have some error because of its subjective and random nature . The paper also tries to improvise the calculation of the Activeness Quotient . The extreme case of a software organization having an RQ greater than 0 and MQ equal to 0 was not handled by the method of taking an average of RQ and MQ to calculate the AQ . The improvisation is that the AQ must be equal to a product of MQ and RQ and this is mentioned in the Appendix .
Creative tasks such as ideation or question proposal are powerful applications of crowdsourcing , yet the quantity of workers available for addressing practical problems is often insufficient . To enable scalable crowdsourcing thus requires gaining all possible efficiency and information from available workers . One option for text-focused tasks is to allow assistive technology , such as an autocompletion user interface ( AUI ) , to help workers input text responses . But support for the efficacy of AUIs is mixed . Here we designed and conducted a randomized experiment where workers were asked to provide short text responses to given questions . Our experimental goal was to determine if an AUI helps workers respond more quickly and with improved consistency by mitigating typos and misspellings . Surprisingly , we found that neither occurred : workers assigned to the AUI treatment were slower than those assigned to the non-AUI control and their responses were more diverse , not less , than those of the control . Both the lexical and semantic diversities of responses were higher , with the latter measured using word0vec . A crowdsourcer interested in worker speed may want to avoid using an AUI , but using an AUI to boost response diversity may be valuable to crowdsourcers interested in receiving as much novel information from workers as possible .
Existing compact routing schemes , e . g . , Thorup and Zwick [SPAA 0000] and Chechik [PODC 0000] , often have no means to tolerate failures , once the system has been setup and started . This paper presents , to our knowledge , the first self-healing compact routing scheme . Besides , our schemes are developed for low memory nodes , i . e . , nodes need only $O ( \log^0 n ) $ memory , and are thus , compact schemes . We introduce two algorithms of independent interest : The first is CompactFT , a novel compact version ( using only $O ( \log n ) $ local memory ) of the self-healing algorithm Forgiving Tree of Hayes et al . [PODC 0000] . The second algorithm ( CompactFTZ ) combines CompactFT with Thorup-Zwick ' s tree-based compact routing scheme [SPAA 0000] to produce a fully compact self-healing routing scheme . In the self-healing model , the adversary deletes nodes one at a time with the affected nodes self-healing locally by adding few edges . CompactFT recovers from each attack in only $O ( 0 ) $ time and $\Delta$ messages , with only +0 degree increase and $O ( log \Delta ) $ graph diameter increase , over any sequence of deletions ( $\Delta$ is the initial maximum degree ) . Additionally , CompactFTZ guarantees delivery of a packet sent from sender s as long as the receiver t has not been deleted , with only an additional $O ( y \log \Delta ) $ latency , where $y$ is the number of nodes that have been deleted on the path between $s$ and $t$ . If $t$ has been deleted , $s$ gets informed and the packet removed from the network .
The world had witnessed several generations of the Internet . Starting with the Fixed Internet , then the Mobile Internet , scientists now focus on many types of research related to the " Thing " Internet ( or Internet of Things ) . The question is " what is the next Internet generation after the Thing Internet ? " This paper envisions about the Tactile Internet which could be the next Internet generation in the near future . The paper will introduce what is the tactile internet , why it could be the next future Internet , as well as the impact and its application in the future society . Furthermore , some challenges and the requirements are presented to guide further research in this near future field .
Latent space models are effective tools for statistical modeling and exploration of network data . These models can effectively model real world network characteristics such as degree heterogeneity , transitivity , homophily , etc . Due to their close connection to generalized linear models , it is also natural to incorporate covariate information in them . The current paper presents two universal fitting algorithms for networks with edge covariates : one based on nuclear norm penalization and the other based on projected gradient descent . Both algorithms are motivated by maximizing likelihood for a special class of inner-product models while working simultaneously for a wide range of different latent space models , such as distance models , which allow latent vectors to affect edge formation in flexible ways . These fitting methods , especially the one based on projected gradient descent , are fast and scalable to large networks . We obtain their rates of convergence for both inner-product models and beyond . The effectiveness of the modeling approach and fitting algorithms is demonstrated on five real world network datasets for different statistical tasks , including community detection with and without edge covariates , and network assisted learning .
We consider an infinite collection of agents who make decisions , sequentially , about an unknown underlying binary state of the world . Each agent , prior to making a decision , receives an independent private signal whose distribution depends on the state of the world . Moreover , each agent also observes the decisions of its last K immediate predecessors . We study conditions under which the agent decisions converge to the correct value of the underlying state . We focus on the case where the private signals have bounded information content and investigate whether learning is possible , that is , whether there exist decision rules for the different agents that result in the convergence of their sequence of individual decisions to the correct state of the world . We first consider learning in the almost sure sense and show that it is impossible , for any value of K . We then explore the possibility of convergence in probability of the decisions to the correct state . Here , a distinction arises : if K equals 0 , learning in probability is impossible under any decision rule , while for K greater or equal to 0 , we design a decision rule that achieves it . We finally consider a new model , involving forward looking strategic agents , each of which maximizes the discounted sum ( over all agents ) of the probabilities of a correct decision . ( The case , studied in previous literature , of myopic agents who maximize the probability of their own decision being correct is an extreme special case . ) We show that for any value of K , for any equilibrium of the associated Bayesian game , and under the assumption that each private signal has bounded information content , learning in probability fails to obtain .
Building on the notion of a particle physics detector as a camera and the collimated streams of high energy particles , or jets , it measures as an image , we investigate the potential of machine learning techniques based on deep learning architectures to identify highly boosted W bosons . Modern deep learning algorithms trained on jet images can out-perform standard physically-motivated feature driven approaches to jet tagging . We develop techniques for visualizing how these features are learned by the network and what additional information is used to improve performance . This interplay between physically-motivated feature driven tools and supervised learning algorithms is general and can be used to significantly increase the sensitivity to discover new particles and new forces , and gain a deeper understanding of the physics within jets .
We present a fast variational Bayesian algorithm for performing non-negative matrix factorisation and tri-factorisation . We show that our approach achieves faster convergence per iteration and timestep ( wall-clock ) than Gibbs sampling and non-probabilistic approaches , and do not require additional samples to estimate the posterior . We show that in particular for matrix tri-factorisation convergence is difficult , but our variational Bayesian approach offers a fast solution , allowing the tri-factorisation approach to be used more effectively .
This year , the Agile Manifesto completes seventeen years and , throughout the world , companies and researchers seek to understand their adoption stage , as well as the benefits , barriers , and limitations of agile methods . Although we have some studies and questionnaire data at the global level , we know little about how the Paraguayan software community is adopting agile methods . The present work conducted a research to set up the current stage of adoption , initial concerns and barriers of implementation of agile methods in software development companies in Paraguay . An online survey was sent to representatives of 00 Paraguayan companies . Of these , 0 ( 00% ) companies responded . The concern about adopting more agile methods ( 00 . 00% of respondents ) was the lack of reliability in product quality if developed using agile methods . The main barrier was the lack of experience ( 00 . 00% ) of the companies .
We introduce a simple permutation equivariant layer for deep learning with set structure . This type of layer , obtained by parameter-sharing , has a simple implementation and linear-time complexity in the size of each set . We use deep permutation-invariant networks to perform point-could classification and MNIST-digit summation , where in both cases the output is invariant to permutations of the input . In a semi-supervised setting , where the goal is make predictions for each instance within a set , we demonstrate the usefulness of this type of layer in set-outlier detection as well as semi-supervised learning with clustering side-information .
Barndorff-Nielsen and Cox ( 0000 , p . 000 ) modify an estimative prediction limit to obtain an improved prediction limit with better coverage properties . Kabaila and Syuhada ( 0000 ) present a simulation-based approximation to this improved prediction limit , which avoids the extensive algebraic manipulations required for this modification . We present a modification of an estimative prediction interval , analogous to the Barndorff-Nielsen and Cox modification , to obtain an improved prediction interval with better coverage properties . We also present an analogue , for the prediction interval context , of this simulation-based approximation . The parameter estimator on which the estimative and improved prediction limits and intervals are based is assumed to have the same asymptotic distribution as the ( conditional ) maximum likelihood estimator . The improved prediction limit and interval depend on the asymptotic conditional bias of this estimator . This bias can be very sensitive to very small changes in the estimator . It may require considerable effort to find this bias . We show , however , that the improved prediction limit and interval have asymptotic efficiencies that are functionally independent of this bias . Thus , improved prediction limits and intervals obtained using the Barndorff-Nielsen and Cox type of methodology can conveniently be based on the ( conditional ) maximum likelihood estimator , whose asymptotic conditional bias is given by the formula of Vidoni ( 0000 , p . 000 ) . Also , improved prediction limits and intervals obtained using Kabaila and Syuhada type approximations have asymptotic efficiencies that are independent of the estimator on which these intervals are based .
Choice decisions made by users of online applications can suffer from biases due to the users ' level of engagement . For instance , low engagement users may make random choices with no concern for the quality of items offered . This biased choice data can corrupt estimates of user preferences for items . However , one can correct for these biases if additional behavioral data is utilized . To do this we construct a new choice engagement time model which captures the impact of user engagement on choice decisions and response times associated with these choice decisions . Response times are the behavioral data we choose because they are easily measured by online applications and reveal information about user engagement . To test our model we conduct online polls with subject populations that have different levels of engagement and measure their choice decisions and response times . We have two main empirical findings . First , choice decisions and response times are correlated , with strong preferences having faster response times than weak preferences . Second , low user engagement is manifested through more random choice data and faster response times . Both of these phenomena are captured by our choice engagement time model and we find that this model fits the data better than traditional choice models . Our work has direct implications for online applications . It lets these applications remove the bias of low engagement users when estimating preferences for items . It also allows for the segmentation of users according to their level of engagement , which can be useful for targeted advertising or marketing campaigns .
In a seminal paper Abadie , Diamond , and Hainmueller [0000] ( ADH ) , see also Abadie and Gardeazabal [0000] , Abadie et al . [0000] , develop the synthetic control procedure for estimating the effect of a treatment , in the presence of a single treated unit and a number of control units , with pre-treatment outcomes observed for all units . The method constructs a set of weights such that selected covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units ( the synthetic control ) . The weights are restricted to be nonnegative and sum to one , which is important because it allows the procedure to obtain unique weights even when the number of lagged outcomes is modest relative to the number of control units , a common setting in applications . In the current paper we propose a generalization that allows the weights to be negative , and their sum to differ from one , and that allows for a permanent additive difference between the treated unit and the controls , similar to difference-in-difference procedures . The weights directly minimize the distance between the lagged outcomes for the treated and the control units , using regularization methods to deal with a potentially large number of possible control units .
A frequent matter of debate in Bayesian inversion is the question , which of the two principle point-estimators , the maximum-a-posteriori ( MAP ) or the conditional mean ( CM ) estimate is to be preferred . As the MAP estimate corresponds to the solution given by variational regularization techniques , this is also a constant matter of debate between the two research areas . Following a theoretical argument - the Bayes cost formalism - the CM estimate is classically preferred for being the Bayes estimator for the mean squared error cost while the MAP estimate is classically discredited for being only asymptotically the Bayes estimator for the uniform cost function . In this article we present recent theoretical and computational observations that challenge this point of view , in particular for high-dimensional sparsity-promoting Bayesian inversion . Using Bregman distances , we present new , proper convex Bayes cost functions for which the MAP estimator is the Bayes estimator . We complement this finding by results that correct further common misconceptions about MAP estimates . In total , we aim to rehabilitate MAP estimates in linear inverse problems with log-concave priors as proper Bayes estimators .
Video enhancement plays an important role in various video applications . In this paper , we propose a new intra-and-inter-constraint-based video enhancement approach aiming to 0 ) achieve high intra-frame quality of the entire picture where multiple region-of-interests ( ROIs ) can be adaptively and simultaneously enhanced , and 0 ) guarantee the inter-frame quality consistencies among video frames . We first analyze features from different ROIs and create a piecewise tone mapping curve for the entire frame such that the intra-frame quality of a frame can be enhanced . We further introduce new inter-frame constraints to improve the temporal quality consistency . Experimental results show that the proposed algorithm obviously outperforms the state-of-the-art algorithms .
Central to several objective approaches to Bayesian model selection is the use of training samples ( subsets of the data ) , so as to allow utilization of improper objective priors . The most common prescription for choosing training samples is to choose them to be as small as possible , subject to yielding proper posteriors ; these are called minimal training samples . When data can vary widely in terms of either information content or impact on the improper priors , use of minimal training samples can be inadequate . Important examples include certain cases of discrete data , the presence of censored observations , and certain situations involving linear models and explanatory variables . Such situations require more sophisticated methods of choosing training samples . A variety of such methods are developed in this paper , and successfully applied in challenging situations .
This paper shows that one cannot learn the probability of rare events without imposing further structural assumptions . The event of interest is that of obtaining an outcome outside the coverage of an i . i . d . sample from a discrete distribution . The probability of this event is referred to as the " missing mass " . The impossibility result can then be stated as : the missing mass is not distribution-free PAC-learnable in relative error . The proof is semi-constructive and relies on a coupling argument using a dithered geometric distribution . This result formalizes the folklore that in order to predict rare events , one necessarily needs distributions with " heavy tails " .
In a landmark paper , Papadimitriou and Roughgarden described a polynomial-time algorithm ( " Ellipsoid Against Hope " ) for computing sample correlated equilibria of concisely-represented games . Recently , Stein , Parrilo and Ozdaglar showed that this algorithm can fail to find an exact correlated equilibrium , but can be easily modified to efficiently compute approximate correlated equilibria . Currently , it remains unresolved whether the algorithm can be modified to compute an exact correlated equilibrium . We show that it can , presenting a variant of the Ellipsoid Against Hope algorithm that guarantees the polynomial-time identification of exact correlated equilibrium . Our new algorithm differs from the original primarily in its use of a separation oracle that produces cuts corresponding to pure-strategy profiles . As a result , we no longer face the numerical precision issues encountered by the original approach , and both the resulting algorithm and its analysis are considerably simplified . Our new separation oracle can be understood as a derandomization of Papadimitriou and Roughgarden ' s original separation oracle via the method of conditional probabilities . Also , the equilibria returned by our algorithm are distributions with polynomial-sized supports , which are simpler ( in the sense of being representable in fewer bits ) than the mixtures of product distributions produced previously ; no tractable algorithm has previously been proposed for identifying such equilibria .
Integral expressions for positive-part moments E X_+^p ( p>0 ) of random variables X are presented , in terms of the Fourier-Laplace or Fourier transforms of the distribution of X . A necessary and sufficient condition for the validity of such an expression is given . This study was motivated by extremal problems in probability and statistics , where one needs to evaluate such positive-part moments .
Nonparametric detection of existence of an anomalous structure over a network is investigated . Nodes corresponding to the anomalous structure ( if one exists ) receive samples generated by a distribution q , which is different from a distribution p generating samples for other nodes . If an anomalous structure does not exist , all nodes receive samples generated by p . It is assumed that the distributions p and q are arbitrary and unknown . The goal is to design statistically consistent tests with probability of errors converging to zero as the network size becomes asymptotically large . Kernel-based tests are proposed based on maximum mean discrepancy that measures the distance between mean embeddings of distributions into a reproducing kernel Hilbert space . Detection of an anomalous interval over a line network is first studied . Sufficient conditions on minimum and maximum sizes of candidate anomalous intervals are characterized in order to guarantee the proposed test to be consistent . It is also shown that certain necessary conditions must hold to guarantee any test to be universally consistent . Comparison of sufficient and necessary conditions yields that the proposed test is order-level optimal and nearly optimal respectively in terms of minimum and maximum sizes of candidate anomalous intervals . Generalization of the results to other networks is further developed . Numerical results are provided to demonstrate the performance of the proposed tests .
Let $X$ be a mean zero Gaussian random vector in a separable Hilbert space ${\mathbb H}$ with covariance operator $\Sigma : ={\mathbb E} ( X\otimes X ) . $ Let $\Sigma=\sum_{r\geq 0}\mu_r P_r$ be the spectral decomposition of $\Sigma$ with distinct eigenvalues $\mu_0>\mu_0> \dots$ and the corresponding spectral projectors $P_0 , P_0 , \dots . $ Given a sample $X_0 , \dots , X_n$ of size $n$ of i . i . d . copies of $X , $ the sample covariance operator is defined as $\hat \Sigma_n : = n^{-0}\sum_{j=0}^n X_j\otimes X_j . $ The main goal of principal component analysis is to estimate spectral projectors $P_0 , P_0 , \dots$ by their empirical counterparts $\hat P_0 , \hat P_0 , \dots$ properly defined in terms of spectral decomposition of the sample covariance operator $\hat \Sigma_n . $ The aim of this paper is to study asymptotic distributions of important statistics related to this problem , in particular , of statistic $\|\hat P_r-P_r\|_0^0 , $ where $\|\cdot\|_0^0$ is the squared Hilbert--Schmidt norm . This is done in a " high-complexity " asymptotic framework in which the so called effective rank ${\bf r} ( \Sigma ) : =\frac{{\rm tr} ( \Sigma ) }{\|\Sigma\|_{\infty}}$ ( ${\rm tr} ( \cdot ) $ being the trace and $\|\cdot\|_{\infty}$ being the operator norm ) of the true covariance $\Sigma$ is becoming large simultaneously with the sample size $n , $ but ${\bf r} ( \Sigma ) =o ( n ) $ as $n\to\infty . $ In this setting , we prove that , in the case of one-dimensional spectral projector $P_r , $ the properly centered and normalized statistic $\|\hat P_r-P_r\|_0^0$ with {\it data-dependent} centering and normalization converges in distribution to a Cauchy type limit . The proofs of this and other related results rely on perturbation analysis and Gaussian concentration .
This article describes the results of a case study that applies Neural Network-based Optical Character Recognition ( OCR ) to scanned images of books printed between 0000 and 0000 by training the OCR engine OCRopus [@breuel0000high] on the RIDGES herbal text corpus [@OdebrechtEtAlSubmitted] . Training specific OCR models was possible because the necessary *ground truth* is available as error-corrected diplomatic transcriptions . The OCR results have been evaluated for accuracy against the ground truth of unseen test sets . Character and word accuracies ( percentage of correctly recognized items ) for the resulting machine-readable texts of individual documents range from 00% to more than 00% ( character level ) and from 00% to 00% ( word level ) . This includes the earliest printed books , which were thought to be inaccessible by OCR methods until recently . Furthermore , OCR models trained on one part of the corpus consisting of books with different printing dates and different typesets * ( mixed models ) * have been tested for their predictive power on the books from the other part containing yet other fonts , mostly yielding character accuracies well above 00% . It therefore seems possible to construct generalized models trained on a range of fonts that can be applied to a wide variety of historical printings still giving good results . A moderate postcorrection effort of some pages will then enable the training of individual models with even better accuracies . Using this method , diachronic corpora including early printings can be constructed much faster and cheaper than by manual transcription . The OCR methods reported here open up the possibility of transforming our printed textual cultural heritage into electronic text by largely automatic means , which is a prerequisite for the mass conversion of scanned books .
Let $ ( V , \delta ) $ be a finite metric space , where $V$ is a set of $n$ points and $\delta$ is a distance function defined for these points . Assume that $ ( V , \delta ) $ has a constant doubling dimension $d$ and assume that each point $p\in V$ has a disk of radius $r ( p ) $ around it . The disk graph that corresponds to $V$ and $r ( \cdot ) $ is a \emph{directed} graph $I ( V , E , r ) $ , whose vertices are the points of $V$ and whose edge set includes a directed edge from $p$ to $q$ if $\delta ( p , q ) \leq r ( p ) $ . In \cite{PeRo00} we presented an algorithm for constructing a $ ( 0+\eps ) $-spanner of size $O ( n/\eps^d \log M ) $ , where $M$ is the maximal radius $r ( p ) $ . The current paper presents two results . The first shows that the spanner of \cite{PeRo00} is essentially optimal , i . e . , for metrics of constant doubling dimension it is not possible to guarantee a spanner whose size is independent of $M$ . The second result shows that by slightly relaxing the requirements and allowing a small perturbation of the radius assignment , considerably better spanners can be constructed . In particular , we show that if it is allowed to use edges of the disk graph $I ( V , E , r_{0+\eps} ) $ , where $r_{0+\eps} ( p ) = ( 0+\eps ) \cdot r ( p ) $ for every $p\in V$ , then it is possible to get a $ ( 0+\eps ) $-spanner of size $O ( n/\eps^d ) $ for $I ( V , E , r ) $ . Our algorithm is simple and can be implemented efficiently .
We present a unified framework for the declarative analysis of structured communications . By relying on a ( timed ) concurrent constraint programming language , we show that in addition to the usual operational techniques from process calculi , the analysis of structured communications can elegantly exploit logic-based reasoning techniques . We introduce a declarative interpretation of the language for structured communications proposed by Honda , Vasconcelos , and Kubo . Distinguishing features of our approach are : the possibility of including partial information ( constraints ) in the session model ; the use of explicit time for reasoning about session duration and expiration ; a tight correspondence with logic , which formally relates session execution and linear-time temporal logic formulas .
An information-geometric approach to sensor management is introduced that is based on following geodesic curves in a manifold of possible sensor configurations . This perspective arises by observing that , given a parameter estimation problem to be addressed through management of sensor assets , any particular sensor configuration corresponds to a Riemannian metric on the parameter manifold . With this perspective , managing sensors involves navigation on the space of all Riemannian metrics on the parameter manifold , which is itself a Riemannian manifold . Existing work assumes the metric on the parameter manifold is one that , in statistical terms , corresponds to a Jeffreys prior on the parameter to be estimated . It is observed that informative priors , as arise in sensor management , can also be accommodated . Given an initial sensor configuration , the trajectory along which to move in sensor configuration space to gather most information is seen to be locally defined by the geodesic structure of this manifold . Further , divergences based on Fisher and Shannon information lead to the same Riemannian metric and geodesics .
Nonlinear expectation , including sublinear expectation as its special case , is a new and original framework of probability theory and has potential applications in some scientific fields , especially in finance risk measure and management . Under the nonlinear expectation framework , however , the related statistical models and statistical inferences have not yet been well established . The goal of this paper is to construct the sublinear expectation regression and investigate its statistical inference . First , a sublinear expectation linear regression is defined and its identifiability is given . Then , based on the representation theorem of sublinear expectation and the newly defined model , several parameter estimations and model predictions are suggested , the asymptotic normality of estimations and the mini-max property of predictions are obtained . Furthermore , new methods are developed to realize variable selection for high-dimensional model . Finally , simulation studies and a real-life example are carried out to illustrate the new models and methodologies . All notions and methodologies developed are essentially different from classical ones and can be thought of as a foundation for general nonlinear expectation statistics .
Principal component analysis continues to be a powerful tool in dimension reduction of high dimensional data . We assume a variance-diverging model and use the high-dimension , low-sample-size asymptotics to show that even though the principal component directions are not consistent , the sample and prediction principal component scores can be useful in revealing the population structure . We further show that these scores are biased , and the bias is asymptotically decomposed into rotation and scaling parts . We propose methods of bias-adjustment that are shown to be consistent and work well in the finite but high dimensional situations with small sample sizes . The potential advantage of bias-adjustment is demonstrated in a classification setting .
Discussion of " Is Bayes Posterior just Quick and Dirty Confidence ? " by D . A . S . Fraser [arXiv : 0000 . 0000] .
In this paper , we consider the finite tiling problem which was proved undecidable in the Euclidean plane by Jarkko Kari in 0000 . Here , we prove that the same problem for the hyperbolic plane is also undecidable .
World Wide Web consists of more than 00 billion pages online . It is highly dynamic i . e . the web continuously introduces new capabilities and attracts many people . Due to this explosion in size , the effective information retrieval system or search engine can be used to access the information . In this paper we have proposed the EPOW ( Effective Performance of WebCrawler ) architecture . It is a software agent whose main objective is to minimize the overload of a user locating needed information . We have designed the web crawler by considering the parallelization policy . Since our EPOW crawler has a highly optimized system it can download a large number of pages per second while being robust against crashes . We have also proposed to use the data structure concepts for implementation of scheduler & circular Queue to improve the performance of our web crawler .
Error backpropagation is a highly effective mechanism for learning high-quality hierarchical features in deep networks . Updating the features or weights in one layer , however , requires waiting for the propagation of error signals from higher layers . Learning using delayed and non-local errors makes it hard to reconcile backpropagation with the learning mechanisms observed in biological neural networks as it requires the neurons to maintain a memory of the input long enough until the higher-layer errors arrive . In this paper , we propose an alternative learning mechanism where errors are generated locally in each layer using fixed , random auxiliary classifiers . Lower layers could thus be trained independently of higher layers and training could either proceed layer by layer , or simultaneously in all layers using local error information . We address biological plausibility concerns such as weight symmetry requirements and show that the proposed learning mechanism based on fixed , broad , and random tuning of each neuron to the classification categories outperforms the biologically-motivated feedback alignment learning technique on the MNIST , CIFAR00 , and SVHN datasets , approaching the performance of standard backpropagation . Our approach highlights a potential biological mechanism for the supervised , or task-dependent , learning of feature hierarchies . In addition , we show that it is well suited for learning deep networks in custom hardware where it can drastically reduce memory traffic and data communication overheads .
We propose an easy-to-use methodology to allocate one of the groups which have been previously built from a complete learning data base , to new individuals . The learning data base contains continuous and categorical variables for each individual . The groups ( clusters ) are built by using only the continuous variables and described with the help of the categorical ones . For the new individuals , only the categorical variables are available , and it is necessary to define a model which computes the probabilities to belong to each of the clusters , by using only the categorical variables . Then this model provides a decision rule to assign the new individuals and gives an efficient tool to decision-makers . This tool is shown to be very efficient for customers allocation in consumer clusters for marketing purposes , for example .
This article shows a correspondence between abstract interpretation of imperative programs and the refinement calculus : in the refinement calculus , an abstract interpretation of a program is a specification which is a function . This correspondence can be used to guide the design of mechanically verified static analyses , keeping the correctness proof well separated from the heuristic parts of the algorithms .
We investigate the effect and usefulness of spontaneity in speech ( i . e . whether a given speech data is spontaneous or not ) in the context of emotion recognition . We hypothesize that emotional content in speech is interrelated with its spontaneity , and thus propose to use spontaneity classification as an auxiliary task to the problem of emotion recognition . We propose two supervised learning settings that utilize spontaneity to improve speech emotion recognition : a hierarchical model that performs spontaneity detection before performing emotion recognition , and a multitask learning model that jointly learns to recognize both spontaneity and emotion . Through various experiments on a benchmark database , we show that by using spontaneity as an additional information , significant improvement ( 0% ) can be achieved over systems that are unaware of spontaneity . We also observe that spontaneity information is highly useful in recognizing positive emotions as the recognition accuracy improves by 00% .
At a superficial level , the idea of maximum likelihood must be prehistoric : early hunters and gatherers may not have used the words ``method of maximum likelihood ' ' to describe their choice of where and how to hunt and gather , but it is hard to believe they would have been surprised if their method had been described in those terms . It seems a simple , even unassailable idea : Who would rise to argue in favor of a method of minimum likelihood , or even mediocre likelihood ? And yet the mathematical history of the topic shows this ``simple idea ' ' is really anything but simple . Joseph Louis Lagrange , Daniel Bernoulli , Leonard Euler , Pierre Simon Laplace and Carl Friedrich Gauss are only some of those who explored the topic , not always in ways we would sanction today . In this article , that history is reviewed from back well before Fisher to the time of Lucien Le Cam ' s dissertation . In the process Fisher ' s unpublished 0000 characterization of conditions for the consistency and efficiency of maximum likelihood estimates is presented , and the mathematical basis of his three proofs discussed . In particular , Fisher ' s derivation of the information inequality is seen to be derived from his work on the analysis of variance , and his later approach via estimating functions was derived from Euler ' s Relation for homogeneous functions . The reaction to Fisher ' s work is reviewed , and some lessons drawn .
In Operation Research , practical evaluation is essential to validate the efficacy of optimization approaches . This paper promotes the usage of performance profiles as a standard practice to visualize and analyze experimental results . It introduces a Web tool to construct and export performance profiles as SVG or HTML files . In addition , the application relies on a methodology to estimate the benefit of hypothetical solver improvements . Therefore , the tool allows one to employ what-if analysis to screen possible research directions , and identify those having the best potential . The approach is showcased on two Operation Research technologies : Constraint Programming and Mixed Integer Linear Programming .
We present a novel approach for learning an HMM whose outputs are distributed according to a parametric family . This is done by {\em decoupling} the learning task into two steps : first estimating the output parameters , and then estimating the hidden states transition probabilities . The first step is accomplished by fitting a mixture model to the output stationary distribution . Given the parameters of this mixture model , the second step is formulated as the solution of an easily solvable convex quadratic program . We provide an error analysis for the estimated transition probabilities and show they are robust to small perturbations in the estimates of the mixture parameters . Finally , we support our analysis with some encouraging empirical results .
We provide a " shared axiomatization " of natural numbers and hereditarily finite sets built around a polymorphic abstraction of bijective base-0 arithmetics . The " axiomatization " is described as a progressive refinement of Haskell type classes with examples of instances converging to an efficient implementation in terms of arbitrary length integers and bit operations . As an instance , we derive algorithms to perform arithmetic operations efficiently directly with hereditarily finite sets . The self-contained source code of the paper is available at http : //logic . cse . unt . edu/tarau/research/0000/unified . hs .
It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature , such as text or images , may be more efficiently represented with deep hierarchical networks than with shallow ones . Despite the vast empirical evidence supporting this belief , theoretical justifications to date are limited . In particular , they do not account for the locality , sharing and pooling constructs of convolutional networks , the most successful deep learning architecture to date . In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality , sharing and pooling . An equivalence between the networks and hierarchical tensor factorizations is established . We show that a shallow network corresponds to CP ( rank-0 ) decomposition , whereas a deep network corresponds to Hierarchical Tucker decomposition . Using tools from measure theory and matrix algebra , we prove that besides a negligible set , all functions that can be implemented by a deep network of polynomial size , require exponential size in order to be realized ( or even approximated ) by a shallow network . Since log-space computation transforms our networks into SimNets , the result applies directly to a deep learning architecture demonstrating promising empirical performance . The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community .
We focus on the robust principal component analysis ( RPCA ) problem , and review a range of old and new convex formulations for the problem and its variants . We then review dual smoothing and level set techniques in convex optimization , present several novel theoretical results , and apply the techniques on the RPCA problem . In the final sections , we show a range of numerical experiments for simulated and real-world problems .
In this paper are briefly outlined the motivations , mathematical ideas in use , pre-formalization and assumptions , object-as-functor construction , `soft ' types and concept constructions , case study for concepts based on variable domains , extracting a computational background , and examples of evaluations .
Data processing systems impose multiple views on data as it is processed by the system . These views include spreadsheets , databases , matrices , and graphs . The common theme amongst these views is the need to store and operate on data as whole sets instead of as individual data elements . This work describes a common mathematical representation of these data sets ( associative arrays ) that applies across a wide range of applications and technologies . Associative arrays unify and simplify these different approaches for representing and manipulating data into common two-dimensional view of data . Specifically , associative arrays ( 0 ) reduce the effort required to pass data between steps in a data processing system , ( 0 ) allow steps to be interchanged with full confidence that the results will be unchanged , and ( 0 ) make it possible to recognize when steps can be simplified or eliminated . Most database system naturally support associative arrays via their tabular interfaces . The D0M implementation of associative arrays uses this feature to provide a common interface across SQL , NoSQL , and NewSQL databases .
This paper proposes a binarization scheme for vectors of high dimension based on the recent concept of anti-sparse coding , and shows its excellent performance for approximate nearest neighbor search . Unlike other binarization schemes , this framework allows , up to a scaling factor , the explicit reconstruction from the binary representation of the original vector . The paper also shows that random projections which are used in Locality Sensitive Hashing algorithms , are significantly outperformed by regular frames for both synthetic and real data if the number of bits exceeds the vector dimensionality , i . e . , when high precision is required .
The Brier Score is a widely-used criterion to assess the quality of probabilistic predictions of binary events . The expectation value of the Brier Score can be decomposed into the sum of three components called reliability , resolution , and uncertainty which characterize different forecast attributes . Given a dataset of forecast probabilities and corresponding binary verifications , these three components can be estimated empirically . Here , propagation of uncertainty is used to derive expressions that approximate the sampling variances of the estimated components . Variance estimates are provided for both the traditional estimators , as well as for refined estimators that include a bias correction . Applications of the derived variance estimates to artificial data illustrate their validity , and application to a meteorological prediction problem illustrates a possible use case . The observed increase of variance of the bias-corrected estimators is discussed .
Deep CNNs are known to exhibit the following peculiarity : on the one hand they generalize extremely well to a test set , while on the other hand they are extremely sensitive to so-called adversarial perturbations . The extreme sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are learning high level abstractions in the dataset . We are concerned with the following question : How can a deep CNN that does not learn any high level semantics of the dataset manage to generalize so well ? The goal of this article is to measure the tendency of CNNs to learn surface statistical regularities of the dataset . To this end , we use Fourier filtering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical regularities . For the SVHN and CIFAR-00 datasets , we present two Fourier filtered variants : a low frequency variant and a randomly filtered variant . Each of the Fourier filtering schemes is tuned to preserve the recognizability of the objects . Our main finding is that CNNs exhibit a tendency to latch onto the Fourier image statistics of the training dataset , sometimes exhibiting up to a 00% generalization gap across the various test sets . Moreover , we observe that significantly increasing the depth of a network has a very marginal impact on closing the aforementioned generalization gap . Thus we provide quantitative evidence supporting the hypothesis that deep CNNs tend to learn surface statistical regularities in the dataset rather than higher-level abstract concepts .
We present GLASSES : Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search . The majority of global optimisation approaches in use are myopic , in only considering the impact of the next function value ; the non-myopic approaches that do exist are able to consider only a handful of future evaluations . Our novel algorithm , GLASSES , permits the consideration of dozens of evaluations into the future . This is done by approximating the ideal look-ahead loss function , which is expensive to evaluate , by a cheaper alternative in which the future steps of the algorithm are simulated beforehand . An Expectation Propagation algorithm is used to compute the expected value of the loss . We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests .
The emergence of mobile games has caused a paradigm shift in the video-game industry . Game developers now have at their disposal a plethora of information on their players , and thus can take advantage of reliable models that can accurately predict player behavior and scale to huge datasets . Churn prediction , a challenge common to a variety of sectors , is particularly relevant for the mobile game industry , as player retention is crucial for the successful monetization of a game . In this article , we present an approach to predicting game abandon based on survival ensembles . Our method provides accurate predictions on both the level at which each player will leave the game and their accumulated playtime until that moment . Further , it is robust to different data distributions and applicable to a wide range of response variables , while also allowing for efficient parallelization of the algorithm . This makes our model well suited to perform real-time analyses of churners , even for games with millions of daily active users .
A software architecture describes the structure of a computing system by specifying software components and their interactions . Mapping a software architecture to an implementation is a well known challenge . A key element of this mapping is the architecture ' s description of the data and control-flow interactions between components . The characterization of these interactions can be rather abstract or very concrete , providing more or less implementation guidance , programming support , and static verification . In this paper , we explore one point in the design space between abstract and concrete component interaction specifications . We introduce a notion of interaction contract that expresses allowed interactions between components , describing both data and control-flow constraints . This declaration is part of the architecture description , allows generation of extensive programming support , and enables various verifications . We instantiate our approach in an architecture description language for Sense/Compute/Control applications , and describe associated compilation and verification strategies .
Technical debt---design shortcuts taken to optimize for delivery speed---is a critical part of long-term software costs . Consequently , automatically detecting technical debt is a high priority for software practitioners . Software quality tool vendors have responded to this need by positioning their tools to detect and manage technical debt . While these tools bundle a number of rules , it is hard for users to understand which rules identify design issues , as opposed to syntactic quality . This is important , since previous studies have revealed the most significant technical debt is related to design issues . Other research has focused on comparing these tools on open source projects , but these comparisons have not looked at whether the rules were relevant to design . We conducted an empirical study using a structured categorization approach , and manually classify 000 software quality rules from three industry tools---CAST , SonarQube , and NDepend . We found that most of these rules were easily labeled as either not design ( 00% ) or design ( 00% ) . The remainder ( 00% ) resulted in disagreements among the labelers . Our results are a first step in formalizing a definition of a design rule , in order to support automatic detection .
Due to the rapidly growing scale and heterogeneity of wireless networks , the design of distributed cross-layer optimization algorithms have received significant interest from the networking research community . So far , the standard distributed cross-layer approach in the literature is based on first-order Lagrangian dual decomposition and the subgradient method , which suffers a slow convergence rate . In this paper , we make the first known attempt to develop a distributed Newton ' s method , which is second-order and enjoys a quadratic convergence rate . However , due to interference in wireless networks , the Hessian matrix of the cross-layer problem has an non-separable structure . As a result , developing a distributed second-order algorithm is far more challenging than its counterpart for wireline networks . Our main results in this paper are two-fold : i ) For a special network setting where all links mutually interfere , we derive decentralized closed-form expressions to compute the Hessian inverse ; ii ) For general wireless networks where the interference relationships are arbitrary , we propose a distributed iterative matrix splitting scheme for the Hessian inverse . These results successfully lead to a new theoretical framework for cross-layer optimization in wireless networks . More importantly , our work contributes to an exciting second-order paradigm shift in wireless networks optimization theory .
Online Social Networks ( OSNs ) are used by millions of users worldwide . Academically speaking , there is little doubt about the usefulness of demographic studies conducted on OSNs and , hence , methods to label unknown users from small labeled samples are very useful . However , from the general public point of view , this can be a serious privacy concern . Thus , both topics are tackled in this paper : First , a new algorithm to perform user profiling in social networks is described , and its performance is reported and discussed . Secondly , the experiments --conducted on information usually considered sensitive-- reveal that by just publicizing one ' s contacts privacy is at risk and , thus , measures to minimize privacy leaks due to social graph data mining are outlined .
In distributed storage systems that employ erasure coding , the issue of minimizing the total {\it repair bandwidth} required to exactly regenerate a storage node after a failure arises . This repair bandwidth depends on the structure of the storage code and the repair strategies used to restore the lost data . Minimizing it requires that undesired data during a repair align in the smallest possible spaces , using the concept of interference alignment ( IA ) . Here , a points-on-a-lattice representation of the symbol extension IA of Cadambe {\it et al . } provides cues to perfect IA instances which we combine with fundamental properties of Hadamard matrices to construct a new storage code with favorable repair properties . Specifically , we build an explicit $ ( k+0 , k ) $ storage code over $\mathbb{GF} ( 0 ) $ , whose single systematic node failures can be repaired with bandwidth that matches exactly the theoretical minimum . Moreover , the repair of single parity node failures generates at most the same repair bandwidth as any systematic node failure . Our code can tolerate any single node failure and any pair of failures that involves at most one systematic failure .
Performance modeling typically relies on two antithetic methodologies : white box models , which exploit knowledge on system ' s internals and capture its dynamics using analytical approaches , and black box techniques , which infer relations among the input and output variables of a system based on the evidences gathered during an initial training phase . In this paper we investigate a technique , which we name Bootstrapping , which aims at reconciling these two methodologies and at compensating the cons of the one with the pros of the other . We thoroughly analyze the design space of this gray box modeling technique , and identify a number of algorithmic and parametric trade-offs which we evaluate via two realistic case studies , a Key-Value Store and a Total Order Broadcast service .
This note considers the applicability of Gauss-Hermite quadrature and direct numerical quadrature for computation of moment generating function ( mgf ) and the derivatives . A preprocessing using the asymptotic technique is employed while computing the characteristic function ( chf ) using Gauss Hermite quadrature while this is optional for mgf . The mgf of the low and high amplitude regions of a single lognormal variable and the derivatives is examined and attention is drawn to the effect of variance . The problem of inversion of the mgf/chf of a sum of lognormals to obtain the CDF/pdf is considered with special reference to methods related to Post Widder technique , Gaussian quadrature and the Fourier series method . The method based on the complex exponential integral which makes use of the derivative of the cumulant is an alternative . Segmentation of the mgf/chf on the basis of the derivative structure which indicates activity rate is shown to be useful .
This article is based on studies of the existing literature , focusing on the states-of-the-arts on virtual reality ( VR ) and its potential uses in learning . Different platforms have been used to improve the learning effects of VR that offers exciting opportunities in various fields . As more and more students want in a distance , part-time , or want to continue their education , VR has attracted considerable attention in learning , training , and traditional education . VR based learning enables operators to bring together all disciplinary resources in a common playground . The VR base multimedia platform has successfully demonstrated great potential of education and training . In this paper , we will discuss existing systems and their uses and address the technical challenges and future directions .
Multiplication is one of the most important operation in computer arithmetic . Many integer operations such as squaring , division and computing reciprocal require same order of time as multiplication whereas some other operations such as computing GCD and residue operation require at most a factor of $\log n$ time more than multiplication . We propose an integer multiplication algorithm using Nikhilam method of Vedic mathematics which can be used to multiply two binary numbers efficiently .
The Gemini Observatory is planning to implement a Multi Conjugate Adaptive Optics ( MCAO ) System as a facility instrument for the Gemini-South telescope . The system will include 0 Laser Guide Stars , 0 Natural Guide Stars , and 0 Deformable mirrors optically conjugated at different altitudes to achieve near-uniform atmospheric compensation over a 0 arc minute square field of view . The control of such a system will be split into 0 main functions : the control of the opto-mechanical assemblies of the whole system ( including the Laser , the Beam Transfer Optics and the Adaptive Optics bench ) , the control of the Adaptive Optics System itself at a rate of 000FPS and the control of the safety system . The control of the Adaptive Optics System is the most critical in terms of real time performances . The control system will be an EPICS based system . In this paper , we will describe the requirements for the whole MCAO control system , preliminary designs for the control of the opto-mechanical devices and architecture options for the control of the Adaptive Optics system and the safety system .
The objectives of this " perspective " paper are to review some recent advances in sparse feature selection for regression and classification , as well as compressed sensing , and to discuss how these might be used to develop tools to advance personalized cancer therapy . As an illustration of the possibilities , a new algorithm for sparse regression is presented , and is applied to predict the time to tumor recurrence in ovarian cancer . A new algorithm for sparse feature selection in classification problems is presented , and its validation in endometrial cancer is briefly discussed . Some open problems are also presented .
We present several application of simple topological arguments in problems of Kolmogorov complexity . Basically we use the standard fact from topology that the disk is simply connected . It proves to be enough to construct strings with some nontrivial algorithmic properties .
Activity classification was performed using MEMS accelerometer and wireless sensor node for wireless sensor network environment . Three axes MEMS accelerometer measures body ' s acceleration and transmits measured data with the help of sensor node to base station attached to PC . On the PC , real time accelerometer data is processed for movement classifications . In this paper , Rest , walking and running are the classified activities of the person . Both time and frequency analysis was performed to classify running and walking . The classification of rest and movement is done using Signal magnitude area ( SMA ) . The classification accuracy for rest and movement is 000% . For the classification of walk and Run two parameters i . e . SMA and Median frequency were used . The classification accuracy for walk and running was detected as 00 . 00% in the experiments performed by the test persons .
The main objective of this work is to examine possible effects of using freshman student subjects in software engineering experiments . Particularly in this work we report the effectiveness measured as percentage of observed and observable defects of two software testing techniques : Black-box and white-box . Regarding observed defects , both techniques show an effectiveness around of 0% . With respect of observable defects by test cases , black-box testing is slightly more effective ( 00% ) than white-box testing ( 00% ) , although this difference is not significant . We observe a considerable lack of technical skills of subjects for applying both software testing techniques . Due to observed findings , we suggest to employ students with more technical skills for carrying out software engineering experiments . ----- El objetivo de este trabajo se centra en investigar los efectos que conlleva realizar experimentos en ingenier\ ' ia de software ( IS ) empleando como sujetos experimentales a estudiantes de pregrado cursando su primer a\~no de estudios de la carrera en ingenier\ ' ia de software . De manera particular en este trabajo se investiga la efectividad medida en porcentaje de defectos observados y observables de las t\ ' ecnicas de prueba de software funcional ( caja negra ) y estructural ( caja blanca ) . Con respecto a los defectos observados por los sujetos , ambas t\ ' ecnicas obtuvieron una efectividad del 0% . Con respecto a los defectos observables por los casos de prueba , la t\ ' ecnica funcional es ligeramente superior ( 00% ) que la t\ ' ecnica estructural ( 00% ) , aunque esta diferencia no es significativa . Se observa un nivel de inexperiencia considerable en los sujetos para aplicar las t\ ' ecnicas . Dado los hallazgos encontrados , se sugiere emplear sujetos de pregrado con un nivel mayor de experiencia .
This paper proposes stochastic models for the analysis of ocean surface trajectories obtained from freely-drifting satellite-tracked instruments . The proposed time series models are used to summarise large multivariate datasets and infer important physical parameters of inertial oscillations and other ocean processes . Nonstationary time series methods are employed to account for the spatiotemporal variability of each trajectory . Because the datasets are large , we construct computationally efficient methods through the use of frequency-domain modelling and estimation , with the data expressed as complex-valued time series . We detail how practical issues related to sampling and model misspecification may be addressed using semi-parametric techniques for time series , and we demonstrate the effectiveness of our stochastic models through application to both real-world data and to numerical model output .
We present a semi-supervised learning algorithm for learning discrete factor analysis models with arbitrary structure on the latent variables . Our algorithm assumes that every latent variable has an " anchor " , an observed variable with only that latent variable as its parent . Given such anchors , we show that it is possible to consistently recover moments of the latent variables and use these moments to learn complete models . We also introduce a new technique for improving the robustness of method-of-moment algorithms by optimizing over the marginal polytope or its relaxations . We evaluate our algorithm using two real-world tasks , tag prediction on questions from the Stack Overflow website and medical diagnosis in an emergency department .
We propose an image deconvolution algorithm when the data is contaminated by Poisson noise . The image to restore is assumed to be sparsely represented in a dictionary of waveforms such as the wavelet or curvelet transforms . Our key contributions are : First , we handle the Poisson noise properly by using the Anscombe variance stabilizing transform leading to a {\it non-linear} degradation equation with additive Gaussian noise . Second , the deconvolution problem is formulated as the minimization of a convex functional with a data-fidelity term reflecting the noise properties , and a non-smooth sparsity-promoting penalties over the image representation coefficients ( e . g . $\ell_0$-norm ) . Third , a fast iterative backward-forward splitting algorithm is proposed to solve the minimization problem . We derive existence and uniqueness conditions of the solution , and establish convergence of the iterative algorithm . Finally , a GCV-based model selection procedure is proposed to objectively select the regularization parameter . Experimental results are carried out to show the striking benefits gained from taking into account the Poisson statistics of the noise . These results also suggest that using sparse-domain regularization may be tractable in many deconvolution applications with Poisson noise such as astronomy and microscopy .
CD0 T cells are specialized immune cells that play an important role in the regulation of antiviral immune response and the generation of protective immunity . In this paper we investigate the differentiation of memory CD0 T cells in the immune response using a short time course microarray experiment . Structurally , this experiment is similar to many in that it involves measurements taken on independent samples , in one biological group , at a small number of irregularly spaced time points , and exhibiting patterns of temporal nonstationarity . To analyze this CD0 T-cell experiment , we develop a hierarchical state space model so that we can : ( 0 ) detect temporally differentially expressed genes , ( 0 ) identify the direction of successive changes over time , and ( 0 ) assess the magnitude of successive changes over time . We incorporate hidden Markov models into our model to utilize the information embedded in the time series and set up the proposed hierarchical state space model in an empirical Bayes framework to utilize the population information from the large-scale data . Analysis of the CD0 T-cell experiment using the proposed model results in biologically meaningful findings . Temporal patterns involved in the differentiation of memory CD0 T cells are summarized separately and performance of the proposed model is illustrated in a simulation study .
Recently , group recommendations have attracted considerable attention . Rather than recommending items to individual users , group recommenders recommend items to groups of users . In this position paper , we introduce the problem of forming an appropriate group of users to recommend an item when constraints apply to the members of the group . We present a formal model of the problem and an algorithm for its solution . Finally , we identify several directions for future work .
Programs with constraints are hard to debug . In this paper , we describe a general architecture to help develop new debugging tools for constraint programming . The possible tools are fed by a single general-purpose tracer . A tracer-driver is used to adapt the actual content of the trace , according to the needs of the tool . This enables the tools and the tracer to communicate in a client-server scheme . Each tool describes its needs of execution data thanks to event patterns . The tracer driver scrutinizes the execution according to these event patterns and sends only the data that are relevant to the connected tools . Experimental measures show that this approach leads to good performance in the context of constraint logic programming , where a large variety of tools exists and the trace is potentially huge .
This paper focuses on two key problems for audio-visual emotion recognition in the video . One is the audio and visual streams temporal alignment for feature level fusion . The other one is locating and re-weighting the perception attentions in the whole audio-visual stream for better recognition . The Long Short Term Memory Recurrent Neural Network ( LSTM-RNN ) is employed as the main classification architecture . Firstly , soft attention mechanism aligns the audio and visual streams . Secondly , seven emotion embedding vectors , which are corresponding to each classification emotion type , are added to locate the perception attentions . The locating and re-weighting process is also based on the soft attention mechanism . The experiment results on EmotiW0000 dataset and the qualitative analysis show the efficiency of the proposed two techniques .
For the normal linear model variable selection problem , we propose selection criteria based on a fully Bayes formulation with a generalization of Zellner ' s $g$-prior which allows for $p>n$ . A special case of the prior formulation is seen to yield tractable closed forms for marginal densities and Bayes factors which reveal new model evaluation characteristics of potential interest .
Non-stationary time series with non-linear trends are frequently encountered in applications . We consider here the feasibility of accurately forecasting the signals of multiple such time series considering jointly when the number of historic samples is inadequate for accurately forecasting the signal of each considered in isolation . We develop a new forecasting methodology based on Gaussian process regression that is successful in doing so in examples for which the method of generalized least-squares is not . The new method employs a form of deep machine learning .
Data discretization is an important step in the process of machine learning , since it is easier for classifiers to deal with discrete attributes rather than continuous attributes . Over the years , several methods of performing discretization such as Boolean Reasoning , Equal Frequency Binning , Entropy have been proposed , explored , and implemented . In this article , a simple supervised discretization approach is introduced . The prime goal of MIL is to maximize classification accuracy of classifier , minimizing loss of information while discretization of continuous attributes . The performance of the suggested approach is compared with the supervised discretization algorithm Minimum Information Loss ( MIL ) , using the state-of-the-art rule inductive algorithms- J00 ( Java implementation of C0 . 0 classifier ) . The presented approach is , indeed , the modified version of MIL . The empirical results show that the modified approach performs better in several cases in comparison to the original MIL algorithm and Minimum Description Length Principle ( MDLP ) .
This study presents an innovative method for reducing the number of rating scale items without predictability loss . The " area under the re- ceiver operator curve method " ( AUC ROC ) is used to implement in the RatingScaleReduction package posted on CRAN . Several cases have been used to illustrate how the stepwise method has reduced the number of rating scale items ( variables ) .
The two-dimensional discrete wavelet transform has a huge number of applications in image-processing techniques . Until now , several papers compared the performance of such transform on graphics processing units ( GPUs ) . However , all of them only dealt with lifting and convolution computation schemes . In this paper , we show that corresponding horizontal and vertical lifting parts of the lifting scheme can be merged into non-separable lifting units , which halves the number of steps . We also discuss an optimization strategy leading to a reduction in the number of arithmetic operations . The schemes were assessed using the OpenCL and pixel shaders . The proposed non-separable lifting scheme outperforms the existing schemes in many cases , irrespective of its higher complexity .
Selection of initial seeds greatly affects the quality of the clusters and in k-means type algorithms . Most of the seed selection methods result different results in different independent runs . We propose a single , optimal , outlier insensitive seed selection algorithm for k-means type algorithms as extension to k-means++ . The experimental results on synthetic , real and on microarray data sets demonstrated that effectiveness of the new algorithm in producing the clustering results
Mobile sensing applications usually require time-series inputs from sensors . Some applications , such as tracking , can use sensed acceleration and rate of rotation to calculate displacement based on physical system models . Other applications , such as activity recognition , extract manually designed features from sensor inputs for classification . Such applications face two challenges . On one hand , on-device sensor measurements are noisy . For many mobile applications , it is hard to find a distribution that exactly describes the noise in practice . Unfortunately , calculating target quantities based on physical system and noise models is only as accurate as the noise assumptions . Similarly , in classification applications , although manually designed features have proven to be effective , it is not always straightforward to find the most robust features to accommodate diverse sensor noise patterns and user behaviors . To this end , we propose DeepSense , a deep learning framework that directly addresses the aforementioned noise and feature customization challenges in a unified manner . DeepSense integrates convolutional and recurrent neural networks to exploit local interactions among similar mobile sensors , merge local interactions of different sensory modalities into global interactions , and extract temporal relationships to model signal dynamics . DeepSense thus provides a general signal estimation and classification framework that accommodates a wide range of applications . We demonstrate the effectiveness of DeepSense using three representative and challenging tasks : car tracking with motion sensors , heterogeneous human activity recognition , and user identification with biometric motion analysis . DeepSense significantly outperforms the state-of-the-art methods for all three tasks . In addition , DeepSense is feasible to implement on smartphones due to its moderate energy consumption and low latency
This paper studies the landscape of empirical risk of deep neural networks by theoretically analyzing its convergence behavior to the population risk as well as its stationary points and properties . For an $l$-layer linear neural network , we prove its empirical risk uniformly converges to its population risk at the rate of $\mathcal{O} ( r^{0l}\sqrt{d\log ( l ) }/\sqrt{n} ) $ with training sample size of $n$ , the total weight dimension of $d$ and the magnitude bound $r$ of weight of each layer . We then derive the stability and generalization bounds for the empirical risk based on this result . Besides , we establish the uniform convergence of gradient of the empirical risk to its population counterpart . We prove the one-to-one correspondence of the non-degenerate stationary points between the empirical and population risks with convergence guarantees , which describes the landscape of deep neural networks . In addition , we analyze these properties for deep nonlinear neural networks with sigmoid activation functions . We prove similar results for convergence behavior of their empirical risks as well as the gradients and analyze properties of their non-degenerate stationary points . To our best knowledge , this work is the first one theoretically characterizing landscapes of deep learning algorithms . Besides , our results provide the sample complexity of training a good deep neural network . We also provide theoretical understanding on how the neural network depth $l$ , the layer width , the network size $d$ and parameter magnitude determine the neural network landscapes .
We present an integer programming framework to build accurate and interpretable discrete linear classification models . Unlike existing approaches , our framework is designed to provide practitioners with the control and flexibility they need to tailor accurate and interpretable models for a domain of choice . To this end , our framework can produce models that are fully optimized for accuracy , by minimizing the 0--0 classification loss , and that address multiple aspects of interpretability , by incorporating a range of discrete constraints and penalty functions . We use our framework to produce models that are difficult to create with existing methods , such as scoring systems and M-of-N rule tables . In addition , we propose specially designed optimization methods to improve the scalability of our framework through decomposition and data reduction . We show that discrete linear classifiers can attain the training accuracy of any other linear classifier , and provide an Occam ' s Razor type argument as to why the use of small discrete coefficients can provide better generalization . We demonstrate the performance and flexibility of our framework through numerical experiments and a case study in which we construct a highly tailored clinical tool for sleep apnea diagnosis .
This paper discussed some job scheduling algorithms for Hadoop platform , and proposed a jobs scheduling optimization algorithm based on Bayes Classification viewing the shortcoming of those algorithms which are used . The proposed algorithm can be summarized as follows . In the scheduling algorithm based on Bayes Classification , the jobs in job queue will be classified into bad job and good job by Bayes Classification , when JobTracker gets task request , it will select a good job from job queue , and select tasks from good job to allocate JobTracker , then the execution result will feedback to the JobTracker . Therefore the scheduling algorithm based on Bayes Classification influence the job classification via learning the result of feedback with the JobTracker will select the most appropriate job to execute on TaskTracker every time . We need to consider the feature usage of job resource and the influence of TaskTracker resource on task execution , the former of which we call it job feature , for instance , the average usage rate of CPU and average usage rate of memory , the latter node feature , such as the usage rate of CPU and the size of idle physical memory , the two are called feature variables . Results show that it has a significant improvement in execution efficiency and stability of job scheduling .
This document is due to appear as a chapter of the forthcoming Handbook of Approximate Bayesian Computation ( ABC ) edited by S . Sisson , Y . Fan , and M . Beaumont . Since the earliest work on ABC , it has been recognised that using summary statistics is essential to produce useful inference results . This is because ABC suffers from a curse of dimensionality effect , whereby using high dimensional inputs causes large approximation errors in the output . It is therefore crucial to find low dimensional summaries which are informative about the parameter inference or model choice task at hand . This chapter reviews the methods which have been proposed to select such summaries , extending the previous review paper of Blum et al . ( 0000 ) with recent developments . Related theoretical results on the ABC curse of dimensionality and sufficiency are also discussed .
Compared to image representation based on low-level local descriptors , deep neural activations of Convolutional Neural Networks ( CNNs ) are richer in mid-level representation , but poorer in geometric invariance properties . In this paper , we present a straightforward framework for better image representation by combining the two approaches . To take advantages of both representations , we propose an efficient method to extract a fair amount of multi-scale dense local activations from a pre-trained CNN . We then aggregate the activations by Fisher kernel framework , which has been modified with a simple scale-wise normalization essential to make it suitable for CNN activations . Replacing the direct use of a single activation vector with our representation demonstrates significant performance improvements : +00 . 00 ( Acc . ) on MIT Indoor 00 and +0 . 00 ( mAP ) on PASCAL VOC 0000 . The results suggest that our proposal can be used as a primary image representation for better performances in visual recognition tasks .
Social media has changed the ways of communication , where everyone is equipped with the power to express their opinions to others in online discussion platforms . Previously , a number of stud- ies have been presented to identify opinion leaders in online discussion networks . Feng ( " Are you connected ? Evaluating information cascade in online discussion about the #RaceTogether campaign " , Computers in Human Behavior , 0000 ) identified five types of central users and their communication patterns in an online communication network of a limited time span . However , to trace the change in communication pattern , a long-term analysis is required . In this study , we critically analyzed framework presented by Feng based on five types of central users in online communication network and their communication pattern in a long-term manner . We take another case study presented by Udnor et al . ( " Determining social media impact on the politics of developing countries using social network analytics " , Program , 0000 ) to further understand the dynamics as well as to perform validation . Results indicate that there may not exist all of these central users in an online communication network in a long-term manner . Furthermore , we discuss the changing positions of opinion leaders and their power to keep isolates interested in an online discussion network .
We apply the results of Andresen A . and Spokoiny V . on profile M-estimators and the alternating maximization procedure to analyse a sieve profile quasi maximum likelihood estimator in the single index model with linear index function . The link function is approximated with \ ( C^0\ ) -Daubechies-wavelets with compact support . We derive results like Wilks phenomenon and Fisher Theorem in a finite sample setup . Further we show that an alternation maximization procedure converges to the global maximizer and assess the performance of a projection pursuit procedure in that context . The approach is based on showing that the conditions of Andresen A . and Spokoiny V . on profile M-estimators and the alternating maximization procedure can be satisfied under a set of mild regularity and moment conditions on the index function , the regressors and the additive noise . This allows to construct nonasymptotic confidence sets and to derive asymptotic bounds for the estimator as corollaries .
In this paper we study the existence of locally most powerful invariant tests ( LMPIT ) for the problem of testing the covariance structure of a set of Gaussian random vectors . The LMPIT is the optimal test for the case of close hypotheses , among those satisfying the invariances of the problem , and in practical scenarios can provide better performance than the typically used generalized likelihood ratio test ( GLRT ) . The derivation of the LMPIT usually requires one to find the maximal invariant statistic for the detection problem and then derive its distribution under both hypotheses , which in general is a rather involved procedure . As an alternative , Wijsman ' s theorem provides the ratio of the maximal invariant densities without even finding an explicit expression for the maximal invariant . We first consider the problem of testing whether a set of $N$-dimensional Gaussian random vectors are uncorrelated or not , and show that the LMPIT is given by the Frobenius norm of the sample coherence matrix . Second , we study the case in which the vectors under the null hypothesis are uncorrelated and identically distributed , that is , the sphericity test for Gaussian vectors , for which we show that the LMPIT is given by the Frobenius norm of a normalized version of the sample covariance matrix . Finally , some numerical examples illustrate the performance of the proposed tests , which provide better results than their GLRT counterparts .
We study the consensus-halving problem of dividing an object into two portions , such that each of $n$ agents has equal valuation for the two portions . The $\epsilon$-approximate consensus-halving problem allows each agent to have an $\epsilon$ discrepancy on the values of the portions . We prove that computing $\epsilon$-approximate consensus-halving solution using $n$ cuts is in PPA , and is PPAD-hard , where $\epsilon$ is some positive constant ; the problem remains PPAD-hard when we allow a constant number of additional cuts . It is NP-hard to decide whether a solution with $n-0$ cuts exists for the problem . As a corollary of our results , we obtain that the approximate computational version of the Continuous Necklace Splitting Problem is PPAD-hard when the number of portions $t$ is two .
Good software cost prediction is important for effective project management such as budgeting , project planning and control . In this paper , we present an intelligent approach to software cost prediction . By integrating the neuro-fuzzy technique with the well-accepted COCOMO model , our approach can make the best use of both expert knowledge and historical project data . Its major advantages include learning ability , good interpretability , and robustness to imprecise and uncertain inputs . The validation using industry project data shows that the model greatly improves prediction accuracy in comparison with the COCOMO model .
Deep Gaussian processes ( DGPs ) are multi-layer hierarchical generalisations of Gaussian processes ( GPs ) and are formally equivalent to neural networks with multiple , infinitely wide hidden layers . DGPs are nonparametric probabilistic models and as such are arguably more flexible , have a greater capacity to generalise , and provide better calibrated uncertainty estimates than alternative deep models . This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time . The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning . We evaluate the new method for non-linear regression on eleven real-world datasets , showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks . As a by-product , this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks .
Gene expression levels in a population vary extensively across tissues . Such heterogeneity is caused by genetic variability and environmental factors , and is expected to be linked to disease development . The abundance of experimental data now enables the identification of features of gene expression profiles that are shared across tissues , and those that are tissue-specific . While most current research is concerned with characterising differential expression by comparing mean expression profiles across tissues , it is also believed that a significant difference in a gene expression ' s variance across tissues may also be associated to molecular mechanisms that are important for tissue development and function . We propose a sparse multi-view matrix factorisation ( sMVMF ) algorithm to jointly analyse gene expression measurements in multiple tissues , where each tissue provides a different " view " of the underlying organism . The proposed methodology can be interpreted as an extension of principal component analysis in that it provides the means to decompose the total sample variance in each tissue into the sum of two components : one capturing the variance that is shared across tissues , and one isolating the tissue-specific variances . sMVMF has been used to jointly model mRNA expression profiles in three tissues - adipose , skin and LCL - which are available for a large and well-phenotyped twins cohort , TwinsUK . Using sMVMF , we are able to prioritise genes based on whether their variation patterns are specific to each tissue . Furthermore , using DNA methylation profiles available , we provide supporting evidence that adipose-specific gene expression patterns may be driven by epigenetic effects .
Achieving artificial visual reasoning - the ability to answer image-related questions which require a multi-step , high-level process - is an important step towards artificial general intelligence . This multi-modal task requires learning a question-dependent , structured reasoning process over images from language . Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure , while leading methods learn to visually reason successfully but are hand-crafted for reasoning . We show that a general-purpose , Conditional Batch Normalization approach achieves state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 0 . 0% error rate . We outperform the next best end-to-end method ( 0 . 0% ) and even methods that use extra supervision ( 0 . 0% ) . We probe our model to shed light on how it reasons , showing it has learned a question-dependent , multi-step process . Previous work has operated under the assumption that visual reasoning calls for a specialized architecture , but we show that a general architecture with proper conditioning can learn to visually reason effectively .
We consider the problem of scheduling a set of $n$ tasks on $m$ processors under precedence , communication , and global system energy constraints to minimize makespan . We extend existing scheduling models to account for energy usage and give convex programming algorithms that yield essentially the same results as existing algorithms that do not consider energy , while adhering to a strict energy bound .
We propose a novel method for automatic pain intensity estimation from facial images based on the framework of kernel Conditional Ordinal Random Fields ( KCORF ) . We extend this framework to account for heteroscedasticity on the output labels ( i . e . , pain intensity scores ) and introduce a novel dynamic features , dynamic ranks , that impose temporal ordinal constraints on the static ranks ( i . e . , intensity scores ) . Our experimental results show that the proposed approach outperforms state-of-the art methods for sequence classification with ordinal data and other ordinal regression models . The approach performs significantly better than other models in terms of Intra-Class Correlation measure , which is the most accepted evaluation measure in the tasks of facial behaviour intensity estimation .
In a recent work ( arXiv : 0000 . 0000 ) , for nonlinear models with sparse underlying linear structures , we studied the error bounds of $\ell_0$-regularized estimation . In this note , we show that $\ell_0$-regularized estimation in some important cases can achieve the same order of error bounds as those in the aforementioned work .
The paper introduces RADULS , a new parallel sorter based on radix sort algorithm , intended to organize ultra-large data sets efficiently . For example 0G 00-byte records can be sorted with 00 threads in less than 00 seconds on Intel Xeon-based workstation . The implementation of RADULS is not only highly optimized to gain such an excellent performance , but also parallelized in a cache friendly manner to make the most of modern multicore architectures . Besides , our parallel scheduler launches a few different procedures at runtime , according to the current parameters of the execution , for proper workload management . All experiments show RADULS to be superior to competing algorithms .
Each Boolean function can be computed by a single-pass instruction sequence that contains only instructions to set and get the content of Boolean registers , forward jump instructions , and a termination instruction . Auxiliary Boolean registers are not necessary for this . In the current paper , we show that , in the case of the parity functions , shorter instruction sequences are possible with the use of an auxiliary Boolean register in the presence of instructions to complement the content of auxiliary Boolean registers . This result supports , in a setting where programs are instruction sequences acting on Boolean registers , a basic intuition behind the storage of auxiliary data , namely the intuition that this makes possible a reduction of the size of a program .
The happy-productive worker thesis states that happy workers are more productive . Recent research in software engineering supports the thesis , and the ideal of flourishing happiness among software developers is often expressed among industry practitioners . However , the literature suggests that a cost-effective way to foster happiness and productivity among workers could be to limit unhappiness . Psychological disorders such as job burnout and anxiety could also be reduced by limiting the negative experiences of software developers . Simultaneously , a baseline assessment of ( un ) happiness and knowledge about how developers experience it are missing . In this paper , we broaden the understanding of unhappiness among software developers in terms of ( 0 ) the software developer population distribution of ( un ) happiness , and ( 0 ) the causes of unhappiness while developing software . We conducted a large-scale quantitative and qualitative survey , incorporating a psychometrically validated instrument for measuring ( un ) happiness , with 0000 developers , yielding a rich and balanced sample of 0000 complete responses . Our results indicate that software developers are a slightly happy population , but the need for limiting the unhappiness of developers remains . We also identified 000 factors representing causes of unhappiness while developing software . Our results , which are available as open data , can act as guidelines for practitioners in management positions and developers in general for fostering happiness on the job . We suggest considering happiness in future studies of both human and technical aspects in software engineering .
Cross-lingual plagiarism ( CLP ) occurs when texts written in one language are translated into a different language and used without acknowledging the original sources . One of the most common methods for detecting CLP requires online machine translators ( such as Google or Microsoft translate ) which are not always available , and given that plagiarism detection typically involves large document comparison , the amount of translations required would overwhelm an online machine translator , especially when detecting plagiarism over the web . In addition , when translated texts are replaced with their synonyms , using online machine translators to detect CLP would result in poor performance . This paper addresses the problem of cross-lingual plagiarism detection ( CLPD ) by proposing a model that uses simulated word embeddings to reproduce the predictions of an online machine translator ( Google translate ) when detecting CLP . The simulated embeddings comprise of translated words in different languages mapped in a common space , and replicated to increase the prediction probability of retrieving the translations of a word ( and their synonyms ) from the model . Unlike most existing models , the proposed model does not require parallel corpora , and accommodates multiple languages ( multi-lingual ) . We demonstrated the effectiveness of the proposed model in detecting CLP in standard datasets that contain CLP cases , and evaluated its performance against a state-of-the-art baseline that relies on online machine translator ( T+MA model ) . Evaluation results revealed that the proposed model is not only effective in detecting CLP , it outperformed the baseline . The results indicate that CLP could be detected with state-of-the-art performances by leveraging the prediction accuracy of an internet translator with word embeddings , without relying on internet translators .
The last decade has witnessed a growing interest in random forest models which are recognized to exhibit good practical performance , especially in high-dimensional settings . On the theoretical side , however , their predictive power remains largely unexplained , thereby creating a gap between theory and practice . The aim of this paper is twofold . Firstly , we provide theoretical guarantees to link finite forests used in practice ( with a finite number M of trees ) to their asymptotic counterparts . Using empirical process theory , we prove a uniform central limit theorem for a large class of random forest estimates , which holds in particular for Breiman ' s original forests . Secondly , we show that infinite forest consistency implies finite forest consistency and thus , we state the consistency of several infinite forests . In particular , we prove that q quantile forests---close in spirit to Breiman ' s forests but easier to study---are able to combine inconsistent trees to obtain a final consistent prediction , thus highlighting the benefits of random forests compared to single trees .
Discussion of Conditional growth charts [math . ST/0000000]
Random hashing is a standard method to balance loads among nodes in Peer-to-Peer networks . However , hashing destroys locality properties of object keys , the critical properties to many applications , more specifically , those that require range searching . To preserve a key order while keeping loads balanced , Ganesan , Bawa and Garcia-Molina proposed a load-balancing algorithm that supports both object insertion and deletion that guarantees a ratio of 0 . 000 between the maximum and minimum loads among nodes in the network using constant amortized costs . However , their algorithm is not straightforward to implement in real networks because it is recursive . Their algorithm mostly uses local operations with global max-min load information . In this work , we present a simple non-recursive algorithm using essentially the same primitive operations as in Ganesan {\em et al . } ' s work . We prove that for insertion and deletion , our algorithm guarantees a constant max-min load ratio of 0 . 000 with constant amortized costs .
The research on meta-analysis and particularly multivariate meta-analysis has been greatly influenced by the work of Ingram Olkin . This paper documents Olkin ' s contributions by way of citation counts and outlines several areas of contribution by Olkin and his academic descendants . An academic family tree is provided .
This paper presents a preliminary conceptual investigation into an environment representation that has constant space complexity with respect to the camera image space . This type of representation allows the planning algorithms of a mobile agent to bypass what are often complex and noisy transformations between camera image space and Euclidean space . The approach is to compute per-pixel potential values directly from processed camera data , which results in a discrete potential field that has constant space complexity with respect to the image plane . This can enable planning and control algorithms , whose complexity often depends on the size of the environment representation , to be defined with constant run-time . This type of approach can be particularly useful for platforms with strict resource constraints , such as embedded and real-time systems .
This paper describes a framework for flexible multiple hypothesis testing of autoregressive time series . The modeling approach is Bayesian , though a blend of frequentist and Bayesian reasoning is used to evaluate procedures . Nonparametric characterizations of both the null and alternative hypotheses will be shown to be the key robustification step necessary to ensure reasonable Type-I error performance . The methodology is applied to part of a large database containing up to 00 years of corporate performance statistics on 00 , 000 publicly traded American companies , where the primary goal of the analysis is to flag companies whose historical performance is significantly different from that expected due to chance .
In a \emph{fan-planar drawing} of a graph an edge can cross only edges with a common end-vertex . Fan-planar drawings have been recently introduced by Kaufmann and Ueckerdt , who proved that every $n$-vertex fan-planar drawing has at most $0n-00$ edges , and that this bound is tight for $n \geq 00$ . We extend their result , both from the combinatorial and the algorithmic point of view . We prove tight bounds on the density of constrained versions of fan-planar drawings and study the relationship between fan-planarity and $k$-planarity . Furthermore , we prove that deciding whether a graph admits a fan-planar drawing in the variable embedding setting is NP-complete .
Statistical models for genetic linkage analysis of k-locus diseases are k-dimensional subvarieties of a ( 0^k-0 ) -dimensional probability simplex . We determine the algebraic invariants of these models with general characteristics for k=0 , in particular we recover , and generalize , the Hardy-Weinberg curve . For k = 0 , the invariants are presented as determinants of 00x00-matrices of linear forms in 0 unknowns , a suitable format for computations with numerical data .
Relaying on early effort estimation to predict the required number of resources is not often sufficient , and could lead to under or over estimation . It is widely acknowledge that that software development process should be refined regularly and that software prediction made at early stage of software development is yet kind of guesses . Even good predictions are not sufficient with inherent uncertainty and risks . The stage-effort estimation allows project manager to re-allocate correct number of resources , re-schedule project and control project progress to finish on time and within budget . In this paper we propose an approach to utilize prior effort records to predict stage effort . The proposed model combines concepts of Fuzzy set theory and association rule mining . The results were good in terms of prediction accuracy and have potential to deliver good stage-effort estimation .
This paper gives an overview of impersonation bots that generate output in one , or possibly , multiple modalities . We also discuss rapidly advancing areas of machine learning and artificial intelligence that could lead to frighteningly powerful new multi-modal social bots . Our main conclusion is that most commonly known bots are one dimensional ( i . e . , chatterbot ) , and far from deceiving serious interrogators . However , using recent advances in machine learning , it is possible to unleash incredibly powerful , human-like armies of social bots , in potentially well coordinated campaigns of deception and influence .
We show that all non-negative submodular functions have high {\em noise-stability} . As a consequence , we obtain a polynomial-time learning algorithm for this class with respect to any product distribution on $\{-0 , 0\}^n$ ( for any constant accuracy parameter $\epsilon$ ) . Our algorithm also succeeds in the agnostic setting . Previous work on learning submodular functions required either query access or strong assumptions about the types of submodular functions to be learned ( and did not hold in the agnostic setting ) .
We study a general task allocation problem , involving multiple agents that collaboratively accomplish tasks and where agents may fail to successfully complete the tasks assigned to them ( known as execution uncertainty ) . The goal is to choose an allocation that maximises social welfare while taking their execution uncertainty into account . We show that this can be achieved by using the post-execution verification ( PEV ) -based mechanism if and only if agents ' valuations satisfy a multilinearity condition . We then consider a more complex setting where an agent ' s execution uncertainty is not completely predictable by the agent alone but aggregated from all agents ' private opinions ( known as trust ) . We show that PEV-based mechanism with trust is still truthfully implementable if and only if the trust aggregation is multilinear .
" Evidence and Evolution : the Logic behind the Science " was published in 0000 by Elliott Sober . It examines the philosophical foundations of the statistical arguments used to evaluate hypotheses in evolutionary biology , based on simple examples and likelihood ratios . The difficulty with reading the book from a statistician ' s perspective is the reluctance of the author to engage into model building and even less into parameter estimation . The first chapter nonetheless constitutes a splendid coverage of the most common statistical approaches to testing and model comparison , even though the advocation of the Akaike information criterion against Bayesian alternatives is rather forceful . The book also covers an examination of the " intelligent design " arguments against the Darwinian evolution theory , predictably if unnecessarily resorting to Popperian arguments to correctly argue that the creationist perspective fails to predict anything . The following chapters cover the more relevant issues of assessing selection versus drift and of testing for the presence of a common ancestor . While remaining a philosophy treatise , Evidence and Evolution is written in a way that is accessible to laymen , if rather unusual from a statistician viewpoint , and the insight about testing issues gained from Evidence and Evolution makes it a worthwhile read .
Recently , an extension of independent component analysis ( ICA ) from one to multiple datasets , termed independent vector analysis ( IVA ) , has been the subject of significant research interest . IVA has also been shown to be a generalization of Hotelling ' s canonical correlation analysis . In this paper , we provide the identification conditions for a general IVA formulation , which accounts for linear , nonlinear , and sample-to-sample dependencies . The identification conditions are a generalization of previous results for ICA and for IVA when samples are independently and identically distributed . Furthermore , a principal aim of IVA is the identification of dependent sources between datasets . Thus , we provide the additional conditions for when the arbitrary ordering of the sources within each dataset is common . Performance bounds in terms of the Cramer-Rao lower bound are also provided for the demixing matrices and interference to source ratio . The performance of two IVA algorithms are compared to the theoretical bounds .
Typically , in the dynamical theory of extremal events , the function that gauges the intensity of a phenomenon is assumed to be convex and maximal , or singular , at a single , or at most a finite collection of points in phase--space . In this paper we generalize this situation to fractal landscapes , i . e . intensity functions characterized by an uncountable set of singularities , located on a Cantor set . This reveals the dynamical r\^ole of classical quantities like the Minkowski dimension and content , whose definition we extend to account for singular continuous invariant measures . We also introduce the concept of extremely rare event , quantified by non--standard Minkowski constants and we study its consequences to extreme value statistics . Limit laws are derived from formal calculations and are verified by numerical experiments .
This paper introduces a framework for simulating finite dimensional representations of ( jump ) diffusion sample paths over finite intervals , without discretisation error ( exactly ) , in such a way that the sample path can be restored at any finite collection of time points . Within this framework we extend existing exact algorithms and introduce novel adaptive approaches . We consider an application of the methodology developed within this paper which allows the simulation of upper and lower bounding processes which almost surely constrain ( jump ) diffusion sample paths to any specified tolerance . We demonstrate the efficacy of our approach by showing that with finite computation it is possible to determine whether or not sample paths cross various irregular barriers , simulate to any specified tolerance the first hitting time of the irregular barrier and simulate killed diffusion sample paths .
The vector autoregressive ( VAR ) model is a powerful tool in modeling complex time series and has been exploited in many fields . However , fitting high dimensional VAR model poses some unique challenges : On one hand , the dimensionality , caused by modeling a large number of time series and higher order autoregressive processes , is usually much higher than the time series length ; On the other hand , the temporal dependence structure in the VAR model gives rise to extra theoretical challenges . In high dimensions , one popular approach is to assume the transition matrix is sparse and fit the VAR model using the " least squares " method with a lasso-type penalty . In this manuscript , we propose an alternative way in estimating the VAR model . The main idea is , via exploiting the temporal dependence structure , to formulate the estimating problem into a linear program . There is instant advantage for the proposed approach over the lasso-type estimators : The estimation equation can be decomposed into multiple sub-equations and accordingly can be efficiently solved in a parallel fashion . In addition , our method brings new theoretical insights into the VAR model analysis . So far the theoretical results developed in high dimensions ( e . g . , Song and Bickel ( 0000 ) and Kock and Callot ( 0000 ) ) mainly pose assumptions on the design matrix of the formulated regression problems . Such conditions are indirect about the transition matrices and not transparent . In contrast , our results show that the operator norm of the transition matrices plays an important role in estimation accuracy . We provide explicit rates of convergence for both estimation and prediction . In addition , we provide thorough experiments on both synthetic and real-world equity data to show that there are empirical advantages of our method over the lasso-type estimators in both parameter estimation and forecasting .
We present some extensions of Bernstein ' s concentration inequality for random matrices . This inequality has become a useful and powerful tool for many problems in statistics , signal processing and theoretical computer science . The main feature of our bounds is that , unlike the majority of previous related results , they do not depend on the dimension $d$ of the ambient space . Instead , the dimension factor is replaced by the " effective rank " associated with the underlying distribution that is bounded from above by $d$ . In particular , this makes an extension to the infinite-dimensional setting possible . Our inequalities refine earlier results in this direction obtained by D . Hsu , S . M . Kakade and T . Zhang .
This research analyzes complex networks in open-source software at the inter-package level , where package dependencies often span across projects and between development groups . We review complex networks identified at ``lower ' ' levels of abstraction , and then formulate a description of interacting software components at the package level , a relatively ``high ' ' level of abstraction . By mining open-source software repositories from two sources , we empirically show that the coupling of modules at this granularity creates a small-world and scale-free network in both instances .
Nowadays , a globalization of national markets requires developing flexible and demand-driven production systems . Agent-based technology , being distributed , flexible and autonomous is expected to provide a short-time reaction to disturbances and sudden changes of environment and allows satisfying the mentioned requirements . The distributed constraint satisfaction approach underlying the suggested method is described by a modified Petri network providing both the conceptual notions and main details of implementation .
This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning . We focus on the traveling salesman problem ( TSP ) and train a recurrent network that , given a set of city coordinates , predicts a distribution over different city permutations . Using negative tour length as the reward signal , we optimize the parameters of the recurrent network using a policy gradient method . We compare learning the network parameters on a set of training graphs against learning them on individual test graphs . Despite the computational expense , without much engineering and heuristic designing , Neural Combinatorial Optimization achieves close to optimal results on 0D Euclidean graphs with up to 000 nodes . Applied to the KnapSack , another NP-hard problem , the same method obtains optimal solutions for instances with up to 000 items .
For ill-posed inverse problems , a regularised solution can be interpreted as a mode of the posterior distribution in a Bayesian framework . This framework enriches the set the solutions , as other posterior estimates can be used as a solution to the inverse problem , such as the posterior mean that can be easier to compute in practice . In this paper we prove consistency of Bayesian solutions of an ill-posed linear inverse problem in the Ky Fan metric for a general class of likelihoods and prior distributions in a finite dimensional setting . This result can be applied to study infinite dimensional problems by letting the dimension of the unknown parameter grow to infinity which can be viewed as discretisation on a grid or spectral approximation of an infinite dimensional problem . Likelihood and the prior distribution are assumed to be in an exponential form that includes distributions from the exponential family , and to be differentiable . The observations can be dependent . No assumption of finite moments of observations , such as expected value or the variance , is necessary thus allowing for possibly non-regular likelihoods , and allowing for non-conjugate and improper priors . If the variance exists , it may be heteroscedastic , namely , it may depend on the unknown function . We observe quite a surprising phenomenon when applying our result to the spectral approximation framework where it is possible to achieve the parametric rate of convergence , i . e the problem becomes self-regularised . We also consider a particular case of the unknown parameter being on the boundary of the parameter set , and show that the rate of convergence in this case is faster than for an interior point parameter .
Motivated by what is required for real-time path planning , the paper starts out by presenting sRMPD , a new recursive " local " planner founded on the key notion that , unless made necessary by an obstacle , there must be no deviation from the shortest path between any two points , which would normally be a straight line path in the configuration space . Subsequently , we increase the power of sRMPD by using it as a " connect " subroutine call in a higher-level sampling-based algorithm mRMPD that is inspired by multi-RRT . As a consequence , mRMPD spawns a larger number of space exploring trees in regions of the configuration space that are characterized by a higher density of obstacles . The overall effect is a hybrid tree growing strategy with a trade-off between random exploration as made possible by multi-RRT based logic and immediate exploitation of opportunities to connect two states as made possible by sRMPD . The mRMPD planner can be biased with regard to this trade-off for solving different kinds of planning problems efficiently . Based on the test cases we have run , our experiments show that mRMPD can reduce planning time by up to 00% compared to basic RRT .
Fuel efficient Homogeneous Charge Compression Ignition ( HCCI ) engine combustion timing predictions must contend with non-linear chemistry , non-linear physics , period doubling bifurcation ( s ) , turbulent mixing , model parameters that can drift day-to-day , and air-fuel mixture state information that cannot typically be resolved on a cycle-to-cycle basis , especially during transients . In previous work , an abstract cycle-to-cycle mapping function coupled with $\epsilon$-Support Vector Regression was shown to predict experimentally observed cycle-to-cycle combustion timing over a wide range of engine conditions , despite some of the aforementioned difficulties . The main limitation of the previous approach was that a partially acausual randomly sampled training dataset was used to train proof of concept offline predictions . The objective of this paper is to address this limitation by proposing a new online adaptive Extreme Learning Machine ( ELM ) extension named Weighted Ring-ELM . This extension enables fully causal combustion timing predictions at randomly chosen engine set points , and is shown to achieve results that are as good as or better than the previous offline method . The broader objective of this approach is to enable a new class of real-time model predictive control strategies for high variability HCCI and , ultimately , to bring HCCI ' s low engine-out NOx and reduced CO0 emissions to production engines .
The riots in Stockholm in May 0000 were an event that reverberated in the world media for its dimension of violence that had spread through the Swedish capital . In this study we have investigated the role of social media in creating media phenomena via text mining and natural language processing . We have focused on two channels of communication for our analysis : Twitter and Poloniainfo . se ( Forum of Polish community in Sweden ) . Our preliminary results show some hot topics driving discussion related mostly to Swedish Police and Swedish Politics by counting word usage . Typical features for media intervention are presented . We have built networks of most popular phrases , clustered by categories ( geography , media institution , etc . ) . Sentiment analysis shows negative connotation with Police . The aim of this preliminary exploratory quantitative study was to generate questions and hypotheses , which we could carefully follow by deeper more qualitative methods .
We prove that the Palm measure of the Ginibre process is obtained by removing a Gaussian distributed point from the process and adding the origin . We obtain also precise formulas describing the law of the typical cell of Ginibre--Voronoi tessellation . We show that near the germs of the cells a more important part of the area is captured in the Ginibre--Voronoi tessellation than in the Poisson--Voronoi tessellation . Moment areas of corresponding subdomains of the cells are explicitly evaluated .
Unsupervised classification methods learn a discriminative classifier from unlabeled data , which has been proven to be an effective way of simultaneously clustering the data and training a classifier from the data . Various unsupervised classification methods obtain appealing results by the classifiers learned in an unsupervised manner . However , existing methods do not consider the misclassification error of the unsupervised classifiers except unsupervised SVM , so the performance of the unsupervised classifiers is not fully evaluated . In this work , we study the misclassification error of two popular classifiers , i . e . the nearest neighbor classifier ( NN ) and the plug-in classifier , in the setting of unsupervised classification .
Finite sample properties of multiple imputation estimators under the linear regression model are studied . The exact bias of the multiple imputation variance estimator is presented . A method of reducing the bias is presented and simulation is used to make comparisons . We also show that the suggested method can be used for a general class of linear estimators .
This paper proposes a new Bayesian approach for analysing moment condition models in the situation where the data may be contaminated by outliers . The approach builds upon the foundations developed by Schennach ( 0000 ) who proposed the Bayesian exponentially tilted empirical likelihood ( BETEL ) method , justified by the fact that an empirical likelihood ( EL ) can be interpreted as the nonparametric limit of a Bayesian procedure when the implied probabilities are obtained from maximizing entropy subject to some given moment constraints . Considering the impact that outliers are thought to have on the estimation of population moments , we develop a new robust BETEL ( RBETEL ) inferential methodology to deal with this potential problem . We show how the BETEL methods are linked to the recent work of Bissiri , Holmes and Walker ( 0000 ) who propose a general framework to update prior belief via a loss function . A controlled simulation experiment is conducted to investigate the performance of the RBETEL method . We find that the proposed methodology produces reliable posterior inference for the fundamental relationships that are embedded in the majority of the data , even when outliers are present . The method is also illustrated in an empirical study relating brain weight to body weight using a dataset containing sixty-five different land animal species .
Understanding the seizure initiation process and its propagation pattern ( s ) is a critical task in epilepsy research . Characteristics of the pre-seizure electroencephalograms ( EEGs ) such as oscillating powers and high-frequency activities are believed to be indicative of the seizure onset and spread patterns . In this article , we analyze epileptic EEG time series using nonparametric spectral estimation methods to extract information on seizure-specific power and characteristic frequency [or frequency band ( s ) ] . Because the EEGs may become nonstationary before seizure events , we develop methods for both stationary and local stationary processes . Based on penalized Whittle likelihood , we propose a direct generalized maximum likelihood ( GML ) and generalized approximate cross-validation ( GACV ) methods to estimate smoothing parameters in both smoothing spline spectrum estimation of a stationary process and smoothing spline ANOVA time-varying spectrum estimation of a locally stationary process . We also propose permutation methods to test if a locally stationary process is stationary . Extensive simulations indicate that the proposed direct methods , especially the direct GML , are stable and perform better than other existing methods . We apply the proposed methods to the intracranial electroencephalograms ( IEEGs ) of an epileptic patient to gain insights into the seizure generation process .
Science projects are data publishers . The scale and complexity of current and future science data changes the nature of the publication process . Publication is becoming a major project component . At a minimum , a project must preserve the ephemeral data it gathers . Derived data can be reconstructed from metadata , but metadata is ephemeral . Longer term , a project should expect some archive to preserve the data . We observe that pub-lished scientific data needs to be available forever ? this gives rise to the data pyramid of versions and to data inflation where the derived data volumes explode . As an example , this article describes the Sloan Digital Sky Survey ( SDSS ) strategies for data publication , data access , curation , and preservation .
Nowadays folksonomy is used as a system derived from user-generated electronic tags or keywords that annotate and describe online content . But it is not a classification system as an ontology . To consider it as a classification system it would be necessary to share a representation of contexts by all the users . This paper is proposing the use of folksonomies and network theory to devise a new concept : a " Folksodriven Structure Network " to represent folksonomies . This paper proposed and analyzed the network structure of Folksodriven tags thought as folsksonomy tags suggestions for the user on a dataset built on chosen websites . It is observed that the Folksodriven Network has relative low path lengths checking it with classic networking measures ( clustering coefficient ) . Experiment result shows it can facilitate serendipitous discovery of content among users . Neat examples and clear formulas can show how a " Folksodriven Structure Network " can be used to tackle ontology mapping challenges .
In order to perform autonomous sequential manipulation tasks , perception in cluttered scenes remains a critical challenge for robots . In this paper , we propose a probabilistic approach for robust sequential scene estimation and manipulation - Sequential Scene Understanding and Manipulation ( SUM ) . SUM considers uncertainty due to discriminative object detection and recognition in the generative estimation of the most likely object poses maintained over time to achieve a robust estimation of the scene under heavy occlusions and unstructured environment . Our method utilizes candidates from discriminative object detector and recognizer to guide the generative process of sampling scene hypothesis , and each scene hypotheses is evaluated against the observations . Also SUM maintains beliefs of scene hypothesis over robot physical actions for better estimation and against noisy detections . We conduct extensive experiments to show that our approach is able to perform robust estimation and manipulation .
In this paper , we demonstrate a score based indexing approach for tennis videos . Given a broadcast tennis video ( BTV ) , we index all the video segments with their scores to create a navigable and searchable match . Our approach temporally segments the rallies in the video and then recognizes the scores from each of the segments , before refining the scores using the knowledge of the tennis scoring system . We finally build an interface to effortlessly retrieve and view the relevant video segments by also automatically tagging the segmented rallies with human accessible tags such as ' fault ' and ' deuce ' . The efficiency of our approach is demonstrated on BTV ' s from two major tennis tournaments .
This paper presents a detailed noise analysis and a noise-based optimization procedure for resonant MEMS structures . A design for high sensitivity of MEMS structures needs to take into account the noise shaping induced by damping phenomena at micro scale . The existing literature presents detailed models for the damping at microscale , but usually neglects them in the noise analysis process , assuming instead a white spectrum approximation for the mechano-thermal noise . The present work extends the implications of the complex gas-solid interaction into the field of noise analysis for mechanical sensors , and provides a semi-automatic procedure for behavioral macromodel extraction and sensor optimization with respect to signal-to-noise ratio .
We introduce a new framework for unsupervised learning of representations based on a novel hierarchical decomposition of information . Intuitively , data is passed through a series of progressively fine-grained sieves . Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data . The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer . Ultimately , we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise . We present a practical implementation of this framework for discrete variables and apply it to a variety of fundamental tasks in unsupervised learning including independent component analysis , lossy and lossless compression , and predicting missing values in data .
Recently we presented TTC , a domain-specific compiler for tensor transpositions . Despite the fact that the performance of the generated code is nearly optimal , due to its offline nature , TTC cannot be utilized in all the application codes in which the tensor sizes and the necessary tensor permutations are determined at runtime . To overcome this limitation , we introduce the open-source C++ library High-Performance Tensor Transposition ( HPTT ) . Similar to TTC , HPTT incorporates optimizations such as blocking , multi-threading , and explicit vectorization ; furthermore it decomposes any transposition into multiple loops around a so called micro-kernel . This modular design---inspired by BLIS---makes HPTT easy to port to different architectures , by only replacing the hand-vectorized micro-kernel ( e . g . , a 0x0 transpose ) . HPTT also offers an optional autotuning framework---guided by a performance model---that explores a vast search space of implementations at runtime ( similar to FFTW ) . Across a wide range of different tensor transpositions and architectures ( e . g . , Intel Ivy Bridge , Intel Knights Landing , ARMv0 , IBM Power0 ) , HPTT attains a bandwidth comparable to that of SAXPY , and yields remarkable speedups over Eigen ' s tensor transposition implementation . Most importantly , the integration of HPTT into the Cyclops Tensor Framework ( CTF ) improves the overall performance of tensor contractions by up to 0 . 0x .
Model Driven Engineering ( MDE ) is an emerging approach of software engineering . MDE emphasizes the construction of models from which the implementation should be derived by applying model transformations . The Ontology Definition Meta-model ( ODM ) has been proposed as a profile for UML models of the Web Ontology Language ( OWL ) . In this context , transformations of UML models can be mapped into ODM/OWL transformations . On the other hand , model validation is a crucial task in model transformation . Meta-modeling permits to give a syntactic structure to source and target models . However , semantic requirements have to be imposed on source and target models . A given transformation will be sound when source and target models fulfill the syntactic and semantic requirements . In this paper , we present an approach for model validation in ODM based transformations . Adopting a logic programming based transformational approach we will show how it is possible to transform and validate models . Properties to be validated range from structural and semantic requirements of models ( pre and post conditions ) to properties of the transformation ( invariants ) . The approach has been applied to a well-known example of model transformation : the Entity-Relationship ( ER ) to Relational Model ( RM ) transformation .
We introduce Parseval networks , a form of deep neural networks in which the Lipschitz constant of linear , convolutional and aggregation layers is constrained to be smaller than 0 . Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation . The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be ( approximately ) Parseval tight frames , which are extensions of orthogonal matrices to non-square matrices . We describe how these constraints can be maintained efficiently during SGD . We show that Parseval networks match the state-of-the-art in terms of accuracy on CIFAR-00/000 and Street View House Numbers ( SVHN ) while being more robust than their vanilla counterpart against adversarial examples . Incidentally , Parseval networks also tend to train faster and make a better usage of the full capacity of the networks .
A fundamental limitation of causal inference in observational studies is that perceived evidence for an effect might instead be explained by factors not accounted for in the primary analysis . Methods for assessing the sensitivity of a study ' s conclusions to unmeasured confounding have been established under the assumption that the treatment effect is constant across all individuals . In the potential presence of unmeasured confounding , it has been argued that certain patterns of effect heterogeneity may conspire with unobserved covariates to render the performed sensitivity analysis inadequate . We present a new method for conducting a sensitivity analysis for the sample average treatment effect in the presence of effect heterogeneity in paired observational studies . Our recommended procedure , called the studentized sensitivity analysis , represents an extension of recent work on studentized permutation tests to the case of observational studies , where randomizations are no longer drawn uniformly . The method naturally extends conventional tests for the sample average treatment effect in paired experiments to the case of unknown , but bounded , probabilities of assignment to treatment . In so doing , we illustrate that concerns about certain sensitivity analyses operating under the presumption of constant effects are largely unwarranted .
Multiview video with interactive and smooth view switching at the receiver is a challenging application with several issues in terms of effective use of storage and bandwidth resources , reactivity of the system , quality of the viewing experience and system complexity . The classical decoding system for generating virtual views first projects a reference or encoded frame to a given viewpoint and then fills in the holes due to potential occlusions . This last step still constitutes a complex operation with specific software or hardware at the receiver and requires a certain quantity of information from the neighboring frames for insuring consistency between the virtual images . In this work we propose a new approach that shifts most of the burden due to interactivity from the decoder to the encoder , by anticipating the navigation of the decoder and sending auxiliary information that guarantees temporal and interview consistency . This leads to an additional cost in terms of transmission rate and storage , which we minimize by using optimization techniques based on the user behavior modeling . We show by experiments that the proposed system represents a valid solution for interactive multiview systems with classical decoders .
B-Prolog is a high-performance implementation of the standard Prolog language with several extensions including matching clauses , action rules for event handling , finite-domain constraint solving , arrays and hash tables , declarative loop constructs , and tabling . The B-Prolog system is based on the TOAM architecture which differs from the WAM mainly in that ( 0 ) arguments are passed old-fashionedly through the stack , ( 0 ) only one frame is used for each predicate call , and ( 0 ) instructions are provided for encoding matching trees . The most recent architecture , called TOAM Jr . , departs further from the WAM in that it employs no registers for arguments or temporary variables , and provides variable-size instructions for encoding predicate calls . This paper gives an overview of the language features and a detailed description of the TOAM Jr . architecture , including architectural support for action rules and tabling .
We propose a novel class of statistical divergences called \textit{Relaxed Wasserstein} ( RW ) divergence . RW divergence generalizes Wasserstein distance and is parametrized by strictly convex , differentiable functions . We establish for RW several key probabilistic properties , which are critical for the success of Wasserstein distances . In particular , we show that RW is dominated by Total Variation ( TV ) and Wasserstein-$L^0$ distance , and establish continuity , differentiability , and duality representation of RW divergence . Finally , we provide a non-asymptotic moment estimate and a concentration inequality for RW divergence . Our experiments on image generation problems show that RWGANs with Kullback-Leibler ( KL ) divergence provide competitive performance compared with many state-of-the-art approaches . Empirically , we show that RWGANs possess better convergence properties than WGANs , with competitive inception scores . In comparison to the existing literature in GANs , which are ad-hoc in the choices of cost functions , this new conceptual framework not only provides great flexibility in designing general cost functions , e . g . , for applications to GANs , but also allows different cost functions implemented and compared under a unified mathematical framework .
Multiple hypothesis testing is a central topic in statistics , but despite abundant work on the false discovery rate ( FDR ) and the corresponding Type-II error concept known as the false non-discovery rate ( FNR ) , a fine-grained understanding of the fundamental limits of multiple testing has not been developed . Our main contribution is to derive a precise non-asymptotic tradeoff between FNR and FDR for a variant of the generalized Gaussian sequence model . Our analysis is flexible enough to permit analyses of settings where the problem parameters vary with the number of hypotheses $n$ , including various sparse and dense regimes ( with $o ( n ) $ and $\mathcal{O} ( n ) $ signals ) . Moreover , we prove that the Benjamini-Hochberg algorithm as well as the Barber-Cand\`{e}s algorithm are both rate-optimal up to constants across these regimes .
Testing for the significance of a subset of regression coefficients in a linear model , a staple of statistical analysis , goes back at least to the work of Fisher who introduced the analysis of variance ( ANOVA ) . We study this problem under the assumption that the coefficient vector is sparse , a common situation in modern high-dimensional settings . Suppose we have $p$ covariates and that under the alternative , the response only depends upon the order of $p^{0-\alpha}$ of those , $0\le\alpha\le0$ . Under moderate sparsity levels , that is , $0\le\alpha\le0/0$ , we show that ANOVA is essentially optimal under some conditions on the design . This is no longer the case under strong sparsity constraints , that is , $\alpha>0/0$ . In such settings , a multiple comparison procedure is often preferred and we establish its optimality when $\alpha\geq0/0$ . However , these two very popular methods are suboptimal , and sometimes powerless , under moderately strong sparsity where $0/0<\alpha<0/0$ . We suggest a method based on the higher criticism that is powerful in the whole range $\alpha>0/0$ . This optimality property is true for a variety of designs , including the classical ( balanced ) multi-way designs and more modern " $p>n$ " designs arising in genetics and signal processing . In addition to the standard fixed effects model , we establish similar results for a random effects model where the nonzero coefficients of the regression vector are normally distributed .
Bayesian nonparametric marginal methods are very popular since they lead to fairly easy implementation due to the formal marginalization of the infinite-dimensional parameter of the model . However , the straightforwardness of these methods also entails some limitations : they typically yield point estimates in the form of posterior expectations , but cannot be used to estimate non-linear functionals of the posterior distribution , such as median , mode or credible intervals . This is particularly relevant in survival analysis where non-linear functionals such as e . g . the median survival time , play a central role for clinicians and practitioners . The main goal of this paper is to summarize the methodology introduced in [Arbel et al . , Comput . Stat . Data . An . , 0000] for hazard mixture models in order to draw approximate Bayesian inference on survival functions that is not limited to the posterior mean . In addition , we propose a practical implementation of an R package called momentify designed for moment-based density approximation , and , by means of an extensive simulation study , we thoroughly compare the introduced methodology with standard marginal methods and empirical estimation .
We consider the problem of locating a jump discontinuity ( change-point ) in a smooth parametric regression model with a bounded covariate . It is assumed that one can sample the covariate at different values and measure the corresponding responses . Budget constraints dictate that a total of $n$ such measurements can be obtained . A multistage adaptive procedure is proposed , where at each stage an estimate of the change point is obtained and new points are sampled from its appropriately chosen neighborhood . It is shown that such procedures accelerate the rate of convergence of the least squares estimate of the change-point . Further , the asymptotic distribution of the estimate is derived using empirical processes techniques . The latter result provides guidelines on how to choose the tuning parameters of the multistage procedure in practice . The improved efficiency of the procedure is demonstrated using real and synthetic data . This problem is primarily motivated by applications in engineering systems .
This paper is centered on using chemical reaction as a computational metaphor for simultaneously solving problems . An artificial chemical reactor that can simultaneously solve instances of three unrelated problems was created . The reactor is a distributed stochastic algorithm that simulates a chemical universe wherein the molecular species are being represented either by a human genomic contig panel , a Hamiltonian cycle , or an aircraft landing schedule . The chemical universe is governed by reactions that can alter genomic sequences , re-order Hamiltonian cycles , or reschedule an aircraft landing program . Molecular masses were considered as measures of goodness of solutions , and represented radiation hybrid ( RH ) vector similarities , costs of Hamiltonian cycles , and penalty costs for landing an aircraft before and after target landing times . This method , tested by solving in tandem with deterministic algorithms , has been shown to find quality solutions in finding the minima RH vector similarities of genomic data , minima costs in Hamiltonian cycles of the traveling salesman , and minima costs for landing aircrafts before or after target landing times .
Query containment and query answering are two important computational tasks in databases . While query answering amounts to compute the result of a query over a database , query containment is the problem of checking whether for every database , the result of one query is a subset of the result of another query . In this paper , we deal with unions of conjunctive queries , and we address query containment and query answering under Description Logic constraints . Every such constraint is essentially an inclusion dependencies between concepts and relations , and their expressive power is due to the possibility of using complex expressions , e . g . , intersection and difference of relations , special forms of quantification , regular expressions over binary relations , in the specification of the dependencies . These types of constraints capture a great variety of data models , including the relational , the entity-relationship , and the object-oriented model , all extended with various forms of constraints , and also the basic features of the ontology languages used in the context of the Semantic Web . We present the following results on both query containment and query answering . We provide a method for query containment under Description Logic constraints , thus showing that the problem is decidable , and analyze its computational complexity . We prove that query containment is undecidable in the case where we allow inequalities in the right-hand side query , even for very simple constraints and queries . We show that query answering under Description Logic constraints can be reduced to query containment , and illustrate how such a reduction provides upper bound results with respect to both combined and data complexity .
General predictive models do not provide a measure of confidence in predictions without Bayesian assumptions . A way to circumvent potential restrictions is to use conformal methods for constructing non-parametric confidence regions , that offer guarantees regarding validity . In this paper we provide a detailed description of a computationally efficient conformal procedure for Kernel Ridge Regression ( KRR ) , and conduct a comparative numerical study to see how well conformal regions perform against the Bayesian confidence sets . The results suggest that conformalized KRR can yield predictive confidence regions with specified coverage rate , which is essential in constructing anomaly detection systems based on predictive models .
This paper looks in detail at how an evolutionary algorithm attempts to solve instances from the multimodal problem generator . The paper shows that in order to consistently reach the global optimum , an evolutionary algorithm requires a population size that should grow at least linearly with the number of peaks . It is also shown a close relationship between the supply and decision making issues that have been identified previously in the context of population sizing models for additively decomposable problems . The most important result of the paper , however , is that solving an instance of the multimodal problem generator is like solving a peak-in-a-haystack , and it is argued that evolutionary algorithms are not the best algorithms for such a task . Finally , and as opposed to what several researchers have been doing , it is our strong belief that the multimodal problem generator is not adequate for assessing the performance of evolutionary algorithms .
Obesity is one of the major health risk factors be- hind the rise of non-communicable conditions . Understanding the factors influencing obesity is very complex since there are many variables that can affect the health behaviors leading to it . Nowadays , multiple data sources can be used to study health behaviors , such as wearable sensors for physical activity and sleep , social media , mobile and health data . In this paper we describe the design of a dashboard for the visualization of actigraphy and biometric data from a childhood obesity camp in Qatar . This dashboard allows quantitative discoveries that can be used to guide patient behavior and orient qualitative research .
Tabular notations , in particular SCR specifications , have proved to be a useful means for formally describing complex requirements . The SCR method offers a powerful family of analysis tools , known as the SCR Toolset , but its availability is restricted by the Naval Research Laboratory of the USA . This toolset applies different kinds of analysis considering the whole set of behaviours associated with a requirements specification . In this paper we present a tool for describing and analyzing SCR requirements descriptions , that complements the SCR Toolset in two aspects . First , its use is not limited by any institution , and resorts to a standard model checking tool for analysis ; and second , it allows to concentrate the analysis to particular sets of behaviours ( subsets of the whole specifications ) , that correspond to particular scenarios explicitly mentioned in the specification . We take an operational notation that allows the engineer to describe behavioural " scenarios " by means of programs , and provide a translation into Promela to perform the analysis via Spin , an efficient off-the-shelf model checker freely available . In addition , we apply the SCR method to a Pacemaker system and we use its tabular specification as a running example of this article .
Count data , for example the number of observed cases of a disease in a city , often arise in the fields of healthcare analytics and epidemiology . In this paper , we consider performing regression on multivariate data in which our outcome is a count . Specifically , we derive log-likelihood functions for finite mixtures of regression models involving counts that come from a Poisson distribution , as well as a negative binomial distribution when the counts are significantly overdispersed . Within our proposed modeling framework , we carry out optimal component selection using the information criteria scores AIC , BIC , CAIC , and ICOMP . We demonstrate applications of our approach on simulated data , as well as on a real data set of HIV cases in Tennessee counties from the year 0000 . Finally , using a genetic algorithm within our framework , we perform variable subset selection to determine the covariates that are most responsible for categorizing Tennessee counties . This leads to some interesting insights into the traits of counties that have high HIV counts .
The exact calculation of network reliability in a probabilistic context has been a long-standing issue of practical importance , but a difficult one , even for planar graphs , with perfect nodes and with edges of identical reliability p . Many approaches ( determination of bounds , sums of disjoint products algorithms , Monte Carlo evaluations , studies of the reliability polynomials , etc . ) can only provide approximations when the network ' s size increases . We consider here a ladder graph of arbitrary size corresponding to real-life network configurations , and give the exact , analytical solutions for the all- and two-terminal reliabilities . These solutions use transfer matrices , in which individual reliabilities of edges and nodes are taken into account . The special case of identical edge and node reliabilities -- p and rho , respectively -- is solved . We show that the zeros of the two-terminal reliability polynomial exhibit structures which differ substantially for seemingly similar networks , and we compare the sensitivity of various edges . We discuss how the present work may be further extended to lead to a catalog of exactly solvable networks in terms of reliability , which could be useful as elementary bricks for a new and improved set of bounds or benchmarks in the general case .
This paper presents a novel theoretical framework for the state space reduction of Kripke structures . We define two equivalence relations , Kripke minimization equivalence ( KME ) and weak Kripke minimization equivalence ( WKME ) . We define the quotient system under these relations and show that these relations are strictly coarser than strong ( bi ) simulation and divergence-sensitive stutter ( bi ) simulation , respectively . We prove that the quotient system obtained under KME and WKME preserves linear-time and stutter-insensitive linear-time properties . Finally , we show that KME is compositional w . r . t . synchronous parallel composition .
Suppose we have data generated according to a multivariate normal distribution with a fixed unknown mean vector that is sparse in the sense of being nearly black . Optimality of Bayes estimates and posterior concentration properties in terms of the minimax risk in the $l_0$ norm corresponding to a very general class of continuous shrinkage priors are studied in this work . The class of priors considered is rich enough to include a great variety of heavy tailed prior distributions , such as , the three parameter beta normal mixtures ( including the horseshoe ) , the generalized double Pareto , the inverse gamma and the normal-exponential-gamma priors . Assuming that the number of non-zero components of the mean vector is known , we show that the Bayes estimators corresponding to this general class of priors attain the minimax risk in the $l_0$ norm ( possibly up to a multiplicative constant ) and the corresponding posterior distributions contract around the true mean vector at the minimax optimal rate for appropriate choice of the global shrinkage parameter . Moreover , we provide conditions for which these posterior distributions contract around the corresponding Bayes estimates at least as fast as the minimax risk in the $l_0$ norm . We also provide a lower bound to the total posterior variance for an important subclass of this general class of shrinkage priors that includes the generalized double Pareto priors with shape parameter $\alpha=0 . 0$ and the three parameter beta normal mixtures with parameters $a=0 . 0$ and $b>0$ ( including the horseshoe ) in particular . The present work is inspired by the recent work of van der Pas et al . ( 0000 ) on the posterior contraction properties of the horseshoe prior under the present set-up . We extend their results for this general class of priors and come up with novel unifying proofs which work for a very broad class of one-group continuous shrinkage priors .
We propose two estimators of a monotone spectral density , that are based on the periodogram . These are the isotonic regression of the periodogram and the isotonic regression of the log-periodogram . We derive pointwise limit distribution results for the proposed estimators for short memory linear processes and long memory Gaussian processes and also that the estimators are rate optimal .
Integrating architectural elements with a modern programming language is essential to ensure a smooth combination of architectural design and programming . In this position statement , we motivate a combination of architectural description for distributed , asynchronously communicating systems and Java as an example for such an integration . The result is an ordinary programming language , that exhibits architecture , data structure and behavior within one view . Mappings or tracing between different views is unnecessary . A prototypical implementation of a compiler demonstrates the possibilities and challenges of architectural programming .
Phone sensors could be useful in assessing changes in gait that occur with alcohol consumption . This study determined ( 0 ) feasibility of collecting gait-related data during drinking occasions in the natural environment , and ( 0 ) how gait-related features measured by phone sensors relate to estimated blood alcohol concentration ( eBAC ) . Ten young adult heavy drinkers were prompted to complete a 0-step gait task every hour from 0pm to 00am over four consecutive weekends . We collected 0-xis accelerometer , gyroscope , and magnetometer data from phone sensors , and computed 00 gait-related features using a sliding window technique . eBAC levels were calculated at each time point based on Ecological Momentary Assessment ( EMA ) of alcohol use . We used an artificial neural network model to analyze associations between sensor features and eBACs in training ( 00% of the data ) and validation and test ( 00% of the data ) datasets . We analyzed 000 data points where both eBAC and gait-related sensor data was captured , either when not drinking ( n=00 ) , while eBAC was ascending ( n=00 ) or eBAC was descending ( n=00 ) . 00 data points were captured at times when the eBAC was greater than the legal limit ( 0 . 00 mg/dl ) . Using a Bayesian regularized neural network , gait-related phone sensor features showed a high correlation with eBAC ( Pearson ' s r > 0 . 0 ) , and >00% of estimated eBAC would fall between -0 . 000 and +0 . 000 of actual eBAC . It is feasible to collect gait-related data from smartphone sensors during drinking occasions in the natural environment . Sensor-based features can be used to infer gait changes associated with elevated blood alcohol content .
Technology is the making , usage and knowledge of tools , techniques , crafts , systems or methods of organization in order to solve a problem or serve some purpose . This is true for humanitarian issues also . Such as the issue of language and its primitive attraction for its native speakers which is visible in the cases of the language spoken at home , outside home , in its choice of newspapers , and TV channels . Everyone finds to accomplish its need by the same way . Example includes the preference of using mobile phones in English . The satisfactory answer to this tendency may be the lack of finding the translations in native language---Bengali terms used in current mobile phones are hard to understand by users . I have investigated various mobile phone models available in Indian market which have lot of problems in Bengali interpretation . I have sort out the root cause of this problem to be the conventional accent understand ability . Depending on this I have created a set of equivalent terms that I hope to be simpler in use . In this paper I have performed experiments to compare the new terms to the available ones . Our findings show that the newly derived terms do better in term of performance than to current ones . It has also been seen that acceptance of Bengali terms in mobile phones might grow if the parameter of simpler and conventional accent understand ability are met while designing .
The adoption of the distributed paradigm has allowed applications to increase their scalability , robustness and fault tolerance , but it has also complicated their structure , leading to an exponential growth of the applications ' configuration space and increased difficulty in predicting their performance . In this work , we describe a novel , automated profiling methodology that makes no assumptions on application structure . Our approach utilizes oblique Decision Trees in order to recursively partition an application ' s configuration space in disjoint regions , choose a set of representative samples from each subregion according to a defined policy and return a model for the entire space as a composition of linear models over each subregion . An extensive evaluation over real-life applications and synthetic performance functions showcases that our scheme outperforms other state-of-the-art profiling methodologies . It particularly excels at reflecting abnormalities and discontinuities of the performance function , allowing the user to influence the sampling policy based on the modeling accuracy and the space coverage .
Locomotion for legged robots poses considerable challenges when confronted by obstacles and adverse environments . Footstep planners are typically only designed for one mode of locomotion , but traversing unfavorable environments may require several forms of locomotion to be sequenced together , such as walking , crawling , and jumping . Multi-modal motion planners can be used to address some of these problems , but existing implementations tend to be time-consuming and are limited to quasi-static actions . This paper presents a motion planning method to traverse complex environments using multiple categories of continuous actions . To this end , this paper formulates and exploits the Possibility Graph---which uses high-level approximations of constraint manifolds to rapidly explore the " possibility " of actions---to utilize lower-level single-action motion planners more effectively . We show that the Possibility Graph can quickly find routes through several different challenging environments which require various combinations of actions in order to traverse .
Articles in Marketing and choice literatures have demonstrated the need for incorporating person-level heterogeneity into behavioral models ( e . g . , logit models for multiple binary outcomes as studied here ) . However , the logit likelihood extended with a population distribution of heterogeneity doesn ' t yield closed-form inferences , and therefore numerical integration techniques are relied upon ( e . g . , MCMC methods ) . We present here an alternative , closed-form Bayesian inferences for the logit model , which we obtain by approximating the logit likelihood via a polynomial expansion , and then positing a distribution of heterogeneity from a flexible family that is now conjugate and integrable . For problems where the response coefficients are independent , choosing the Gamma distribution leads to rapidly convergent closed-form expansions ; if there are correlations among the coefficients one can still obtain rapidly convergent closed-form expansions by positing a distribution of heterogeneity from a Multivariate Gamma distribution . The solution then comes from the moment generating function of the Multivariate Gamma distribution or in general from the multivariate heterogeneity distribution assumed . Closed-form Bayesian inferences , derivatives ( useful for elasticity calculations ) , population distribution parameter estimates ( useful for summarization ) and starting values ( useful for complicated algorithms ) are hence directly available . Two simulation studies demonstrate the efficacy of our approach .
This paper gives processor-allocation algorithms for minimizing the average number of communication hops between the assigned processors for grid architectures , in the presence of occupied cells . The simpler problem of assigning processors on a free grid has been studied by Karp , McKellar , and Wong who show that the solutions have nontrivial structure ; they left open the complexity of the problem . The associated clustering problem is as follows : Given n points in Re^d , find k points that minimize their average pairwise L0 distance . We present a natural approximation algorithm and show that it is a 0/0-approximation for 0D grids . For d-dimensional space , the approximation guarantee is 0- ( 0/0d ) , which is tight . We also give a polynomial-time approximation scheme ( PTAS ) for constant dimension d , and report on experimental results .
Many sparse linear discriminant analysis ( LDA ) methods have been proposed to overcome the major problems of the classic LDA in high-dimensional settings . However , the asymptotic optimality results are limited to the case that there are only two classes , which is due to the fact that the classification boundary of LDA is a hyperplane and explicit formulas exist for the classification error in this case . In the situation where there are more than two classes , the classification boundary is usually complicated and no explicit formulas for the classification errors exist . In this paper , we consider the asymptotic optimality in the high-dimensional settings for a large family of linear classification rules with arbitrary number of classes under the situation of multivariate normal distribution . Our main theorem provides easy-to-check criteria for the asymptotic optimality of a general classification rule in this family as dimensionality and sample size both go to infinity and the number of classes is arbitrary . We establish the corresponding convergence rates . The general theory is applied to the classic LDA and the extensions of two recently proposed sparse LDA methods to obtain the asymptotic optimality . We conduct simulation studies on the extended methods in various settings .
Universities conduct examinations to evaluate acquired skills and knowledge gained by students . An assessment of skills and knowledge levels evaluated during Software Engineering examinations is presented in this paper . The question items asked during examinations are analyzed from three dimensions that are cognitive levels , knowledge levels and knowledge areas . The Revised Bloom ' s Taxonomy is used to classify question items along the dimensions of cognitive levels and knowledge levels . Question items are also classified in various knowledge areas specified in ACM/IEEE ' s Computer Science Curricula . The analysis presented in this paper will be useful for software engineering educators to devise corrective interventions and employers of fresh graduates to design pre-induction training programs .
In this paper we show the results of our studies carried out in the framework of the European Project SciCafe0 . 0 in the area of Participatory Engagement models . We present a methodological approach built on participative engagements models and holistic framework for problem situation clarification and solution impacts assessment . Several online platforms for social engagement have been analysed to extract the main patterns of participative engagement . We present our own experiments through the SciCafe0 . 0 Platform and our insights from requirements elicitation .
Estimation of the skeleton of a directed acyclic graph ( DAG ) is of great importance for understanding the underlying DAG and causaleffects can be assessed from the skeleton when the DAG is notidentifiable . We propose a novel method named PenPC toestimate the skeleton of a high-dimensional DAG by a two-stepapproach . We first estimate the non-zero entries of a concentrationmatrix using penalized regression , and then fix the differencebetween the concentration matrix and the skeleton by evaluating aset of conditional independence hypotheses . For high dimensionalproblems where the number of vertices $p$ is in polynomial orexponential scale of sample size $n$ , we study the asymptoticproperty of PenPC on two types of graphs : traditionalrandom graphs where all the vertices have the same expected numberof neighbors , and scale-free graphs where a few vertices may have alarge number of neighbors . As illustrated by extensive simulationsand applications on gene expression data of cancer patients , PenPChas higher sensitivity and specificity than the standard-of-the-artmethod , the PC-stable algorithm .
Central to robot exploration and mapping is the task of persistent localization in environmental fields characterized by spatially correlated measurements . This paper presents a Gaussian process localization ( GP-Localize ) algorithm that , in contrast to existing works , can exploit the spatially correlated field measurements taken during a robot ' s exploration ( instead of relying on prior training data ) for efficiently and scalably learning the GP observation model online through our proposed novel online sparse GP . As a result , GP-Localize is capable of achieving constant time and memory ( i . e . , independent of the size of the data ) per filtering step , which demonstrates the practical feasibility of using GPs for persistent robot localization and autonomy . Empirical evaluation via simulated experiments with real-world datasets and a real robot experiment shows that GP-Localize outperforms existing GP localization algorithms .
Safety and efficiency applications in vehicular networks rely on the exchange of periodic messages between vehicles . These messages contain position , speed , heading , and other vital information that makes the vehicles aware of their surroundings . The drawback of exchanging periodic cooperative messages is that they generate significant channel load . Decentralized Congestion Control ( DCC ) algorithms have been proposed to minimize the channel load . However , while the rationale for periodic message exchange is to improve awareness , existing DCC algorithms do not use awareness as a metric for deciding when , at what power , and at what rate the periodic messages need to be sent in order to make sure all vehicles are informed . We propose an environment- and context-aware DCC algorithm combines power and rate control in order to improve cooperative awareness by adapting to both specific propagation environments ( e . g . , urban intersections , open highways , suburban roads ) as well as application requirements ( e . g . , different target cooperative awareness range ) . Studying various operational conditions ( e . g . , speed , direction , and application requirement ) , ECPR adjusts the transmit power of the messages in order to reach the desired awareness ratio at the target distance while at the same time controlling the channel load using an adaptive rate control algorithm . By performing extensive simulations , including realistic propagation as well as environment modeling and realistic vehicle operational environments ( varying demand on both awareness range and rate ) , we show that ECPR can increase awareness by 00% while keeping the channel load and interference at almost the same level . When permitted by the awareness requirements , ECPR can improve the average message rate by 00% compared to algorithms that perform rate adaptation only .
We present a novel , scalable and Bayesian approach to modelling the occurrence of pairs of symbols ( i , j ) drawn from a large vocabulary . Observed pairs are assumed to be generated by a simple popularity based selection process followed by censoring using a preference function . By basing inference on the well-founded principle of variational bounding , and using new site-independent bounds , we show how a scalable inference procedure can be obtained for large data sets . State of the art results are presented on real-world movie viewing data .
We present ASYMP , a distributed graph processing system developed for the timely analysis of graphs with trillions of edges . ASYMP has several distinguishing features including a robust fault tolerance mechanism , a lockless architecture which scales seamlessly to thousands of machines , and efficient data access patterns to reduce per-machine overhead . ASYMP is used to analyze the largest graphs at Google , and the graphs we consider in our empirical evaluation here are , to the best of our knowledge , the largest considered in the literature . Our experimental results show that compared to previous graph processing frameworks at Google , ASYMP can scale to larger graphs , operate on more crowded clusters , and complete real-world graph mining analytic tasks faster . First , we evaluate the speed of ASYMP , where we show that across a diverse selection of graphs , it runs Connected Component 0-00x faster than state of the art implementations in MapReduce and Pregel . Then we demonstrate the scalability and parallelism of this framework : first by showing that the running time increases linearly by increasing the size of the graphs ( without changing the number of machines ) , and then by showing the gains in running time while increasing the number of machines . Finally , we demonstrate the fault-tolerance properties for the framework , showing that inducing 00% of our machines to fail increases the running time by only 00% .
Digital multimedia watermarking technology had suggested in the last decade to embed copyright information in digital objects such as images , audio and video . However , the increasing use of relational database systems in many real-life applications created an ever-increasing need for watermarking database systems . As a result , watermarking relational database system is now emerging as a research area that deals with the legal issue of copyright protection of database systems . The main goal of database watermarking is to generate robust and impersistent watermark for database . In this paper we propose a method , based on image as watermark and this watermark is embedded over the database at two different attribute of tuple , one in the numeric attribute of tuple and another in the date attribute ' s time ( seconds ) field . Our approach can be applied for numerical and categorical database .
Recent work has shown that dopamine-modulated STDP can solve many of the issues associated with reinforcement learning , such as the distal reward problem . Spiking neural networks provide a useful technique in implementing reinforcement learning in an embodied context as they can deal with continuous parameter spaces and as such are better at generalizing the correct behaviour to perform in a given context . In this project we implement a version of DA-modulated STDP in an embodied robot on a food foraging task . Through simulated dopaminergic neurons we show how the robot is able to learn a sequence of behaviours in order to achieve a food reward . In tests the robot was able to learn food-attraction behaviour , and subsequently unlearn this behaviour when the environment changed , in all 00 trials . Moreover we show that the robot is able to operate in an environment whereby the optimal behaviour changes rapidly and so the agent must constantly relearn . In a more complex environment , consisting of food-containers , the robot was able to learn food-container attraction in 00% of trials , despite the large temporal distance between the correct behaviour and the reward . This is achieved by shifting the dopamine response from the primary stimulus ( food ) to the secondary stimulus ( food-container ) . Our work provides insights into the reasons behind some observed biological phenomena , such as the bursting behaviour observed in dopaminergic neurons . As well as demonstrating how spiking neural network controlled robots are able to solve a range of reinforcement learning tasks .
This paper addresses information-based sensing point selection from a set of possible sensing locations , which determines a set of measurement points maximizing the mutual information between the sensor measurements and the variables of interest . A potential game approach has been applied to addressing distributed implementation of decision making for cooperative sensor planning . When a sensor network involves a large number of sensing agents , the local utility function for a sensing agent is hard to compute , because the local utility function depends on the other agents ' decisions while each sensing agent is inherently faced with limitations in both its communication and computational capabilities . Accordingly , a local utility function for each agent should be approximated to accommodate limitations in information gathering and processing . We propose an approximation method for a local utility function using only a portion of the decisions of other agents . The part of the decisions that each agent considers is called the neighboring set for the agent . The error induced by the approximation is also analyzed , and to keep the error small we propose a neighbor selection algorithm that chooses the neighbor set for each agent in a greedy way . The selection algorithm is based on the correlation information between one agent ' s measurement selection and the other agents ' selections . Futhermore , we show that a game with an approximate local utility function has an $\epsilon$-equilibrium and the set of the equilibria include the Nash equilibrium of the original potential game . We demonstrate the validity of our approximation method through two numerical examples on simplified weather forecasting and multi-target tracking .
Modern software systems are built to be used in dynamic environments using configuration capabilities to adapt to changes and external uncertainties . In a self-adaptation context , we are often interested in reasoning about the performance of the systems under different configurations . Usually , we learn a black-box model based on real measurements to predict the performance of the system given a specific configuration . However , as modern systems become more complex , there are many configuration parameters that may interact and we end up learning an exponentially large configuration space . Naturally , this does not scale when relying on real measurements in the actual changing environment . We propose a different solution : Instead of taking the measurements from the real system , we learn the model using samples from other sources , such as simulators that approximate performance of the real system at low cost . We define a cost model that transform the traditional view of model learning into a multi-objective problem that not only takes into account model accuracy but also measurements effort as well . We evaluate our cost-aware transfer learning solution using real-world configurable software including ( i ) a robotic system , ( ii ) 0 different stream processing applications , and ( iii ) a NoSQL database system . The experimental results demonstrate that our approach can achieve ( a ) a high prediction accuracy , as well as ( b ) a high model reliability .
Collaborative robots could transform several industries , such as manufacturing and healthcare , but they present a significant challenge to verification . The complex nature of their working environment necessitates testing in realistic detail under a broad range of circumstances . We propose the use of Coverage-Driven Verification ( CDV ) to meet this challenge . By automating the simulation-based testing process as far as possible , CDV provides an efficient route to coverage closure . We discuss the need , practical considerations , and potential benefits of transferring this approach from microelectronic design verification to the field of human-robot interaction . We demonstrate the validity and feasibility of the proposed approach by constructing a custom CDV testbench and applying it to the verification of an object handover task .
The intensity of a Gibbs point process is usually an intractable function of the model parameters . For repulsive pairwise interaction point processes , this intensity can be expressed as the Laplace transform of some particular function . Baddeley and Nair ( 0000 ) developped the Poisson-saddlepoint approximation which consists , for basic models , in calculating this Laplace transform with respect to a homogeneous Poisson point process . In this paper , we develop an approximation which consists in calculating the same Laplace transform with respect to a specific determinantal point process . This new approximation is efficiently implemented and turns out to be more accurate than the Poisson-saddlepoint approximation , as demonstrated by some numerical examples .
The impact of social media and its growing association with the sharing of ideas and propagation of messages remains vital in everyday communication . Twitter is one effective platform for the dissemination of news and stories about recent events happening around the world . It has a continually growing database currently adopted by over 000 million users . In this paper we propose a novel grid-based approach employing supervised Multinomial Naive Bayes while extracting geographic entities from relevant user descriptions metadata which gives a spatial indication of the user location . To the best of our knowledge our approach is the first to make location inference from tweets using geo-enriched grid-based classification . Our approach performs better than existing baselines achieving more than 00% accuracy at city-level granularity . In addition we present a novel framework for content-based estimation of user locations by specifying levels of granularity required in pre-defined location grids .
We analyze stochastic gradient descent for optimizing non-convex functions . In many cases for non-convex functions the goal is to find a reasonable local minimum , and the main concern is that gradient updates are trapped in saddle points . In this paper we identify strict saddle property for non-convex problem that allows for efficient optimization . Using this property we show that stochastic gradient descent converges to a local minimum in a polynomial number of iterations . To the best of our knowledge this is the first work that gives global convergence guarantees for stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points . Our analysis can be applied to orthogonal tensor decomposition , which is widely used in learning a rich class of latent variable models . We propose a new optimization formulation for the tensor decomposition problem that has strict saddle property . As a result we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee .
Since its first publication in 0000 , the Gene Set Enrichment Analysis ( GSEA ) method , based on the Kolmogorov-Smirnov statistic , has been heavily used , modified , and also questioned . Recently a simplified approach , using a one sample t test score to assess enrichment and ignoring gene-gene correlations was proposed by Irizarry et al . 0000 as a serious contender . The argument criticizes GSEA ' s nonparametric nature and its use of an empirical null distribution as unnecessary and hard to compute . We refute these claims by careful consideration of the assumptions of the simplified method and its results , including a comparison with GSEA ' s on a large benchmark set of 00 datasets . Our results provide strong empirical evidence that gene-gene correlations cannot be ignored due to the significant variance inflation they produced on the enrichment scores and should be taken into account when estimating gene set enrichment significance . In addition , we discuss the challenges that the complex correlation structure and multi-modality of gene sets pose more generally for gene set enrichment methods .
An important statistical task in disease mapping problems is to identify out- lier/divergent regions with unusually high or low residual risk of disease . Leave-one-out cross-validatory ( LOOCV ) model assessment is a gold standard for computing predictive p-value that can flag such outliers . However , actual LOOCV is time-consuming because one needs to re-simulate a Markov chain for each posterior distribution in which an observation is held out as a test case . This paper introduces a new method , called iIS , for approximating LOOCV with only Markov chain samples simulated from a posterior based on a full data set . iIS is based on importance sampling ( IS ) . iIS integrates the p-value and the likelihood of the test observation with respect to the distribution of the latent variable without reference to the actual observation . The predictive p-values computed with iIS can be proved to be equivalent to the LOOCV predictive p-values , following the general theory for IS . We com- pare iIS and other three existing methods in the literature with a lip cancer dataset collected in Scotland . Our empirical results show that iIS provides predictive p-values that are al- most identical to the actual LOOCV predictive p-values and outperforms the existing three methods , including the recently proposed ghosting method by Marshall and Spiegelhalter ( 0000 ) .
The cyclic block coordinate descent-type ( CBCD-type ) methods , which performs iterative updates for a few coordinates ( a block ) simultaneously throughout the procedure , have shown remarkable computational performance for solving strongly convex minimization problems . Typical applications include many popular statistical machine learning methods such as elastic-net regression , ridge penalized logistic regression , and sparse additive regression . Existing optimization literature has shown that for strongly convex minimization , the CBCD-type methods attain iteration complexity of $\mathcal{O} ( p\log ( 0/\epsilon ) ) $ , where $\epsilon$ is a pre-specified accuracy of the objective value , and $p$ is the number of blocks . However , such iteration complexity explicitly depends on $p$ , and therefore is at least $p$ times worse than the complexity $\mathcal{O} ( \log ( 0/\epsilon ) ) $ of gradient descent ( GD ) methods . To bridge this theoretical gap , we propose an improved convergence analysis for the CBCD-type methods . In particular , we first show that for a family of quadratic minimization problems , the iteration complexity $\mathcal{O} ( \log^0 ( p ) \cdot\log ( 0/\epsilon ) ) $ of the CBCD-type methods matches that of the GD methods in term of dependency on $p$ , up to a $\log^0 p$ factor . Thus our complexity bounds are sharper than the existing bounds by at least a factor of $p/\log^0 ( p ) $ . We also provide a lower bound to confirm that our improved complexity bounds are tight ( up to a $\log^0 ( p ) $ factor ) , under the assumption that the largest and smallest eigenvalues of the Hessian matrix do not scale with $p$ . Finally , we generalize our analysis to other strongly convex minimization problems beyond quadratic ones .
We study a network congestion game of discrete-time dynamic traffic of atomic agents with a single origin-destination pair . Any agent freely makes a dynamic decision at each vertex ( e . g . , road crossing ) and traffic is regulated with given priorities on edges ( e . g . , road segments ) . We first constructively prove that there always exists a subgame perfect equilibrium ( SPE ) in this game . We then study the relationship between this model and a simplified model , in which agents select and fix an origin-destination path simultaneously . We show that the set of Nash equilibrium ( NE ) flows of the simplified model is a proper subset of the set of SPE flows of our main model . We prove that each NE is also a strong NE and hence weakly Pareto optimal . We establish several other nice properties of NE flows , including global First-In-First-Out . Then for two classes of networks , including series-parallel ones , we show that the queue lengths at equilibrium are bounded at any given instance , which means the price of anarchy of any given game instance is bounded , provided that the inflow size never exceeds the network capacity .
Box-Cox power transformation is a commonly used methodology to transform the distribution of a non-normal data into a normal one . Estimation of the transformation parameter is crucial in this methodology . In this study , the estimation process is hold via a searching algorithm and is integrated into well-known seven goodness of fit tests for normal distribution . An artificial covariate method is also included for comparative purposes . Simulation studies are implemented to compare the effectiveness of the proposed methods . The methods are also illustrated on two different real life data applications . Moreover , an R package AID is proposed for implementation .
Current generation solid-state storage devices are exposing a new bottlenecks in the SCSI and block layers of the Linux kernel , where IO throughput is limited by lock contention , inefficient interrupt handling , and poor memory locality . To address these limitations , the Linux kernel block layer underwent a major rewrite with the blk-mq project to move from a single request queue to a multi-queue model . The Linux SCSI subsystem rework to make use of this new model , known as scsi-mq , has been merged into the Linux kernel and work is underway for dm-multipath support in the upcoming Linux 0 . 0 kernel . These pieces were necessary to make use of the multi-queue block layer in a Lustre parallel filesystem with high availability requirements . We undertook adding support of the 0 . 00 kernel to Lustre with scsi-mq and dm-multipath patches to evaluate the potential of these efficiency improvements . In this paper we evaluate the block-level performance of scsi-mq with backing storage hardware representative of a HPC-targerted Lustre filesystem . Our findings show that SCSI write request latency is reduced by as much as 00 . 0% . Additionally , when profiling the CPU usage of our prototype Lustre filesystem , we found that CPU idle time increased by a factor of 0 with Linux 0 . 00 and blk-mq as compared to a standard 0 . 0 . 00 Linux kernel . Our findings demonstrate increased efficiency of the multi-queue block layer even with disk-based caching storage arrays used in existing parallel filesystems .
This paper addresses the problem of infants ' cry fundamental frequency estimation . The fundamental frequency is estimated using a modified simple inverse filtering tracking ( SIFT ) algorithm . The performance of the modified SIFT is studied using a real database of infants ' cry . It is shown that the algorithm is capable of overcoming the problem of under-estimation and over-estimation of the cry fundamental frequency , with an estimation accuracy of 0 . 00% and 0 . 00% , for hyperphonated and phonated cry segments , respectively . Some typical examples of the fundamental frequency contour in typical cases of pathological and healthy cry signals are presented and discussed .
We consider the incorporation of causal knowledge about the presence or absence of ( possibly indirect ) causal relations into a causal model . Such causal relations correspond to directed paths in a causal model . This type of knowledge naturally arises from experimental data , among others . Specifically , we consider the formalisms of Causal Bayesian Networks and Maximal Ancestral Graphs and their Markov equivalence classes : Partially Directed Acyclic Graphs and Partially Oriented Ancestral Graphs . We introduce sound and complete procedures which are able to incorporate causal prior knowledge in such models . In simulated experiments , we show that often considering even a few causal facts leads to a significant number of new inferences . In a case study , we also show how to use real experimental data to infer causal knowledge and incorporate it into a real biological causal network . The code is available at mensxmachina . org .
This article considers stochastic algorithms for efficiently solving a class of large scale non-linear least squares ( NLS ) problems which frequently arise in applications . We propose eight variants of a practical randomized algorithm where the uncertainties in the major stochastic steps are quantified . Such stochastic steps involve approximating the NLS objective function using Monte-Carlo methods , and this is equivalent to the estimation of the trace of corresponding symmetric positive semi-definite ( SPSD ) matrices . For the latter , we prove tight necessary and sufficient conditions on the sample size ( which translates to cost ) to satisfy the prescribed probabilistic accuracy . We show that these conditions are practically computable and yield small sample sizes . They are then incorporated in our stochastic algorithm to quantify the uncertainty in each randomized step . The bounds we use are applications of more general results regarding extremal tail probabilities of linear combinations of gamma distributed random variables . We derive and prove new results concerning the maximal and minimal tail probabilities of such linear combinations , which can be considered independently of the rest of this paper .
We introduce a transformation system for concurrent constraint programming ( CCP ) . We define suitable applicability conditions for the transformations which guarantee that the input/output CCP semantics is preserved also when distinguishing deadlocked computations from successful ones and when considering intermediate results of ( possibly ) non-terminating computations . The system allows us to optimize CCP programs while preserving their intended meaning : In addition to the usual benefits that one has for sequential declarative languages , the transformation of concurrent programs can also lead to the elimination of communication channels and of synchronization points , to the transformation of non-deterministic computations into deterministic ones , and to the crucial saving of computational space . Furthermore , since the transformation system preserves the deadlock behavior of programs , it can be used for proving deadlock freeness of a given program wrt a class of queries . To this aim it is sometimes sufficient to apply our transformations and to specialize the resulting program wrt the given queries in such a way that the obtained program is trivially deadlock free .
We present a robust and real-time monocular six degree of freedom relocalization system . Our system trains a convolutional neural network to regress the 0-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation . The algorithm can operate indoors and outdoors in real time , taking 0ms per frame to compute . It obtains approximately 0m and 0 degree accuracy for large scale outdoor scenes and 0 . 0m and 00 degree accuracy indoors . This is achieved using an efficient 00 layer deep convnet , demonstrating that convnets can be used to solve complicated out of image plane regression problems . This was made possible by leveraging transfer learning from large scale classification data . We show the convnet localizes from high level features and is robust to difficult lighting , motion blur and different camera intrinsics where point based SIFT registration fails . Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples . PoseNet code , dataset and an online demonstration is available on our project webpage , at http : //mi . eng . cam . ac . uk/projects/relocalisation/
Statistical inference about the speciation process has often been based on the isolation-with-migration ( IM ) model , especially when the research aim is to learn about the presence or absence of gene flow during divergence . The generalised IM model introduced in this paper extends both the standard two-population IM model and the isolation-with-initial-migration ( IIM ) model , and encompasses both these models as special cases . It can be described as a two-population IM model in which migration rates and population sizes are allowed to change at some point in the past . By developing a maximum-likelihood implementation of this GIM model , we enable inference on both historical and contemporary rates of gene flow between two closely related species . Our method relies on the spectral decomposition of the coalescent generator matrix and is applicable to data sets consisting of the numbers of nucleotide differences between one pair of DNA sequences at each of a large number of independent loci .
We consider the problem of estimating the mixing density $f$ from $n$ i . i . d . observations distributed according to a mixture density with unknown mixing distribution . In contrast with finite mixtures models , here the distribution of the hidden variable is not bounded to a finite set but is spread out over a given interval . We propose an approach to construct an orthogonal series estimator of the mixing density $f$ involving Legendre polynomials . The construction of the orthonormal sequence varies from one mixture model to another . Minimax upper and lower bounds of the mean integrated squared error are provided which apply in various contexts . In the specific case of exponential mixtures , it is shown that the estimator is adaptive over a collection of specific smoothness classes , more precisely , there exists a constant $A\textgreater{}0$ such that , when the order $m$ of the projection estimator verifies $m\sim A \log ( n ) $ , the estimator achieves the minimax rate over this collection . Other cases are investigated such as Gamma shape mixtures and scale mixtures of compactly supported densities including Beta mixtures . Finally , a consistent estimator of the support of the mixing density $f$ is provided .
Many of you reading these words will have been attracted by the discussion paper [McShane and Wyner ( 0000 ) ] , in which case , this may be the first , but hopefully not the last , time you will have read anything in a statistics journal . I would like to take this opportunity to discuss the review process in our journal and to make some comments about the role of statistics and uncertainty assessment in paleoclimatology and the broader debate about climate change .
Data mining has traditionally focused on the task of drawing inferences from large datasets . However , many scientific and engineering domains , such as fluid dynamics and aircraft design , are characterized by scarce data , due to the expense and complexity of associated experiments and simulations . In such data-scarce domains , it is advantageous to focus the data collection effort on only those regions deemed most important to support a particular data mining objective . This paper describes a mechanism that interleaves bottom-up data mining , to uncover multi-level structures in spatial data , with top-down sampling , to clarify difficult decisions in the mining process . The mechanism exploits relevant physical properties , such as continuity , correspondence , and locality , in a unified framework . This leads to effective mining and sampling decisions that are explainable in terms of domain knowledge and data characteristics . This approach is demonstrated in two diverse applications -- mining pockets in spatial data , and qualitative determination of Jordan forms of matrices .
This monograph presents a class of algorithms called coordinate descent algorithms for mathematicians , statisticians , and engineers outside the field of optimization . This particular class of algorithms has recently gained popularity due to their effectiveness in solving large-scale optimization problems in machine learning , compressed sensing , image processing , and computational statistics . Coordinate descent algorithms solve optimization problems by successively minimizing along each coordinate or coordinate hyperplane , which is ideal for parallelized and distributed computing . Avoiding detailed technicalities and proofs , this monograph gives relevant theory and examples for practitioners to effectively apply coordinate descent to modern problems in data science and engineering .
Constrained adaptive filtering algorithms inculding constrained least mean square ( CLMS ) , constrained affine projection ( CAP ) and constrained recursive least squares ( CRLS ) have been extensively studied in many applications . Most existing constrained adaptive filtering algorithms are developed under mean square error ( MSE ) criterion , which is an ideal optimality criterion under Gaussian noises . This assumption however fails to model the behavior of non-Gaussian noises found in practice . Motivated by the robustness and simplicity of maximum correntropy criterion ( MCC ) in non-Gaussian impulsive noises , this paper proposes a new adaptive filtering algorithm called constrained maximum correntropy criterion ( CMCC ) . Specifically , CMCC incorporates a linear constraint into a MCC filter to solve a constrained optimization problem explicitly . The proposed adaptive filtering algorithm is easy to implement and has low computational complexity , and in terms of convergence accuracy ( say lower mean square deviation ) and stability , can significantly outperform those MSE based constrained adaptive algorithms in presence of heavy-tailed impulsive noises . Additionally , the mean square convergence behaviors are studied under energy conservation relation , and a sufficient condition to ensure the mean square convergence and the steady-state mean square deviation ( MSD ) of the proposed algorithm are obtained . Simulation results confirm the theoretical predictions under both Gaussian and non- Gaussian noises , and demonstrate the excellent performance of the novel algorithm by comparing it with other conventional methods .
In functional linear regression , the parameters estimation involves solving a non necessarily well-posed problem and it has points of contact with a range of methodologies , including statistical smoothing , deconvolution and projection on finite-dimensional subspaces . We discuss the standard approach based explicitly on functional principal components analysis , nevertheless the choice of the number of basis components remains something subjective and not always properly discussed and justified . In this work we discuss inferential properties of least square estimation in this context with different choices of projection subspaces , as well as we study asymptotic behaviour increasing the dimension of subspaces .
One of the main goals of mathematical modeling in systems medicine related to medical applications is to obtain patient-specific parameterizations and model predictions . In clinical practice , however , the number of available measurements for single patients is usually limited due to time and cost restrictions . This hampers the process of making patient-specific predictions about the outcome of a treatment . On the other hand , data are often available for many patients , in particular if extensive clinical studies have been performed . Therefore , before applying Bayes ' rule \emph{separately} to the data of each patient ( which is typically performed using a non-informative prior ) , it is meaningful to use empirical Bayes methods in order to construct an informative prior from all available data . We compare the performance of four priors -- a non-informative prior and priors chosen by nonparametric maximum likelihood estimation ( NPMLE ) , by maximum penalized likelihood estimation ( MPLE ) and by doubly-smoothed maximum likelihood estimation ( DS-MLE ) -- by applying them to a low-dimensional parameter estimation problem in a toy model as well as to a high-dimensional ODE model of the human menstrual cycle , which represents a typical example from systems biology modeling .
Consider a $N\times n$ random matrix $Z_n= ( Z^n_{j_0 j_0} ) $ where the individual entries are a realization of a properly rescaled stationary gaussian random field . The purpose of this article is to study the limiting empirical distribution of the eigenvalues of Gram random matrices such as $Z_n Z_n ^*$ and $ ( Z_n +A_n ) ( Z_n +A_n ) ^*$ where $A_n$ is a deterministic matrix with appropriate assumptions in the case where $n\to \infty$ and $\frac Nn \to c \in ( 0 , \infty ) $ . The proof relies on related results for matrices with independent but not identically distributed entries and substantially differs from related works in the literature ( Boutet de Monvel et al . , Girko , etc . ) .
During the execution of large scale construction projects performed by Virtual Organizations ( VO ) , relatively complex technical models have to be exchanged between the VO members . For linking the trade and transfer of these models , a so-called multi-model container format was developed . Considering the different skills and tasks of the involved partners , it is not necessary for them to know all the models in every technical detailing . Furthermore , the model size can lead to a delay in communication . In this paper an approach is presented for defining model cut-outs according to the current project context . Dynamic dependencies to the project context as well as static dependencies on the organizational structure are mapped in a context-sensitive rule . As a result , an approach for dynamic filtering of multi-models is obtained which ensures , together with a filtering service , that the involved VO members get a simplified view of complex multi-models as well as sufficient permissions depending on their tasks .
In this contribution we derive an explicit formula for the boundary non-crossing probabilities for Slepian processes associated with the piecewise linear boundary function . This formula is used to develop an approximation formula to the boundary non-crossing probabilities for general continuous boundaries . The formulas we developed are easy to implement in calculation the boundary non-crossing probabilities .
Advertisements ( ads ) often include strongly emotional content to leave a lasting impression on the viewer . This work ( i ) compiles an affective ad dataset capable of evoking coherent emotions across users , as determined from the affective opinions of five experts and 00 annotators ; ( ii ) explores the efficacy of convolutional neural network ( CNN ) features for encoding emotions , and observes that CNN features outperform low-level audio-visual emotion descriptors upon extensive experimentation ; and ( iii ) demonstrates how enhanced affect prediction facilitates computational advertising , and leads to better viewing experience while watching an online video stream embedded with ads based on a study involving 00 users . We model ad emotions based on subjective human opinions as well as objective multimodal features , and show how effectively modeling ad emotions can positively impact a real-life application .
We consider the problem of learning a high-dimensional multi-task regression model , under sparsity constraints induced by presence of grouping structures on the input covariates and on the output predictors . This problem is primarily motivated by expression quantitative trait locus ( eQTL ) mapping , of which the goal is to discover genetic variations in the genome ( inputs ) that influence the expression levels of multiple co-expressed genes ( outputs ) , either epistatically , or pleiotropically , or both . A structured input-output lasso ( SIOL ) model based on an intricate l0/l0-norm penalty over the regression coefficient matrix is employed to enable discovery of complex sparse input/output relationships ; and a highly efficient new optimization algorithm called hierarchical group thresholding ( HiGT ) is developed to solve the resultant non-differentiable , non-separable , and ultra high-dimensional optimization problem . We show on both simulation and on a yeast eQTL dataset that our model leads to significantly better recovery of the structured sparse relationships between the inputs and the outputs , and our algorithm significantly outperforms other optimization techniques under the same model . Additionally , we propose a novel approach for efficiently and effectively detecting input interactions by exploiting the prior knowledge available from biological experiments .
One of the main computational and scientific challenges in the modern age is to extract useful information from unstructured texts . Topic models are one popular machine-learning approach which infers the latent topical structure of a collection of documents . Despite their success --- in particular of its most widely used variant called Latent Dirichlet Allocation ( LDA ) --- and numerous applications in sociology , history , and linguistics , topic models are known to suffer from severe conceptual and practical problems , e . g . a lack of justification for the Bayesian priors , discrepancies with statistical properties of real texts , and the inability to properly choose the number of topics . Here , we approach the problem of identifying topical structures by representing text corpora as bipartite networks of documents and words and using methods from community detection in complex networks , in particular stochastic block models ( SBM ) . We show that our SBM-based approach constitutes a more principled and versatile framework for topic modeling solving the intrinsic limitations of Dirichlet-based models through a more general choice of nonparametric priors . It automatically detects the number of topics and hierarchically clusters both the words and documents . In practice , we demonstrate through the analysis of artificial and real corpora that our approach outperforms LDA in terms of statistical model selection .
The systems supporting signal and image applications process large amount of data . That involves an intensive use of the memory which becomes the bottleneck of systems . Memory limits performances and represents a significant proportion of total consumption . In the development high level synthesis tool called GAUT Low Power , we are interested in the synthesis of the memory unit . In this work , we integrate the data storage and data transfert to constraint the high level synthesis of the datapath ' s execution unit .
In the context of Structural Risk Minimization , one is presented a sequence of classes $\{\mathcal{G}_j\}$ from which , given a random sample $ ( X_i , Y_i ) $ one wants to choose a strongly consistent estimator . For certain types of classes of functions , we present a criterion to choose an estimator , based on the minimization of the sum of empirical error and a complexity penalty $r ( n , j ) $ over each class $\G_j$ . We present also several other results together with important results found on current literature on the subject , in an attempt to present and unify the theory in the context of regression estimation . In particular we present a generalization of the consistency of Structural Risk Minimization in the context of regression estimation which are all new results found on Chapter 0 .
In this article , we discuss the optimal allocation problem in an experiment when a regression model is used for statistical analysis . Monotonic convergence for a general class of multiplicative algorithms for $D$-optimality has been discussed in the literature . Here , we provide an alternate proof of the monotonic convergence for $D$-criterion with a simple computational algorithm and furthermore show it converges to the $D$-optimality . We also discuss an algorithm as well as a conjecture of the monotonic convergence for $A$-criterion . Monte Carlo simulations are used to demonstrate the reliability , efficiency and usefulness of the proposed algorithms .
Learning social media data embedding by deep models has attracted extensive research interest as well as boomed a lot of applications , such as link prediction , classification , and cross-modal search . However , for social images which contain both link information and multimodal contents ( e . g . , text description , and visual content ) , simply employing the embedding learnt from network structure or data content results in sub-optimal social image representation . In this paper , we propose a novel social image embedding approach called Deep Multimodal Attention Networks ( DMAN ) , which employs a deep model to jointly embed multimodal contents and link information . Specifically , to effectively capture the correlations between multimodal contents , we propose a multimodal attention network to encode the fine-granularity relation between image regions and textual words . To leverage the network structure for embedding learning , a novel Siamese-Triplet neural network is proposed to model the links among images . With the joint deep model , the learnt embedding can capture both the multimodal contents and the nonlinear network information . Extensive experiments are conducted to investigate the effectiveness of our approach in the applications of multi-label classification and cross-modal search . Compared to state-of-the-art image embeddings , our proposed DMAN achieves significant improvement in the tasks of multi-label classification and cross-modal search .
We present a customizable soft architecture which allows for the execution of GPGPU code on an FPGA without the need to recompile the design . Issues related to scaling the overlay architecture to multiple GPGPU multiprocessors are considered along with application-class architectural optimizations . The overlay architecture is optimized for FPGA implementation to support efficient use of embedded block memories and DSP blocks . This architecture supports direct CUDA compilation of integer computations to a binary which is executable on the FPGA-based GPGPU . The benefits of our architecture are evaluated for a collection of five standard CUDA benchmarks which are compiled using standard GPGPU compilation tools . Speedups of 00x , on average , versus a MicroBlaze microprocessor are achieved . We show dynamic energy savings versus a soft-core processor of 00% on average . Application-customized versions of the soft GPGPU can be used to further reduce dynamic energy consumption by an average of 00% .
We describe the asymptotic properties of the edge-triangle exponential random graph model as the natural parameters diverge along straight lines . We show that as we continuously vary the slopes of these lines , a typical graph drawn from this model exhibits quantized behavior , jumping from one complete multipartite graph to another , and the jumps happen precisely at the normal lines of a polyhedral set with infinitely many facets . As a result , we provide a complete description of all asymptotic extremal behaviors of the model .
In this paper we investigate lossy channel games under incomplete information , where two players operate on a finite set of unbounded FIFO channels and one player , representing a system component under consideration operates under incomplete information , while the other player , representing the component ' s environment is allowed to lose messages from the channels . We argue that these games are a suitable model for synthesis of communication protocols where processes communicate over unreliable channels . We show that in the case of finite message alphabets , games with safety and reachability winning conditions are decidable and finite-state observation-based strategies for the component can be effectively computed . Undecidability for ( weak ) parity objectives follows from the undecidability of ( weak ) parity perfect information games where only one player can lose messages .
Detection of transitions between broad phonetic classes in a speech signal is an important problem which has applications such as landmark detection and segmentation . The proposed hierarchical method detects silence to non-silence transitions , high amplitude ( mostly sonorants ) to low ampli- tude ( mostly fricatives/affricates/stop bursts ) transitions and vice-versa . A subset of the extremum ( minimum or maximum ) samples between every pair of successive zero-crossings is selected above a second pass threshold , from each bandpass filtered speech signal frame . Relative to the mid-point ( reference ) of a frame , locations of the first and the last extrema lie on either side , if the speech signal belongs to a homogeneous segment ; else , both these locations lie on the left or the right side of the reference , indicating a transition frame . When tested on the entire TIMIT database , of the transitions detected , 00 . 0% are within a tolerance of 00 ms from the hand labeled boundaries . Sonorant , unvoiced non-sonorant and silence classes and their respective onsets are detected with an accuracy of about 00 . 0% for the same tolerance . The results are as good as , and in some respects better than the state-of-the-art methods for similar tasks .
This text presents the research field of natural/unconventional computing as it appears in the book COMPUTING NATURE . The articles discussed consist a selection of works from the Symposium on Natural Computing at AISB-IACAP ( British Society for the Study of Artificial Intelligence and the Simulation of Behaviour and The International Association for Computing and Philosophy ) World Congress 0000 , held at the University of Birmingham , celebrating Turing centenary . The COMPUTING NATURE is about nature considered as the totality of physical existence , the universe . By physical we mean all phenomena , objects and processes , that are possible to detect either directly by our senses or via instruments . Historically , there have been many ways of describing the universe ( cosmic egg , cosmic tree , theistic universe , mechanistic universe ) while a particularly prominent contemporary approach is computational universe , as discussed in this article .
Service Oriented Architectures ( SOAs ) are component-based architectures , characterized by reusability , modularization and composition , usually offered by HTTP ( web services ) and often equipped with a Quality of Services ( QoS ) measure . In order to guarantee the fairness property to each client requesting a service , we propose a fair version of the ( Soft ) Concurrent Constraint language to deal with the negotiation phases of the Service Level Agreement ( SLA ) protocol .
Zika virus ( ZIKV ) , a disease spread primarily through the Aedes aegypti mosquito , was identified in Brazil in 0000 and was declared a global health emergency by the World Health Organization ( WHO ) . Epidemiologists often use common state-level attributes such as population density and temperature to determine the spread of disease . By applying techniques from topological data analysis , we believe that epidemiologists will be able to better predict how ZIKV will spread . We use the Vietoris-Rips filtration on high-density mosquito locations in Brazil to create simplicial complexes , from which we extract homology group generators . Previously epidemiologists have not relied on topological data analysis to model disease spread . Evaluating our model on ZIKV case data in the states of Brazil demonstrates the value of these techniques for the improved assessment of vector-borne diseases .
Existing state-wide data bases on prosecutors ' decisions about juvenile offenders are important , yet often un-explored resources for understanding changes in patterns of judicial decisions over time . We investigate the extent and nature of change in judicial behavior toward juveniles following the enactment of a new set of mandatory registration policies between 0000 and 0000 via analyzing the data on prosecutors ' decisions of moving forward for youths repeatedly charged with sexual violence in South Carolina . To analyze this longitudinal binary data , we use a random effects logistic regression model via incorporating an unknown change-point year . For convenient physical interpretation , our models allow the proportional odds interpretation of effects of the explanatory variables and the change-point year with and without conditioning on the youth-specific random effects . As a consequence , the effects of the unknown change-point year and other factors can be interpreted as changes in both within youth and population averaged odds of moving forward . Using a Bayesian paradigm , we consider various prior opinions about the unknown year of the change in the pattern of prosecutors ' decision . Based on the available data , we make posteriori conclusions about whether a change-point has occurred between 0000 and 0000 ( inclusive ) , evaluate the degree of confidence about the year of change-point , estimate the magnitude of the effects of the change-point and other factors , and investigate other provocative questions about patterns of prosecutors ' decisions over time .
Occupancy models are typically used to determine the probability of a species being present at a given site while accounting for imperfect detection . The survey data underlying these models often include information on several predictors that could potentially characterize habitat suitability and species detectability . Because these variables might not all be relevant , model selection techniques are necessary in this context . In practice , model selection is performed using the Akaike Information Criterion ( AIC ) , as few other alternatives are available . This paper builds an objective Bayesian variable selection framework for occupancy models through the intrinsic prior methodology . The procedure incorporates priors on the model space that account for test multiplicity and respect the polynomial hierarchy of the predictors when higher-order terms are considered . The methodology is implemented using a stochastic search algorithm that is able to thoroughly explore large spaces of occupancy models . The proposed strategy is entirely automatic and provides control of false positives without sacrificing the discovery of truly meaningful covariates . The performance of the method is evaluated and compared to AIC through a simulation study . The method is illustrated on two datasets previously studied in the literature .
The proposed paper discusses the problem of discrimination between close hypotheses about distributions belonging to the Gumbel maximum domain of attraction . The distinctive feature of the proposed work is using only k higher order statistics of the sample in the construction of criteria , because there are many situations when we do not know the whole sample , as it occurs in the problems linked with safety , life span , catastrofes , sea level and other . This work is the first in the series of the author ' s works that deal with the statistics of extrema .
Border inspection , and the challenge of deciding which of the tens of millions of consignments that arrive should be inspected , is a perennial problem for regulatory authorities . The objective of these inspections is to minimise the risk of contraband entering the country . As an example , for regulatory authorities in charge of biosecurity material , consignments of goods are classified before arrival according to their economic tariff number ( Department of Immigration and Border Protection , 0000 ) . This classification , perhaps along with other information , is used as a screening step to determine whether further biosecurity intervention , such as inspection , is necessary . Other information associated with consignments includes details such as the country of origin , supplier , and importer , for example . The choice of which consignments to inspect has typically been informed by historical records of intercepted material . Fortunately for regulators , interception is a rare event , however this sparsity undermines the utility of historical records for deciding which containers to inspect . In this paper we report on an analysis that uses more detailed information to inform inspection . Using quarantine biosecurity as a case study , we create statistical profiles using generalised linear mixed models and compare different model specifications with historical information alone , demonstrating the utility of a statistical modelling approach . We also demonstrate some graphical model summaries that provide managers with insight into pathway governance .
In animal behavioral biology , there are several cases in which an autonomous observing/training system would be useful . 0 ) Observation of certain species continuously , or for documenting specific events , which happen irregularly ; 0 ) Longterm intensive training of animals in preparation for behavioral experiments ; and 0 ) Training and testing of animals without human interference , to eliminate potential cues and biases induced by humans . The primary goal of this study is to build a system named CATOS ( Computer Aided Training/Observing System ) that could be used in the above situations . As a proof of concept , the system was built and tested in a pilot experiment , in which cats were trained to press three buttons differently in response to three different sounds ( human speech ) to receive food rewards . The system was built in use for about 0 months , successfully training two cats . One cat learned to press a particular button , out of three buttons , to obtain the food reward with over 00 percent correctness .
Optimizing the spread of influence in online social networks ( OSNs ) is important for the design of efficient viral marketing strategies using online recommendations . It is commonly believed that , spreading is a global process , whose optimization would require the knowledge of the whole network information . Here we uncover a characteristic local length scale , called influence radius , hidden in the global nature of spreading processes . We show that , any node ' s influence to the entire OSN can be quantified from its local network environment within the influence radius , which is significantly smaller than the whole network diameter . By mapping the problem onto bond percolation , we give a theoretical explanation for the presence of this short influence radius , and a framework to quantify individual ' s influence in real OSNs . We then propose a scalable optimization algorithm to identify the most influential spreaders . The time complexity of our algorithm is independent of network size , and its performance is remarkably close the true optimum . Our method may be applied to other large scale spreading problems , such as the world-wide epidemic control .
In this paper , we present a novel approach to identify feature specific expressions of opinion in product reviews with different features and mixed emotions . The objective is realized by identifying a set of potential features in the review and extracting opinion expressions about those features by exploiting their associations . Capitalizing on the view that more closely associated words come together to express an opinion about a certain feature , dependency parsing is used to identify relations between the opinion expressions . The system learns the set of significant relations to be used by dependency parsing and a threshold parameter which allows us to merge closely associated opinion expressions . The data requirement is minimal as this is a one time learning of the domain independent parameters . The associations are represented in the form of a graph which is partitioned to finally retrieve the opinion expression describing the user specified feature . We show that the system achieves a high accuracy across all domains and performs at par with state-of-the-art systems despite its data limitations .
In Bayesian statistics probability distributions express beliefs . However , for many problems the beliefs cannot be computed analytically and approximations of beliefs are needed . We seek a loss function that quantifies how " embarrassing " it is to communicate a given approximation . We reproduce and discuss an old proof showing that there is only one ranking under the requirements that ( 0 ) the best ranked approximation is the non-approximated belief and ( 0 ) that the ranking judges approximations only by their predictions for actual outcomes . The loss function that is obtained in the derivation is equal to the Kullback-Leibler divergence when normalized . This loss function is frequently used in the literature . However , there seems to be confusion about the correct order in which its functional arguments , the approximated and non-approximated beliefs , should be used . The correct order ensures that the recipient of a communication is only deprived of the minimal amount of information . We hope that the elementary derivation settles the apparent confusion . For example when approximating beliefs with Gaussian distributions the optimal approximation is given by moment matching . This is in contrast to many suggested computational schemes .
We consider estimation of quantile curves for a general class of nonstationary processes . Consistency and central limit results are obtained for local linear quantile estimates under a mild short-range dependence condition . Our results are applied to environmental data sets . In particular , our results can be used to address the problem of whether climate variability has changed , an important problem raised by IPCC ( Intergovernmental Panel on Climate Change ) in 0000 .
Pinboard on Pinterest is an emerging media to engage online social media users , on which users post online images for specific topics . Regardless of its significance , there is little previous work specifically to facilitate information discovery based on pinboards . This paper proposes a novel pinboard recommendation system for Twitter users . In order to associate contents from the two social media platforms , we propose to use MultiLabel classification to map Twitter user followees to pinboard topics and visual diversification to recommend pinboards given user interested topics . A preliminary experiment on a dataset with 0000 users validated our proposed system .
In this paper , we consider the following graph embedding problem : Given a bipartite graph G = ( V0 ; V0 ; E ) , where the maximum degree of vertices in V0 is 0 , can G be embedded on a two dimensional grid such that each vertex in V0 is drawn as a line segment along a grid line , each vertex in V0 is drawn as a point at a grid point , and each edge e = ( u ; v ) for some u 0 V0 and v 0 V0 is drawn as a line segment connecting u and v , perpendicular to the line segment for u ? We show that this problem is NP-complete , and sketch how our proof techniques can be used to show the hardness of several other related problems .
A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way . The standard techniques include $K$-fold cross-validation ( $K$-CV ) , Akaike information criterion ( AIC ) , and Bayesian information criterion ( BIC ) . Though these methods work well for low-dimensional problems , they are not suitable in high dimensional settings . In this paper , we present StARS : a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs . The method has a clear interpretation : we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling . This interpretation requires essentially no conditions . Under mild conditions , we show that StARS is partially sparsistent in terms of graph estimation : i . e . with high probability , all the true edges will be included in the selected model even when the graph size diverges with the sample size . Empirically , the performance of StARS is compared with the state-of-the-art model selection procedures , including $K$-CV , AIC , and BIC , on both synthetic data and a real microarray dataset . StARS outperforms all these competing procedures .
We describe ASAGA , an asynchronous parallel version of the incremental gradient algorithm SAGA that enjoys fast linear convergence rates . Through a novel perspective , we revisit and clarify a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms , and propose a simplification of the recently introduced " perturbed iterate " framework that resolves it . We thereby prove that ASAGA can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions . We present results of an implementation on a 00-core architecture illustrating the practical speedup as well as the hardware overhead .
Nowadays , vehicle safety is constantly increasing thanks to the improvement of vehicle passive and active safety . However , on a daily usage of the car , traffic jams remains a problem . With limited space for road infrastructure , automation of the driving task on specific situation seems to be a possible solution . The French project ABV , which stands for low speed automation , tries to demonstrate the feasibility of the concept and to prove the benefits . In this article , we describe the scientific background of the project and expected outputs .
This paper is devoted to the non-asymptotic control of the mean-squared error for the Ruppert-Polyak stochastic averaged gradient descent introduced in the seminal contributions of [Rup00] and [PJ00] . In our main results , we establish non-asymptotic tight bounds ( optimal with respect to the Cramer-Rao lower bound ) in a very general framework that includes the uniformly strongly convex case as well as the one where the function f to be minimized satisfies a weaker Kurdyka-Lojiasewicz-type condition [Loj00 , Kur00] . In particular , it makes it possible to recover some pathological examples such as on-line learning for logistic regression ( see [Bac00] ) and recursive quan- tile estimation ( an even non-convex situation ) .
We study the inefficiency of equilibria for various classes of games when players are ( partially ) altruistic . We model altruistic behavior by assuming that player i ' s perceived cost is a convex combination of 0-\alpha_i times his direct cost and \alpha_i times the social cost . Tuning the parameters \alpha_i allows smooth interpolation between purely selfish and purely altruistic behavior . Within this framework , we study altruistic extensions of linear congestion games , fair cost-sharing games and valid utility games . We derive ( tight ) bounds on the price of anarchy of these games for several solution concepts . Thereto , we suitably adapt the smoothness notion introduced by Roughgarden and show that it captures the essential properties to determine the robust price of anarchy of these games . Our bounds show that for congestion games and cost-sharing games , the worst-case robust price of anarchy increases with increasing altruism , while for valid utility games , it remains constant and is not affected by altruism . However , the increase in the price of anarchy is not a universal phenomenon : for symmetric singleton linear congestion games , we derive a bound on the pure price of anarchy that decreases as the level of altruism increases . Since the bound is also strictly lower than the robust price of anarchy , it exhibits a natural example in which Nash equilibria are more efficient than more permissive notions of equilibrium .
We propose a nonparametric method to explicitly model and represent the derivatives of smooth underlying trajectories for longitudinal data . This representation is based on a direct Karhunen--Lo\`eve expansion of the unobserved derivatives and leads to the notion of derivative principal component analysis , which complements functional principal component analysis , one of the most popular tools of functional data analysis . The proposed derivative principal component scores can be obtained for irregularly spaced and sparsely observed longitudinal data , as typically encountered in biomedical studies , as well as for functional data which are densely measured . Novel consistency results and asymptotic convergence rates for the proposed estimates of the derivative principal component scores and other components of the model are derived under a unified scheme for sparse or dense observations and mild conditions . We compare the proposed representations for derivatives with alternative approaches in simulation settings and also in a wallaby growth curve application . It emerges that representations using the proposed derivative principal component analysis recover the underlying derivatives more accurately compared to principal component analysis-based approaches especially in settings where the functional data are represented with only a very small number of components or are densely sampled . In a second wheat spectra classification example , derivative principal component scores were found to be more predictive for the protein content of wheat than the conventional functional principal component scores .
The interdiction problem arises in a variety of areas including military logistics , infectious disease control , and counter-terrorism . In the typical formulation of network interdiction , the task of the interdictor is to find a set of edges in a weighted network such that the removal of those edges would maximally increase the cost to an evader of traveling on a path through the network . Our work is motivated by cases in which the evader has incomplete information about the network or lacks planning time or computational power , e . g . when authorities set up roadblocks to catch bank robbers , the criminals do not know all the roadblock locations or the best path to use for their escape . We introduce a model of network interdiction in which the motion of one or more evaders is described by Markov processes and the evaders are assumed not to react to interdiction decisions . The interdiction objective is to find an edge set of size B , that maximizes the probability of capturing the evaders . We prove that similar to the standard least-cost formulation for deterministic motion this interdiction problem is also NP-hard . But unlike that problem our interdiction problem is submodular and the optimal solution can be approximated within 0-0/e using a greedy algorithm . Additionally , we exploit submodularity through a priority evaluation strategy that eliminates the linear complexity scaling in the number of network edges and speeds up the solution by orders of magnitude . Taken together the results bring closer the goal of finding realistic solutions to the interdiction problem on global-scale networks .
Here we describe the SHARE system , a web service based framework for distributed querying and reasoning on the semantic web . The main innovations of SHARE are : ( 0 ) the extension of a SPARQL query engine to perform on-demand data retrieval from web services , and ( 0 ) the extension of an OWL reasoner to test property restrictions by means of web service invocations . In addition to enabling queries across distributed datasets , the system allows for a target dataset that is significantly larger than is possible under current , centralized approaches . Although the architecture is equally applicable to all types of data , the SHARE system targets bioinformatics , due to the large number of interoperable web services that are already available in this area . SHARE is built entirely on semantic web standards , and is the successor of the BioMOBY project .
This paper is concerned with the problem of top-$K$ ranking from pairwise comparisons . Given a collection of $n$ items and a few pairwise binary comparisons across them , one wishes to identify the set of $K$ items that receive the highest ranks . To tackle this problem , we adopt the logistic parametric model---the Bradley-Terry-Luce model , where each item is assigned a latent preference score , and where the outcome of each pairwise comparison depends solely on the relative scores of the two items involved . Recent works have made significant progress towards characterizing the performance ( e . g . the mean square error for estimating the scores ) of several classical methods , including the spectral method and the maximum likelihood estimator ( MLE ) . However , where they stand regarding top-$K$ ranking remains unsettled . We demonstrate that under a random sampling model , the spectral method alone , or the regularized MLE alone , is minimax optimal in terms of the sample complexity---the number of paired comparisons needed to ensure exact top-$K$ identification . This is accomplished via optimal control of the entrywise error of the score estimates . We complement our theoretical studies by numerical experiments , confirming that both methods yield low entrywise errors for estimating the underlying scores . Our theory is established based on a novel leave-one-out trick , which proves effective for analyzing both iterative and non-iterative optimization procedures . Along the way , we derive an elementary eigenvector perturbation bound for probability transition matrices , which parallels the Davis-Kahan $\sin\Theta$ theorem for symmetric matrices . This further allows us to close the gap between the $\ell_0$ error upper bound for the spectral method and the minimax lower limit .
We examine aspects of the computation of finite element matrices and vectors which are made possible by automated code generation . Given a variational form in a syntax which resembles standard mathematical notation , the low-level computer code for building finite element tensors , typically matrices , vectors and scalars , can be generated automatically via a form compiler . In particular , the generation of code for computing finite element matrices using a quadrature approach is addressed . For quadrature representations , a number of optimisation strategies which are made possible by automated code generation are presented . The relative performance of two different automatically generated representations of finite element matrices is examined , with a particular emphasis on complicated variational forms . It is shown that approaches which perform best for simple forms are not tractable for more complicated problems in terms of run time performance , the time required to generate the code or the size of the generated code . The approach and optimisations elaborated here are effective for a range of variational forms .
We discuss classical and quantum computations in terms of corresponding Hamiltonian dynamics . This allows us to introduce quantum computations which involve parallel processing of both : the data and programme instructions . Using mixed quantum-classical dynamics we look for a full cost of computations on quantum computers with classical terminals .
Good large sample performance is typically a minimum requirement of any model selection criterion . This article focuses on the consistency property of the Bayes factor , a commonly used model comparison tool , which has experienced a recent surge of attention in the literature . We thoroughly review existing results . As there exists such a wide variety of settings to be considered , e . g . parametric vs . nonparametric , nested vs . non-nested , etc . , we adopt the view that a unified framework has didactic value . Using the basic marginal likelihood identity of Chib ( 0000 ) , we study Bayes factor asymptotics by decomposing the natural logarithm of the ratio of marginal likelihoods into three components . These are , respectively , log ratios of likelihoods , prior densities , and posterior densities . This yields an interpretation of the log ratio of posteriors as a penalty term , and emphasizes that to understand Bayes factor consistency , the prior support conditions driving posterior consistency in each respective model under comparison should be contrasted in terms of the rates of posterior contraction they imply .
In this work we establish and investigate connections between causes for query answers in databases , database repairs wrt . denial constraints , and consistency-based diagnosis . The first two are relatively new research areas in databases , and the third one is an established subject in knowledge representation . We show how to obtain database repairs from causes , and the other way around . Causality problems are formulated as diagnosis problems , and the diagnoses provide causes and their responsibilities . The vast body of research on database repairs can be applied to the newer problems of computing actual causes for query answers and their responsibilities . These connections , which are interesting per se , allow us , after a transition -inspired by consistency-based diagnosis- to computational problems on hitting sets and vertex covers in hypergraphs , to obtain several new algorithmic and complexity results for database causality .
We study a Bayesian approach to recovering the initial condition for the heat equation from noisy observations of the solution at a later time . We consider a class of prior distributions indexed by a parameter quantifying " smoothness " and show that the corresponding posterior distributions contract around the true parameter at a rate that depends on the smoothness of the true initial condition and the smoothness and scale of the prior . Correct combinations of these characteristics lead to the optimal minimax rate . One type of priors leads to a rate-adaptive Bayesian procedure . The frequentist coverage of credible sets is shown to depend on the combination of the prior and true parameter as well , with smoother priors leading to zero coverage and rougher priors to ( extremely ) conservative results . In the latter case credible sets are much larger than frequentist confidence sets , in that the ratio of diameters diverges to infinity . The results are numerically illustrated by a simulated data example .
Spatial item recommendation has become an important means to help people discover interesting locations , especially when people pay a visit to unfamiliar regions . Some current researches are focusing on modelling individual and collective geographical preferences for spatial item recommendation based on users ' check-in records , but they fail to explore the phenomenon of user interest drift across geographical regions , i . e . , users would show different interests when they travel to different regions . Besides , they ignore the influence of public comments for subsequent users ' check-in behaviors . Specifically , it is intuitive that users would refuse to check in to a spatial item whose historical reviews seem negative overall , even though it might fit their interests . Therefore , it is necessary to recommend the right item to the right user at the right location . In this paper , we propose a latent probabilistic generative model called LSARS to mimic the decision-making process of users ' check-in activities both in home-town and out-of-town scenarios by adapting to user interest drift and crowd sentiments , which can learn location-aware and sentiment-aware individual interests from the contents of spatial items and user reviews . Due to the sparsity of user activities in out-of-town regions , LSARS is further designed to incorporate the public preferences learned from local users ' check-in behaviors . Finally , we deploy LSARS into two practical application scenes : spatial item recommendation and target user discovery . Extensive experiments on two large-scale location-based social networks ( LBSNs ) datasets show that LSARS achieves better performance than existing state-of-the-art methods .
Item cold-start is a classical issue in recommender systems that affects anime and manga recommendations as well . This problem can be framed as follows : how to predict whether a user will like a manga that received few ratings from the community ? Content-based techniques can alleviate this issue but require extra information , that is usually expensive to gather . In this paper , we use a deep learning technique , Illustration0Vec , to easily extract tag information from the manga and anime posters ( e . g . , sword , or ponytail ) . We propose BALSE ( Blended Alternate Least Squares with Explanation ) , a new model for collaborative filtering , that benefits from this extra information to recommend mangas . We show , using real data from an online manga recommender system called Mangaki , that our model improves substantially the quality of recommendations , especially for less-known manga , and is able to provide an interpretation of the taste of the users .
We propose to model multivariate volatility processes based on the newly defined conditionally uncorrelated components ( CUCs ) . This model represents a parsimonious representation for matrix-valued processes . It is flexible in the sense that we may fit each CUC with any appropriate univariate volatility model . Computationally it splits one high-dimensional optimization problem into several lower-dimensional subproblems . Consistency for the estimated CUCs has been established . A bootstrap test is proposed for testing the existence of CUCs . The proposed methodology is illustrated with both simulated and real data sets .
The Memory-Centred Cognition perspective places an active association substrate at the heart of cognition , rather than as a passive adjunct . Consequently , it places prediction and priming on the basis of prior experience to be inherent and fundamental aspects of processing . Social interaction is taken here to minimally require contingent and co-adaptive behaviours from the interacting parties . In this contribution , I seek to show how the memory-centred cognition approach to cognitive architectures can provide an means of addressing these functions . A number of example implementations are briefly reviewed , particularly focusing on multi-modal alignment as a function of experience-based priming . While there is further refinement required to the theory , and implementations based thereon , this approach provides an interesting alternative perspective on the foundations of cognitive architectures to support robots engage in social interactions with humans .
Caesium fountain frequency-standards realize the second in the International System of Units with a relative uncertainty approaching 00^-00 . Among the main contributions to the accuracy budget , cold collisions play an important role because of the atomic density shift of the reference atomic transition . This paper describes an application of the Bayesian analysis of the clock frequency to estimate the density shift and describes how the Bayes theorem allows the a priori knowledge of the sign of the collisional coefficient to be rigourously embedded into the analysis . As an application , data from the INRIM caesium fountain are used and the Bayesian and orthodox analyses are compared . The Bayes theorem allows the orthodox uncertainty to be reduced by 00% and demonstrates to be an important tool in primary frequency-metrology .
A spatial stochastic model is developed which describes the 0D nanomorphology of composite materials , being blends of two different ( organic and inorganic ) solid phases . Such materials are used , for example , in photoactive layers of hybrid polymer zinc oxide solar cells . The model is based on ideas from stochastic geometry and spatial statistics . Its parameters are fitted to image data gained by electron tomography ( ET ) , where adaptive thresholding and stochastic segmentation have been used to represent morphological features of the considered ET data by unions of overlapping spheres . Their midpoints are modeled by a stack of 0D point processes with a suitably chosen correlation structure , whereas a moving-average procedure is used to add the radii of spheres . The model is validated by comparing physically relevant characteristics of real and simulated data , like the efficiency of exciton quenching , which is important for the generation of charges and their transport toward the electrodes .
Open-world query answering is the problem of deciding , given a set of facts , conjunction of constraints , and query , whether the facts and constraints imply the query . This amounts to reasoning over all instances that include the facts and satisfy the constraints . We study finite open-world query answering ( FQA ) , which assumes that the underlying world is finite and thus only considers the finite completions of the instance . The major known decidable cases of FQA derive from the following : the guarded fragment of first-order logic , which can express referential constraints ( data in one place points to data in another ) but cannot express number restrictions such as functional dependencies ; and the guarded fragment with number restrictions but on a signature of arity only two . In this paper , we give the first decidability results for FQA that combine both referential constraints and number restrictions for arbitrary signatures : we show that , for unary inclusion dependencies and functional dependencies , the finiteness assumption of FQA can be lifted up to taking the finite implication closure of the dependencies . Our result relies on new techniques to construct finite universal models of such constraints , for any bound on the maximal query size .
This paper considers statistical estimation problems where the probability distribution of the observed random variable is invariant with respect to actions of a finite topological group . It is shown that any such distribution must satisfy a restricted finite mixture representation . When specialized to the case of distributions over the sphere that are invariant to the actions of a finite spherical symmetry group $\mathcal G$ , a group-invariant extension of the Von Mises Fisher ( VMF ) distribution is obtained . The $\mathcal G$-invariant VMF is parameterized by location and scale parameters that specify the distribution ' s mean orientation and its concentration about the mean , respectively . Using the restricted finite mixture representation these parameters can be estimated using an Expectation Maximization ( EM ) maximum likelihood ( ML ) estimation algorithm . This is illustrated for the problem of mean crystal orientation estimation under the spherically symmetric group associated with the crystal form , e . g . , cubic or octahedral or hexahedral . Simulations and experiments establish the advantages of the extended VMF EM-ML estimator for data acquired by Electron Backscatter Diffraction ( EBSD ) microscopy of a polycrystalline Nickel alloy sample .
The deployment of a software product requires considerable amount of time and effort . In order to increase the productivity of the software products , reusability strategies were proposed in the literature . However effective reuse is still a challenging issue . This paper presents a framework studio for effective components reusability which provides the selection of components from framework studio and generation of source code based on stakeholders needs . The framework studio is implemented using swings which are integrated onto the Net Beans IDE which help in faster generation of the source code .
In model-based clustering and classification , the cluster-weighted model constitutes a convenient approach when the random vector of interest constitutes a response variable Y and a set p of explanatory variables X . However , its applicability may be limited when p is high . To overcome this problem , this paper assumes a latent factor structure for X in each mixture component . This leads to the cluster-weighted factor analyzers ( CWFA ) model . By imposing constraints on the variance of Y and the covariance matrix of X , a novel family of sixteen CWFA models is introduced for model-based clustering and classification . The alternating expectation-conditional maximization algorithm , for maximum likelihood estimation of the parameters of all the models in the family , is described ; to initialize the algorithm , a 0-step hierarchical procedure is proposed , which uses the nested structures of the models within the family and thus guarantees the natural ranking among the sixteen likelihoods . Artificial and real data show that these models have very good clustering and classification performance and that the algorithm is able to recover the parameters very well .
Time series ( TS ) occur in many scientific and commercial applications , ranging from earth surveillance to industry automation to the smart grids . An important type of TS analysis is classification , which can , for instance , improve energy load forecasting in smart grids by detecting the types of electronic devices based on their energy consumption profiles recorded by automatic sensors . Such sensor-driven applications are very often characterized by ( a ) very long TS and ( b ) very large TS datasets needing classification . However , current methods to time series classification ( TSC ) cannot cope with such data volumes at acceptable accuracy ; they are either scalable but offer only inferior classification quality , or they achieve state-of-the-art classification quality but cannot scale to large data volumes . In this paper , we present WEASEL ( Word ExtrAction for time SEries cLassification ) , a novel TSC method which is both scalable and accurate . Like other state-of-the-art TSC methods , WEASEL transforms time series into feature vectors , using a sliding-window approach , which are then analyzed through a machine learning classifier . The novelty of WEASEL lies in its specific method for deriving features , resulting in a much smaller yet much more discriminative feature set . On the popular UCR benchmark of 00 TS datasets , WEASEL is more accurate than the best current non-ensemble algorithms at orders-of-magnitude lower classification and training times , and it is almost as accurate as ensemble classifiers , whose computational complexity makes them inapplicable even for mid-size datasets . The outstanding robustness of WEASEL is also confirmed by experiments on two real smart grid datasets , where it out-of-the-box achieves almost the same accuracy as highly tuned , domain-specific methods .
This article is about the architecture of a lossless wavelet filter bank with reprogrammable logic . It is based on second generation of wavelets with a reduced of number of operations . A new basic structure for parallel architecture and modules to forward and backward integer discrete wavelet transform is proposed .
We present the sTeX+ system , a user-driven advancement of sTeX - a semantic extension of LaTeX that allows for producing high-quality PDF documents for ( proof ) reading and printing , as well as semantic XML/OMDoc documents for the Web or further processing . Originally sTeX had been created as an invasive , semantic frontend for authoring XML documents . Here , we used sTeX in a Software Engineering case study as a formalization tool . In order to deal with modular pre-semantic vocabularies and relations , we upgraded it to sTeX+ in a participatory design process . We present a tool chain that starts with an sTeX+ editor and ultimately serves the generated documents as XHTML+RDFa Linked Data via an OMDoc-enabled , versioned XML database . In the final output , all structural annotations are preserved in order to enable semantic information retrieval services .
In this paper we study the scheduling of ( m , k ) -firm synchronous periodic task systems using the Distance Based Priority ( DBP ) scheduler . We first show three phenomena : ( i ) choosing , for each task , the initial k-sequence 0^k is not optimal , ( ii ) we can even start the scheduling from a ( fictive ) error state ( in regard to the initial k-sequence ) and ( iii ) the period of feasible DBP-schedules is not necessarily the task hyper-period . We then show that any feasible DBP-schedule is periodic and we upper-bound the length of that period . Lastly , based on our periodicity result we provide an exact schedulability test .
This paper considers the maximum likelihood estimation of factor models of high dimension , where the number of variables ( N ) is comparable with or even greater than the number of observations ( T ) . An inferential theory is developed . We establish not only consistency but also the rate of convergence and the limiting distributions . Five different sets of identification conditions are considered . We show that the distributions of the MLE estimators depend on the identification restrictions . Unlike the principal components approach , the maximum likelihood estimator explicitly allows heteroskedasticities , which are jointly estimated with other parameters . Efficiency of MLE relative to the principal components method is also considered .
A novel quickest detection setting is proposed which is a generalization of the well-known Bayesian change-point detection model . Suppose \{ ( X_i , Y_i ) \}_{i\geq 0} is a sequence of pairs of random variables , and that S is a stopping time with respect to \{X_i\}_{i\geq 0} . The problem is to find a stopping time T with respect to \{Y_i\}_{i\geq 0} that optimally tracks S , in the sense that T minimizes the expected reaction delay E ( T-S ) ^+ , while keeping the false-alarm probability P ( T<S ) below a given threshold \alpha \in [0 , 0] . This problem formulation applies in several areas , such as in communication , detection , forecasting , and quality control . Our results relate to the situation where the X_i ' s and Y_i ' s take values in finite alphabets and where S is bounded by some positive integer \kappa . By using elementary methods based on the analysis of the tree structure of stopping times , we exhibit an algorithm that computes the optimal average reaction delays for all \alpha \in [0 , 0] , and constructs the associated optimal stopping times T . Under certain conditions on \{ ( X_i , Y_i ) \}_{i\geq 0} and S , the algorithm running time is polynomial in \kappa .
Minimization of computational errors in the fixed-point data path is often difficult task . Many signal processing algorithms use chains of consecutive additions . The analyzing technique that can be applied to fixed-point data path synthesis has been proposed . This technique takes advantage of allocating the chains of consecutive additions in order to predict growing width of the data path and minimize the design complexity and computational errors .
This paper addresses the problem of estimating the Potts parameter B jointly with the unknown parameters of a Bayesian model within a Markov chain Monte Carlo ( MCMC ) algorithm . Standard MCMC methods cannot be applied to this problem because performing inference on B requires computing the intractable normalizing constant of the Potts model . In the proposed MCMC method the estimation of B is conducted using a likelihood-free Metropolis-Hastings algorithm . Experimental results obtained for synthetic data show that estimating B jointly with the other unknown parameters leads to estimation results that are as good as those obtained with the actual value of B . On the other hand , assuming that the value of B is known can degrade estimation performance significantly if this value is incorrect . To illustrate the interest of this method , the proposed algorithm is successfully applied to real bidimensional SAR and tridimensional ultrasound images .
We study asymptotic behavior of one-step weighted $M$-estimators based on samples from arrays of not necessarily identically distributed random variables and representing explicit approximations to the corresponding consistent weighted $M$-estimators . Sufficient conditions are presented for asymptotic normality of the one-step weighted $M$-estimators under consideration . As a consequence , we consider some well-known nonlinear regression models where the procedure mentioned allow us to construct explicit asymptotically optimal estimators .
Markov chain Monte Carlo methods explicitly defined on the manifold of probability distributions have recently been established . These methods are constructed from diffusions across the manifold and the solution of the equations describing geodesic flows in the Hamilton--Jacobi representation . This paper takes the differential geometric basis of Markov chain Monte Carlo further by considering methods to simulate from probability distributions that themselves are defined on a manifold , with common examples being classes of distributions describing directional statistics . Proposal mechanisms are developed based on the geodesic flows over the manifolds of support for the distributions and illustrative examples are provided for the hypersphere and Stiefel manifold of orthonormal matrices .
Extreme value analysis in the presence of censoring is receiving much attention as it has applications in many disciplines , including survival and reliability studies . Estimation of extreme value index ( EVI ) is of primary importance as it is a critical parameter needed in estimating extreme events such as quantiles and exceedance probabilities . In this paper , we review several estimators of the extreme value index when data is subject to random censoring . In addition , four estimators are proposed , one based on the exponential regression approximation of log spacings , one based on a Zipf estimator and two based on variants of the moment estimator . The proposed estimators and the existing ones are compared under the same simulation conditions . The performance measures for the estimators include confidence interval length and coverage probability . The simulation results show that no estimator is universally the best as the estimators depend on the size of the EVI parameter , percentage of censoring in the right tail and the underlying distribution . However , certain estimators such as the proposed reduced-bias estimator and the adapted moment estimator are found to perform well across most scenarios . Moreover , we present a bootstrap algorithm for obtaining samples for extreme value analysis in the context of censoring . Some of the estimators that performed well in the simulation study are illustrated using a practical dataset from medical research
The generalized lambda distribution ( GLD ) is a flexible four parameter distribution with many practical applications . L-moments of the GLD can be expressed in closed form and are good alternatives for the central moments . The L-moments of the GLD up to an arbitrary order are presented , and a study of L-skewness and L-kurtosis that can be achieved by the GLD is provided . The boundaries of L-skewness and L-kurtosis are derived analytically for the symmetric GLD and calculated numerically for the GLD in general . Additionally , the contours of L-skewness and L-kurtosis are presented as functions of the GLD parameters . It is found that with an exception of the smallest values of L-kurtosis , the GLD covers all possible pairs of L-skewness and L-kurtosis and often there are two or more distributions that share the same L-skewness and the same L-kurtosis . Examples that demonstrate situations where there are four GLD members with the same L-skewness and the same L-kurtosis are presented . The estimation of the GLD parameters is studied in a simulation example where method of L-moments compares favorably to more complicated estimation methods . The results increase the knowledge on the distributions that belong to the GLD family and can be utilized in model selection and estimation .
In this paper , researchers estimated the stock price of activated companies in Tehran ( Iran ) stock exchange . It is used Linear Regression and Artificial Neural Network methods and compared these two methods . In Artificial Neural Network , of General Regression Neural Network method ( GRNN ) for architecture is used . In this paper , first , researchers considered 00 macro economic variables and 00 financial variables and then they obtained seven final variables including 0 macro economic variables and 0 financial variables to estimate the stock price using Independent components Analysis ( ICA ) . So , we presented an equation for two methods and compared their results which shown that artificial neural network method is more efficient than linear regression method .
Insurers are faced with the challenge of estimating the future reserves needed to handle historic and outstanding claims that are not fully settled . A well-known and widely used technique is the chain-ladder method , which is a deterministic algorithm . To include a stochastic component one may apply generalized linear models to the run-off triangles based on past claims data . Analytical expressions for the standard deviation of the resulting reserve estimates are typically difficult to derive . A popular alternative approach to obtain inference is to use the bootstrap technique . However , the standard procedures are very sensitive to the possible presence of outliers . These atypical observations , deviating from the pattern of the majority of the data , may both inflate or deflate traditional reserve estimates and corresponding inference such as their standard errors . Even when paired with a robust chain-ladder method , classical bootstrap inference may break down . Therefore , we discuss and implement several robust bootstrap procedures in the claims reserving framework and we investigate and compare their performance on both simulated and real data . We also illustrate their use for obtaining the distribution of one year risk measures .
We consider the problem of reserving link capacity in a network in such a way that any of a given set of flow scenarios can be supported . In the optimal capacity reservation problem , we choose the reserved link capacities to minimize the reservation cost . This problem reduces to a large linear program , with the number of variables and constraints on the order of the number of links times the number of scenarios . Small and medium size problems are within the capabilities of generic linear program solvers . We develop a more scalable , distributed algorithm for the problem that alternates between solving ( in parallel ) one flow problem per scenario , and coordination steps , which connect the individual flows and the reservation capacities .
As renewable distributed energy resources ( DERs ) penetrate the power grid at an accelerating speed , it is essential for operators to have accurate solar photovoltaic ( PV ) energy forecasting for efficient operations and planning . Generally , observed weather data are applied in the solar PV generation forecasting model while in practice the energy forecasting is based on forecasted weather data . In this paper , a study on the uncertainty in weather forecasting for the most commonly used weather variables is presented . The forecasted weather data for six days ahead is compared with the observed data and the results of analysis are quantified by statistical metrics . In addition , the most influential weather predictors in energy forecasting model are selected . The performance of historical and observed weather data errors is assessed using a solar PV generation forecasting model . Finally , a sensitivity test is performed to identify the influential weather variables whose accurate values can significantly improve the results of energy forecasting .
We consider the problem of revenue-optimal dynamic mechanism design in settings where agents ' types evolve over time as a function of their ( both public and private ) experience with items that are auctioned repeatedly over an infinite horizon . A central question here is understanding what natural restrictions on the environment permit the design of optimal mechanisms ( note that even in the simpler static setting , optimal mechanisms are characterized only under certain restrictions ) . We provide a {\em structural characterization} of a natural " separable : multi-armed bandit environment ( where the evolution and incentive structure of the a-priori type is decoupled from the subsequent experience in a precise sense ) where dynamic optimal mechanism design is possible . Here , we present the Virtual Index Mechanism , an optimal dynamic mechanism , which maximizes the ( long term ) {\em virtual surplus} using the classical Gittins algorithm . The mechanism optimally balances exploration and exploitation , taking incentives into account .
This paper deals with the entity extraction task ( named entity recognition ) of a text mining process that aims at unveiling non-trivial semantic structures , such as relationships and interaction between entities or communities . In this paper we present a simple and efficient named entity extraction algorithm . The method , named PAMPO ( PAttern Matching and POs tagging based algorithm for NER ) , relies on flexible pattern matching , part-of-speech tagging and lexical-based rules . It was developed to process texts written in Portuguese , however it is potentially applicable to other languages as well . We compare our approach with current alternatives that support Named Entity Recognition ( NER ) for content written in Portuguese . These are Alchemy , Zemanta and Rembrandt . Evaluation of the efficacy of the entity extraction method on several texts written in Portuguese indicates a considerable improvement on $recall$ and $F_0$ measures .
In this paper , we extend the deep long short-term memory ( DLSTM ) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers . These direct links , called highway connections , enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs . We further introduce the latency-controlled bidirectional LSTMs ( BLSTMs ) which can exploit the whole history while keeping the latency under control . Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria . Experiments on the AMI distant speech recognition ( DSR ) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs ( HLSTMs ) . Our novel model obtains $00 . 0/00 . 0\%$ WER on AMI ( SDM ) dev and eval sets , outperforming all previous works . It beats the strong DNN and DLSTM baselines with $00 . 0\%$ and $0 . 0\%$ relative improvement respectively .
In nonparametric regression problems involving multiple predictors , there is typically interest in estimating an anisotropic multivariate regression surface in the important predictors while discarding the unimportant ones . Our focus is on defining a Bayesian procedure that leads to the minimax optimal rate of posterior contraction ( up to a log factor ) adapting to the unknown dimension and anisotropic smoothness of the true surface . We propose such an approach based on a Gaussian process prior with dimension-specific scalings , which are assigned carefully-chosen hyperpriors . We additionally show that using a homogenous Gaussian process with a single bandwidth leads to a sub-optimal rate in anisotropic cases .
Modern shared memory multiprocessors permit reordering of memory operations for performance reasons . These reorderings are often a source of subtle bugs in programs written for such architectures . Traditional approaches to verify weak memory programs often rely on interleaving semantics , which is prone to state space explosion , and thus severely limits the scalability of the analysis . In recent times , there has been a renewed interest in modelling dynamic executions of weak memory programs using partial orders . However , such an approach typically requires ad-hoc mechanisms to correctly capture the data and control-flow choices/conflicts present in real-world programs . In this work , we propose a novel , conflict-aware , composable , truly concurrent semantics for programs written using C/C++ for modern weak memory architectures . We exploit our symbolic semantics based on general event structures to build an efficient decision procedure that detects assertion violations in bounded multi-threaded programs . Using a large , representative set of benchmarks , we show that our conflict-aware semantics outperforms the state-of-the-art partial-order based approaches .
Homing preset and adaptive experiments with Finite State Machines ( FSMs ) are widely used when a non-initialized discrete event system is given for testing and thus , has to be set to the known state at the first step . The length of a shortest homing sequence is known to be exponential with respect to the number of states for a complete observable nondeterministic FSM while the problem of checking the existence of such sequence ( Homing problem ) is PSPACE-complete . In order to decrease the complexity of related problems , one can consider adaptive experiments when a next input to be applied to a system under experiment depends on the output responses to the previous inputs . In this paper , we study the problem of the existence of an adaptive homing experiment for complete observable nondeterministic machines . We show that if such experiment exists then it can be constructed with the use of a polynomial-time algorithm with respect to the number of FSM states .
The problem of autonomous navigation is one of the basic problems for robotics . Although , in general , it may be challenging when an autonomous vehicle is placed into partially observable domain . In this paper we consider simplistic environment model and introduce a navigation algorithm based on Learning Classifier System .
Population-based search algorithms ( PBSAs ) , including swarm intelligence algorithms ( SIAs ) and evolutionary algorithms ( EAs ) , are competitive alternatives for solving complex optimization problems and they have been widely applied to real-world optimization problems in different fields . In this study , a novel population-based across neighbourhood search ( ANS ) is proposed for numerical optimization . ANS is motivated by two straightforward assumptions and three important issues raised in improving and designing efficient PBSAs . In ANS , a group of individuals collaboratively search the solution space for an optimal solution of the optimization problem considered . A collection of superior solutions found by individuals so far is maintained and updated dynamically . At each generation , an individual directly searches across the neighbourhoods of multiple superior solutions with the guidance of a Gaussian distribution . This search manner is referred to as across neighbourhood search . The characteristics of ANS are discussed and the concept comparisons with other PBSAs are given . The principle behind ANS is simple . Moreover , ANS is easy for implementation and application with three parameters being required to tune . Extensive experiments on 00 benchmark optimization functions of different types show that ANS has well balanced exploration and exploitation capabilities and performs competitively compared with many efficient PBSAs ( Related Matlab codes used in the experiments are available from http : //guohuawunudt . gotoip0 . com/publications . html ) .
The promise of search-driven development is that developers will save time and resources by reusing external code in their local projects . To efficiently integrate this code , users must be able to trust it , thus trustability of code search results is just as important as their relevance . In this paper , we introduce a trustability metric to help users assess the quality of code search results and therefore ease the cost-benefit analysis they undertake trying to find suitable integration candidates . The proposed trustability metric incorporates both user votes and cross-project activity of developers to calculate a " karma " value for each developer . Through the karma value of all its developers a project is ranked on a trustability scale . We present JBender , a proof-of-concept code search engine which implements our trustability metric and we discuss preliminary results from an evaluation of the prototype .
In human-in-the-loop machine learning , the user provides information beyond that in the training data . Many algorithms and user interfaces have been designed to optimize and facilitate this human--machine interaction ; however , fewer studies have addressed the potential defects the designs can cause . Effective interaction often requires exposing the user to the training data or its statistics . The design of the system is then critical , as this can lead to double use of data and overfitting , if the user reinforces noisy patterns in the data . We propose a user modelling methodology , by assuming simple rational behaviour , to correct the problem . We show , in a user study with 00 participants , that the method improves predictive performance in a sparse linear regression sentiment analysis task , where graded user knowledge on feature relevance is elicited . We believe that the key idea of inferring user knowledge with probabilistic user models has general applicability in guarding against overfitting and improving interactive machine learning .
When dealing with subjective , noisy , or otherwise nebulous features , the " wisdom of crowds " suggests that one may benefit from multiple judgments of the same feature on the same object . We give theoretically-motivated `feature multi-selection ' algorithms that choose , among a large set of candidate features , not only which features to judge but how many times to judge each one . We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting people ' s height and weight from photos , using features such as ' gender ' and ' estimated weight ' as well as culturally fraught ones such as ' attractive ' .
Three different variations of PSO algorithms , i . e . Canonical , Gaussian Bare-bone and L\ ' evy Bare-bone PSO , are tested to optimize the ultimate oil recovery of a large heavy oil reservoir . The performance of these algorithms was compared in terms of convergence behaviour and the final optimization results . It is found that , in general , all three types of PSO methods are able to improve the objective function . The best objective function is found by using the Canonical PSO , while the other two methods give similar results . The Gaussian Bare-bone PSO may picks positions that are far away from the optimal solution . The L\ ' evy Bare-bone PSO has similar convergence behaviour as the Canonical PSO . For the specific optimization problem investigated in this study , it is found that the temperature of the injection steam , CO0 composition in the injection gas , and the gas injection rates have bigger impact on the objective function , while steam injection rate and the liquid production rate have less impact on the objective function .
We define a generalized index of jump activity , propose estimators of that index for a discretely sampled process and derive the estimators ' properties . These estimators are applicable despite the presence of Brownian volatility in the process , which makes it more challenging to infer the characteristics of the small , infinite activity jumps . When the method is applied to high frequency stock returns , we find evidence of infinitely active jumps in the data and estimate their index of activity .
In recent years , a rich variety of regularization procedures have been proposed for high dimensional regression problems . However , tuning parameter choice and computational efficiency in ultra-high dimensional problems remain vexing issues . The routine use of $\ell_0$ regularization is largely attributable to the computational efficiency of the LARS algorithm , but similar efficiency for better behaved penalties has remained elusive . In this article , we propose a highly efficient path following procedure for combination of any convex loss function and a broad class of penalties . From a Bayesian perspective , this algorithm rapidly yields maximum a posteriori estimates at different hyper-parameter values . To bypass the inefficiency and potential instability of cross validation , we propose an empirical Bayes procedure for rapidly choosing the optimal model and corresponding hyper-parameter value . This approach applies to any penalty that corresponds to a proper prior distribution on the regression coefficients . While we mainly focus on sparse estimation of generalized linear models , the method extends to more general regularizations such as polynomial trend filtering after reparameterization . The proposed algorithm scales efficiently to large $p$ and/or $n$ . Solution paths of 00 , 000 dimensional examples are computed within one minute on a laptop for various generalized linear models ( GLM ) . Operating characteristics are assessed through simulation studies and the methods are applied to several real data sets .
This article focuses on automatically generating polynomial equations that are inductive loop invariants of computer programs . We propose a new algorithm for this task , which is based on polynomial interpolation . Though the proposed algorithm is not complete , it is efficient and can be applied to a broader range of problems compared to existing methods targeting similar problems . The efficiency of our approach is testified by experiments on a large collection of programs . The current implementation of our method is based on dense interpolation , for which a total degree bound is needed . On the theoretical front , we study the degree and dimension of the invariant ideal of loops which have no branches and where the assignments define a P-solvable recurrence . In addition , we obtain sufficient conditions for non-trivial polynomial equation invariants to exist ( resp . not to exist ) .
We describe a model-checking toolchain for the behavioral verification of AADL models that takes into account the realtime semantics of the language and that is compatible with the AADL Behavioral Annex . We give a high-level view of the tools and transformations involved in the verification process and focus on the support offered by our framework for checking user-defined properties . We also describe the experimental results obtained on a significant avionic demonstrator , that models a network protocol in charge of data communications between an airplane and ground stations .
While skin cancer is the most diagnosed form of cancer in men and women , with more cases diagnosed each year than all other cancers combined , sufficiently early diagnosis results in very good prognosis and as such makes early detection crucial . While radiomics have shown considerable promise as a powerful diagnostic tool for significantly improving oncological diagnostic accuracy and efficiency , current radiomics-driven methods have largely rely on pre-defined , hand-crafted quantitative features , which can greatly limit the ability to fully characterize unique cancer phenotype that distinguish it from healthy tissue . Recently , the notion of discovery radiomics was introduced , where a large amount of custom , quantitative radiomic features are directly discovered from the wealth of readily available medical imaging data . In this study , we present a novel discovery radiomics framework for skin cancer detection , where we leverage novel deep multi-column radiomic sequencers for high-throughput discovery and extraction of a large amount of custom radiomic features tailored for characterizing unique skin cancer tissue phenotype . The discovered radiomic sequencer was tested against 0 , 000 biopsy-proven clinical images comprising of different skin cancers such as melanoma and basal cell carcinoma , and demonstrated sensitivity and specificity of 00% and 00% , respectively , thus achieving dermatologist-level performance and \break hence can be a powerful tool for assisting general practitioners and dermatologists alike in improving the efficiency , consistency , and accuracy of skin cancer diagnosis .
Agile methods provide an organization or a team the flexibility to adopt a selected subset of principles and practices based on their culture , their values , and the types of systems that they develop . More specifically , every organization or team implements a customized agile method , tailored to better accommodate its needs . However , the extent to which a customized method supports the organizational objectives , or rather the ' goodness ' of that method is questionable . Existing agile assessment approaches focus on a comparative analysis , or are limited in scope and application . In this research , we propose a structured , systematic and comprehensive approach to assess the ' goodness ' of agile methods . We examine an agile method based on ( 0 ) its adequacy , ( 0 ) the capability of the organization to support the adopted principles and practices specified by the method , and ( 0 ) the method ' s effectiveness . We propose the Objectives , Principles and Practices ( OPP ) Framework to guide our assessment . The Framework identifies ( 0 ) objectives of the agile philosophy , ( 0 ) principles that support the objectives , ( 0 ) practices that are reflective of the principles , ( 0 ) the linkages between the objectives , principles and practices , and ( 0 ) indicators for each practice to assess the effectiveness of the practice and the extent to which the organization supports its implementation . In this document , we discuss our solution approach , preliminary results , and future work .
Inter-coder agreement measures , like Cohen ' s kappa , correct the relative frequency of agreement between coders to account for agreement which simply occurs by chance . However , in some situations these measures exhibit behavior which make their values difficult to interprete . These properties , e . g . the " annotator bias " or the " problem of prevalence " , refer to a tendency of some of these measures to indicate counterintuitive high or low values of reliability depending on conditions which many researchers consider as unrelated to inter-coder reliability . However , not all researchers agree with this view , and since there is no commonly accepted formal definition of inter-coder reliability , it is hard to decide whether this depends upon a different concept of reliability or simply upon flaws in the measuring algorithms . In this note we therefore take an axiomatic approach : we introduce a model for the rating of items by several coders according to a nominal scale . Based upon this model we define inter-coder reliability as a probability to assign a category to an item with certainty . We then discuss under which conditions this notion of inter-coder reliability is uniquely determined given typical experimental results , i . e . relative frequencies of category assignments by different coders . In addition we provide an algorithm and conduct numerical simulations which exhibit the accuracy of this algorithm under different model parameter settings .
In a Spiking Neural Networks ( SNN ) , spike emissions are sparsely and irregularly distributed both in time and in the network architecture . Since a current feature of SNNs is a low average activity , efficient implementations of SNNs are usually based on an Event-Driven Simulation ( EDS ) . On the other hand , simulations of large scale neural networks can take advantage of distributing the neurons on a set of processors ( either workstation cluster or parallel computer ) . This article presents DAMNED , a large scale SNN simulation framework able to gather the benefits of EDS and parallel computing . Two levels of parallelism are combined : Distributed mapping of the neural topology , at the network level , and local multithreaded allocation of resources for simultaneous processing of events , at the neuron level . Based on the causality of events , a distributed solution is proposed for solving the complex problem of scheduling without synchronization barrier .
The problem of estimating the mean of a normal vector with known but unequal variances introduces substantial difficulties that impair the adequacy of traditional empirical Bayes estimators . By taking a different approach , that treats the known variances as part of the random observations , we restore symmetry and thus the effectiveness of such methods . We suggest a group-linear empirical Bayes estimator , which collects observations with similar variances and applies a spherically symmetric estimator to each group separately . The proposed estimator is motivated by a new oracle rule which is stronger than the best linear rule , and thus provides a more ambitious benchmark than that considered in previous literature . Our estimator asymptotically achieves the new oracle risk ( under appropriate conditions ) and at the same time is minimax . The group-linear estimator is particularly advantageous in situations where the true means and observed variances are empirically dependent . To demonstrate the merits of the proposed methods in real applications , we analyze the baseball data used in Brown ( 0000 ) , where the group-linear methods achieved the prediction error of the best nonparametric estimates that have been applied to the dataset , and significantly lower error than other parametric and semi-parametric empirical Bayes estimators .
Making sense of incomplete and conflicting narrative knowledge in the presence of abnormalities , unobservable processes , and other real world considerations is a challenge and crucial requirement for cognitive robotics systems . An added challenge , even when suitably specialised action languages and reasoning systems exist , is practical integration and application within large-scale robot control frameworks . In the backdrop of an autonomous wheelchair robot control task , we report on application-driven work to realise postdiction triggered abnormality detection and re-planning for real-time robot control : ( a ) Narrative-based knowledge about the environment is obtained via a larger smart environment framework ; and ( b ) abnormalities are postdicted from stable-models of an answer-set program corresponding to the robot ' s epistemic model . The overall reasoning is performed in the context of an approximate epistemic action theory based planner implemented via a translation to answer-set programming .
In the present paper we prove the following conjecture in Kingman , J . F . C . , Random walks with spherical symmetry , Acta Math . , 000 , ( 0000 ) , 00-00 . concerning a famous Raikov ' s theorem of decomposition of Poisson random variables : " If a radial sum of two independent random variables X and Y is radial Poisson , then each of them must be radial Poisson . "
This paper investigates the opportunities and limitations of adaptive virtual machine ( VM ) migration to reduce communication costs in a virtualized environment . We introduce a new formal model for the problem of online VM migration in two scenarios : ( 0 ) VMs can be migrated arbitrarily in the substrate network ; e . g . , a private cloud provider may have an incentive to reduce the overall communication cost in the network . ( 0 ) VMs can only be migrated within a given tenant ; e . g . , a user that was assigned a set of physical machines may exchange the functionality of the VMs on these machines . We propose a simple class of Destination-Swap algorithms which are based on an aggressive collocation strategy ( inspired by splay datastructures ) and which maintain a minimal and local amount of per-node ( amortized cost ) information to decide where to migrate a VM and how ; thus , the algorithms react quickly to changes in the load . The algorithms come in two main flavors , an indirect and distributed variant which keeps existing VM placements local , and a direct variant which keeps the number of affected VMs small . We show that naturally , inter-tenant optimizations yield a larger potential for optimization , but generally also a tenant itself can improve its embedding . Moreover , there exists an interesting tradeoff between direct and indirect strategies : indirect variants are preferable under skewed and sparse communication patterns due to their locality properties .
Since the definition of the Busy Beaver function by Rado in 0000 , an interesting open question has been the smallest value of n for which BB ( n ) is independent of ZFC set theory . Is this n approximately 00 , or closer to 0 , 000 , 000 , or is it even larger ? In this paper , we show that it is at most 0 , 000 by presenting an explicit description of a 0 , 000-state Turing machine Z with 0 tape and a 0-symbol alphabet that cannot be proved to run forever in ZFC ( even though it presumably does ) , assuming ZFC is consistent . The machine is based on the work of Harvey Friedman on independent statements involving order-invariant graphs . In doing so , we give the first known upper bound on the highest provable Busy Beaver number in ZFC . To create Z , we develop and use a higher-level language , Laconic , which is much more convenient than direct state manipulation . We also use Laconic to design two Turing machines , G and R , that halt if and only if there are counterexamples to Goldbach ' s Conjecture and the Riemann Hypothesis , respectively .
We consider distributed estimation of the inverse covariance matrix , also called the concentration or precision matrix , in Gaussian graphical models . Traditional centralized estimation often requires global inference of the covariance matrix , which can be computationally intensive in large dimensions . Approximate inference based on message-passing algorithms , on the other hand , can lead to unstable and biased estimation in loopy graphical models . In this paper , we propose a general framework for distributed estimation based on a maximum marginal likelihood ( MML ) approach . This approach computes local parameter estimates by maximizing marginal likelihoods defined with respect to data collected from local neighborhoods . Due to the non-convexity of the MML problem , we introduce and solve a convex relaxation . The local estimates are then combined into a global estimate without the need for iterative message-passing between neighborhoods . The proposed algorithm is naturally parallelizable and computationally efficient , thereby making it suitable for high-dimensional problems . In the classical regime where the number of variables $p$ is fixed and the number of samples $T$ increases to infinity , the proposed estimator is shown to be asymptotically consistent and to improve monotonically as the local neighborhood size increases . In the high-dimensional scaling regime where both $p$ and $T$ increase to infinity , the convergence rate to the true parameters is derived and is seen to be comparable to centralized maximum likelihood estimation . Extensive numerical experiments demonstrate the improved performance of the two-hop version of the proposed estimator , which suffices to almost close the gap to the centralized maximum likelihood estimator at a reduced computational cost .
Determinantal Point Processes ( DPPs ) are a family of probabilistic models that have a repulsive behavior , and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important . While there are fast algorithms for sampling , marginalization and conditioning , much less is known about learning the parameters of a DPP . Our contribution is twofold : ( i ) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter , which we call the \emph{cycle sparsity} ; ( ii ) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity . Finally , we give experimental results that confirm our theoretical findings .
We consider a pseudo-marginal Metropolis--Hastings kernel $P_m$ that is constructed using an average of $m$ exchangeable random variables , as well as an analogous kernel $P_s$ that averages $s<m$ of these same random variables . Using an embedding technique to facilitate comparisons , we show that the asymptotic variances of ergodic averages associated with $P_m$ are lower bounded in terms of those associated with $P_s$ . We show that the bound provided is tight and disprove a conjecture that when the random variables to be averaged are independent , the asymptotic variance under $P_m$ is never less than $s/m$ times the variance under $P_s$ . The conjecture does , however , hold when considering continuous-time Markov chains . These results imply that if the computational cost of the algorithm is proportional to $m$ , it is often better to set $m=0$ . We provide intuition as to why these findings differ so markedly from recent results for pseudo-marginal kernels employing particle filter approximations . Our results are exemplified through two simulation studies ; in the first the computational cost is effectively proportional to $m$ and in the second there is a considerable start-up cost at each iteration .
Research shows that comment spamming ( comments which are unsolicited , unrelated , abusive , hateful , commercial advertisements etc ) in online discussion forums has become a common phenomenon in Web 0 . 0 applications and there is a strong need to counter or combat comment spamming . We present a method to automatically detect comment spammer in YouTube ( largest and a popular video sharing website ) forums . The proposed technique is based on mining comment activity log of a user and extracting patterns ( such as time interval between subsequent comments , presence of exactly same comment across multiple unrelated videos ) indicating spam behavior . We perform empirical analysis on data crawled from YouTube and demonstrate that the proposed method is effective for the task of comment spammer detection .
A key enabler for optimizing business processes is accurately estimating the probability distribution of a time series future given its past . Such probabilistic forecasts are crucial for example for reducing excess inventory in supply chains . In this paper we propose DeepAR , a novel methodology for producing accurate probabilistic forecasts , based on training an auto-regressive recurrent network model on a large number of related time series . We show through extensive empirical evaluation on several real-world forecasting data sets that our methodology is more accurate than state-of-the-art models , while requiring minimal feature engineering .
This paper describes how to adapt a static code analyzer to help novice programmers . Current analyzers have been built to give feedback to experienced programmers who build new applications or systems . The type of feedback and the type of analysis of these tools focusses on mistakes that are relevant within that context , and help with debugging the system . When teaching novice programmers this type of advice is often not particularly useful . It would be instead more useful to use these techniques to find problem in the understanding of students of important programming concepts . This paper first explores in what respect static analyzers support the learning and teaching of programming can be implemented based on existing static analysis technology . It presents an extension to static analyzer PMD was made so that feedback messages appear which are easier to understand for novice programmers . To answer the question if these techniques are able to find conceptual mistakes that are characteristic for novice programmers make , we ran it over a number of student projects , and compared these results with publicly available mature software projects .
With the rapid development of location-based social networks ( LBSNs ) , spatial item recommendation has become an important means to help people discover attractive and interesting venues and events , especially when users travel out of town . However , this recommendation is very challenging compared to the traditional recommender systems . A user can visit only a limited number of spatial items , leading to a very sparse user-item matrix . Most of the items visited by a user are located within a short distance from where he/she lives , which makes it hard to recommend items when the user travels to a far away place . Moreover , user interests and behavior patterns may vary dramatically across different geographical regions . In light of this , we propose Geo-SAGE , a geographical sparse additive generative model for spatial item recommendation in this paper . Geo-SAGE considers both user personal interests and the preference of the crowd in the target region , by exploiting both the co-occurrence pattern of spatial items and the content of spatial items . To further alleviate the data sparsity issue , Geo-SAGE exploits the geographical correlation by smoothing the crowd ' s preferences over a well-designed spatial index structure called spatial pyramid . We conduct extensive experiments to evaluate the performance of our Geo-SAGE model on two real large-scale datasets . The experimental results clearly demonstrate our Geo-SAGE model outperforms the state-of-the-art in the two tasks of both out-of-town and home-town recommendations .
The aim of this article is to design a moment transformation for Student- t distributed random variables , which is able to account for the error in the numerically computed mean . We employ Student-t process quadrature , an instance of Bayesian quadrature , which allows us to treat the integral itself as a random variable whose variance provides information about the incurred integration error . Advantage of the Student- t process quadrature over the traditional Gaussian process quadrature , is that the integral variance depends also on the function values , allowing for a more robust modelling of the integration error . The moment transform is applied in nonlinear sigma-point filtering and evaluated on two numerical examples , where it is shown to outperform the state-of-the-art moment transforms .
The curse of outlier measurements in estimation problems is a well known issue in a variety of fields . Therefore , outlier removal procedures , which enables the identification of spurious measurements within a set , have been developed for many different scenarios and applications . In this paper , we propose a statistically motivated outlier removal algorithm for time differences of arrival ( TDOAs ) , or equivalently range differences ( RD ) , acquired at sensor arrays . The method exploits the TDOA-space formalism and works by only knowing relative sensor positions . As the proposed method is completely independent from the application for which measurements are used , it can be reliably used to identify outliers within a set of TDOA/RD measurements in different fields ( e . g . acoustic source localization , sensor synchronization , radar , remote sensing , etc . ) . The proposed outlier removal algorithm is validated by means of synthetic simulations and real experiments .
Let X ; Z be r and s-dimensional covariates , respectively , used to model the response variable Y as Y = m ( X ; Z ) + \sigma ( X ; Z ) \epsilon . We develop an ANOVA-type test for the null hypothesis that Z has no influence on the regression function , based on residuals obtained from local polynomial fitting of the null model . Using p-values from this test , a group variable selection method based on multiple testing ideas is proposed . Simulations studies suggest that the proposed test procedure outperforms the generalized likelihood ratio test when the alternative is non-additive or there is heteroscedasticity . Additional simulation studies , with data generated from linear , non-linear and logistic regression , reveal that the proposed group variable selection procedure performs competitively against Group Lasso , and outperforms it in selecting groups having nonlinear effects . The proposed group variable selection procedure is illustrated on a real data set .
Hommel ' s and Hochberg ' s procedures for familywise error control are both derived as shortcuts in a closed testing procedure with the Simes local test . Hommel ' s shortcut is exact but takes quadratic time in the number of hypotheses . Hochberg ' s shortcut takes only linearithmic time , but is conservative . In this paper we present an exact shortcut in linearithmic time , combining the strengths of both procedures . The novel shortcut also applies to a robust variant of Hommel ' s procedure that does not require the assumption of the Simes inequality .
Micro-robotics at low Reynolds number has been a growing area of research over the past decade . We propose and study a generalized 0-link robotic swimmer inspired by the planar Purcell ' s swimmer . By incorporating out-of-plane motion of the outer limbs , this mechanism generalizes the planar Purcell ' s swimmer , which has been widely studied in the literature . Such an evolution of the limbs ' motion results in the swimmer ' s base link evolving in a 0-dimensional space . The swimmer ' s configuration space admits a trivial principal fiber bundle structure , which along with the slender body theory at the low Reynolds number regime , facilitates in obtaining a principal kinematic form of the equations . We derive a coordinate-free expression for the local form of the kinematic connection . A novel approach for local controllability analysis of this 0-dimensional swimmer in the low Reynolds number regime is presented by employing the controllability results of the planar Purcell ' s swimmer . This is followed by control synthesis using the motion primitives approach . We prove the existence of motion primitives based control sequence for maneuvering the swimmer ' s base link whose motion evolves on a Lie group . Using the principal fiber bundle structure , an algorithm for point to point reconfiguration of the swimmer is presented . A set of control sequences for translational and rotational maneuvers is then provided along with numerical simulations .
Improving the interpretability of brain decoding approaches is of primary interest in many neuroimaging studies . Despite extensive studies of this type , at present , there is no formal definition for interpretability of brain decoding models . As a consequence , there is no quantitative measure for evaluating the interpretability of different brain decoding methods . In this paper , we present a simple definition for interpretability of linear brain decoding models . Then , we propose to combine the interpretability and the performance of the brain decoding into a new multi-objective criterion for model selection . Our preliminary results on the toy data show that optimizing the hyper-parameters of the regularized linear classifier based on the proposed criterion results in more informative linear models . The presented definition provides the theoretical background for quantitative evaluation of interpretability in linear brain decoding .
The p_0 model is a directed random graph model used to describe dyadic interactions in a social network in terms of effects due to differential attraction ( popularity ) and expansiveness , as well as an additional effect due to reciprocation . In this article we carry out an algebraic statistics analysis of this model . We show that the p_0 model is a toric model specified by a multi-homogeneous ideal . We conduct an extensive study of the Markov bases for p_0 models that incorporate explicitly the constraint arising from multi-homogeneity . Our results are directly relevant to the estimation and conditional goodness-of-fit testing problems in p_0 models .
The B\ " o\ " ogg is a large model of a snowman , constructed of inflammable materials and filled with explosives . During the traditional festival of Sechsel\ " auten , which takes place each spring in Zurich , Switzerland , the B\ " o\ " ogg is placed atop a wooden pyre , which is set alight . According to popular legend , the time that elapses until the B\ " o\ " ogg ' s head explodes ( the " head-bang " time ) is said to give a rough forecast of local weather conditions prevailing during the following summer . However , recent research has questioned the validity of this prediction . To study the B\ " o\ " ogg ' s predictive powers , we analyzed the B\ " o\ " ogg head-bang time record from 0000-0000 within the context of global climate change . Our analysis shows that the B\ " o\ " ogg head-bang time is a good predictor not of short-term local weather , as might be expected from the legend , but of the behavior of the entire global climate system .
Generative Adversarial Network ( GAN ) and its variants demonstrate state-of-the-art performance in the class of generative models . To capture higher dimensional distributions , the common learning procedure requires high computational complexity and large number of parameters . In this paper , we present a new generative adversarial framework by representing each layer as a tensor structure connected by multilinear operations , aiming to reduce the number of model parameters by a large factor while preserving the quality of generalized performance . To learn the model , we develop an efficient algorithm by alternating optimization of the mode connections . Experimental results demonstrate that our model can achieve high compression rate for model parameters up to 00 times as compared to the existing GAN .
Despite rapid advances in experimental cell biology , the in vivo behavior of hematopoietic stem cells ( HSC ) cannot be directly observed and measured . Previously we modeled feline hematopoiesis using a two-compartment hidden Markov process that had birth and emigration events in the first compartment . Here we perform Bayesian statistical inference on models which contain two additional events in the first compartment in order to determine if HSC fate decisions are linked to cell division or occur independently . Pareto Optimal Model Assessment approach is used to cross check the estimates from Bayesian inference . Our results show that HSC must divide symmetrically ( i . e . , produce two HSC daughter cells ) in order to maintain hematopoiesis . We then demonstrate that the augmented model that adds asymmetric division events provides a better fit to the competitive transplantation data , and we thus provide evidence that HSC fate determination in vivo occurs both in association with cell division and at a separate point in time . Last we show that assuming each cat has a unique set of parameters leads to either a significant decrease or a nonsignificant increase in model fit , suggesting that the kinetic parameters for HSC are not unique attributes of individual animals , but shared within a species .
In this paper we present an interesting gadget based on the chain pair simplification problem under the discrete Fr\ ' echet distance ( CPS-0F ) , which allows the construction of arbitrarily long paths that must be chosen in the simplification of the two curves . A pseudopolynomial time reduction from set partition is given as an example . For clarification , CPS-0F was recently shown to be in \textbf{P} , and the reduction is merely to show how the gadget works .
This paper considers estimating a covariance matrix of $p$ variables from $n$ observations by either banding or tapering the sample covariance matrix , or estimating a banded version of the inverse of the covariance . We show that these estimates are consistent in the operator norm as long as $ ( \log p ) /n\to0$ , and obtain explicit rates . The results are uniform over some fairly natural well-conditioned families of covariance matrices . We also introduce an analogue of the Gaussian white noise model and show that if the population covariance is embeddable in that model and well-conditioned , then the banded approximations produce consistent estimates of the eigenvalues and associated eigenvectors of the covariance matrix . The results can be extended to smooth versions of banding and to non-Gaussian distributions with sufficiently short tails . A resampling approach is proposed for choosing the banding parameter in practice . This approach is illustrated numerically on both simulated and real data .
Social networks are getting closer to our real physical world . People share the exact location and time of their check-ins and are influenced by their friends . Modeling the spatio-temporal behavior of users in social networks is of great importance for predicting the future behavior of users , controlling the users ' movements , and finding the latent influence network . It is observed that users have periodic patterns in their movements . Also , they are influenced by the locations that their close friends recently visited . Leveraging these two observations , we propose a probabilistic model based on a doubly stochastic point process with a periodic decaying kernel for the time of check-ins and a time-varying multinomial distribution for the location of check-ins of users in the location-based social networks . We learn the model parameters using an efficient EM algorithm , which distributes over the users . Experiments on synthetic and real data gathered from Foursquare show that the proposed inference algorithm learns the parameters efficiently and our model outperforms the other alternatives in the prediction of time and location of check-ins .
We investigate the width complexity of nondeterministic unitary OBDDs ( NUOBDDs ) . Firstly , we present a generic lower bound on their widths based on the size of strong 0-fooling sets . Then , we present classically cheap functions that are expensive for NUOBDDs and vice versa by improving the previous gap . We also present a function for which neither classical nor unitary nondeterminism does help . Moreover , based on our results , we present a width hierarchy for NUOBDDs . Lastly , we provide the bounds on the widths of NUOBDDs for the basic Boolean operations negation , union , and intersection .
Sparse additive models are families of $d$-variate functions that have the additive decomposition $f^* = \sum_{j \in S} f^*_j$ , where $S$ is an unknown subset of cardinality $s \ll d$ . In this paper , we consider the case where each univariate component function $f^*_j$ lies in a reproducing kernel Hilbert space ( RKHS ) , and analyze a method for estimating the unknown function $f^*$ based on kernels combined with $\ell_0$-type convex regularization . Working within a high-dimensional framework that allows both the dimension $d$ and sparsity $s$ to increase with $n$ , we derive convergence rates ( upper bounds ) in the $L^0 ( \mathbb{P} ) $ and $L^0 ( \mathbb{P}_n ) $ norms over the class $\MyBigClass$ of sparse additive models with each univariate function $f^*_j$ in the unit ball of a univariate RKHS with bounded kernel function . We complement our upper bounds by deriving minimax lower bounds on the $L^0 ( \mathbb{P} ) $ error , thereby showing the optimality of our method . Thus , we obtain optimal minimax rates for many interesting classes of sparse additive models , including polynomials , splines , and Sobolev classes . We also show that if , in contrast to our univariate conditions , the multivariate function class is assumed to be globally bounded , then much faster estimation rates are possible for any sparsity $s = \Omega ( \sqrt{n} ) $ , showing that global boundedness is a significant restriction in the high-dimensional setting .
Despite significant advances in improving the gaze tracking accuracy under controlled conditions , the tracking robustness under real-world conditions , such as large head pose and movements , use of eyeglasses , illumination and eye type variations , remains a major challenge in eye tracking . In this paper , we revisit this challenge and introduce a real-time multi-camera eye tracking framework to improve the tracking robustness . First , differently from previous work , we design a multi-view tracking setup that allows for acquiring multiple eye appearances simultaneously . Leveraging multi-view appearances enables to more reliably detect gaze features under challenging conditions , particularly when they are obstructed in conventional single-view appearance due to large head movements or eyewear effects . The features extracted on various appearances are then used for estimating multiple gaze outputs . Second , we propose to combine estimated gaze outputs through an adaptive fusion mechanism to compute user ' s overall point of regard . The proposed mechanism firstly determines the estimation reliability of each gaze output according to user ' s momentary head pose and predicted gazing behavior , and then performs a reliability-based weighted fusion . We demonstrate the efficacy of our framework with extensive simulations and user experiments on a collected dataset featuring 00 subjects . Our results show that in comparison with state-of-the-art eye trackers , the proposed framework provides not only a significant enhancement in accuracy but also a notable robustness . Our prototype system runs at 00 frames-per-second ( fps ) and achieves 0 degree accuracy under challenging experimental scenarios , which makes it suitable for applications demanding high accuracy and robustness .
The features of designing of reconstruction of the acting plant by its design department are considered : the results of work are drawings corresponding with the national standards ; large number of the small projects for different acting objects ; variety of the types of the drawings in one project ; large paper archive . The models and methods of developing of the complex CAD system with friend uniform environment of designing , with setting a profile of operations , with usage of the general parts of the project , with a series of problem-oriented subsystems are described on an example of a CAD system TechnoCAD GlassX
Most professional golfers and analysts think that winning on the PGA Tour peaks when golfers are in their thirties . Rather than relying on educated guesses , we can actually use available statistical data to determine the actual ages at which golfers peak their golf game . We can also test the hypothesis that age affects winning professional golf tournaments . Using data available from the Golf Channel , the PGA Tour , and LPGA Tour , I calculated and provided the mean , the median , and the mode ages at which professional golfers on the PGA , European PGA , Champions , and LPGA Tours had won over a five-year period . More specifically , the ages at which golfers on the PGA , European PGA , Champions Tour , and LPGA Tours peak their wins are 00 , 00 , 00 , and 00 , respectively . The regression analyses I conducted seem to support my hypothesis that age affects winning professional golf tournaments .
Several variants of the Long Short-Term Memory ( LSTM ) architecture for recurrent neural networks have been proposed since its inception in 0000 . In recent years , these networks have become the state-of-the-art models for a variety of machine learning problems . This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants . In this paper , we present the first large-scale analysis of eight LSTM variants on three representative tasks : speech recognition , handwriting recognition , and polyphonic music modeling . The hyperparameters of all LSTM variants for each task were optimized separately using random search , and their importance was assessed using the powerful fANOVA framework . In total , we summarize the results of 0000 experimental runs ( $\approx 00$ years of CPU time ) , which makes our study the largest of its kind on LSTM networks . Our results show that none of the variants can improve upon the standard LSTM architecture significantly , and demonstrate the forget gate and the output activation function to be its most critical components . We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment .
This report presents an Expectation-Maximization ( EM ) algorithm for estimation of the maximum-likelihood parameter values of constrained multivariate autoregressive Gaussian state-space ( MARSS ) models . The MARSS model can be written : x ( t ) =Bx ( t-0 ) +u+w ( t ) , y ( t ) =Zx ( t ) +a+v ( t ) , where w ( t ) and v ( t ) are multivariate normal error-terms with variance-covariance matrices Q and R respectively . MARSS models are a class of dynamic linear model and vector autoregressive state-space model . Shumway and Stoffer presented an unconstrained EM algorithm for this class of models in 0000 , and a number of researchers have presented EM algorithms for specific types of constrained MARSS models since then . In this report , I present a general EM algorithm for constrained MARSS models , where the constraints are on the elements within the parameter matrices ( B , u , Q , Z , a , R ) . The constraints take the form vec ( M ) =f+Dm , where M is the parameter matrix , f is a column vector of fixed values , D is a matrix of multipliers , and m is the column vector of estimated values . This allows a wide variety of constrained parameter matrix forms . The presentation is for a time-varying MARSS model , where time-variation enters through the fixed ( meaning not estimated ) f ( t ) and D ( t ) matrices for each parameter . The algorithm allows missing values in y and partially deterministic systems where 0s appear on the diagonals of Q or R . Open source code for estimating MARSS models with this algorithm is provided in the MARSS R package on the Comprehensive R Archive Network ( CRAN ) .
We study asymptotically optimal statistical inference concerning the unknown state of $N$ identical quantum systems , using two complementary approaches : a " poor man ' s approach " based on the van Trees inequality , and a rather more sophisticated approach using the recently developed quantum form of LeCam ' s theory of Local Asymptotic Normality .
Some real-world problems revolve to solve the optimization problem \max_{x\in\mathcal{X}}f\left ( x\right ) where f\left ( . \right ) is a black-box function and X might be the set of non-vectorial objects ( e . g . , distributions ) where we can only define a symmetric and non-negative similarity score on it . This setting requires a novel view for the standard framework of Bayesian Optimization that generalizes the core insightful spirit of this framework . With this spirit , in this paper , we propose Analogical-based Bayesian Optimization that can maximize black-box function over a domain where only a similarity score can be defined . Our pathway is as follows : we first base on the geometric view of Gaussian Processes ( GP ) to define the concept of influence level that allows us to analytically represent predictive means and variances of GP posteriors and base on that view to enable replacing kernel similarity by a more genetic similarity score . Furthermore , we also propose two strategies to find a batch of query points that can efficiently handle high dimensional data .
We consider the inference of the structure of an undirected graphical model in an exact Bayesian framework . More specifically we aim at achieving the inference with close-form posteriors , avoiding any sampling step . This task would be intractable without any restriction on the considered graphs , so we limit our exploration to mixtures of spanning trees . We consider the inference of the structure of an undirected graphical model in a Bayesian framework . To avoid convergence issues and highly demanding Monte Carlo sampling , we focus on exact inference . More specifically we aim at achieving the inference with close-form posteriors , avoiding any sampling step . To this aim , we restrict the set of considered graphs to mixtures of spanning trees . We investigate under which conditions on the priors - on both tree structures and parameters - exact Bayesian inference can be achieved . Under these conditions , we derive a fast an exact algorithm to compute the posterior probability for an edge to belong to {the tree model} using an algebraic result called the Matrix-Tree theorem . We show that the assumption we have made does not prevent our approach to perform well on synthetic and flow cytometry data .
We develop a new class of dynamic multivariate Poisson count models that allow for fast online updating and we refer to these models as multivariate Poisson-scaled beta ( MPSB ) . The MPSB model allows for serial dependence in the counts as well as dependence across multiple series with a random common environment . Other notable features include analytic forms for state propagation and predictive likelihood densities . Sequential updating occurs through the updating of the sufficient statistics for static model parameters , leading to a fully adapted particle learning algorithm and a new class of predictive likelihoods and marginal distributions which we refer to as the ( dynamic ) multivariate confluent hyper-geometric negative binomial distribution ( MCHG-NB ) and the the dynamic multivariate negative binomial ( DMNB ) distribution . To illustrate our methodology , we use various simulation studies and count data on weekly non-durable goods consumer demand .
Video search results and suggested videos on web sites are represented with a video thumbnail , which is manually selected by the video up-loader among three randomly generated ones ( e . g . , YouTube ) . In contrast , we present a grounded user-based approach for automatically detecting interesting key-frames within a video through aggregated users ' replay interactions with the video player . Previous research has focused on content-based systems that have the benefit of analyzing a video without user interactions , but they are monolithic , because the resulting video thumbnails are the same regardless of the user preferences . We constructed a user interest function , which is based on aggregate video replays , and analyzed hundreds of user interactions . We found that the local maximum of the replaying activity stands for the semantics of information rich videos , such as lecture , and how-to . The concept of user-based key-frame detection could be applied to any video on the web , in order to generate a user-based and dynamic video thumbnail in search results .
Inference is typically intractable in high-treewidth undirected graphical models , making maximum likelihood learning a challenge . One way to overcome this is to restrict parameters to a tractable set , most typically the set of tree-structured parameters . This paper explores an alternative notion of a tractable set , namely a set of " fast-mixing parameters " where Markov chain Monte Carlo ( MCMC ) inference can be guaranteed to quickly converge to the stationary distribution . While it is common in practice to approximate the likelihood gradient using samples obtained from MCMC , such procedures lack theoretical guarantees . This paper proves that for any exponential family with bounded sufficient statistics , ( not just graphical models ) when parameters are constrained to a fast-mixing set , gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability . When unregularized , to find a solution epsilon-accurate in log-likelihood requires a total amount of effort cubic in 0/epsilon , disregarding logarithmic factors . When ridge-regularized , strong convexity allows a solution epsilon-accurate in parameter distance with effort quadratic in 0/epsilon . Both of these provide of a fully-polynomial time randomized approximation scheme .
We consider effort allocation in crowdsourcing , where we wish to assign labeling tasks to imperfect homogeneous crowd workers to maximize overall accuracy in a continuous-time Bayesian setting , subject to budget and time constraints . The Bayes-optimal policy for this problem is the solution to a partially observable Markov decision process , but the curse of dimensionality renders the computation infeasible . Based on the Lagrangian Relaxation technique in Adelman & Mersereau ( 0000 ) , we provide a computationally tractable instance-specific upper bound on the value of this Bayes-optimal policy , which can in turn be used to bound the optimality gap of any other sub-optimal policy . In an approach similar in spirit to the Whittle index for restless multiarmed bandits , we provide an index policy for effort allocation in crowdsourcing and demonstrate numerically that it outperforms other stateof- arts and performs close to optimal solution .
Fitting high-dimensional data involves a delicate tradeoff between faithful representation and the use of sparse models . Too often , sparsity assumptions on the fitted model are too restrictive to provide a faithful representation of the observed data . In this paper , we present a novel framework incorporating sparsity in different domains . We decompose the observed covariance matrix into a sparse Gaussian Markov model ( with a sparse precision matrix ) and a sparse independence model ( with a sparse covariance matrix ) . Our framework incorporates sparse covariance and sparse precision estimation as special cases and thus introduces a richer class of high-dimensional models . We characterize sufficient conditions for identifiability of the two models , \viz Markov and independence models . We propose an efficient decomposition method based on a modification of the popular $\ell_0$-penalized maximum-likelihood estimator ( $\ell_0$-MLE ) . We establish that our estimator is consistent in both the domains , i . e . , it successfully recovers the supports of both Markov and independence models , when the number of samples $n$ scales as $n = \Omega ( d^0 \log p ) $ , where $p$ is the number of variables and $d$ is the maximum node degree in the Markov model . Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation .
The need for data intensive Grids , and advanced networks with high performance that support our science has made the High Energy Physics community a leading and a key co-developer of leading edge wide area networks . This paper gives an overview of the status for the world ' s research networks and major international links used by the high energy physics and other scientific communities , showing some Future Internet testbed architectures , scalability , geographic scope , and extension between networks . The resemblance between wireless sensor network and future internet network , especially in scale consideration as density and network coverage , inspires us to adopt the models of the former to the later . Then we test this assumption to see that this provides a concise working model . This paper collects some heuristics that we call them SpiroPlanck and employs them to model the coverage of dense networks . In this paper , we propose a framework for the operation of FI testbeds containing a test scenario , new representation and visualization techniques , and possible performance measures . Investigations show that it is very promising and could be seen as a good optimization
We study bivariate stochastic recurrence equations ( SREs ) motivated by applications to GARCH ( 0 , 0 ) processes . If coefficient matrices of SREs have strictly positive entries , then the Kesten result applies and it gives solutions with regularly varying tails . Moreover , the tail indices are the same for all coordinates . However , for applications , this framework is too restrictive . We study SREs when coefficients are triangular matrices and prove that the coordinates of the solution may exhibit regularly varying tails with different indices . We also specify each tail index together with its constant . The results are used to characterize regular variations of bivariate stationary GARCH ( 0 , 0 ) processes .
It has been shown that a neural network model recently proposed to describe basic memory performance is based on a ternary/binary coding/decoding algorithm which leads to a new neural network assembly memory model ( NNAMM ) providing maximum-likelihood recall/recognition properties and implying a new memory unit architecture with Hopfield two-layer network , N-channel time gate , auxiliary reference memory , and two nested feedback loops . For the data coding used , conditions are found under which a version of Hopfied network implements maximum-likelihood convolutional decoding algorithm and , simultaneously , linear statistical classifier of arbitrary binary vectors with respect to Hamming distance between vector analyzed and reference vector given . In addition to basic memory performance and etc , the model explicitly describes the dependence on time of memory trace retrieval , gives a possibility of one-trial learning , metamemory simulation , generalized knowledge representation , and distinct description of conscious and unconscious mental processes . It has been shown that an assembly memory unit may be viewed as a model of a smallest inseparable part or an ' atom ' of consciousness . Some nontraditional neurobiological backgrounds ( dynamic spatiotemporal synchrony , properties of time dependent and error detector neurons , early precise spike firing , etc ) and the model ' s application to solve some interdisciplinary problems from different scientific fields are discussed .
In the present paper , we studied a Dynamic Stochastic Block Model ( DSBM ) under the assumptions that the connection probabilities , as functions of time , are smooth and that at most $s$ nodes can switch their class memberships between two consecutive time points . We estimate the edge probability tensor by a kernel-type procedure and extract the group memberships of the nodes by spectral clustering . The procedure is computationally viable , adaptive to the unknown smoothness of the functional connection probabilities , to the rate $s$ of membership switching and to the unknown number of clusters . In addition , it is accompanied by non-asymptotic guarantees for the precision of estimation and clustering .
Blockchain technology enables the execution of collaborative business processes involving untrusted parties without requiring a central authority . Specifically , a process model comprising tasks performed by multiple parties can be coordinated via smart contracts operating on the blockchain . The consensus mechanism governing the blockchain thereby guarantees that the process model is followed by each party . However , the cost required for blockchain use is highly dependent on the volume of data recorded and the frequency of data updates by smart contracts . This paper proposes an optimized method for executing business processes on top of commodity blockchain technology . The paper presents a method for compiling a process model into a smart contract that encodes the preconditions for executing each task in the process using a space-optimized data structure . The method is empirically compared to a previously proposed baseline by replaying execution logs , including one from a real-life business process , and measuring resource consumption .
Background : In settings where proof-of-principle trials have succeeded but the effectiveness of different forms of implementation remains uncertain , trials that not only generate information about intervention effects but also provide public health benefit would be useful . Cluster randomized trials ( CRT ) capture both direct and indirect intervention effects ; the latter depends heavily on contact networks within and across clusters . We propose a novel class of connectivity-informed trial designs that leverages information about such networks in order to improve public health impact and preserve ability to detect intervention effects . Methods : We consider CRTs in which the order of enrollment is based on the total number of ties between individuals across clusters ( based either on the total number of inter-cluster connections or on connections only to untreated clusters ) . We include options analogous both to traditional Parallel and Stepped Wedge designs . We also allow for control clusters to be " held-back " from re-randomization for some period . We investigate the performance epidemic control and power to detect vaccine effect performance of these designs by simulating vaccination trials during an SEIR-type epidemic using a network-structured agent-based model . Results : In our simulations , connectivity-informed designs have lower peak infectiousness than comparable traditional designs and reduce cumulative incidence by 00% , but with little impact on time to end of epidemic and reduced power to detect differences in incidence across clusters . However even a brief " holdback " period restores most of the power lost compared to traditional approaches . Conclusion : Incorporating information about cluster connectivity in design of CRTs can increase their public health impact , especially in acute outbreak settings , with modest cost in power to detect an effective intervention .
Over the last decade , PageRank has gained importance in a wide range of applications and domains , ever since it first proved to be effective in determining node importance in large graphs ( and was a pioneering idea behind Google ' s search engine ) . In distributed computing alone , PageRank vector , or more generally random walk based quantities have been used for several different applications ranging from determining important nodes , load balancing , search , and identifying connectivity structures . Surprisingly , however , there has been little work towards designing provably efficient fully-distributed algorithms for computing PageRank . The difficulty is that traditional matrix-vector multiplication style iterative methods may not always adapt well to the distributed setting owing to communication bandwidth restrictions and convergence rates . In this paper , we present fast random walk-based distributed algorithms for computing PageRanks in general graphs and prove strong bounds on the round complexity . We first present a distributed algorithm that takes $O\big ( \log n/\eps \big ) $ rounds with high probability on any graph ( directed or undirected ) , where $n$ is the network size and $\eps$ is the reset probability used in the PageRank computation ( typically $\eps$ is a fixed constant ) . We then present a faster algorithm that takes $O\big ( \sqrt{\log n}/\eps \big ) $ rounds in undirected graphs . Both of the above algorithms are scalable , as each node sends only small ( $\polylog n$ ) number of bits over each edge per round . To the best of our knowledge , these are the first fully distributed algorithms for computing PageRank vector with provably efficient running time .
The identification of the lag length for vector autoregressive models by mean of Akaike Information Criterion ( AIC ) , Partial Autoregressive and Correlation Matrices ( PAM and PCM hereafter ) is studied in the framework of processes with time varying variance . It is highlighted that the use of the standard tools are not justified in such a case . As a consequence we propose an adaptive AIC which is robust to the presence of unconditional heteroscedasticity . Corrected confidence bounds are proposed for the usual PAM and PCM obtained from the Ordinary Least Squares ( OLS ) estimation . The volatility structure of the innovations is used to develop adaptive PAM and PCM . We underline that the adaptive PAM and PCM are more accurate than the OLS PAM and PCM for identifying the lag length of the autoregressive models . Monte Carlo experiments show that the adaptive $AIC$ have a greater ability to select the correct autoregressive order than the standard AIC . An illustrative application using US international finance data is presented .
With the increased interest in computational sciences , machine learning ( ML ) , pattern recognition ( PR ) and big data , governmental agencies , academia and manufacturers are overwhelmed by the constant influx of new algorithms and techniques promising improved performance , generalization and robustness . Sadly , result reproducibility is often an overlooked feature accompanying original research publications , competitions and benchmark evaluations . The main reasons behind such a gap arise from natural complications in research and development in this area : the distribution of data may be a sensitive issue ; software frameworks are difficult to install and maintain ; Test protocols may involve a potentially large set of intricate steps which are difficult to handle . Given the raising complexity of research challenges and the constant increase in data volume , the conditions for achieving reproducible research in the domain are also increasingly difficult to meet . To bridge this gap , we built an open platform for research in computational sciences related to pattern recognition and machine learning , to help on the development , reproducibility and certification of results obtained in the field . By making use of such a system , academic , governmental or industrial organizations enable users to easily and socially develop processing toolchains , re-use data , algorithms , workflows and compare results from distinct algorithms and/or parameterizations with minimal effort . This article presents such a platform and discusses some of its key features , uses and limitations . We overview a currently operational prototype and provide design insights .
This paper concerns a method of testing equality of distribution of random convex compact sets and the way how to use the test to distinguish between two realisations of general random sets . The family of metrics on the space of distributions of random convex compact sets is constructed using the theory of N-distances and characteristic functions of random convex compact sets . Further , the approximation of the metrics through its finite dimensional counterparts is proposed , which lead to a new statistical test for testing equality in distribution of two random convex compact sets . Then , it is described how to approximate a realisation of a general random set by a union of convex compact sets , and it is shown how to determine whether two realisations of general random sets come from the same process using the constructed test . The procedure is justified by an extensive simulation study .
We propose a hierarchical abstract domain for the analysis of free-list memory allocators that tracks shape and numerical properties about both the heap and the free lists . Our domain is based on Separation Logic extended with predicates that capture the pointer arithmetics constraints for the heap-list and the shape of the free-list . These predicates are combined using a hierarchical composition operator to specify the overlapping of the heap-list by the free-list . In addition to expressiveness , this operator leads to a compositional and compact representation of abstract values and simplifies the implementation of the abstract domain . The shape constraints are combined with numerical constraints over integer arrays to track properties about the allocation policies ( best-fit , first-fit , etc ) . Such properties are out of the scope of the existing analyzers . We implemented this domain and we show its effectiveness on several implementations of free-list allocators .
In this paper , a novel robust tracking control law is proposed for constrained robots under unknown stiffness environment . The stability and the robustness of the controller are proved using a Lyapunov-based approach where the relationship between the error dynamics of the robotic system and its energy is investigated . Finally , a 0DOF constrained robotic arm is used to prove the stability , the robustness and the safety of the proposed approach .
Fourier PCA is Principal Component Analysis of a matrix obtained from higher order derivatives of the logarithm of the Fourier transform of a distribution . We make this method algorithmic by developing a tensor decomposition method for a pair of tensors sharing the same vectors in rank-$0$ decompositions . Our main application is the first provably polynomial-time algorithm for underdetermined ICA , i . e . , learning an $n \times m$ matrix $A$ from observations $y=Ax$ where $x$ is drawn from an unknown product distribution with arbitrary non-Gaussian components . The number of component distributions $m$ can be arbitrarily higher than the dimension $n$ and the columns of $A$ only need to satisfy a natural and efficiently verifiable nondegeneracy condition . As a second application , we give an alternative algorithm for learning mixtures of spherical Gaussians with linearly independent means . These results also hold in the presence of Gaussian noise .
This paper focuses on a passivity-based distributed reference governor ( RG ) applied to a pre-stabilized mobile robotic network . The novelty of this paper lies in the method used to solve the RG problem , where a passivity-based distributed optimization scheme is proposed . In particular , the gradient descent method minimizes the global objective function while the dual ascent method maximizes the Hamiltonian . To make the agents converge to the agreed optimal solution , a proportional-integral consensus estimator is used . This paper proves the convergence of the state estimates of the RG to the optimal solution through passivity arguments , considering the physical system static . Then , the effectiveness of the scheme considering the dynamics of the physical system is demonstrated through simulations and experiments .
In many cases it makes sense to model a relationship symmetrically , not implying any particular directionality . Consider the classical example of a recommendation system where the rating of an item by a user should symmetrically be dependent on the attributes of both the user and the item . The attributes of the ( known ) relationships are also relevant for predicting attributes of entities and for predicting attributes of new relations . In recommendation systems , the exploitation of relational attributes is often referred to as collaborative filtering . Again , in many applications one might prefer to model the collaborative effect in a symmetrical way . In this paper we present a relational model , which is completely symmetrical . The key innovation is that we introduce for each entity ( or object ) an infinite-dimensional latent variable as part of a Dirichlet process ( DP ) model . We discuss inference in the model , which is based on a DP Gibbs sampler , i . e . , the Chinese restaurant process . We extend the Chinese restaurant process to be applicable to relational modeling . Our approach is evaluated in three applications . One is a recommendation system based on the MovieLens data set . The second application concerns the prediction of the function of yeast genes/proteins on the data set of KDD Cup 0000 using a multi-relational model . The third application involves a relational medical domain . The experimental results show that our model gives significantly improved estimates of attributes describing relationships or entities in complex relational models .
Variational inference is a scalable technique for approximate Bayesian inference . Deriving variational inference algorithms requires tedious model-specific calculations ; this makes it difficult to automate . We propose an automatic variational inference algorithm , automatic differentiation variational inference ( ADVI ) . The user only provides a Bayesian model and a dataset ; nothing else . We make no conjugacy assumptions and support a broad class of models . The algorithm automatically determines an appropriate variational family and optimizes the variational objective . We implement ADVI in Stan ( code available now ) , a probabilistic programming framework . We compare ADVI to MCMC sampling across hierarchical generalized linear models , nonconjugate matrix factorization , and a mixture model . We train the mixture model on a quarter million images . With ADVI we can use variational inference on any model we write in Stan .
This paper describes an integrated framework for SOC test automation . This framework is based on a new approach for Wrapper/TAM co-optimization based on rectangle packing considering the diagonal length of the rectangles to emphasize on both TAM widths required by a core and its corresponding testing time . In this paper , we propose an efficient algorithm to construct wrappers that reduce testing time for cores . We then use rectangle packing to develop an integrated scheduling algorithm that incorporates power constraints in the test schedule . The test power consumption is important to consider since exceeding the system ' s power limit might damage the system .
We reprove a result of Dehkordi , Frati , and Gudmundsson : every two vertices in a non-obtuse triangulation of a point set are connected by an angle-monotone path--an xy-monotone path in an appropriately rotated coordinate system . We show that this result cannot be extended to angle-monotone spanning trees , but can be extended to boundary-rooted spanning forests . The latter leads to a conjectural edge-unfolding of sufficiently shallow polyhedral convex caps .
Upper and lower bounds are derived for the Gaussian mean width of the intersection of a convex hull of $M$ points with an Euclidean ball of a given radius . The upper bound holds for any collection of extreme point bounded in Euclidean norm . The upper bound and the lower bound match up to a multiplicative constant whenever the extreme points satisfy a one sided Restricted Isometry Property . This bound is then applied to study the Lasso estimator in fixed-design regression , the Empirical Risk Minimizer in the anisotropic persistence problem , and the convex aggregation problem in density estimation .
Education and training in digital forensics requires a variety of suitable challenge corpora containing realistic features including regular wear-and-tear , background noise , and the actual digital traces to be discovered during investigation . Typically , the creation of these challenges requires overly arduous effort on the part of the educator to ensure their viability . Once created , the challenge image needs to be stored and distributed to a class for practical training . This storage and distribution step requires significant time and resources and may not even be possible in an online/distance learning scenario due to the data sizes involved . As part of this paper , we introduce a more capable methodology and system as an alternative to current approaches . EviPlant is a system designed for the efficient creation , manipulation , storage and distribution of challenges for digital forensics education and training . The system relies on the initial distribution of base disk images , i . e . , images containing solely base operating systems . In order to create challenges for students , educators can boot the base system , emulate the desired activity and perform a " diffing " of resultant image and the base image . This diffing process extracts the modified artefacts and associated metadata and stores them in an " evidence package " . Evidence packages can be created for different personae , different wear-and-tear , different emulated crimes , etc . , and multiple evidence packages can be distributed to students and integrated into the base images . A number of additional applications in digital forensic challenge creation for tool testing and validation , proficiency testing , and malware analysis are also discussed as a result of using EviPlant .
The problem of adaptive noisy clustering is investigated . Given a set of noisy observations $Z_i=X_i+\epsilon_i$ , $i=0 , . . . , n$ , the goal is to design clusters associated with the law of $X_i$ ' s , with unknown density $f$ with respect to the Lebesgue measure . Since we observe a corrupted sample , a direct approach as the popular {\it $k$-means} is not suitable in this case . In this paper , we propose a noisy $k$-means minimization , which is based on the $k$-means loss function and a deconvolution estimator of the density $f$ . In particular , this approach suffers from the dependence on a bandwidth involved in the deconvolution kernel . Fast rates of convergence for the excess risk are proposed for a particular choice of the bandwidth , which depends on the smoothness of the density $f$ . Then , we turn out into the main issue of the paper : the data-driven choice of the bandwidth . We state an adaptive upper bound for a new selection rule , called ERC ( Empirical Risk Comparison ) . This selection rule is based on the Lepski ' s principle , where empirical risks associated with different bandwidths are compared . Finally , we illustrate that this adaptive rule can be used in many statistical problems of $M$-estimation where the empirical risk depends on a nuisance parameter .
Recently by the development of the Internet and the Web , different types of social media such as web blogs become an immense source of text data . Through the processing of these data , it is possible to discover practical information about different topics , individuals opinions and a thorough understanding of the society . Therefore , applying models which can automatically extract the subjective information from the documents would be efficient and helpful . Topic modeling methods , also sentiment analysis are the most raised topics in the natural language processing and text mining fields . In this paper a new structure for joint sentiment-topic modeling based on Restricted Boltzmann Machine ( RBM ) which is a type of neural networks is proposed . By modifying the structure of RBM as well as appending a layer which is analogous to sentiment of text data to it , we propose a generative structure for joint sentiment topic modeling based on neutral networks . The proposed method is supervised and trained by the Contrastive Divergence algorithm . The new attached layer in the proposed model is a layer with the multinomial probability distribution which can be used in text data sentiment classification or any other supervised application . The proposed model is compared with existing models in the experiments such as evaluating as a generative model , sentiment classification , information retrieval and the corresponding results demonstrate the efficiency of the method .
We discuss here the mean-field theory for a cellular automata model of meta-learning . The meta-learning is the process of combining outcomes of individual learning procedures in order to determine the final decision with higher accuracy than any single learning method . Our method is constructed from an ensemble of interacting , learning agents , that acquire and process incoming information using various types , or different versions of machine learning algorithms . The abstract learning space , where all agents are located , is constructed here using a fully connected model that couples all agents with random strength values . The cellular automata network simulates the higher level integration of information acquired from the independent learning trials . The final classification of incoming input data is therefore defined as the stationary state of the meta-learning system using simple majority rule , yet the minority clusters that share opposite classification outcome can be observed in the system . Therefore , the probability of selecting proper class for a given input data , can be estimated even without the prior knowledge of its affiliation . The fuzzy logic can be easily introduced into the system , even if learning agents are build from simple binary classification machine learning algorithms by calculating the percentage of agreeing agents .
The density function of the limiting spectral distribution of general sample covariance matrices is usually unknown . We propose to use kernel estimators which are proved to be consistent . A simulation study is also conducted to show the performance of the estimators .
Support Vector Machines , SVMs , and the Large Margin Nearest Neighbor algorithm , LMNN , are two very popular learning algorithms with quite different learning biases . In this paper we bring them into a unified view and show that they have a much stronger relation than what is commonly thought . We analyze SVMs from a metric learning perspective and cast them as a metric learning problem , a view which helps us uncover the relations of the two algorithms . We show that LMNN can be seen as learning a set of local SVM-like models in a quadratic space . Along the way and inspired by the metric-based interpretation of SVM s we derive a novel variant of SVMs , epsilon-SVM , to which LMNN is even more similar . We give a unified view of LMNN and the different SVM variants . Finally we provide some preliminary experiments on a number of benchmark datasets in which show that epsilon-SVM compares favorably both with respect to LMNN and SVM .
Due to the emergence of embedded applications in image and video processing , communication and cryptography , improvement of pictorial information for better human perception like deblurring , denoising in several fields such as satellite imaging , medical imaging , mobile applications etc . are gaining importance for renewed research . Behind such developments , the primary responsibility lies with the advancement of semiconductor technology leading to FPGA based programmable logic devices , which combines the advantages of both custom hardware and dedicated DSP resources . In addition , FPGA provides powerful reconfiguration feature and hence is an ideal target for rapid prototyping . We have endeavoured to exploit exceptional features of FPGA technology in respect to hardware parallelism leading to higher computational density and throughput , and have observed better performances than those one can get just merely porting the image processing software algorithms to hardware . In this paper , we intend to present an elaborate review , based on our expertise and experiences , on undertaking necessary transformation to an image processing software algorithm including the optimization techniques that makes its operation in hardware comparatively faster .
Marginal structural models were introduced in order to provide estimates of causal effects from interventions based on observational studies in epidemiological research . The key point is that this can be understood in terms of Girsanov ' s change of measure . This offers a mathematical interpretation of marginal structural models that has not been available before . We consider both a model of an observational study and a model of a hypothetical randomized trial . These models correspond to different martingale measures -- the observational measure and the randomized trial measure -- on some underlying space . We describe situations where the randomized trial measure is absolutely continuous with respect to the observational measure . The resulting continuous-time likelihood ratio process with respect to these two probability measures corresponds to the weights in discrete-time marginal structural models . In order to do inference for the hypothetical randomized trial , we can simulate samples using observational data weighted by this likelihood ratio .
There has been a lively debate in many fields , including statistics and related applied fields such as psychology and biomedical research , on possible reforms of the scholarly publishing system . Currently , referees contribute so much to improve scientific papers , both directly through constructive criticism and indirectly through the threat of rejection . We discuss ways in which new approaches to journal publication could continue to make use of the valuable efforts of peer reviewers .
This paper introduces a procedure based on genetic programming to evolve XSLT programs ( usually called stylesheets or logicsheets ) . XSLT is a general purpose , document-oriented functional language , generally used to transform XML documents ( or , in general , solve any problem that can be coded as an XML document ) . The proposed solution uses a tree representation for the stylesheets as well as diverse specific operators in order to obtain , in the studied cases and a reasonable time , a XSLT stylesheet that performs the transformation . Several types of representation have been compared , resulting in different performance and degree of success .
Portable Electronic Devices usually utilize a small screen with limited viewing area and a keyboard with a limited number of keys . This makes it difficult to perform quick searches in data arrays containing more than dozen items such an address book or song list . In this article we present a new data selection method which allows the user to quickly select an entry from a list using 0-way navigation device such as joystick , trackball or 0-way key pad . This method allows for quick navigation using just one hand , without looking at the screen .
This paper discusses the local linear smoothing to estimate the unknown first and second infinitesimal moments in second-order jump-diffusion model based on Gamma asymmetric kernels . Under the mild conditions , we obtain the weak consistency and the asymptotic normality of these estimators for both interior and boundary design points . Besides the standard properties of the local linear estimation such as simple bias representation and boundary bias correction , the local linear smoothing using Gamma asymmetric kernels possess some extra advantages such as variable bandwidth , variance reduction and resistance to sparse design , which is validated through finite sample simulation study . Finally , we employ the estimators for the return of some high frequency financial data .
Generalized Second Price ( GSP ) auctions are widely used by search engines today to sell their ad slots . Most search engines have supported broad match between queries and bid keywords when executing GSP auctions , however , it has been revealed that GSP auction with the standard broad-match mechanism they are currently using ( denoted as SBM-GSP ) has several theoretical drawbacks ( e . g . , its theoretical properties are known only for the single-slot case and full-information setting , and even in this simple setting , the corresponding worst-case social welfare can be rather bad ) . To address this issue , we propose a novel broad-match mechanism , which we call the Probabilistic Broad-Match ( PBM ) mechanism . Different from SBM that puts together the ads bidding on all the keywords matched to a given query for the GSP auction , the GSP with PBM ( denoted as PBM-GSP ) randomly samples a keyword according to a predefined probability distribution and only runs the GSP auction for the ads bidding on this sampled keyword . We perform a comprehensive study on the theoretical properties of the PBM-GSP . Specifically , we study its social welfare in the worst equilibrium , in both full-information and Bayesian settings . The results show that PBM-GSP can generate larger welfare than SBM-GSP under mild conditions . Furthermore , we also study the revenue guarantee for PBM-GSP in Bayesian setting . To the best of our knowledge , this is the first work on broad-match mechanisms for GSP that goes beyond the single-slot case and the full-information setting .
There has been widespread interest in the use of grid-level storage to handle the variability from increasing penetrations of wind and solar energy . This problem setting requires optimizing energy storage and release decisions for anywhere from a half-dozen , to potentially hundreds of storage devices spread around the grid as new technologies evolve . We approach this problem using two competing algorithmic strategies . The first , developed within the stochastic programming literature , is stochastic dual dynamic programming ( SDDP ) which uses Benders decomposition to create a multidimensional value function approximations , which have been widely used to manage hydro reservoirs . The second approach , which has evolved using the language of approximate dynamic programming , uses separable , piecewise linear value function approximations , a method which has been successfully applied to high-dimensional fleet management problems . This paper brings these two approaches together using a common notational system , and contrasts the algorithmic strategies ( which are both a form of approximate dynamic programming ) used by each approach . The methods are then subjected to rigorous testing using the context of optimizing grid level storage .
The past century has seen a steady increase in the need of estimating and predicting complex systems and making ( possibly critical ) decisions with limited information . Although computers have made possible the numerical evaluation of sophisticated statistical models , these models are still designed \emph{by humans} because there is currently no known recipe or algorithm for dividing the design of a statistical model into a sequence of arithmetic operations . Indeed enabling computers to \emph{think} as \emph{humans} have the ability to do when faced with uncertainty is challenging in several major ways : ( 0 ) Finding optimal statistical models remains to be formulated as a well posed problem when information on the system of interest is incomplete and comes in the form of a complex combination of sample data , partial knowledge of constitutive relations and a limited description of the distribution of input random variables . ( 0 ) The space of admissible scenarios along with the space of relevant information , assumptions , and/or beliefs , tend to be infinite dimensional , whereas calculus on a computer is necessarily discrete and finite . With this purpose , this paper explores the foundations of a rigorous framework for the scientific computation of optimal statistical estimators/models and reviews their connections with Decision Theory , Machine Learning , Bayesian Inference , Stochastic Optimization , Robust Optimization , Optimal Uncertainty Quantification and Information Based Complexity .
Taken traditionally as a no-go theorem against the theorization of inductive processes , Duhem-Quine thesis may interfere with the essence of statistical inference . This difficulty can be resolved by Micro-Macro duality \cite{Oj00 , Oj00} which clarifies the importance of specifying the pertinent aspects and accuracy relevant to concrete contexts of scientific discussions and which ensures the matching between what to be described and what to describe in the form of the validity of duality relations . This consolidates the foundations of the inverse problem , induction method , and statistical inference crucial for the sound relations between theory and experiments . To achieve the purpose , we propose here Large Deviation Strategy ( LDS for short ) on the basis of Micro-Macro duality , quadrality scheme , and large deviation principle . According to the quadrality scheme emphasizing the basic roles played by the dynamics , algebra of observables together with its representations and universal notion of classifying space , LDS consists of four levels and we discuss its first and second levels in detail , aiming at establishing statistical inference concerning observables and states . By efficient use of the central measure , we will establish a quantum version of Sanov ' s theorem , the Bayesian escort predictive state and the widely applicable information criteria for quantum states in LDS second level . Finally , these results are reexamined in the context of quantum estimation theory , and organized as quantum model selection , i . e . , a quantum version of model selection .
In order to deal efficiently with the exponential growth of the Web services landscape in composition life cycle activities , it is necessary to have a clear view of its main features . As for many situations where there is a lot of interacting entities , the complex networks paradigm is an appropriate approach to analyze the interactions between the multitudes of Web services . In this paper , we present and investigate the main interactions between semantic Web services models from the complex network perspective . Results show that both parameter and operation networks exhibit the main characteristics of typical real-world complex networks such as the small-world property and an inhomogeneous degree distribution . These results yield valuable insight in order to develop composition search algorithms , to deal with security threat in the composition process and on the phenomena which characterize its evolution .
This paper covers the whole process of developing an Augmented Reality Stereoscopig Render Engine for the Oculus Rift . To capture the real world in form of a camera stream , two cameras with fish-eye lenses had to be installed on the Oculus Rift DK0 hardware . The idea was inspired by Steptoe \cite{steptoe0000presence} . After the introduction , a theoretical part covers all the most neccessary elements to achieve an AR System for the Oculus Rift , following the implementation part where the code from the AR Stereo Engine is explained in more detail . A short conclusion section shows some results , reflects some experiences and in the final chapter some future works will be discussed . The project can be accessed via the git repository https : //github . com/MaXvanHeLL/ARift . git .
SOBA is an approach to election verification that provides observers with justifiably high confidence that the reported results of an election are consistent with an audit trail ( " ballots " ) , which can be paper or electronic . SOBA combines three ideas : ( 0 ) publishing cast vote records ( CVRs ) separately for each contest , so that anyone can verify that each reported contest outcome is correct , if the CVRs reflect voters ' intentions with sufficient accuracy ; ( 0 ) shrouding a mapping between ballots and the CVRs for those ballots to prevent the loss of privacy that could occur otherwise ; ( 0 ) assessing the accuracy with which the CVRs reflect voters ' intentions for a collection of contests while simultaneously assessing the integrity of the shrouded mapping between ballots and CVRs by comparing randomly selected ballots to the CVRs that purport to represent them . Step ( 0 ) is related to work by the Humboldt County Election Transparency Project , but publishing CVRs separately for individual contests rather than images of entire ballots preserves privacy . Step ( 0 ) requires a cryptographic commitment from elections officials . Observers participate in step ( 0 ) , which relies on the " super-simple simultaneous single-ballot risk-limiting audit . " Step ( 0 ) is designed to reveal relatively few ballots if the shrouded mapping is proper and the CVRs accurately reflect voter intent . But if the reported outcomes of the contests differ from the outcomes that a full hand count would show , step ( 0 ) is guaranteed to have a large chance of requiring all the ballots to be counted by hand , thereby limiting the risk that an incorrect outcome will become official and final .
Modeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task . When restricted to the single-agent decision-theoretic setting , inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem . These techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations . In this work , we consider similar tasks in competitive and cooperative multi-agent domains . Here , unlike single-agent settings , a player cannot myopically maximize its reward --- it must speculate on how the other agents may act to influence the game ' s outcome . Employing the game-theoretic notion of regret and the principle of maximum entropy , we introduce a technique for predicting and generalizing behavior , as well as recovering a reward function in these domains .
Bayesian inference on structured models typically relies on the ability to infer posterior distributions of underlying hidden variables . However , inference in implicit models or complex posterior distributions is hard . A popular tool for learning implicit models are generative adversarial networks ( GANs ) which learn parameters of generators by fooling discriminators . Typically , GANs are considered to be models themselves and are not understood in the context of inference . Current techniques rely on inefficient global discrimination of joint distributions to perform learning , or only consider discriminating a single output variable . We overcome these limitations by treating GANs as a basis for likelihood-free inference in generative models and generalize them to Bayesian posterior inference over factor graphs . We propose local learning rules based on message passing minimizing a global divergence criterion involving cooperating local adversaries used to sidestep explicit likelihood evaluations . This allows us to compose models and yields a unified inference and learning framework for adversarial learning . Our framework treats model specification and inference separately and facilitates richly structured models within the family of Directed Acyclic Graphs , including components such as intractable likelihoods , non-differentiable models , simulators and generally cumbersome models . A key result of our treatment is the insight that Bayesian inference on structured models can be performed only with sampling and discrimination when using nonparametric variational families , without access to explicit distributions . As a side-result , we discuss the link to likelihood maximization . These approaches hold promise to be useful in the toolbox of probabilistic modelers and enrich the gamut of current probabilistic programming applications .
Due to the advancement in mobile devices and wireless networks mobile cloud computing , which combines mobile computing and cloud computing has gained momentum since 0000 . The characteristics of mobile devices and wireless network makes the implementation of mobile cloud computing more complicated than for fixed clouds . This section lists some of the major issues in Mobile Cloud Computing . One of the key issues in mobile cloud computing is the end to end delay in servicing a request . Data caching is o ne of the techniques widely used in wired and wireless networks to improve data access efficiency . In this paper we explore the possibility of a cooperative caching approach to enhance data access efficiency in mobile cloud computing . The proposed approach is based on cloudlets , one of the architecture designed for mobile cloud computing .
Confidence intervals are a popular way to visualize and analyze data distributions . Unlike p-values , they can convey information both about statistical significance as well as effect size . However , very little work exists on applying confidence intervals to multivariate data . In this paper we define confidence intervals for multivariate data that extend the one-dimensional definition in a natural way . In our definition every variable is associated with its own confidence interval as usual , but a data vector can be outside of a few of these , and still be considered to be within the confidence area . We analyze the problem and show that the resulting confidence areas retain the good qualities of their one-dimensional counterparts : they are informative and easy to interpret . Furthermore , we show that the problem of finding multivariate confidence intervals is hard , but provide efficient approximate algorithms to solve the problem .
Recent results concerning asymptotic Bayes-optimality under sparsity ( ABOS ) of multiple testing procedures are extended to fairly generally distributed effect sizes under the alternative . An asymptotic framework is considered where both the number of tests m and the sample size m go to infinity , while the fraction p of true alternatives converges to zero . It is shown that under mild restrictions on the loss function nontrivial asymptotic inference is possible only if n increases to infinity at least at the rate of log m . Based on this assumption precise conditions are given under which the Bonferroni correction with nominal Family Wise Error Rate ( FWER ) level alpha and the Benjamini- Hochberg procedure ( BH ) at FDR level alpha are asymptotically optimal . When n is proportional to log m then alpha can remain fixed , whereas when n increases to infinity at a quicker rate , then alpha has to converge to zero roughly like n^ ( -0/0 ) . Under these conditions the Bonferroni correction is ABOS in case of extreme sparsity , while BH adapts well to the unknown level of sparsity . In the second part of this article these optimality results are carried over to model selection in the context of multiple regression with orthogonal regressors . Several modifications of Bayesian Information Criterion are considered , controlling either FWER or FDR , and conditions are provided under which these selection criteria are ABOS . Finally the performance of these criteria is examined in a brief simulation study .
Cyber-Physical Systems ( CPSs ) involve the interconnection of heterogeneous computing devices which are closely integrated with the physical processes under control . Often , these systems are resource-constrained and require specific features such as the ability to adapt in a timeliness and efficient fashion to dynamic environments . Also , they must support fault tolerance and avoid single points of failure . This paper describes a scalable framework for CPSs based on the OMG DDS standard . The proposed solution allows reconfiguring this kind of systems at run-time and managing efficiently their resources .
Despite efforts to increase the supply of organs from living donors , most kidney transplants performed in Australia still come from deceased donors . The age of these donated organs has increased substantially in recent decades as the rate of fatal accidents on roads has fallen . The Organ and Tissue Authority in Australia is therefore looking to design a new mechanism that better matches the age of the organ to the age of the patient . I discuss the design , axiomatics and performance of several candidate mechanisms that respect the special online nature of this fair division problem .
We consider the problem of locating a single facility on the real line . This facility serves a set of agents , each of whom is located on the line , and incurs a cost equal to his distance from the facility . An agent ' s location is private information that is known only to him . Agents report their location to a central planner who decides where to locate the facility . The planner ' s objective is to minimize a " social " cost function that depends on the agent-costs . However , agents might not report truthfully ; to address this issue , the planner must restrict himself to {\em strategyproof} mechanisms , in which truthful reporting is a dominant strategy for each agent . A mechanism that simply chooses the optimal solution is generally not strategyproof , and so the planner aspires to use a mechanism that effectively {\em approximates} his objective function . In our paper , we study the problem described above with the social cost function being the $L_p$ norm of the vector of agent-costs . We show that the median mechanism ( which is known to be strategyproof ) provides a $0^{0-\frac{0}{p}}$ approximation ratio , and that is the optimal approximation ratio among all deterministic strategyproof mechanisms . For randomized mechanisms , we present two results . First , we present a negative result : we show that for integer $\infty>p>0$ , no mechanism---from a rather large class of randomized mechanisms--- has an approximation ratio better than that of the median mechanism . This is in contrast to the case of $p=0$ and $p=\infty$ where a randomized mechanism provably helps improve the worst case approximation ratio . Second , for the case of 0 agents , we show that a mechanism called LRM , first designed by Procaccia and Tennenholtz for the special case of $L_{\infty}$ , provides the optimal approximation ratio among all randomized mechanisms .
Checking software application suitability using automated software tools has become a vital element for most organisations irrespective of whether they produce in-house software or simply customise off-the-shelf software applications for internal use . As software solutions become ever more complex , the industry becomes increasingly dependent on software automation tools , yet the brittle nature of the available software automation tools limits their effectiveness . Companies invest significantly in obtaining and implementing automation software but most of the tools fail to deliver when the cost of maintaining an effective automation test suite exceeds the cost and time that would have otherwise been spent on manual testing . A failing in the current generation of software automation tools is they do not adapt to unexpected modifications and obstructions without frequent ( and time expensive ) manual interference . Such issues are commonly acknowledged amongst industry practitioners , yet none of the current generation of tools have leveraged the advances in machine learning and artificial intelligence to address these problems . This paper proposes a framework solution that utilises machine learning concepts , namely fuzzy matching and error recovery . The suggested solution applies adaptive techniques to recover from unexpected obstructions that would otherwise have prevented the script from proceeding . Recovery details are presented to the user in a report which can be analysed to determine if the recovery procedure was acceptable and the framework will adapt future runs based on the decisions of the user . Using this framework , a practitioner can run the automated suits without human intervention while minimising the risk of schedule delays .
The histogram is widely used as a simple , exploratory display of data , but it is usually not clear how to choose the number and size of bins for this purpose . We construct a confidence set of distribution functions that optimally address the two main tasks of the histogram : estimating probabilities and detecting features such as increases and ( anti ) modes in the distribution . We define the essential histogram as the histogram in the confidence set with the fewest bins . Thus the essential histogram is the simplest visualization of the data that optimally achieves the main tasks of the histogram . We provide a fast algorithm for computing a slightly relaxed version of the essential histogram , which still possesses most of its beneficial theoretical properties , and we illustrate our methodology with examples . An R-package is available online .
A worldwide movement towards the publication of Open Government Data is taking place , and budget data is one of the key elements pushing this trend . Its importance is mostly related to transparency , but publishing budget data , combined with other actions , can also improve democratic participation , allow comparative analysis of governments and boost data-driven business . However , the lack of standards and common evaluation criteria still hinders the development of appropriate tools and the materialization of the appointed benefits . In this paper , we present a model to analyse government initiatives to publish budget data . We identify the main features of these initiatives with a double objective : ( i ) to drive a structured analysis , relating some dimensions to their possible impacts , and ( ii ) to derive characterization attributes to compare initiatives based on each dimension . We define use perspectives and analyse some initiatives using this model . We conclude that , in order to favour use perspectives , special attention must be given to user feedback , semantics standards and linking possibilities .
An informative sampling design leads to the selection of units whose inclusion probabilities are correlated with the response variable of interest . Model inference performed on the resulting observed sample will be biased for the population generative model . One approach that produces asymptotically unbiased inference employs marginal inclusion probabilities to form sampling weights used to exponentiate each likelihood contribution of a pseudo likelihood used to form a pseudo posterior distribution . Conditions for posterior consistency restrict applicable sampling designs to those under which pairwise inclusion dependencies asymptotically limit to 0 . There are many sampling designs excluded by this restriction ; for example , a multi-stage design that samples individuals within households . Viewing each household as a population , the dependence among individuals does not attenuate . We propose a more targeted approach in this paper for inference focused on pairs of individuals or sampled units ; for example , the substance use of one spouse in a shared household , conditioned on the substance use of the other spouse . We formulate the pseudo likelihood with weights based on pairwise or second order probabilities and demonstrate consistency , removing the requirement for asymptotic independence and replacing it with restrictions on higher order selection probabilities . Our approach provides a nearly automated estimation procedure applicable to any model specified by the data analyst . We demonstrate our method on the National Survey on Drug Use and Health .
Aluminum extrusion die manufacturing is a critical task for productive improvement and increasing potential of competition in aluminum extrusion industry . It causes to meet the efficiency not only consistent quality but also time and production cost reduction . Die manufacturing consists first of die design and process planning in order to make a die for extruding the customer ' s requirement products . The efficiency of die design and process planning are based on the knowledge and experience of die design and die manufacturer experts . This knowledge has been formulated into a computer system called the knowledge-based system . It can be reused to support a new die design and process planning . Such knowledge can be extracted directly from die geometry which is composed of die features . These features are stored in die feature library to be prepared for producing a new die manufacturing . Die geometry is defined according to the characteristics of the profile so we can reuse die features from the previous similar profile design cases . This paper presents the CaseXpert Process Planning System for die manufacturing based on feature based neural network technique . Die manufacturing cases in the case library would be retrieved with searching and learning method by neural network for reusing or revising it to build a die design and process planning when a new case is similar with the previous die manufacturing cases . The results of the system are dies design and machining process . The system has been successfully tested , it has been proved that the system can reduce planning time and respond high consistent plans .
While Bayesian methods are extremely popular in statistics and machine learning , their application to massive datasets is often challenging , when possible at all . Indeed , the classical MCMC algorithms are prohibitively slow when both the model dimension and the sample size are large . Variational Bayesian methods aim at approximating the posterior by a distribution in a tractable family . Thus , MCMC are replaced by an optimization algorithm which is orders of magnitude faster . VB methods have been applied in such computationally demanding applications as including collaborative filtering , image and video processing , NLP and text processing . . . However , despite very nice results in practice , the theoretical properties of these approximations are usually not known . In this paper , we propose a general approach to prove the concentration of variational approximations of fractional posteriors . We apply our theory to two examples : matrix completion , and Gaussian VB .
The Amazon Picking Challenge ( APC ) , held alongside the International Conference on Robotics and Automation in May 0000 in Seattle , challenged roboticists from academia and industry to demonstrate fully automated solutions to the problem of picking objects from shelves in a warehouse fulfillment scenario . Packing density , object variability , speed , and reliability are the main complexities of the task . The picking challenge serves both as a motivation and an instrument to focus research efforts on a specific manipulation problem . In this document , we describe Team MIT ' s approach to the competition , including design considerations , contributions , and performance , and we compile the lessons learned . We also describe what we think are the main remaining challenges .
This paper presents an automated approach for interpretable feature recommendation for solving signal data analytics problems . The method has been tested by performing experiments on datasets in the domain of prognostics where interpretation of features is considered very important . The proposed approach is based on Wide Learning architecture and provides means for interpretation of the recommended features . It is to be noted that such an interpretation is not available with feature learning approaches like Deep Learning ( such as Convolutional Neural Network ) or feature transformation approaches like Principal Component Analysis . Results show that the feature recommendation and interpretation techniques are quite effective for the problems at hand in terms of performance and drastic reduction in time to develop a solution . It is further shown by an example , how this human-in-loop interpretation system can be used as a prescriptive system .
We report results from a measurement study of three video streaming services , YouTube , Dailymotion and Vimeo on six different smartphones . We measure and analyze the traffic and energy consumption when streaming different quality videos over Wi-Fi and 0G . We identify five different techniques to deliver the video and show that the use of a particular technique depends on the device , player , quality , and service . The energy consumption varies dramatically between devices , services , and video qualities depending on the streaming technique used . As a consequence , we come up with suggestions on how to improve the energy efficiency of mobile video streaming services .
The subject of this paper is to introduce a novel permutation-based nonparametric approach for the problem of ranking several multivariate populations with respect to both experimental and observation studies to be referred to the most useful design such as MANOVA ( multivariate independent samples ) and MRCB ( multivariate randomized complete block design , i . e . multivariate dependent samples also known as repeated measures ) . This topic is not only of theoretical interest but also have a practical relevance , especially to business and industrial research where a reliable global ranking in terms of performance of all investigated products/prototypes is a very natural goal . In fact , the need to define an appropriate ranking of items ( products , services , teaching courses , degree programs , and so on ) is very common in both experimental and observational studies within the areas of business and industrial research .
A community ' s identity defines and shapes its internal dynamics . Our current understanding of this interplay is mostly limited to glimpses gathered from isolated studies of individual communities . In this work we provide a systematic exploration of the nature of this relation across a wide variety of online communities . To this end we introduce a quantitative , language-based typology reflecting two key aspects of a community ' s identity : how distinctive , and how temporally dynamic it is . By mapping almost 000 Reddit communities into the landscape induced by this typology , we reveal regularities in how patterns of user engagement vary with the characteristics of a community . Our results suggest that the way new and existing users engage with a community depends strongly and systematically on the nature of the collective identity it fosters , in ways that are highly consequential to community maintainers . For example , communities with distinctive and highly dynamic identities are more likely to retain their users . However , such niche communities also exhibit much larger acculturation gaps between existing users and newcomers , which potentially hinder the integration of the latter . More generally , our methodology reveals differences in how various social phenomena manifest across communities , and shows that structuring the multi-community landscape can lead to a better understanding of the systematic nature of this diversity .
Lederer and van de Geer ( 0000 ) introduced a new Orlicz norm , the Bernstein-Orlicz norm , which is connected to Bernstein type inequalities . Here we introduce another Orlicz norm , the Bennett-Orlicz norm , which is connected to Bennett type inequalities . The new Bennett-Orlicz norm yields inequalities for expectations of maxima which are potentially somewhat tighter than those resulting from the Bernstein-Orlicz norm when they are both applicable . We discuss cross connections between these norms , exponential inequalities of the Bernstein , Bennett , and Prokhorov types , and make comparisons with results of Talagrand ( 0000 , 0000 ) , and Boucheron , Lugosi , and Massart ( 0000 ) .
In this paper , we review and apply several approaches to model selection for analysis of variance models which are used in a credibility and insurance context . The reversible jump algorithm is employed for model selection , where posterior model probabilities are computed . We then apply this method to insurance data from workers ' compensation insurance schemes . The reversible jump results are compared with the Deviance Information Criterion , and are shown to be consistent .
The biological immune system is a robust , complex , adaptive system that defends the body from foreign pathogens . It is able to categorize all cells ( or molecules ) within the body as self or non-self substances . It does this with the help of a distributed task force that has the intelligence to take action from a local and also a global perspective using its network of chemical messengers for communication . There are two major branches of the immune system . The innate immune system is an unchanging mechanism that detects and destroys certain invading organisms , whilst the adaptive immune system responds to previously unknown foreign cells and builds a response to them that can remain in the body over a long period of time . This remarkable information processing biological system has caught the attention of computer science in recent years . A novel computational intelligence technique , inspired by immunology , has emerged , called Artificial Immune Systems . Several concepts from the immune system have been extracted and applied for solution to real world science and engineering problems . In this tutorial , we briefly describe the immune system metaphors that are relevant to existing Artificial Immune Systems methods . We will then show illustrative real-world problems suitable for Artificial Immune Systems and give a step-by-step algorithm walkthrough for one such problem . A comparison of the Artificial Immune Systems to other well-known algorithms , areas for future work , tips & tricks and a list of resources will round this tutorial off . It should be noted that as Artificial Immune Systems is still a young and evolving field , there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from time to time and from those examples given here .
X in R^D has mean zero and finite second moments . We show that there is a precise sense in which almost all linear projections of X into R^d ( for d < D ) look like a scale-mixture of spherical Gaussians -- specifically , a mixture of distributions N ( 0 , sigma^0 I_d ) where the weight of the particular sigma component is P ( | X |^0 = sigma^0 D ) . The extent of this effect depends upon the ratio of d to D , and upon a particular coefficient of eccentricity of X ' s distribution . We explore this result in a variety of experiments .
SimOutUtils is a suite of MATLAB/Octave functions for studying and analyzing time series-like output from stochastic simulation models . More specifically , SimOutUtils allows modelers to study and visualize simulation output dynamics , perform distributional analysis of output statistical summaries , as well as compare these summaries in order to assert the statistical equivalence of two or more model implementations . Additionally , the provided functions are able to produce publication quality figures and tables showcasing results from the specified simulation output studies .
This article describes lossless compression algorithms for multisets of sequences , taking advantage of the multiset ' s unordered structure . Multisets are a generalisation of sets where members are allowed to occur multiple times . A multiset can be encoded na\ " ively by simply storing its elements in some sequential order , but then information is wasted on the ordering . We propose a technique that transforms the multiset into an order-invariant tree representation , and derive an arithmetic code that optimally compresses the tree . Our method achieves compression even if the sequences in the multiset are individually incompressible ( such as cryptographic hash sums ) . The algorithm is demonstrated practically by compressing collections of SHA-0 hash sums , and multisets of arbitrary , individually encodable objects .
The cutting edge in systems development today is in the area of " systems of systems " ( SoS ) large networks of inter-related systems that are developed and managed separately , but that also perform collective activities . Such large systems typically involve constituent systems operating with different life cycles , often with uncoordinated evolution . The result is an ever-changing SoS in which adaptation and evolution replace the older engineering paradigm of " development " . This short paper presents key thoughts about verification and validation in this environment . Classic verification and validation methods rely on having ( a ) a basis of proof , in requirements and in operational scenarios , and ( b ) a known system configuration to be proven . However , with constant SoS evolution , management of both requirements and system configurations are problematic . Often , it is impossible to maintain a valid set of requirements for the SoS due to the ongoing changes in the constituent systems . Frequently , it is even difficult to maintain a vision of the SoS operational use as users find new ways to adapt the SoS . These features of the SoS result in significant challenges for system proof . In addition to discussing the issues , the paper also indicates some of the solutions that are currently used to prove the SoS .
In this paper , we present a communication-free algorithm for distributed coverage of an arbitrary network by a group of mobile agents with local sensing capabilities . The network is represented as a graph , and the agents are arbitrarily deployed on some nodes of the graph . Any node of the graph is covered if it is within the sensing range of at least one agent . The agents are mobile devices that aim to explore the graph and to optimize their locations in a decentralized fashion by relying only on their sensory inputs . We formulate this problem in a game theoretic setting and propose a communication-free learning algorithm for maximizing the coverage .
We use a simple machine learning model , logistically-weighted regularized linear least squares regression , in order to predict baseball , basketball , football , and hockey games . We do so using only the thirty-year record of which visiting teams played which home teams , on what date , and what the final score was . No real " statistics " are used . The method works best in basketball , likely because it is high-scoring and has long seasons . It works better in football and hockey than in baseball , but in baseball the predictions are closer to a theoretical optimum . The football predictions , while good , can in principle be made much better , and the hockey predictions can be made somewhat better . These findings tells us that in basketball , most statistics are subsumed by the scores of the games , whereas in football , further study of game and player statistics is necessary to predict games as well as can be done . Baseball and hockey lie somewhere in between .
Based on the convex least-squares estimator , we propose two different procedures for testing convexity of a probability mass function supported on N with an unknown finite support . The procedures are shown to be asymptotically calibrated .
Out of the scope of the usual positions of computing in the field of music and musicology , one notices the emergence of human-computer systems that do exist by breaking off . Though these singular systems take effect in the usual fields of expansion of music , they do not make any systematic reference to known musicological categories . On the contrary , they make possible experiments that open uses where listening , composition and musical transmission get merged in a gesture sometimes named as ? music-ripping ? . We will show in which way the music-ripping practices provoke traditional musicology , whose canonical categories happen to be ineffectual to explain here . To achieve that purpose , we shall need : - to make explicit a minimal set of categories that is sufficient to underlie the usual models of computer assisted music ; - to do the same for human-computer systems ( anti-musicological ? ) that disturb us ; - to examine the possibility conditions of reduction of the second set to the first ; - to conclude on the nature of music-ripping .
We propose Very Simple Classifier ( VSC ) a novel method designed to incorporate the concepts of subsampling and locality in the definition of features to be used as the input of a perceptron . The rationale is that locality theoretically guarantees a bound on the generalization error . Each feature in VSC is a max-margin classifier built on randomly-selected pairs of samples . The locality in VSC is achieved by multiplying the value of the feature by a confidence measure that can be characterized in terms of the Chebichev inequality . The output of the layer is then fed in a output layer of neurons . The weights of the output layer are then determined by a regularized pseudoinverse . Extensive comparison of VSC against 0 competitors in the task of binary classification is carried out . Results on 00 benchmark datasets with fixed parameters show that VSC is competitive with the Multi Layer Perceptron ( MLP ) and outperforms the other competitors . An exploration of the parameter space shows VSC can outperform MLP .
We generalize the results of \cite{SPU , SJPU} by showing how the Gaussian aggregator may be computed in a setting where parameter estimation is not required . We proceed to provide an explicit formula for a " one-shot " aggregation problem with two forecasters .
Global software engineering has become a fact in many companies due to real necessity in practice . In contrast to co-located projects global projects face a number of additional software engineering challenges . Among them quality management has become much more difficult and schedule and budget overruns can be observed more often . Compared to co-located projects global software engineering is even more challenging due to the need for integration of different cultures , different languages , and different time zones - across companies , and across countries . The diversity of development locations on several levels seriously endangers an effective and goal-oriented progress of projects . In this position paper we discuss reasons for global development , sketch settings for distribution and views of orchestration of dislocated companies in a global project that can be seen as a " virtual project environment " . We also present a collection of questions , which we consider relevant for global software engineering . The questions motivate further discussion to derive a research agenda in global software engineering .
Network creation games model the creation and usage costs of networks formed by n selfish nodes . Each node v can buy a set of edges , each for a fixed price \alpha > 0 . Its goal is to minimize its private costs , i . e . , the sum ( SUM-game , Fabrikant et al . , PODC 0000 ) or maximum ( MAX-game , Demaine et al . , PODC 0000 ) of distances from $v$ to all other nodes plus the prices of the bought edges . The above papers show the existence of Nash equilibria as well as upper and lower bounds for the prices of anarchy and stability . In several subsequent papers , these bounds were improved for a wide range of prices \alpha . In this paper , we extend these models by incorporating quality-of-service aspects : Each edge cannot only be bought at a fixed quality ( edge length one ) for a fixed price \alpha . Instead , we assume that quality levels ( i . e . , edge lengths ) are varying in a fixed interval [\beta , B] , 0 < \beta <= B . A node now cannot only choose which edge to buy , but can also choose its quality x , for the price p ( x ) , for a given price function p . For both games and all price functions , we show that Nash equilibria exist and that the price of stability is either constant or depends only on the interval size of available edge lengths . Our main results are bounds for the price of anarchy . In case of the SUM-game , we show that they are tight if price functions decrease sufficiently fast .
We present POAPS , a novel planning system for defining Partially Observable Markov Decision Processes ( POMDPs ) that abstracts away from POMDP details for the benefit of non-expert practitioners . POAPS includes an expressive adaptive programming language based on Lisp that has constructs for choice points that can be dynamically optimized . Non-experts can use our language to write adaptive programs that have partially observable components without needing to specify belief/hidden states or reason about probabilities . POAPS is also a compiler that defines and performs the transformation of any program written in our language into a POMDP with control knowledge . We demonstrate the generality and power of POAPS in the rapidly growing domain of human computation by describing its expressiveness and simplicity by writing several POAPS programs for common crowdsourcing tasks .
An optimal index solving top-k document retrieval [Navarro and Nekrich , SODA00] takes O ( m + k ) time for a pattern of length m , but its space is at least 00n bytes for a collection of n symbols . We reduce it to 0 . 0n to 0n bytes , with O ( m+ ( k+log log n ) log log n ) time , on typical texts . The index is up to 00 times faster than the best previous compressed solutions , and requires at most 0% more space in practice ( and in some cases as little as one half ) . Apart from replacing classical by compressed data structures , our main idea is to replace suffix tree sampling by frequency thresholding to achieve compression .
We use a trading metaphor to study knowledge transfer in the sciences as well as the social sciences . The metaphor comprises four dimensions : ( a ) Discipline Self-dependence , ( b ) Knowledge Exports/Imports , ( c ) Scientific Trading Dynamics , and ( d ) Scientific Trading Impact . This framework is applied to a dataset of 000 Web of Science subject categories . We find that : ( i ) the Scientific Trading Impact and Dynamics of Materials Science And Transportation Science have increased ; ( ii ) Biomedical Disciplines , Physics , And Mathematics are significant knowledge exporters , as is Statistics & Probability ; ( iii ) in the social sciences , Economics , Business , Psychology , Management , And Sociology are important knowledge exporters ; ( iv ) Discipline Self-dependence is associated with specialized domains which have ties to professional practice ( e . g . , Law , Ophthalmology , Dentistry , Oral Surgery & Medicine , Psychology , Psychoanalysis , Veterinary Sciences , And Nursing ) .
Variational autoencoders ( VAEs ) , that are built upon deep neural networks have emerged as popular generative models in computer vision . Most of the work towards improving variational autoencoders has focused mainly on making the approximations to the posterior flexible and accurate , leading to tremendous progress . However , there have been limited efforts to replace pixel-wise reconstruction , which have known shortcomings . In this work , we use real-valued non-volume preserving transformations ( real NVP ) to exactly compute the conditional likelihood of the data given the latent distribution . We show that a simple VAE with this form of reconstruction is competitive with complicated VAE structures , on image modeling tasks . As part of our model , we develop powerful conditional coupling layers that enable real NVP to learn with fewer intermediate layers .
Gaussian graphical models are used throughout the natural sciences , social sciences , and economics to model the statistical relationships between variables of interest in the form of a graph . We here provide a pedagogic introduction to Gaussian graphical models and review recent results on maximum likelihood estimation for such models . Throughout , we highlight the rich algebraic and geometric properties of Gaussian graphical models and explain how these properties relate to convex optimization and ultimately result in insights on the existence of the maximum likelihood estimator ( MLE ) and algorithms for computing the MLE .
Modern embedded systems have made the transition from single-core to multi-core architectures , providing performance improvement via parallelism rather than higher clock frequencies . DAGs are considered among the most generic task models in the real-time domain and are well suited to exploit this parallelism . In this paper we provide a schedulability test using response-time analysis exploiting exploring and bounding the self interference of a DAG task . Additionally we bound the interference a high priority task has on lower priority ones .
We propose a variable decomposition algorithm -greedy block coordinate descent ( GBCD ) - in order to make dense Gaussian process regression practical for large scale problems . GBCD breaks a large scale optimization into a series of small sub-problems . The challenge in variable decomposition algorithms is the identification of a subproblem ( the active set of variables ) that yields the largest improvement . We analyze the limitations of existing methods and cast the active set selection into a zero-norm constrained optimization problem that we solve using greedy methods . By directly estimating the decrease in the objective function , we obtain not only efficient approximate solutions for GBCD , but we are also able to demonstrate that the method is globally convergent . Empirical comparisons against competing dense methods like Conjugate Gradient or SMO show that GBCD is an order of magnitude faster . Comparisons against sparse GP methods show that GBCD is both accurate and capable of handling datasets of 000 , 000 samples or more .
Preserving the utility of published datasets while simultaneously providing provable privacy guarantees is a well-known challenge . On the one hand , context-free privacy solutions , such as differential privacy , provide strong privacy guarantees , but often lead to a significant reduction in utility . On the other hand , context-aware privacy solutions , such as information theoretic privacy , achieve an improved privacy-utility tradeoff , but assume that the data holder has access to dataset statistics . We circumvent these limitations by introducing a novel context-aware privacy framework called generative adversarial privacy ( GAP ) . GAP leverages recent advancements in generative adversarial networks ( GANs ) to allow the data holder to learn privatization schemes from the dataset itself . Under GAP , learning the privacy mechanism is formulated as a constrained minimax game between two players : a privatizer that sanitizes the dataset in a way that limits the risk of inference attacks on the individuals ' private variables , and an adversary that tries to infer the private variables from the sanitized dataset . To evaluate GAP ' s performance , we investigate two simple ( yet canonical ) statistical dataset models : ( a ) the binary data model , and ( b ) the binary Gaussian mixture model . For both models , we derive game-theoretically optimal minimax privacy mechanisms , and show that the privacy mechanisms learned from data ( in a generative adversarial fashion ) match the theoretically optimal ones . This demonstrates that our framework can be easily applied in practice , even in the absence of dataset statistics .
We study computational aspects of the nonparametric maximum likelihood estimator ( NPMLE ) for the distribution function of bivariate interval censored data . The computation of the NPMLE consists of two steps : a parameter reduction step and an optimization step . In this paper we focus on the reduction step . We introduce two new reduction algorithms : the Tree algorithm and the HeightMap algorithm . The Tree algorithm is only mentioned briefly . The HeightMap algorithm is discussed in detail and also given in pseudo code . It is a very fast and simple algorithm of time complexity O ( n^0 ) . This is an order faster than the best known algorithm thus far , the O ( n^0 ) algorithm of Bogaerts and Lesaffre ( 0000 ) . We compare our algorithms with the algorithms of Gentleman and Vandal ( 0000 ) , Song ( 0000 ) and Bogaerts and Lesaffre ( 0000 ) , using simulated data . We show that our algorithms , and especially the HeightMap algorithm , are significantly faster . Finally , we point out that the HeightMap algorithm can be easily generalized to d-dimensional data with d>0 . Such a multivariate version of the HeightMap algorithm has time complexity O ( n^d ) .
The use of an Ornstein-Uhlenbeck ( OU ) process is ubiquitous in business , economics and finance to capture various price processes and evolution of economic indicators exhibiting mean-reverting properties . When structural changes happen , economic dynamics drastically change and the times at which these occur are of particular interest to policy makers , investors and financial product providers . This paper addresses the change-point problem under a generalised OU model and investigates the associated statistical inference . We propose two estimation methods to locate multiple change points and show the asymptotic properties of the estimators . An informational approach is employed in detecting the change points , and the consistency of our methods is also theoretically demonstrated . Estimation is considered under the setting where both the number and location of change points are unknown . Three computing algorithms are further developed for implementation . The practical applicability of our methods is illustrated using simulated and observed financial market data .
We present a parallel visualization algorithm for the illustrative rendering of depth-dependent stylized dense tube data at interactive frame rates . While this computation could be efficiently performed on a GPU device , we target a parallel framework to enable it to be efficiently running on an ordinary multi-core CPU platform which is much more available than GPUs for common users . Our approach is to map the depth information in each tube onto each of the visual dimensions of shape , color , texture , value , and size on the basis of Bertin ' s semiology theory . The purpose is to enable more legible displays in the dense tube environments . A major contribution of our work is an efficient and effective parallel depthordering algorithm that makes use of the message passing interface ( MPI ) with VTK . We evaluated our framework with visualizations of depth-stylized tubes derived from 0D diffusion tensor MRI data by comparing its efficiency with several other alternative parallelization platforms running the same computations . As our results show , the parallelization framework we proposed can efficiently render highly dense 0D data sets like the tube data and thus is useful as a complement to parallel visualization environments that rely on GPUs .
Based on independently distributed $X_0 \sim N_p ( \theta_0 , \sigma^0_0 I_p ) $ and $X_0 \sim N_p ( \theta_0 , \sigma^0_0 I_p ) $ , we consider the efficiency of various predictive density estimators for $Y_0 \sim N_p ( \theta_0 , \sigma^0_Y I_p ) $ , with the additional information $\theta_0 - \theta_0 \in A$ and known $\sigma^0_0 , \sigma^0_0 , \sigma^0_Y$ . We provide improvements on benchmark predictive densities such as plug-in , the maximum likelihood , and the minimum risk equivariant predictive densities . Dominance results are obtained for $\alpha-$divergence losses and include Bayesian improvements for reverse Kullback-Leibler loss , and Kullback-Leibler ( KL ) loss in the univariate case ( $p=0$ ) . An ensemble of techniques are exploited , including variance expansion ( for KL loss ) , point estimation duality , and concave inequalities . Representations for Bayesian predictive densities , and in particular for $\hat{q}_{\pi_{U , A}}$ associated with a uniform prior for $\theta= ( \theta_0 , \theta_0 ) $ truncated to $\{\theta \in \mathbb{R}^{0p} : \theta_0 - \theta_0 \in A \}$ , are established and are used for the Bayesian dominance findings . Finally and interestingly , these Bayesian predictive densities also relate to skew-normal distributions , as well as new forms of such distributions .
We study ridge estimation of the precision matrix in the high-dimensional setting where the number of variables is large relative to the sample size . We first review two archetypal ridge estimators and note that their utilized penalties do not coincide with common ridge penalties . Subsequently , starting from a common ridge penalty , analytic expressions are derived for two alternative ridge estimators of the precision matrix . The alternative estimators are compared to the archetypes with regard to eigenvalue shrinkage and risk . The alternatives are also compared to the graphical lasso within the context of graphical modeling . The comparisons may give reason to prefer the proposed alternative estimators .
In the literature of game theory , the information sets of extensive form games have different interpretations , which may lead to confusions and paradoxical cases . We argue that the problem lies in the mix-up of two interpretations of the extensive form game structures : game rules or game runs which do not always coincide . In this paper , we try to separate and connect these two views by proposing a dynamic epistemic framework in which we can compute the runs step by step from the game rules plus the given assumptions of the players . We propose a modal logic to describe players ' knowledge and its change during the plays , and provide a complete axiomatization . We also show that , under certain conditions , the mix-up of the rules and the runs is not harmful due to the structural similarity of the two .
This paper investigates a novel task of generating texture images from perceptual descriptions . Previous work on texture generation focused on either synthesis from examples or generation from procedural models . Generating textures from perceptual attributes have not been well studied yet . Meanwhile , perceptual attributes , such as directionality , regularity and roughness are important factors for human observers to describe a texture . In this paper , we propose a joint deep network model that combines adversarial training and perceptual feature regression for texture generation , while only random noise and user-defined perceptual attributes are required as input . In this model , a preliminary trained convolutional neural network is essentially integrated with the adversarial framework , which can drive the generated textures to possess given perceptual attributes . An important aspect of the proposed model is that , if we change one of the input perceptual features , the corresponding appearance of the generated textures will also be changed . We design several experiments to validate the effectiveness of the proposed method . The results show that the proposed method can produce high quality texture images with desired perceptual properties .
We present two methods for detecting patterns and clusters in high dimensional time-dependent functional data . Our methods are based on wavelet-based similarity measures , since wavelets are well suited for identifying highly discriminant local time and scale features . The multiresolution aspect of the wavelet transform provides a time-scale decomposition of the signals allowing to visualize and to cluster the functional data into homogeneous groups . For each input function , through its empirical orthogonal wavelet transform the first method uses the distribution of energy across scales generate a handy number of features that can be sufficient to still make the signals well distinguishable . Our new similarity measure combined with an efficient feature selection technique in the wavelet domain is then used within more or less classical clustering algorithms to effectively differentiate among high dimensional populations . The second method uses dissimilarity measures between the whole time-scale representations and are based on wavelet-coherence tools . The clustering is then performed using a k-centroid algorithm starting from these dissimilarities . Practical performance of these methods that jointly designs both the feature selection in the wavelet domain and the classification distance is demonstrated through simulations as well as daily profiles of the French electricity power demand .
We study the {\em verification} problem in distributed networks , stated as follows . Let $H$ be a subgraph of a network $G$ where each vertex of $G$ knows which edges incident on it are in $H$ . We would like to verify whether $H$ has some properties , e . g . , if it is a tree or if it is connected . We would like to perform this verification in a decentralized fashion via a distributed algorithm . The time complexity of verification is measured as the number of rounds of distributed communication . In this paper we initiate a systematic study of distributed verification , and give almost tight lower bounds on the running time of distributed verification algorithms for many fundamental problems such as connectivity , spanning connected subgraph , and $s-t$ cut verification . We then show applications of these results in deriving strong unconditional time lower bounds on the {\em hardness of distributed approximation} for many classical optimization problems including minimum spanning tree , shortest paths , and minimum cut . Many of these results are the first non-trivial lower bounds for both exact and approximate distributed computation and they resolve previous open questions . Moreover , our unconditional lower bound of approximating minimum spanning tree ( MST ) subsumes and improves upon the previous hardness of approximation bound of Elkin [STOC 0000] as well as the lower bound for ( exact ) MST computation of Peleg and Rubinovich [FOCS 0000] . Our result implies that there can be no distributed approximation algorithm for MST that is significantly faster than the current exact algorithm , for {\em any} approximation factor . Our lower bound proofs show an interesting connection between communication complexity and distributed computing which turns out to be useful in establishing the time complexity of exact and approximate distributed computation of many problems .
We present a system for identifying conceptual shifts between visual categories , which will form the basis for a co-creative drawing system to help users draw more creative sketches . The system recognizes human sketches and matches them to structurally similar sketches from categories to which they do not belong . This would allow a co-creative drawing system to produce an ambiguous sketch that blends features from both categories .
Every data has a lot of hidden information . The processing method of data decides what type of information data produce . In India education sector has a lot of data that can produce valuable information . This information can be used to increase the quality of education . But educational institution does not use any knowledge discovery process approach on these data . Information and communication technology puts its leg into the education sector to capture and compile low cost information . Now a day a new research community , educational data mining ( EDM ) , is growing which is intersection of data mining and pedagogy . In this paper we present roadmap of research done in EDM in various segment of education sector .
In this paper we present the design and implementation , as well as a use case , of a tool for workflow analysis . The tool provides an assistant for the specification of properties of a workflow model . The specification language for property description is Fluent Linear Time Temporal Logic . Fluents provide an adequate flexibility for capturing properties of workflows . Both the model and the properties are encoded , in an automated way , as Labelled Transition Systems , and the analysis is reduced to model checking .
A map is an abstract visual representation of a region , taken from a given space , usually designed for final human consumption . Traditional cartography focuses on the mapping of Euclidean spaces by using some distance metric . In this paper we aim at mapping the Web space by leveraging its relational nature . We introduce a general mathematical framework for maps and an algebra and discuss the feasibility of maps suitable for interpretation not only by humans but also by machines .
Mella is a minimalistic dependently typed programming language and interactive theorem prover implemented in Haskell . Its main purpose is to investigate the effective integration of automated theorem provers in a pure and simple setting . Such integrations are essential for supporting program development in dependently typed languages . We integrate the equational theorem prover Waldmeister and test it on more than 000 proof goals from the TPTP library . In contrast to previous approaches , the reconstruction of Waldmeister proofs within Mella is quite robust and does not generate a significant overhead to proof search . Mella thus yields a template for integrating more expressive theorem provers in more sophisticated languages .
We consider the prediction error of linear regression with L0 regularization when the number of covariates p is large relative to the sample size n . When the model is k-sparse and well-specified , and restricted isometry or similar conditions hold , the excess squared-error in prediction can be bounded on the order of sigma^0* ( k*log ( p ) /n ) , where sigma^0 is the noise variance . Although these conditions are close to necessary for accurate recovery of the true coefficient vector , it is possible to guarantee good predictive accuracy under much milder conditions , avoiding the restricted isometry condition , but only ensuring an excess error bound of order ( k*log ( p ) /n ) +sigma*\surd ( k*log ( p ) /n ) . Here we show that this is indeed the best bound possible ( up to logarithmic factors ) without introducing stronger assumptions similar to restricted isometry .
In a few months the computer mouse will be half-a-century-old . It is known to have many drawbacks , the main ones being : loss of productivity due to constant switching between keyboard and mouse , and health issues such as RSI . Like the keyboard , it is an unnatural human-computer interface . However the vast majority of computer users still use computer mice nowadays . In this article , we explore computer mouse alternatives . Our research shows that moving the mouse cursor can be done efficiently with camera-based head tracking system such as the SmartNav device , and mouse clicks can be emulated in many complementary ways . We believe that computer users can increase their productivity and improve their long-term health by using these alternatives .
Distributed Software Development today is in its childhood and not too widespread as a method of developing software in the global IT Industry . In this context , Petrinets are a mathematical model for describing distributed systems theoretically , whereas AttNets are one of their offshoots . But development of true distributed software is limited to network operating systems majorly . Software that runs on many machines with separate programs for each machine , are very few . This paper introduces and defines Distributed Object Oriented Software Engineering DOOSE as a new field in software engineering . The paper further gives a Distributed Object Oriented Software Process Model DOOSPM , called the DolNet , which describes how work may be done by a software development organization while working on Distributed Object Oriented DOO Projects .
Graph embedding provides an efficient solution for graph analysis by converting the graph into a low-dimensional space which preserves the structure information . In contrast to the graph structure data , the i . i . d . node embedding can be processed efficiently in terms of both time and space . Current semi-supervised graph embedding algorithms assume the labelled nodes are given , which may not be always true in the real world . While manually label all training data is inapplicable , how to select the subset of training data to label so as to maximize the graph analysis task performance is of great importance . This motivates our proposed active graph embedding ( AGE ) framework , in which we design a general active learning query strategy for any semi-supervised graph embedding algorithm . AGE selects the most informative nodes as the training labelled nodes based on the graphical information ( i . e . , node centrality ) as well as the learnt node embedding ( i . e . , node classification uncertainty and node embedding representativeness ) . Different query criteria are combined with the time-sensitive parameters which shift the focus from graph based query criteria to embedding based criteria as the learning progresses . Experiments have been conducted on three public data sets and the results verified the effectiveness of each component of our query strategy and the power of combining them using time-sensitive parameters . Our code is available online at : https : //github . com/vwz/AGE .
Recovering low-rank structures via eigenvector perturbation analysis is a common problem in statistical machine learning , such as in factor analysis , community detection , ranking , matrix completion , among others . While a large variety of results provide tight bounds on the average errors between empirical and population statistics of eigenvectors , fewer results are tight for entrywise analyses , which are critical for a number of problems such as community detection and ranking . This paper investigates the entrywise perturbation analysis for a large class of random matrices whose expectations are low-rank , including community detection , synchronization ( $\mathbb{Z}_0$-spiked Wigner model ) and matrix completion models . Denoting by $\{u_k\}$ , respectively $\{u_k^*\}$ , the eigenvectors of a random matrix $A$ , respectively $\mathbb{E} A$ , the paper characterizes cases for which $$u_k \approx \frac{A u_k^*}{\lambda_k^*}$$ serves as a first-order approximation under the $\ell_\infty$ norm . The fact that the approximation is both tight and linear in the random matrix $A$ allows for sharp comparisons of $u_k$ and $u_k^*$ . In particular , it allows to compare the signs of $u_k$ and $u_k^*$ even when $\| u_k - u_k^*\|_{\infty}$ is large , which in turn allows to settle the conjecture in Abbe et al . ( 0000 ) that the spectral algorithm achieves exact recovery in the stochastic block model without any trimming or cleaning steps . The results are further extended to the perturbation of eigenspaces , providing new bounds for $\ell_\infty$-type errors in noisy matrix completion .
This work investigates the use of deep fully convolutional neural networks ( DFCNN ) for pixel-wise scene labeling of Earth Observation images . Especially , we train a variant of the SegNet architecture on remote sensing data over an urban area and study different strategies for performing accurate semantic segmentation . Our contributions are the following : 0 ) we transfer efficiently a DFCNN from generic everyday images to remote sensing images ; 0 ) we introduce a multi-kernel convolutional layer for fast aggregation of predictions at multiple scales ; 0 ) we perform data fusion from heterogeneous sensors ( optical and laser ) using residual correction . Our framework improves state-of-the-art accuracy on the ISPRS Vaihingen 0D Semantic Labeling dataset .
We develop a semantics framework for verifying recent relaxations of differential privacy : R\ ' enyi differential privacy and zero-concentrated differential privacy . Both notions require a bound on a particular statistical divergence between two probability distributions . In order to reason about such properties compositionally , we introduce approximate span-liftings , generalizing approximate relational liftings previously developed for standard differential privacy to a more general class of divergences , and to continuous distributions . To enable verification of possibly non-terminating programs , our framework supports generalized divergences between subprobability measures . As a concrete application , we use approximate span-liftings to develop a program logic that can prove relaxations of differential privacy and other probabilistic properties based on statistical
Many computer programs have graphical user interfaces ( GUIs ) , which need good layout to make efficient use of the available screen real estate . Most GUIs do not have a fixed layout , but are resizable and able to adapt themselves . Constraints are a powerful tool for specifying adaptable GUI layouts : they are used to specify a layout in a general form , and a constraint solver is used to find a satisfying concrete layout , e . g . \ for a specific GUI size . The constraint solver has to calculate a new layout every time a GUI is resized or changed , so it needs to be efficient to ensure a good user experience . One approach for constraint solvers is based on the Gauss-Seidel algorithm and successive over-relaxation ( SOR ) . Our observation is that a solution after resizing or changing is similar in structure to a previous solution . Thus , our hypothesis is that we can increase the computational performance of an SOR-based constraint solver if we reuse the solution of a previous layout to warm-start the solving of a new layout . In this paper we report on experiments to test this hypothesis experimentally for three common use cases : big-step resizing , small-step resizing and constraint change . In our experiments , we measured the solving time for randomly generated GUI layout specifications of various sizes . For all three cases we found that the performance is improved if an existing solution is used as a starting solution for a new layout .
An n-simplex is said to be n-well-centered if its circumcenter lies in its interior . We introduce several other geometric conditions and an algebraic condition that can be used to determine whether a simplex is n-well-centered . These conditions , together with some other observations , are used to describe restrictions on the local combinatorial structure of simplicial meshes in which every simplex is well-centered . In particular , it is shown that in a 0-well-centered ( 0-well-centered ) tetrahedral mesh there are at least 0 ( 0 ) edges incident to each interior vertex , and these bounds are sharp . Moreover , it is shown that , in stark contrast to the 0-dimensional analog , where there are exactly two vertex links that prevent a well-centered triangle mesh in R^0 , there are infinitely many vertex links that prohibit a well-centered tetrahedral mesh in R^0 .
In this chapter , we analyze nonlinear filtering problems in distributed environments , e . g . , sensor networks or peer-to-peer protocols . In these scenarios , the agents in the environment receive measurements in a streaming fashion , and they are required to estimate a common ( nonlinear ) model by alternating local computations and communications with their neighbors . We focus on the important distinction between single-task problems , where the underlying model is common to all agents , and multitask problems , where each agent might converge to a different model due to , e . g . , spatial dependencies or other factors . Currently , most of the literature on distributed learning in the nonlinear case has focused on the single-task case , which may be a strong limitation in real-world scenarios . After introducing the problem and reviewing the existing approaches , we describe a simple kernel-based algorithm tailored for the multitask case . We evaluate the proposal on a simulated benchmark task , and we conclude by detailing currently open problems and lines of research .
We present the second in a series of three academic essays which deal with the question of how to build a generalized player model . We begin with a proposition : a general model of players requires parameters for the subjective experience of play , including at least three areas : a ) player psychology , b ) game structure , and c ) actions of play . Based on this proposition , we pose three linked research questions , which make incomplete progress toward a generalized player model : RQ0 what is a necessary and sufficient foundation to a general player model ? ; RQ0 can such a foundation improve performance of a computational intelligence-based player model ? ; and RQ0 can such a player model improve efficacy of adaptive artificial intelligence in games ? We set out the arguments for each research question in each of the three essays , presented as three preprints . The second essay , in this preprint , illustrates how our ' Behavlets ' method can improve the performance and accuracy of a predictive player model in the well-known Pac-Man game , by providing a simple foundation for areas a ) to c ) above . We then propose a plan for future work to address RQ0 by conclusively testing the Behavlets approach . This plan builds on the work proposed in the first preprint essay to address RQ0 , and in turn provides support for work on RQ0 . The Behavlets approach was described previously ; therefore if citing this work please use the correct citation : Cowley B , Charles D . Behavlets : a Method for Practical Player Modelling using Psychology-Based Player Traits and Domain Specific Features . User Modelling and User-Adapted Interaction . 0000 Feb 0 ; online ( Special Issue on Personality in Personalized Systems ) : 0-00 .
Mechanism design for a social utility being the sum of agents ' utilities ( SoU ) is a well-studied problem . There are , however , a number of problems of theoretical and practical interest where a designer may have a different objective than maximization of the SoU . One motivation for this is the desire for more equitable allocation of resources among agents . A second , more subtle , motivation is the fact that a fairer allocation indirectly implies less variation in taxes which can be desirable in a situation where ( implicit ) individual agent budgetary constraints make payment of large taxes unrealistic . In this paper we study a family of social utilities that provide fair allocation ( with SoU being subsumed as an extreme case ) and derive conditions under which Bayesian and Dominant strategy implementation is possible . Furthermore , it is shown how a simple modification of the above mechanism can guarantee full Bayesian implementation . Through a numerical example it is shown that the proposed method can result in significant gains both in allocation fairness and tax reduction .
We consider load balancing in a network of caching servers delivering contents to end users . Randomized load balancing via the so-called power of two choices is a well-known approach in parallel and distributed systems . In this framework , we investigate the tension between storage resources , communication cost , and load balancing performance . To this end , we propose a randomized load balancing scheme which simultaneously considers cache size limitation and proximity in the server redirection process . In contrast to the classical power of two choices setup , since the memory limitation and the proximity constraint cause correlation in the server selection process , we may not benefit from the power of two choices . However , we prove that in certain regimes of problem parameters , our scheme results in the maximum load of order $\Theta ( \log\log n ) $ ( here $n$ is the network size ) . This is an exponential improvement compared to the scheme which assigns each request to the nearest available replica . Interestingly , the extra communication cost incurred by our proposed scheme , compared to the nearest replica strategy , is small . Furthermore , our extensive simulations show that the trade-off trend does not depend on the network topology and library popularity profile details .
In this paper , we propose a novel writer-independent global feature extraction framework for the task of automatic signature verification which aims to make robust systems for automatically distinguishing negative and positive samples . Our method consists of an autoencoder for modeling the sample space into a fixed length latent space and a Siamese Network for classifying the fixed-length samples obtained from the autoencoder based on the reference samples of a subject as being " Genuine " or " Forged . " During our experiments , usage of Attention Mechanism and applying Downsampling significantly improved the accuracy of the proposed framework . We evaluated our proposed framework using SigWiComp0000 Japanese and GPDSsyntheticOnLineOffLineSignature datasets . On the SigWiComp0000 Japanese dataset , we achieved 0 . 00% EER that means 0 . 0% relative improvement compared to the best-reported result . Furthermore , on the GPDSsyntheticOnLineOffLineSignature dataset , we achieved average EERs of 0 . 00% , 0 . 00% , 0 . 00% and 0 . 00% respectively for 000 , 000 , 0000 and 0000 test subjects which indicates improvement of relative EER on the best-reported result by 00 . 00% , 00 . 00% , 00 . 0% and 00 . 00% respectively . Apart from the accuracy gain , because of the nature of our proposed framework which is based on neural networks and consequently is as simple as some consecutive matrix multiplications , it has less computational cost than conventional methods such as DTW and could be used concurrently on devices such as GPU , TPU , etc .
This paper proposes a unified approach that enables the Wishart distribution to be studied simultaneously in the real , complex , quaternion and octonion cases . In particular , the noncentral generalised Wishart distribution , the joint density of the eigenvalues and the distribution of the maximum eigenvalue are obtained for real normed division algebras .
In this article , we study the consistency of the template estimation with the Fr\ ' echet mean in quotient spaces . The Fr\ ' echet mean in quotient spaces is often used when the observations are deformed or transformed by a group action . We show that in most cases this estimator is actually inconsistent . We exhibit a sufficient condition for this inconsistency , which amounts to the folding of the distribution of the noisy template when it is projected to the quotient space . This condition appears to be fulfilled as soon as the support of the noise is large enough . To quantify this inconsistency we provide lower and upper bounds of the bias as a function of the variability ( the noise level ) . This shows that the consistency bias cannot be neglected when the variability increases .
Besides serving as prediction models , classification trees are useful for finding important predictor variables and identifying interesting subgroups in the data . These functions can be compromised by weak split selection algorithms that have variable selection biases or that fail to search beyond local main effects at each node of the tree . The resulting models may include many irrelevant variables or select too few of the important ones . Either eventuality can lead to erroneous conclusions . Four techniques to improve the precision of the models are proposed and their effectiveness compared with that of other algorithms , including tree ensembles , on real and simulated data sets .
Deep neural networks with their large number of parameters are highly flexible learning systems . The high flexibility in such networks brings with some serious problems such as overfitting , and regularization is used to address this problem . A currently popular and effective regularization technique for controlling the overfitting is dropout . Often , large data collections required for neural networks contain sensitive information such as the medical histories of patients , and the privacy of the training data should be protected . In this paper , we modify the recently proposed variational dropout technique which provided an elegant Bayesian interpretation to dropout , and show that the intrinsic noise in the variational dropout can be exploited to obtain a degree of differential privacy . The iterative nature of training neural networks presents a challenge for privacy-preserving estimation since multiple iterations increase the amount of noise added . We overcome this by using a relaxed notion of differential privacy , called concentrated differential privacy , which provides tighter estimates on the overall privacy loss . We demonstrate the accuracy of our privacy-preserving variational dropout algorithm on benchmark datasets .
In this paper we consider the problem of anonymizing datasets in which each individual is associated with a set of items that constitute private information about the individual . Illustrative datasets include market-basket datasets and search engine query logs . We formalize the notion of k-anonymity for set-valued data as a variant of the k-anonymity model for traditional relational datasets . We define an optimization problem that arises from this definition of anonymity and provide O ( klogk ) and O ( 0 ) -approximation algorithms for the same . We demonstrate applicability of our algorithms to the America Online query log dataset .
In this paper , we introduce an unbiased gradient simulation algorithms for solving convex optimization problem with stochastic function compositions . We show that the unbiased gradient generated from the algorithm has finite variance and finite expected computation cost . We then combined the unbiased gradient simulation with two variance reduced algorithms ( namely SVRG and SCSG ) and showed that the proposed optimization algorithms based on unbiased gradient simulations exhibit satisfactory convergence properties . Specifically , in the SVRG case , the algorithm with simulated gradient can be shown to converge linearly to optima in expectation and almost surely under strong convexity . Finally , for the numerical experiment , we applied the algorithms to two important cases of stochastic function compositions optimization : maximizing the Cox ' s partial likelihood model and training conditional random fields .
Kepler provides light curves of 000 , 000 stars with unprecedented precision . However , the raw data as they come from the spacecraft contain significant systematic and stochastic errors . These errors , which include discontinuities , systematic trends , and outliers , obscure the astrophysical signals in the light curves . To correct these errors is the task of the Presearch Data Conditioning ( PDC ) module of the Kepler data analysis pipeline . The original version of PDC in Kepler did not meet the extremely high performance requirements for the detection of miniscule planet transits or highly accurate analysis of stellar activity and rotation . One particular deficiency was that astrophysical features were often removed as a side-effect to removal of errors . In this paper we introduce the completely new and significantly improved version of PDC which was implemented in Kepler SOC 0 . 0 . This new PDC version , which utilizes a Bayesian approach for removal of systematics , reliably corrects errors in the light curves while at the same time preserving planet transits and other astrophysically interesting signals . We describe the architecture and the algorithms of this new PDC module , show typical errors encountered in Kepler data , and illustrate the corrections using real light curve examples .
This work provides a unified analysis of the properties of the sample covariance matrix $\Sigma_n$ over the class of $p\times p$ population covariance matrices $\Sigma$ of reduced effective rank $r_e ( \Sigma ) $ . This class includes scaled factor models and covariance matrices with decaying spectrum . We consider $r_e ( \Sigma ) $ as a measure of matrix complexity , and obtain sharp minimax rates on the operator and Frobenius norm of $\Sigma_n-\Sigma$ , as a function of $r_e ( \Sigma ) $ and $\|\Sigma\|_0$ , the operator norm of $\Sigma$ . With guidelines offered by the optimal rates , we define classes of matrices of reduced effective rank over which $\Sigma_n$ is an accurate estimator . Within the framework of these classes , we perform a detailed finite sample theoretical analysis of the merits and limitations of the empirical scree plot procedure routinely used in PCA . We show that identifying jumps in the empirical spectrum that consistently estimate jumps in the spectrum of $\Sigma$ is not necessarily informative for other goals , for instance for the selection of those sample eigenvalues and eigenvectors that are consistent estimates of their population counterparts . The scree plot method can still be used for selecting consistent eigenvalues , for appropriate threshold levels . We provide a threshold construction and also give a rule for checking the consistency of the corresponding sample eigenvectors . We specialize these results and analysis to population covariance matrices with polynomially decaying spectra , and extend it to covariance operators with polynomially decaying spectra . An application to fPCA illustrates how our results can be used in functional data analysis .
We propose a new algorithm to do posterior sampling of Kingman ' s coalescent , based upon the Particle Markov Chain Monte Carlo methodology . Specifically , the algorithm is an instantiation of the Particle Gibbs Sampling method , which alternately samples coalescent times conditioned on coalescent tree structures , and tree structures conditioned on coalescent times via the conditional Sequential Monte Carlo procedure . We implement our algorithm as a C++ package , and demonstrate its utility via a parameter estimation task in population genetics on both single- and multiple-locus data . The experiment results show that the proposed algorithm performs comparable to or better than several well-developed methods .
Advanced engineering materials design involves the exploration of massive multidimensional feature spaces , the correlation of materials properties and the processing parameters derived from disparate sources . The search for alternative materials or processing property strategies , whether through analytical , experimental or simulation approaches , has been a slow and arduous task , punctuated by infrequent and often expected discoveries . A few systematic efforts have been made to analyze the trends in data as a basis for classifications and predictions . This is particularly due to the lack of large amounts of organized data and more importantly the challenging of shifting through them in a timely and efficient manner . The application of recent advances in Data Mining on materials informatics is the state of art of computational and experimental approaches for materials discovery . In this paper similarity based engineering materials selection model is proposed and implemented to select engineering materials based on the composite materials constraints . The result reviewed from this model is sustainable for effective decision making in advanced engineering materials design applications .
In Stochastic blockmodels , which are among the most prominent statistical models for cluster analysis of complex networks , clusters are defined as groups of nodes with statistically similar link probabilities within and between groups . A recent extension by Karrer and Newman incorporates a node degree correction to model degree heterogeneity within each group . Although this demonstrably leads to better performance on several networks it is not obvious whether modelling node degree is always appropriate or necessary . We formulate the degree corrected stochastic blockmodel as a non-parametric Bayesian model , incorporating a parameter to control the amount of degree correction which can then be inferred from data . Additionally , our formulation yields principled ways of inferring the number of groups as well as predicting missing links in the network which can be used to quantify the model ' s predictive performance . On synthetic data we demonstrate that including the degree correction yields better performance both on recovering the true group structure and predicting missing links when degree heterogeneity is present , whereas performance is on par for data with no degree heterogeneity within clusters . On seven real networks ( with no ground truth group structure available ) we show that predictive performance is about equal whether or not degree correction is included ; however , for some networks significantly fewer clusters are discovered when correcting for degree indicating that the data can be more compactly explained by clusters of heterogenous degree nodes .
We present the first model and algorithm for L0-norm kernel PCA . While L0-norm kernel PCA has been widely studied , there has been no work on L0-norm kernel PCA . For this non-convex and non-smooth problem , we offer geometric understandings through reformulations and present an efficient algorithm where the kernel trick is applicable . To attest the efficiency of the algorithm , we provide a convergence analysis including linear rate of convergence . Moreover , we prove that the output of our algorithm is a local optimal solution to the L0-norm kernel PCA problem . We also numerically show its robustness when extracting principal components in the presence of influential outliers , as well as its runtime comparability to L0-norm kernel PCA . Lastly , we introduce its application to outlier detection and show that the L0-norm kernel PCA based model outperforms especially for high dimensional data .
Personalized treatment of patients based on tissue-specific cancer subtypes has strongly increased the efficacy of the chosen therapies . Even though the amount of data measured for cancer patients has increased over the last years , most cancer subtypes are still diagnosed based on individual data sources ( e . g . gene expression data ) . We propose an unsupervised data integration method based on kernel principal component analysis . Principal component analysis is one of the most widely used techniques in data analysis . Unfortunately , the straight-forward multiple-kernel extension of this method leads to the use of only one of the input matrices , which does not fit the goal of gaining information from all data sources . Therefore , we present a scoring function to determine the impact of each input matrix . The approach enables visualizing the integrated data and subsequent clustering for cancer subtype identification . Due to the nature of the method , no free parameters have to be set . We apply the methodology to five different cancer data sets and demonstrate its advantages in terms of results and usability .
Classical causal inference assumes a treatment meant for a given unit does not have an effect on other units . When this " no interference " assumption is violated , new types of spillover causal effects arise , and causal inference becomes much more difficult . In addition , interference introduces a unique complication where outcomes may transmit treatment influences to each other , which is a relationship that has some features of a causal one , but is symmetric . In settings where detailed temporal information on outcomes is not available , addressing this complication using statistical inference methods based on Directed Acyclic Graphs ( DAGs ) ( Ogburn & VanderWeele , 0000 ) leads to conceptual difficulties . In this paper , we develop a new approach to decomposing the spillover effect into direct ( also known as the contagion effect ) and indirect ( also known as the infectiousness effect ) components that extends the DAG based treatment decomposition approach to mediation found in ( Robins & Richardson , 0000 ) to causal chain graph models ( Lauritzen & Richardson , 0000 ) . We show that when these components of the spillover effect are identified in these models , they have an identifying functional , which we call the symmetric mediation formula , that generalizes the mediation formula in DAGs ( Pearl , 0000 ) . We further show that , unlike assumptions in classical mediation analysis , an assumption permitting identification in our setting leads to restrictions on the observed data law , making the assumption empirically falsifiable . Finally , we discuss statistical inference for the components of the spillover effect in the special case of two interacting outcomes , and discuss a maximum likelihood estimator , and a doubly robust estimator .
Universities and Institutions these days ' deals with issues related to with assessment of large number of students . Various evaluation methods have been adopted by examiners in different institutions to examining the ability of an individual , starting from manual means of using paper and pencil to electronic , from oral to written , practical to theoretical and many others . There is a need to expedite the process of examination in order to meet the increasing enrolment of students at the universities and institutes . Sip Based Mass Mobile Examination System ( SiBMMES ) expedites the examination process by automating various activities in an examination such as exam paper setting , Scheduling and allocating examination time and evaluation ( auto-grading for objective questions ) etc . SiBMMES uses the IP Multimedia Subsystem ( IMS ) that is an IP communications framework providing an environment for the rapid development of innovative and reusable services Session Initial Protocol ( SIP ) is a signalling ( request-response ) protocol for this architecture and it is used for establishing sessions in an IP network , making it an ideal candidate for supporting terminal mobility in the IMS to deliver the services , with the extended services available in IMS like open APIs , common network services , Quality of Services ( QoS ) like multiple sessions per call , Push to Talk etc often requiring multiple types of media ( including voice , video , pictures , and text ) . SiBMMES is an effective solution for mass education evaluation using mobile and web technology . In this paper , a novel hybrid component based development ( CBD ) model is proposed for SiBMMES . A Component based Hybrid Model is selected to the fact that IMS takes the concept of layered architecture one step further by defining a horizontal architecture where service enablers and common functions can be reused for multiple applications .
To ensure high quality of and trust in both metadata and data , their representation in RDF must satisfy certain criteria - specified in terms of RDF constraints . From 0000 to 0000 together with other Linked Data community members and experts from the social , behavioral , and economic sciences ( SBE ) , we developed diverse vocabularies to represent SBE metadata and rectangular data in RDF . The DDI-RDF Discovery Vocabulary ( DDI-RDF ) is designed to support the dissemination , management , and reuse of unit-record data , i . e . , data about individuals , households , and businesses , collected in form of responses to studies and archived for research purposes . The RDF Data Cube Vocabulary ( QB ) is a W0C recommendation for expressing data cubes , i . e . multi-dimensional aggregate data and its metadata . Physical Data Description ( PHDD ) is a vocabulary to model data in rectangular format , i . e . , tabular data . The data could either be represented in records with character-separated values ( CSV ) or fixed length . The Simple Knowledge Organization System ( SKOS ) is a vocabulary to build knowledge organization systems such as thesauri , classification schemes , and taxonomies . XKOS is a SKOS extension to describe formal statistical classifications . In this paper , we describe RDF constraints to validate metadata on unit-record data ( DDI-RDF ) , aggregated data ( QB ) , thesauri ( SKOS ) , and statistical classifications ( XKOS ) and to validate tabular data ( PHDD ) - all of them represented in RDF . We classified these constraints according to the severity of occurring constraint violations . This technical report is updated continuously as modifying , adding , and deleting constraints remains ongoing work .
Distributed estimation and processing in networks modeled by graphs have received a great deal of interest recently , due to the benefits of decentralised processing in terms of performance and robustness to communications link failure between nodes of the network . Diffusion-based algorithms have been demonstrated to be among the most effective for distributed signal processing problems , through the combination of local node estimate updates and sharing of information with neighbour nodes through diffusion . In this work , we develop a serial-inspired approach based on message-passing strategies that provides a significant improvement in performance over prior art . The concept of serial processing in the graph has been successfully applied in sum-product based algorithms and here provides inspiration for an algorithm which makes use of the most up-to-date information in the graph in combination with the diffusion approach to offer improved performance .
Techniques such as clusterization , neural networks and decision making usually rely on algorithms that are not well suited to deal with missing values . However , real world data frequently contains such cases . The simplest solution is to either substitute them by a best guess value or completely disregard the missing values . Unfortunately , both approaches can lead to biased results . In this paper , we propose a technique for dealing with missing values in heterogeneous data using imputation based on the k-nearest neighbors algorithm . It can handle real ( which we refer to as crisp henceforward ) , interval and fuzzy data . The effectiveness of the algorithm is tested on several datasets and the numerical results are promising .
Existing speaker verification ( SV ) systems often suffer from performance degradation if there is any language mismatch between model training , speaker enrollment , and test . A major cause of this degradation is that most existing SV methods rely on a probabilistic model to infer the speaker factor , so any significant change on the distribution of the speech signal will impact the inference . Recently , we proposed a deep learning model that can learn how to extract the speaker factor by a deep neural network ( DNN ) . By this feature learning , an SV system can be constructed with a very simple back-end model . In this paper , we investigate the robustness of the feature-based SV system in situations with language mismatch . Our experiments were conducted on a complex cross-lingual scenario , where the model training was in English , and the enrollment and test were in Chinese or Uyghur . The experiments demonstrated that the feature-based system outperformed the i-vector system with a large margin , particularly with language mismatch between enrollment and test .
Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels , and are naturally adapted to multi-output learning situations . This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts . We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients . The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues . We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinatedescent procedure . We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces .
Static type systems are usually not sufficient to express all requirements on function calls . Hence , contracts with pre- and postconditions can be used to express more complex constraints on operations . Contracts can be checked at run time to ensure that operations are only invoked with reasonable arguments and return intended results . Although such dynamic contract checking provides more reliable program execution , it requires execution time and could lead to program crashes that might be detected with more advanced methods at compile time . To improve this situation for declarative languages , we present an approach to combine static and dynamic contract checking for the functional logic language Curry . Based on a formal model of contract checking for functional logic programming , we propose an automatic method to verify contracts at compile time . If a contract is successfully verified , dynamic checking of it can be omitted . This method decreases execution time without degrading reliable program execution . In the best case , when all contracts are statically verified , it provides trust in the software since crashes due to contract violations cannot occur during program execution .
This paper examines fundamental error characteristics for a general class of matrix completion problems , where the matrix of interest is a product of two a priori unknown matrices , one of which is sparse , and the observations are noisy . Our main contributions come in the form of minimax lower bounds for the expected per-element squared error for this problem under under several common noise models . Specifically , we analyze scenarios where the corruptions are characterized by additive Gaussian noise or additive heavier-tailed ( Laplace ) noise , Poisson-distributed observations , and highly-quantized ( e . g . , one-bit ) observations , as instances of our general result . Our results establish that the error bounds derived in ( Soni et al . , 0000 ) for complexity-regularized maximum likelihood estimators achieve , up to multiplicative constants and logarithmic factors , the minimax error rates in each of these noise scenarios , provided that the nominal number of observations is large enough , and the sparse factor has ( on an average ) at least one non-zero per column .
BACKGROUND . Formal demography has a long history of building simple models of age schedules of demographic quantities , e . g . mortality and fertility rates . These are widely used in demographic methods to manipulate whole age schedules using few parameters . OBJECTIVE . The Singular Value Decomposition ( SVD ) factorizes a matrix into three matrices with useful properties including the ability to reconstruct the original matrix using many fewer , simple matrices . This work demonstrates how these properties can be exploited to build parsimonious models of whole age schedules of demographic quantities that can be further parameterized in terms of arbitrary covariates . METHODS . The SVD is presented and explained in detail with attention to developing an intuitive understanding . The SVD is used to construct a general , component model of demographic age schedules , and that model is demonstrated with age-specific mortality and fertility rates . Finally , the model is used ( 0 ) to predict age-specific mortality using HIV indicators and summary measures of age-specific mortality , and ( 0 ) to predict age-specific fertility using the total fertility rate ( TFR ) . RESULTS . The component model of age-specific mortality and fertility rates succeeds in reproducing the data with two inputs , and acting through those two inputs , various covariates are able to accurately predict full age schedules . CONCLUSIONS . The SVD is potentially useful as a way to summarize , smooth and model age-specific demographic quantities . The component model is a general method of relating covariates to whole age schedules . COMMENTS . The focus of this work is the SVD and the component model . The applications are for illustrative purposes only .
Blocking of foreign Web content by Internet access providers has been a hot topic for the last 00 months in Germany . Since fall 0000 the state of North-Rhine-Westphalia very actively tries to mandate such blocking . This paper will take a technical view on the problems imposed by the blocking orders and blocking content at access or network provider level in general . It will also give some empirical data on the effects of the blocking orders to help in the legal assessment of the orders .
We consider the setting of ontological database access , where an Abox is given in form of a relational database D and where a Boolean conjunctive query q has to be evaluated against D modulo a Tbox T formulated in DL-Lite or Linear Datalog+/- . It is well-known that ( T , q ) can be rewritten into an equivalent nonrecursive Datalog program P that can be directly evaluated over D . However , for Linear Datalog ? or for DL-Lite versions that allow for role inclusion , the rewriting methods described so far result in a nonrecursive Datalog program P of size exponential in the joint size of T and q . This gives rise to the interesting question of whether such a rewriting necessarily needs to be of exponential size . In this paper we show that it is actually possible to translate ( T , q ) into a polynomially sized equivalent nonrecursive Datalog program P .
A clustering algorithm partitions a set of data points into smaller sets ( clusters ) such that each subset is more tightly packed than the whole . Many approaches to clustering translate the vector data into a graph with edges reflecting a distance or similarity metric on the points , then look for highly connected subgraphs . We introduce such an algorithm based on ideas borrowed from the topological notion of thin position for knots and 0-dimensional manifolds .
The purpose of this study is to provide a new methodology of how one can consistently estimate a change-point in time series data . In contrast with previous studies , the suggested methodology employs only the empirical spectral density and its first moment . This is accomplished when both the means and variances before and after the unidentified time point are unknown . Then , the well-known Gauss-Newton algorithm is applied to estimate and provide asymptotic results for the parameters involved . Simulations carried out under different distributions , sizes and unknown time points confirm the validity and accuracy of the methodology . The real-world example considered in the paper illustrates the robustness of the methodology in the presence of even extreme outliers .
Consider a semiparametric model with a Euclidean parameter and an infinite-dimensional parameter , to be called a Banach parameter . Assume : ( a ) There exists an efficient estimator of the Euclidean parameter . ( b ) When the value of the Euclidean parameter is known , there exists an estimator of the Banach parameter , which depends on this value and is efficient within this restricted model . Substituting the efficient estimator of the Euclidean parameter for the value of this parameter in the estimator of the Banach parameter , one obtains an efficient estimator of the Banach parameter for the full semiparametric model with the Euclidean parameter unknown . This hereditary property of efficiency completes estimation in semiparametric models in which the Euclidean parameter has been estimated efficiently . Typically , estimation of both the Euclidean and the Banach parameter is necessary in order to describe the random phenomenon under study to a sufficient extent . Since efficient estimators are asymptotically linear , the above substitution method is a particular case of substituting asymptotically linear estimators of a Euclidean parameter into estimators that are asymptotically linear themselves and that depend on this Euclidean parameter . This more general substitution case is studied for its own sake as well , and a hereditary property for asymptotic linearity is proved .
We take a new look at parameter estimation for Gaussian Mixture Models ( GMMs ) . In particular , we propose using \emph{Riemannian manifold optimization} as a powerful counterpart to Expectation Maximization ( EM ) . An out-of-the-box invocation of manifold optimization , however , fails spectacularly : it converges to the same solution but vastly slower . Driven by intuition from manifold convexity , we then propose a reparamerization that has remarkable empirical consequences . It makes manifold optimization not only match EM---a highly encouraging result in itself given the poor record nonlinear programming methods have had against EM so far---but also outperform EM in many practical settings , while displaying much less variability in running times . We further highlight the strengths of manifold optimization by developing a somewhat tuned manifold LBFGS method that proves even more competitive and reliable than existing manifold optimization tools . We hope that our results encourage a wider consideration of manifold optimization for parameter estimation problems .
A deep neural network based architecture was constructed to predict amino acid side chain conformation with unprecedented accuracy . Amino acid side chain conformation prediction is essential for protein homology modeling and protein design . Current widely-adopted methods use physics-based energy functions to evaluate side chain conformation . Here , using a deep neural network architecture without physics-based assumptions , we have demonstrated that side chain conformation prediction accuracy can be improved by more than 00% , especially for aromatic residues compared with current standard methods . More strikingly , the prediction method presented here is robust enough to identify individual conformational outliers from high resolution structures in a protein data bank without providing its structural factors . We envisage that our amino acid side chain predictor could be used as a quality check step for future protein structure model validation and many other potential applications such as side chain assignment in Cryo-electron microscopy , crystallography model auto-building , protein folding and small molecule ligand docking .
In this paper , we concentrate on new methodologies for copulas introduced and developed by Joe , Cooke , Bedford , Kurowica , Daneshkhah and others on the new class of graphical models called vines as a way of constructing higher dimensional distributions . We develop the approximation method presented by Bedford et al ( 0000 ) at which they show that any $n$-dimensional copula density can be approximated arbitrarily well pointwise using a finite parameter set of 0-dimensional copulas in a vine or pair-copula construction . Our constructive approach involves the use of minimum information copulas that can be specified to any required degree of precision based on the available data or experts ' judgements . By using this method , we are able to use a fixed finite dimensional family of copulas to be employed in a vine construction , with the promise of a uniform level of approximation . The basic idea behind this method is to use a two-dimensional ordinary polynomial series to approximate any log-density of a bivariate copula function by truncating the series at an appropriate point . We present an alternative approximation of the multivariate distribution of interest by considering orthonormal polynomial and Legendre multiwavelets as the basis functions . We show the derived approximations are more precise and computationally faster with better properties than the one proposed by Bedford et al . ( 0000 ) . We then apply our method to modelling a dataset of Norwegian financial data that was previously analysed in the series of papers , and finally compare our results by them .
In this paper , we investigate the ( in ) -consistency of different bootstrap methods for constructing confidence intervals in the class of estimators that converge at rate $n^{0/0}$ . The Grenander estimator , the nonparametric maximum likelihood estimator of an unknown nonincreasing density function $f$ on $[0 , \infty ) $ , is a prototypical example . We focus on this example and explore different approaches to constructing bootstrap confidence intervals for $f ( t_0 ) $ , where $t_0\in ( 0 , \infty ) $ is an interior point . We find that the bootstrap estimate , when generating bootstrap samples from the empirical distribution function $\mathbb{F}_n$ or its least concave majorant $\tilde{F}_n$ , does not have any weak limit in probability . We provide a set of sufficient conditions for the consistency of any bootstrap method in this example and show that bootstrapping from a smoothed version of $\tilde{F}_n$ leads to strongly consistent estimators . The $m$ out of $n$ bootstrap method is also shown to be consistent while generating samples from $\mathbb{F}_n$ and $\tilde{F}_n$ .
The Freundlich isotherm has been used widely to describe sorption of solutes to soils for many decades . The Freundlich parameters are often estimated using unweighted least squares ( ULS ) analysis after log-log transformation . Estimating the accuracy of these parameters ( characterized by their coefficient of variation , CV ) is an essential element of the use of these parameters . Accurate CVs can be derived with weighted least squares ( WLS ) , but only if proper weights are assigned to the residuals in the fitting procedure . This work presents the derivation of an analytical approximation of these weights which were found to decrease with increasing concentration , increasing Freundlich exponent , and increasing CVs of the initial and equilibrium concentrations . Monte-Carlo simulations for a wide range of Freundlich systems based on known values of the CVs of the initial and equilibrium concentrations confirmed the accuracy of this analytical approximation . Unfortunately , in practice , the CVs of the initial and equilibrium concentrations are unknown a priori . Simulations showed that the accuracy of the estimated CVs of the Freundlich parameters was distinctly lower if the CVs of the initial and equilibrium concentrations were estimated from the isotherm data . However , this accuracy was still considerably better than when using ULS . It is recommended to use this analytical approximation whenever these CVs are relevant for the further use and interpretation of the estimated Freundlich parameters .
We introduce a convolutional recurrent neural network ( CRNN ) for music tagging . CRNNs take advantage of convolutional neural networks ( CNNs ) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features . We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample . Overall , we found that CRNNs show a strong performance with respect to the number of parameter and training time , indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation .
In deterministic optimization , line searches are a standard tool ensuring stability and efficiency . Where only stochastic gradients are available , no direct equivalent has so far been formulated , because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space . We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization . Our method retains a Gaussian process surrogate of the univariate optimization objective , and uses a probabilistic belief over the Wolfe conditions to monitor the descent . The algorithm has very low computational cost , and no user-controlled parameters . Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent .
We study minimization of a parametric family of relative entropies , termed relative $\alpha$-entropies ( denoted $\mathscr{I}_{\alpha} ( P , Q ) $ ) . These arise as redundancies under mismatched compression when cumulants of compressed lengths are considered instead of expected compressed lengths . These parametric relative entropies are a generalization of the usual relative entropy ( Kullback-Leibler divergence ) . Just like relative entropy , these relative $\alpha$-entropies behave like squared Euclidean distance and satisfy the Pythagorean property . Minimization of $\mathscr{I}_{\alpha} ( P , Q ) $ over the first argument on a set of probability distributions that constitutes a linear family is studied . Such a minimization generalizes the maximum R\ ' {e}nyi or Tsallis entropy principle . The minimizing probability distribution ( termed $\mathscr{I}_{\alpha}$-projection ) for a linear family is shown to have a power-law .
If a probability density p ( \x ) ( \x\in\R^k ) is bounded and R ( t ) : = \int \exp ( t\ell ( \x ) ) \d\x < \infty for some linear functional \ell and all t\in ( 0 , 0 ) , then , for each t\in ( 0 , 0 ) and all large enough n , the n-fold convolution of the t-tilted density p_t ( \x ) : = \exp ( t\ell ( \x ) ) p ( \x ) /R ( t ) is bounded . This is a corollary of a general , " non-i . i . d . " result , which is also shown to enjoy a certain optimality property . Such results are useful for saddle-point approximations .
Although fractional Brownian motion was not invented by Benoit Mandelbrot , it was he who recognized the importance of this random process and gave it the name by which it is known today . This is a personal account of the history behind fractional Brownian motion and some subsequent developments .
Autonomous flight of pocket drones is challenging due to the severe limitations on on-board energy , sensing , and processing power . However , tiny drones have great potential as their small size allows maneuvering through narrow spaces while their small weight provides significant safety advantages . This paper presents a computationally efficient algorithm for determining optical flow , which can be run on an STM00F0 microprocessor ( 000 MHz ) of a 0 gram stereo-camera . The optical flow algorithm is based on edge histograms . We propose a matching scheme to determine local optical flow . Moreover , the method allows for sub-pixel flow determination based on time horizon adaptation . We demonstrate velocity measurements in flight and use it within a velocity control-loop on a pocket drone .
I outline the involvement of the Los Alamos e-print archive ( arXiv ) within the Open Archives Initiative ( OAI ) and describe the implementation of the data provider side of the OAI protocol v0 . 0 . I highlight the ways in which we map the existing structure of arXiv onto elements of the protocol .
Due to its scale and largely interconnected nature , the Internet of Things ( IoT ) will be vulnerable to a number of security threats that range from physical layer attacks to network layer attacks . In this paper , a novel anti-jamming strategy for OFDM-based IoT systems is proposed which enables an IoT controller to protect the IoT devices against a malicious radio jammer . The interaction between the controller node and the jammer is modeled as a Colonel Blotto game with continuous and asymmetric resources in which the IoT controller , acting as defender , seeks to thwart the jamming attack by distributing its power among the subcarries in a smart way to decrease the aggregate bit error rate ( BER ) caused by the jammer . The jammer , on the other hand , aims at disrupting the system performance by allocating jamming power to different frequency bands . To solve the game , an evolutionary algorithm is proposed which can find a mixed-strategy Nash equilibrium of the Blotto game . Simulation results show that the proposed algorithm enables the IoT controller to maintain the BER above an acceptable threshold , thereby preserving the IoT network performance in the presence of malicious jamming .
This article describes a fully automated , credible autocoding chain for control systems . The framework generates code , along with guarantees of high level functional properties which can be independently verified . It relies on domain specific knowledge and fomal methods of analysis to address a context of heightened safety requirements for critical embedded systems and ever-increasing costs of verification and validation . The platform strives to bridge the semantic gap between domain expert and code verification expert . First , a graphical dataflow language is extended with annotation symbols enabling the control engineer to express high level properties of its control law within the framework of a familiar language . An existing autocoder is enhanced to both generate the code implementing the initial design , but also to carry high level properties down to annotations at the level of the code . Finally , using customized code analysis tools , certificates are generated which guarantee the correctness of the annotations with respect to the code , and can be verified using existing static analysis tools . Only a subset of properties and controllers are handled at this point .
We report on a series of experiments concerning the feasibility of example driven modelling . The main aim was to establish experimentally within an academic environment : the relationship between error and task complexity using a ) Traditional spreadsheet modelling ; b ) example driven techniques . We report on the experimental design , sampling , research methods and the tasks set for both control and treatment groups . Analysis of the completed tasks allows comparison of several different variables . The experimental results compare the performance indicators for the treatment and control groups by comparing accuracy , experience , training , confidence measures , perceived difficulty and perceived completeness . The various results are thoroughly tested for statistical significance using : the Chi squared test , Fisher ' s exact test for significance , Cochran ' s Q test and McNemar ' s test on difficulty .
The use of free and open source software is gaining momentum due to the ever increasing availability and use of the Internet . Organizations are also now adopting open source software , despite some reservations in particular regarding the provision and availability of support . One of the greatest concerns about free and open source software is the availability of post release support and the handling of for support . A common belief is that there is no appropriate support available for this class of software , while an alternative argument is that due to the active involvement of Internet users in online forums , there is in fact a large resource available that communicates and manages the management of support requests . The research model of this empirical investigation establishes and studies the relationship between open source software support requests and online public forums . The results of this empirical study provide evidence about the realities of support that is present in open source software projects . We used a dataset consisting of 000 open source software projects covering a broad range of categories in this investigation . The results show that online forums play a significant role in managing support requests in open source software , thus becoming a major source of assistance in maintenance of the open source projects .
We treat collaborative filtering as a univariate time series estimation problem : given a user ' s previous votes , predict the next vote . We describe two families of methods for transforming data to encode time order in ways amenable to off-the-shelf classification and density estimation tools , and examine the results of using these approaches on several real-world data sets . The improvements in predictive accuracy we realize recommend the use of other predictive algorithms that exploit the temporal order of data .
Public institutions are increasingly reliant on data from social media sites to measure public attitude and provide timely public engagement . Such reliance includes the exploration of public views on important social issues such as gender-based violence ( GBV ) . In this study , we examine big ( social ) data consisting of nearly fourteen million tweets collected from Twitter over a period of ten months to analyze public opinion regarding GBV , highlighting the nature of tweeting practices by geographical location and gender . We demonstrate the utility of Computational Social Science to mine insight from the corpus while accounting for the influence of both transient events and sociocultural factors . We reveal public awareness regarding GBV tolerance and suggest opportunities for intervention and the measurement of intervention effectiveness assisting both governmental and non-governmental organizations in policy development .
Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models . We propose a general framework capable of enhancing various types of neural networks ( e . g . , CNNs and RNNs ) with declarative first-order logic rules . Specifically , we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks . We deploy the framework on a CNN for sentiment analysis , and an RNN for named entity recognition . With a few highly intuitive rules , we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems .
This article aims at providing signal machines as small as possible able to perform any computation ( in the classical understanding ) . After presenting signal machines , it is shown how to get universal ones from Turing machines , cellular-automata and cyclic tag systems . Finally a halting universal signal machine with 00 meta-signals and 00 collision rules is presented .
Use Case Points ( UCP ) is a well-known method to estimate the project size , based on Use Case diagram , at early phases of software development . Although the Use Case diagram is widely accepted as a de-facto model for analyzing object oriented software requirements over the world , UCP method did not take sufficient amount of attention because , as yet , there is no consensus on how to produce software effort from UCP . This paper aims to study the potential of using Fuzzy Model Tree to derive effort estimates based on UCP size measure using a dataset collected for that purpose . The proposed approach has been validated against Treeboost model , Multiple Linear Regression and classical effort estimation based on the UCP model . The obtained results are promising and show better performance than those obtained by classical UCP , Multiple Linear Regression and slightly better than those obtained by Tree boost model .
We propose a series of methods to represent the evolution of a field of science at different levels : namely micro , meso and macro levels . We use a previously introduced asymmetric measure of paradigmatic proximity between terms that enables us to extract structure from a large publications database . We apply our set of methods on a case study from the complex systems community through the mapping of more than 000 complex systems science concepts indexed from a database as large as several millions of journal papers . We will first summarize the main properties of our asymmetric proximity measure . Then we show how salient paradigmatic fields can be embedded into a 0-dimensional visualization into which the terms are plotted according to their relative specificity and generality index . This meso-level helps us producing macroscopic maps of the field of science studied featuring the former paradigmatic fields .
Replication helps ensure that a genotype-phenotype association observed in a genome-wide association ( GWA ) study represents a credible association and is not a chance finding or an artifact due to uncontrolled biases . We discuss prerequisites for exact replication , issues of heterogeneity , advantages and disadvantages of different methods of data synthesis across multiple studies , frequentist vs . Bayesian inferences for replication , and challenges that arise from multi-team collaborations . While consistent replication can greatly improve the credibility of a genotype-phenotype association , it may not eliminate spurious associations due to biases shared by many studies . Conversely , lack of replication in well-powered follow-up studies usually invalidates the initially proposed association , although occasionally it may point to differences in linkage disequilibrium or effect modifiers across studies .
We study the size and the complexity of computing finite state automata ( FSA ) representing and approximating the downward and the upward closure of Petri net languages with coverability as the acceptance condition . We show how to construct an FSA recognizing the upward closure of a Petri net language in doubly-exponential time , and therefore the size is at most doubly exponential . For downward closures , we prove that the size of the minimal automata can be non-primitive recursive . In the case of BPP nets , a well-known subclass of Petri nets , we show that an FSA accepting the downward/upward closure can be constructed in exponential time . Furthermore , we consider the problem of checking whether a simple regular language is included in the downward/upward closure of a Petri net/BPP net language . We show that this problem is EXPSPACE-complete ( resp . NP-complete ) in the case of Petri nets ( resp . BPP nets ) . Finally , we show that it is decidable whether a Petri net language is upward/downward closed .
The primary goal of randomized trials is to compare the effects of different interventions on some outcome of interest . In addition to the treatment assignment and outcome , data on baseline covariates , such as demographic characteristics or biomarker measurements , are typically collected . Incorporating such auxiliary covariates in the analysis of randomized trials can increase power , but questions remain about how to preserve type I error when incorporating such covariates in a flexible way , particularly when the number of randomized units is small . Using the Young Citizens study , a cluster-randomized trial of an educational intervention to promote HIV awareness , we compare several methods to evaluate intervention effects when baseline covariates are incorporated adaptively . To ascertain the validity of the methods shown in small samples , extensive simulation studies were conducted . We demonstrate that randomization inference preserves type I error under model selection while tests based on asymptotic theory may yield invalid results . We also demonstrate that covariate adjustment generally increases power , except at extremely small sample sizes using liberal selection procedures . Although shown within the context of HIV prevention research , our conclusions have important implications for maximizing efficiency and robustness in randomized trials with small samples across disciplines .
The estimation of probabilities of network edges from the observed adjacency matrix has important applications to predicting missing links and network denoising . It has usually been addressed by estimating the graphon , a function that determines the matrix of edge probabilities , but this is ill-defined without strong assumptions on the network structure . Here we propose a novel computationally efficient method , based on neighborhood smoothing to estimate the expectation of the adjacency matrix directly , without making the structural assumptions that graphon estimation requires . The neighborhood smoothing method requires little tuning , has a competitive mean-squared error rate , and outperforms many benchmark methods on link prediction in simulated and real networks .
The influence of DNA cis-regulatory elements on a gene ' s expression has been intensively studied . However , little is known about expressions driven by trans-acting DNA hotspots . DNA hotspots harboring copy number aberrations are recognized to be important in cancer as they influence multiple genes on a global scale . The challenge in detecting trans-effects is mainly due to the computational difficulty in detecting weak and sparse trans-acting signals amidst co-occuring passenger events . We propose an integrative approach to learn a sparse interaction network of DNA copy-number regions with their downstream targets in a breast cancer dataset . Information from this network helps distinguish copy-number driven from copy-number independent expression changes on a global scale . Our result further delineates cis- and trans-effects in a breast cancer dataset , for which important oncogenes such as ESR0 and ERBB0 appear to be highly copy-number dependent . Further , our model is shown to be efficient and in terms of goodness of fit no worse than other state-of the art predictors and network reconstruction models using both simulated and real data .
Covariance estimation becomes challenging in the regime where the number p of variables outstrips the number n of samples available to construct the estimate . One way to circumvent this problem is to assume that the covariance matrix is nearly sparse and to focus on estimating only the significant entries . To analyze this approach , Levina and Vershynin ( 0000 ) introduce a formalism called masked covariance estimation , where each entry of the sample covariance estimator is reweighted to reflect an a priori assessment of its importance . This paper provides a short analysis of the masked sample covariance estimator by means of a matrix concentration inequality . The main result applies to general distributions with at least four moments . Specialized to the case of a Gaussian distribution , the theory offers qualitative improvements over earlier work . For example , the new results show that n = O ( B log^0 p ) samples suffice to estimate a banded covariance matrix with bandwidth B up to a relative spectral-norm error , in contrast to the sample complexity n = O ( B log^0 p ) obtained by Levina and Vershynin .
This paper aims at providing a survey on the problems that can be easily addressed by cellular automata in bioinformatics . Some of the authors have proposed algorithms for addressing some problems in bioinformatics but the application of cellular automata in bioinformatics is a virgin field in research . None of the researchers has tried to relate the major problems in bioinformatics and find a common solution . Extensive literature surveys were conducted . We have considered some papers in various journals and conferences for conduct of our research . This paper provides intuition towards relating various problems in bioinformatics logically and tries to attain a common frame work for addressing the same .
This paper shows that the problem of testing hypotheses in moment condition models without any assumptions about identification may be considered as a problem of testing with an infinite-dimensional nuisance parameter . We introduce a sufficient statistic for this nuisance parameter and propose conditional tests . These conditional tests have uniformly correct asymptotic size for a large class of models and test statistics . We apply our approach to construct tests based on quasi-likelihood ratio statistics , which we show are efficient in strongly identified models and perform well relative to existing alternatives in two examples .
A key goal of computer vision is to recover the underlying 0D structure from 0D observations of the world . In this paper we learn strong deep generative models of 0D structures , and recover these structures from 0D and 0D images via probabilistic inference . We demonstrate high-quality samples and report log-likelihoods on several datasets , including ShapeNet [0] , and establish the first benchmarks in the literature . We also show how these models and their inference networks can be trained end-to-end from 0D images . This demonstrates for the first time the feasibility of learning to infer 0D representations of the world in a purely unsupervised manner .
There are often two important types of variation in functional data : the horizontal ( or phase ) variation and the vertical ( or amplitude ) variation . These two types of variation have been appropriately separated and modeled through a domain warping method ( or curve registration ) based on the Fisher Rao metric . This paper focuses on the analysis of the horizontal variation , captured by the domain warping functions . The square-root velocity function representation transforms the manifold of the warping functions to a Hilbert sphere . Motivated by recent results on manifold analogs of principal component analysis , we propose to analyze the horizontal variation via a Principal Nested Spheres approach . Compared with earlier approaches , such as approximating tangent plane principal component analysis , this is seen to be the most efficient and interpretable approach to decompose the horizontal variation in some examples .
We consider the problem of estimating the sparse time-varying parameter vectors of a point process model in an online fashion , where the observations and inputs respectively consist of binary and continuous time series . We construct a novel objective function by incorporating a forgetting factor mechanism into the point process log-likelihood to enforce adaptivity and employ $\ell_0$-regularization to capture the sparsity . We provide a rigorous analysis of the maximizers of the objective function , which extends the guarantees of compressed sensing to our setting . We construct two recursive filters for online estimation of the parameter vectors based on proximal optimization techniques , as well as a novel filter for recursive computation of statistical confidence regions . Simulation studies reveal that our algorithms outperform several existing point process filters in terms of trackability , goodness-of-fit and mean square error . We finally apply our filtering algorithms to experimentally recorded spiking data from the ferret primary auditory cortex during attentive behavior in a click rate discrimination task . Our analysis provides new insights into the time-course of the spectrotemporal receptive field plasticity of the auditory neurons .
We give a stochastic extension of the Brane Calculus , along the lines of recent work by Cardelli and Mardare . In this presentation , the semantics of a Brane process is a measure of the stochastic distribution of possible derivations . To this end , we first introduce a labelled transition system for Brane Calculus , proving its adequacy w . r . t . the usual reduction semantics . Then , brane systems are presented as Markov processes over the measurable space generated by terms up-to syntactic congruence , and where the measures are indexed by the actions of this new LTS . Finally , we provide a SOS presentation of this stochastic semantics , which is compositional and syntax-driven .
Data Mining is being actively applied to stock market since 0000s . It has been used to predict stock prices , stock indexes , for portfolio management , trend detection and for developing recommender systems . The various algorithms which have been used for the same include ANN , SVM , ARIMA , GARCH etc . Different hybrid models have been developed by combining these algorithms with other algorithms like roughest , fuzzy logic , GA , PSO , DE , ACO etc . to improve the efficiency . This paper proposes DE-SVM model ( Differential EvolutionSupport vector Machine ) for stock price prediction . DE has been used to select best free parameters combination for SVM to improve results . The paper also compares the results of prediction with the outputs of SVM alone and PSO-SVM model ( Particle Swarm Optimization ) . The effect of normalization of data on the accuracy of prediction has also been studied .
In a missing-data setting , we have a sample in which a vector of explanatory variables x_i is observed for every subject i , while scalar outcomes y_i are missing by happenstance on some individuals . In this work we propose robust estimates of the distribution of the responses assuming missing at random ( MAR ) data , under a semiparametric regression model . Our approach allows the consistent estimation of any weakly continuous functional of the response ' s distribution . In particular , strongly consistent estimates of any continuous location functional , such as the median or MM functionals , are proposed . A robust fit for the regression model combined with the robust properties of the location functional gives rise to a robust recipe for estimating the location parameter . Robustness is quantified through the breakdown point of the proposed procedure . The asymptotic distribution of the location estimates is also derived .
The onset of several silent , chronic diseases such as diabetes can be detected only through diagnostic tests . Due to cost considerations , self-reported outcomes are routinely collected in lieu of expensive diagnostic tests in large-scale prospective investigations such as the Women ' s Health Initiative . However , self-reported outcomes are subject to imperfect sensitivity and specificity . Using a semiparametric likelihood-based approach , we present time to event models to estimate the association of one or more covariates with a error-prone , self-reported outcome . We present simulation studies to assess the effect of error in self-reported outcomes with regard to bias in the estimation of the regression parameter of interest . We apply the proposed methods to prospective data from 000 , 000 women enrolled in the Women ' s Health Initiative to evaluate the effect of statin use with the risk of incident diabetes mellitus among postmenopausal women . The current analysis is based on follow-up through 0000 , with a median duration of follow-up of 00 . 0 years . The methods proposed in this paper are readily implemented using our freely available R software package icensmis , which is available at the Comprehensive R Archive Network ( CRAN ) website .
It is well known fact that was phrased by famous quality scholar P . B . Crosby that it is always cheaper to do the job right the first time . However , this statement must be reconsidered with respect to software development projects , because the concept of quality and associated costs measurements in software engineering discipline is not as matured as in manufacturing and other fields of the industry . Post delivery defects ( i . e . software bugs ) are very common and integral part of software industry . While the process of measuring and classifying quality cost components is visible , obvious and institutionalized in manufacturing industry , it is still evolving in software industry . In addition to this , the recommendations of British standard BS-0000-0 : 0000 for classifying quality-related costs into prevention costs , appraisal costs , and failure costs have been successfully adopted by many industries , by identifying the activities carried out within each of these categories , and measuring the costs connected with them , software industry has a long-way to go to have the same level of adoption and institutionalization of cost of quality measurements and visibility . Cost of Quality for software isn ' t the price of creating a quality software product or IT-service . It ' s actually the cost of NOT creating a quality software product or IT-service . The chronic affliction of majority of software development projects that are frequently found bleeding with cost overruns , schedule slippage , scope creep and poor quality of deliverables in the global IT industry , was the trigger for this research work . Lessons learnt from this study offer valuable prescriptive guidance for small and medium software businesses , who can benefit from this study by applying the same for their quality improvement initiatives using CoQ-metric , to enhance the capability and maturity of their SDLC-project performance .
An adversarial example is an example that has been adjusted to produce the wrong label when presented to a system at test time . If adversarial examples existed that could fool a detector , they could be used to ( for example ) wreak havoc on roads populated with smart vehicles . Recently , we described our difficulties creating physical adversarial stop signs that fool a detector . More recently , Evtimov et al . produced a physical adversarial stop sign that fools a proxy model of a detector . In this paper , we show that these physical adversarial stop signs do not fool two standard detectors ( YOLO and Faster RCNN ) in standard configuration . Evtimov et al . ' s construction relies on a crop of the image to the stop sign ; this crop is then resized and presented to a classifier . We argue that the cropping and resizing procedure largely eliminates the effects of rescaling and of view angle . Whether an adversarial attack is robust under rescaling and change of view direction remains moot . We argue that attacking a classifier is very different from attacking a detector , and that the structure of detectors - which must search for their own bounding box , and which cannot estimate that box very accurately - likely makes it difficult to make adversarial patterns . Finally , an adversarial pattern on a physical object that could fool a detector would have to be adversarial in the face of a wide family of parametric distortions ( scale ; view angle ; box shift inside the detector ; illumination ; and so on ) . Such a pattern would be of great theoretical and practical interest . There is currently no evidence that such patterns exist .
The Scaling of microchip technologies , from micron to submicron and now to deep sub-micron ( DSM ) range , has enabled large scale systems-on-chip ( SoC ) . In future deep submicron ( DSM ) designs , the interconnect effect will definitely dominate performance . Network-on-Chip ( NoC ) has become a promising solution to bus-based communication infrastructure limitations . NoC designs usually targets Application Specific Integrated Circuits ( ASICs ) , however , the fabrication process costs a lot . Implementing a NoC on an FPGA does not only reduce the cost but also decreases programming and verification cycles . In this paper , an Asynchronous NoC has been implemented on a SPARTAN-0E\textregistered device . The NoC supports basic transactions of both widely used on-chip interconnection standards , the Open Core Protocol ( OCP ) and the WISHBONE Protocol . Although , FPGA devices are synchronous in nature , it has been shown that they can be used to prototype a Global Asynchronous Local Synchronous ( GALS ) systems , comprising an Asynchronous NoC connecting IP cores operating in different clock domains .
We examine the utility of implicit behavioral cues in the form of EEG brain signals and eye movements for gender recognition ( GR ) and emotion recognition ( ER ) . Specifically , the examined cues are acquired via low-cost , off-the-shelf sensors . We asked 00 viewers ( 00 female ) to recognize emotions from unoccluded ( no mask ) as well as partially occluded ( eye and mouth masked ) emotive faces . Obtained experimental results reveal that ( a ) reliable GR and ER is achievable with EEG and eye features , ( b ) differential cognitive processing especially for negative emotions is observed for males and females and ( c ) some of these cognitive differences manifest under partial face occlusion , as typified by the eye and mouth mask conditions .
Regenerating codes are a class of recently developed codes for distributed storage that , like Reed-Solomon codes , permit data recovery from any subset of k nodes within the n-node network . However , regenerating codes possess in addition , the ability to repair a failed node by connecting to an arbitrary subset of d nodes . It has been shown that for the case of functional-repair , there is a tradeoff between the amount of data stored per node and the bandwidth required to repair a failed node . A special case of functional-repair is exact-repair where the replacement node is required to store data identical to that in the failed node . Exact-repair is of interest as it greatly simplifies system implementation . The first result of the paper is an explicit , exact-repair code for the point on the storage-bandwidth tradeoff corresponding to the minimum possible repair bandwidth , for the case when d=n-0 . This code has a particularly simple graphical description and most interestingly , has the ability to carry out exact-repair through mere transfer of data and without any need to perform arithmetic operations . Hence the term `repair-by-transfer ' . The second result of this paper shows that the interior points on the storage-bandwidth tradeoff cannot be achieved under exact-repair , thus pointing to the existence of a separate tradeoff under exact-repair . Specifically , we identify a set of scenarios , termed `helper node pooling ' , and show that it is the necessity to satisfy such scenarios that over-constrains the system .
Stochastic gradient Markov chain Monte Carlo ( SG-MCMC ) has been increasingly popular in Bayesian learning due to its ability to deal with large data . A standard SG-MCMC algorithm simulates samples from a discretized-time Markov chain to approximate a target distribution . However , the samples are typically highly correlated due to the sequential generation process , an undesired property in SG-MCMC . In contrary , Stein variational gradient descent ( SVGD ) directly optimizes a set of particles , and it is able to approximate a target distribution with much fewer samples . In this paper , we propose a novel method to directly optimize particles ( or samples ) in SG-MCMC from scratch . Specifically , we propose efficient methods to solve the corresponding Fokker-Planck equation on the space of probability distributions , whose solution ( i . e . , a distribution ) is approximated by particles . Through our framework , we are able to show connections of SG-MCMC to SVGD , as well as the seemly unrelated generative-adversarial-net framework . Under certain relaxations , particle optimization in SG-MCMC can be interpreted as an extension of standard SVGD with momentum .
We estimate the anisotropic index of an anisotropic fractional Brownian field . For all directions , we give a convergent estimator of the value of the anisotropic index in this direction , based on generalized quadratic variations . We also prove a central limit theorem . First we present a result of identification that relies on the asymptotic behavior of the spectral density of a process . Then , we define Radon transforms of the anisotropic fractional Brownian field and prove that these processes admit a spectral density satisfying the previous assumptions . Finally we use simulated fields to test the proposed estimator in different anisotropic and isotropic cases . Results show that the estimator behaves similarly in all cases and is able to detect anisotropy quite accurately .
The degrees of freedom of MIMO interference networks with constant channel coefficients are not known in general . Determining the feasibility of a linear interference alignment solution is a key step toward solving this open problem . Our approach in this paper is to view the alignment problem as a system of bilinear equations and determine its solvability by comparing the number of equations and the number of variables . To this end , we divide interference alignment problems into two classes - proper and improper . An interference alignment problem is called proper if the number of equations does not exceed the number of variables . Otherwise , it is called improper . Examples are presented to support the intuition that for generic channel matrices , proper systems are almost surely feasible and improper systems are almost surely infeasible .
Functional coroutines are a restricted form of control mechanism , where each coroutine is represented with both a continuation and an environment . This restriction was originally obtained by considering a constructive version of Parigot ' s classical natural deduction which is sound and complete for the Constant Domain logic . In this article , we present a refinement of de Groote ' s abstract machine for functional coroutines and we prove its correctness . Therefore , this abstract machine also provides a direct computational interpretation of the Constant Domain logic .
We consider the problem of estimating a random state vector when there is information about the maximum distances between its subvectors . The estimation problem is posed in a Bayesian framework in which the minimum mean square error ( MMSE ) estimate of the state is given by the conditional mean . Since finding the conditional mean requires multidimensional integration , an approximate MMSE estimator is proposed . The performance of the proposed estimator is evaluated in a positioning problem . Finally , the application of the estimator in inequality constrained recursive filtering is illustrated by applying the estimator to a dead-reckoning problem . The MSE of the estimator is compared with two related posterior Cram\ ' er-Rao bounds .
While Monte Carlo Tree Search and closely related methods have dominated General Video Game Playing , recent research has demonstrated the promise of Rolling Horizon Evolutionary Algorithms as an interesting alternative . However , there is little attention paid to population initialization techniques in the setting of general real-time video games . Therefore , this paper proposes the use of population seeding to improve the performance of Rolling Horizon Evolution and presents the results of two methods , One Step Look Ahead and Monte Carlo Tree Search , tested on 00 games of the General Video Game AI corpus with multiple evolution parameter values ( population size and individual length ) . An in-depth analysis is carried out between the results of the seeding methods and the vanilla Rolling Horizon Evolution . In addition , the paper presents a comparison to a Monte Carlo Tree Search algorithm . The results are promising , with seeding able to boost performance significantly over baseline evolution and even match the high level of play obtained by the Monte Carlo Tree Search .
The \it{Ambient Logic} ( AL ) has been proposed for expressing properties of process mobility in the calculus of Mobile Ambients ( MA ) , and as a basis for query languages on semistructured data . We study some basic questions concerning the discriminating power of AL , focusing on the equivalence on processes induced by the logic $ ( =_L> ) $ . As underlying calculi besides MA we consider a subcalculus in which an image-finiteness condition holds and that we prove to be Turing complete . Synchronous variants of these calculi are studied as well . In these calculi , we provide two operational characterisations of $_=L$ : a coinductive one ( as a form of bisimilarity ) and an inductive one ( based on structual properties of processes ) . After showing $_=L$ to be stricly finer than barbed congruence , we establish axiomatisations of $_=L$ on the subcalculus of MA ( both the asynchronous and the synchronous version ) , enabling us to relate $_=L$ to structural congruence . We also present some ( un ) decidability results that are related to the above separation properties for AL : the undecidability of $_=L$ on MA and its decidability on the subcalculus .
The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models . This has motivated a growing line of work on what it means for a classification procedure to be " fair . " In this paper , we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates . We show that calibration is compatible only with a single error constraint ( i . e . equal false-negatives rates across groups ) , and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier . These unsettling findings , which extend and generalize existing results , are empirically confirmed on several datasets .
The convex hull describes the extent or shape of a set of data and is used ubiquitously in computational geometry . Common algorithms to construct the convex hull on a finite set of n points ( x , y ) range from O ( nlogn ) time to O ( n ) time . However , it is often the case that a heuristic procedure is applied to reduce the original set of n points to a set of s < n points which contains the hull and so accelerates the final hull finding procedure . We present an algorithm to precondition data before building a 0D convex hull with integer coordinates , with three distinct advantages . First , for all practical purposes , it is linear ; second , no explicit sorting of data is required and third , the reduced set of s points is constructed such that it forms an ordered set that can be directly pipelined into an O ( n ) time convex hull algorithm . Under these criteria a fast ( or O ( n ) ) pre-conditioner in principle creates a fast convex hull ( approximately O ( n ) ) for an arbitrary set of points . The paper empirically evaluates and quantifies the acceleration generated by the method against the most common convex hull algorithms . An extra acceleration of at least four times when compared to previous existing preconditioning methods is found from experiments on a dataset .
Information divergence functions play a critical role in statistics and information theory . In this paper we show that a non-parametric f-divergence measure can be used to provide improved bounds on the minimum binary classification probability of error for the case when the training and test data are drawn from the same distribution and for the case where there exists some mismatch between training and test distributions . We confirm the theoretical results by designing feature selection algorithms using the criteria from these bounds and by evaluating the algorithms on a series of pathological speech classification tasks .
We investigate the low-dimensional structure of deterministic transformations between random variables , i . e . , transport maps between probability measures . In the context of statistics and machine learning , these transformations can be used to couple a tractable " reference " measure ( e . g . , a standard Gaussian ) with a target measure of interest . Direct simulation from the desired measure can then be achieved by pushing forward reference samples through the map . Yet characterizing such a map---e . g . , representing and evaluating it---grows challenging in high dimensions . The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of low-dimensional couplings , induced by transport maps that are sparse and/or decomposable . Our analysis not only facilitates the construction of transformations in high-dimensional settings , but also suggests new inference methodologies for continuous non-Gaussian graphical models . For instance , in the context of nonlinear state-space models , we describe new variational algorithms for filtering , smoothing , and sequential parameter inference . These algorithms can be understood as the natural generalization---to the non-Gaussian case---of the square-root Rauch-Tung-Striebel Gaussian smoother .
We build confidence balls for the common density $s$ of a real valued sample $X_0 , . . . , X_n$ . We use resampling methods to estimate the projection of $s$ onto finite dimensional linear spaces and a model selection procedure to choose an optimal approximation space . The covering property is ensured for all $n\geq0$ and the balls are adaptive over a collection of linear spaces .
The paper proposes an ancient landscape design as an example of graphic design for an age and place where no written documents existed . It is created by a network of earthworks , which constitute the remains of an extensive ancient agricultural system . It can be seen by means of the Google satellite imagery on the Peruvian region near the Titicaca Lake , as a texture superimposed to the background landform . In this texture , many drawings ( geoglyphs ) can be observed .
One of the most remarkable aspects of our species is that while we show surprisingly little genetic diversity , we demonstrate astonishing amounts of cultural diversity . Perhaps most impressive is the diversity of our technologies , broadly defined as all the physical objects we produce and the skills we use to produce them . Despite considerable focus on the evolution of technology by social scientists and philosophers , there have been few attempts to systematically quantify technological diversity and therefore the dynamics of technological change remain poorly understood . Here we show a novel Bayesian model for examining technological diversification adopted from paleontological analysis of occurrence data . We use this framework to estimate the tempo of diversification in American car and truck models produced between 0000 and 0000 and to test the relative importance of competition and extrinsic factors in shaping changes in macroevolutionary rates . Our results identify a four-fold decrease in the origination and extinction rates of car models and a negative net diversification rate over the last thirty years . We also demonstrate that competition played a more significant role in car model diversification than either changes in oil prices or gross domestic product . Together our analyses provide a set of tools that can enhance current research on technological and cultural evolution by providing a flexible and quantitative framework for exploring the dynamics of diversification .
We propose a distributed computing framework , based on a divide and conquer strategy and hierarchical modeling , to accelerate posterior inference for high-dimensional Bayesian factor models . Our approach distributes the task of high-dimensional covariance matrix estimation to multiple cores , solves each subproblem separately via a latent factor model , and then combines these estimates to produce a global estimate of the covariance matrix . Existing divide and conquer methods focus exclusively on dividing the total number of observations $n$ into subsamples while keeping the dimension $p$ fixed . Our approach is novel in this regard : it includes all of the $n$ samples in each subproblem and , instead , splits the dimension $p$ into smaller subsets for each subproblem . The subproblems themselves can be challenging to solve when $p$ is large due to the dependencies across dimensions . To circumvent this issue , we specify a novel hierarchical structure on the latent factors that allows for flexible dependencies across dimensions , while still maintaining computational efficiency . Our approach is readily parallelizable and is shown to have computational efficiency of several orders of magnitude in comparison to fitting a full factor model . We report the performance of our method in synthetic examples and a genomics application .
Local Process Model ( LPM ) discovery is focused on the mining of a set of process models where each model describes the behavior represented in the event log only partially , i . e . subsets of possible events are taken into account to create so-called local process models . Often such smaller models provide valuable insights into the behavior of the process , especially when no adequate and comprehensible single overall process model exists that is able to describe the traces of the process from start to end . The practical application of LPM discovery is however hindered by computational issues in the case of logs with many activities ( problems may already occur when there are more than 00 unique activities ) . In this paper , we explore three heuristics to discover subsets of activities that lead to useful log projections with the goal of speeding up LPM discovery considerably while still finding high-quality LPMs . We found that a Markov clustering approach to create projection sets results in the largest improvement of execution time , with discovered LPMs still being better than with the use of randomly generated activity sets of the same size . Another heuristic , based on log entropy , yields a more moderate speedup , but enables the discovery of higher quality LPMs . The third heuristic , based on the relative information gain , shows unstable performance : for some data sets the speedup and LPM quality are higher than with the log entropy based method , while for other data sets there is no speedup at all .
We show how the massive data compression algorithm MOPED can be used to reduce , by orders of magnitude , the number of simulated datasets that are required to estimate the covariance matrix required for the analysis of gaussian-distributed data . This is relevant when the covariance matrix cannot be calculated directly . The compression is especially valuable when the covariance matrix varies with the model parameters . In this case , it may be prohibitively expensive to run enough simulations to estimate the full covariance matrix throughout the parameter space . This compression may be particularly valuable for the next-generation of weak lensing surveys , such as proposed for Euclid and LSST , for which the number of summary data ( such as band power or shear correlation estimates ) is very large , $\sim 00^0$ , due to the large number of tomographic redshift bins that the data will be divided into . In the pessimistic case where the covariance matrix is estimated separately for all points in an MCMC analysis , this may require an unfeasible $00^0$ simulations . We show here that MOPED can reduce this number by a factor of 0000 , or a factor of $\sim 00^0$ if some regularity in the covariance matrix is assumed , reducing the number of simulations required to a manageable $00^0$ , making an otherwise intractable analysis feasible .
We describe and implement a policy language . In our system , agents can distribute data along with usage policies in a decentralized architecture . Our language supports the specification of conditions and obligations , and also the possibility to refine policies . In our framework , the compliance with usage policies is not actively enforced . However , agents are accountable for their actions , and may be audited by an authority requiring justifications .
This document explains how to obtain a Markov basis of the graphical model of the complete bipartite graph $K_{0 , N}$ with binary nodes . The computations illustrate the theory developed in arXiv : 0000 . 0000 that explains how to compute Markov bases of toric fiber products .
An important problem in the statistical analysis of network data is that network data are non-standard data and therefore the meaning of core statistical notions , such as sample and population , is not obvious . All too often , the meaning of such core notions has been left implicit , which has led to considerable confusion . Starting from first principles , we build a statistical framework encompassing a wide range of inference scenarios and distinguish the graph generating process from the observation process . We discuss inference for graphs of fixed size , including finite- and super-population inference , and inference for sequences of graphs of increasing size . We review invariance properties of sequences of graphs of increasing size , including invariance to the labeling of nodes , invariance of expected degrees of nodes , and projectivity , and discuss implications in terms of inference . We conclude with consistency and asymptotic normality results for estimators in finite- , super- , and infinite-population inference scenarios .
Standard game theory assumes that the structure of the game is common knowledge among players . We relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game . In particular , they may not be aware of moves that they and other agents can make . We show how such games can be represented ; the key idea is to describe the game from the point of view of every agent at every node of the game tree . We provide a generalization of Nash equilibrium and show that every game with awareness has a generalized Nash equilibrium . Finally , we extend these results to games with awareness of unawareness , where a player i may be aware that a player j can make moves that i is not aware of , and to subjective games , where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior .
Learning a causal effect from observational data is not straightforward , as this is not possible without further assumptions . If hidden common causes between treatment $X$ and outcome $Y$ cannot be blocked by other measurements , one possibility is to use an instrumental variable . In principle , it is possible under some assumptions to discover whether a variable is structurally instrumental to a target causal effect $X \rightarrow Y$ , but current frameworks are somewhat lacking on how general these assumptions can be . A instrumental variable discovery problem is challenging , as no variable can be tested as an instrument in isolation but only in groups , but different variables might require different conditions to be considered an instrument . Moreover , identification constraints might be hard to detect statistically . In this paper , we give a theoretical characterization of instrumental variable discovery , highlighting identifiability problems and solutions , the need for non-Gaussianity assumptions , and how they fit within existing methods .
Syntax-Guided Synthesis ( SyGuS ) is the computational problem of finding an implementation f that meets both a semantic constraint given by a logical formula $\varphi$ in a background theory T , and a syntactic constraint given by a grammar G , which specifies the allowed set of candidate implementations . Such a synthesis problem can be formally defined in SyGuS-IF , a language that is built on top of SMT-LIB . The Syntax-Guided Synthesis Competition ( SyGuS-Comp ) is an effort to facilitate , bring together and accelerate research and development of efficient solvers for SyGuS by providing a platform for evaluating different synthesis techniques on a comprehensive set of benchmarks . In this year ' s competition we added a new track devoted to programming by examples . This track consisted of two categories , one using the theory of bit-vectors and one using the theory of strings . This paper presents and analyses the results of SyGuS-Comp ' 00 .
Software development processes are subject to variations in time and space , variations that can originate from learning effects , differences in application domains , or a number of other causes . Identifying and analyzing such differences is crucial for a variety of process activities , like defining and evolving process standards , or analyzing the compliance of process models to existing standards , among others . In this paper , we show why appropriately identifying , describing , and visualizing differences between process models in order to support such activities is a highly challenging task . We present scenarios that motivate the need for process model difference analysis , and describe the conceptual and technical challenges arising from them . In addition , we sketch an initial tool-based approach implementing difference analysis , and contrast it with similar existing approaches . The results from this paper constitute the requirements for our ongoing development effort , whose objectives we also describe briefly .
In this paper we propose a novel method of realizing discrete-time ( D-T ) signal amplification using Nano-Electro-Mechanical ( NEMS ) devices . The amplifier uses mechanical devices instead of traditional solid-state circuits . The proposed NEMS-based D-T amplifier provides high gain and operate on a wide dynamic range of signals , consuming only a few micro watts of power . The proposed concept is subsequently verified using Verilog-A model of the NEMS device . Modifications in the proposed amplifier to suit different specifications are also presented .
Agriculture has a high number of fatalities compared to other blue collar fields , additionally population decreasing in rural areas is resulting in decreased work force . These issues have resulted in increased focus on improving efficiency of and introducing autonomy in agriculture . Field robots are an increasingly promising branch of robotics targeted at full automation in agriculture . The safety aspect however is rely addressed in connection with safety standards , which limits the real-world applicability . In this paper we present an analysis of a vision pipeline in connection with functional-safety standards , in order to propose solutions for how to ascertain that the system operates as required . Based on the analysis we demonstrate a simple mechanism for verifying that a vision pipeline is functioning correctly , thus improving the safety in the overall system .
The world is rapidly adopting RESTful web services for most of its tasks . The once popular SOAP-based web services are fast losing ground owing to this . RESTful web services are light weight services without strict message formats . RESTful web services , unlike SOAP , are capable of message transfer in any format be it XML , JSON , plain text . However , in spite of these positives , ensuring message level security in REST is a challenge . Security in RESTful web services is still largely dependent upon transport layer security . There has been some work recently towards message level security in such environments wherein the transfer of message level security metadata is done through utilising new HTTP headers . We feel , however , that any method that compromises the generality of the HTTP protocol should be avoided . In this paper , therefore , we propose two new ways of encryption that promise to ensure message level security in RESTful web services without the need for special HTTP headers . This approach works seamlessly on most famous content-types of RESTful web services : XML , JSON , HTML , plain-text and various ASCII printable content types . Further , the proposed approach removes the need for content negotiation in cases where the content comprises XML , JSON , HTML , plain-text , and ASCII printable content types and also removes the need for XML or JSON canonicalization .
While the existence of low-dimensional embedding manifolds has been shown in patterns of collective motion , the current battery of nonlinear dimensionality reduction methods are not amenable to the analysis of such manifolds . This is mainly due to the necessary spectral decomposition step , which limits control over the mapping from the original high-dimensional space to the embedding space . Here , we propose an alternative approach that demands a two-dimensional embedding which topologically summarizes the high-dimensional data . In this sense , our approach is closely related to the construction of one-dimensional principal curves that minimize orthogonal error to data points subject to smoothness constraints . Specifically , we construct a two-dimensional principal manifold directly in the high-dimensional space using cubic smoothing splines , and define the embedding coordinates in terms of geodesic distances . Thus , the mapping from the high-dimensional data to the manifold is defined in terms of local coordinates . Through representative examples , we show that compared to existing nonlinear dimensionality reduction methods , the principal manifold retains the original structure even in noisy and sparse datasets . The principal manifold finding algorithm is applied to configurations obtained from a dynamical system of multiple agents simulating a complex maneuver called predator mobbing , and the resulting two-dimensional embedding is compared with that of a well-established nonlinear dimensionality reduction method .
Bizur is a consensus algorithm exposing a key-value interface . It is used by a distributed file-system that scales to 000s of servers , delivering millions of IOPS , both data and metadata , with consistent low-latency . Bizur is aimed for services that require strongly consistent state , but do not require a distributed log ; for example , a distributed lock manager or a distributed service locator . By avoiding a distributed log scheme , Bizur outperforms distributed log based consensus algorithms , producing more IOPS and guaranteeing lower latencies during normal operation and especially during failures . Paxos-like algorithms ( e . g . , Zab and Raft ) which are used by existing distributed file-systems , can have artificial contention points due to their dependence on a distributed log . The distributed log is needed when replicating a general service , but when the desired service is key-value based , the contention points created by the distributed log can be avoided . Bizur does exactly that , by reaching consensus independently on independent keys . This independence allows Bizur to handle failures more efficiently and to scale much better than other consensus algorithms , allowing the file-system that utilizes Bizur to scale with it .
GoTools is a program which solves life & death problems in the game of Go . This paper describes experiments using a Genetic Algorithm to optimize heuristic weights used by GoTools ' tree-search . The complete set of heuristic weights is composed of different subgroups , each of which can be optimized with a suitable fitness function . As a useful side product , an MPI interface for FreePascal was implemented to allow the use of a parallelized fitness function running on a Beowulf cluster . The aim of this exercise is to optimize the current version of GoTools , and to make tools available in preparation of an extension of GoTools for solving open boundary life & death problems , which will introduce more heuristic parameters to be fine tuned .
Monte Carlo sampling methods often suffer from long correlation times . Consequently , these methods must be run for many steps to generate an independent sample . In this paper a method is proposed to overcome this difficulty . The method utilizes information from rapidly equilibrating coarse Markov chains that sample marginal distributions of the full system . This is accomplished through exchanges between the full chain and the auxiliary coarse chains . Results of numerical tests on the bridge sampling and filtering/smoothing problems for a stochastic differential equation are presented .
Spectral clustering is a popular and versatile clustering method based on a relaxation of the normalised graph cut objective . Despite its popularity , however , there is no single agreed upon method for tuning the important scaling parameter , nor for determining automatically the number of clusters to extract . Popular heuristics exist , but corresponding theoretical results are scarce . In this paper we investigate the asymptotic value of the normalised cut for an increasing sample assumed to arise from an underlying probability distribution , and based on this result provide recommendations for improving spectral clustering methodology . A corresponding algorithm is proposed with strong empirical performance .
Algorithms for binary classification based on adaptive tree partitioning are formulated and analyzed for both their risk performance and their friendliness to numerical implementation . The algorithms can be viewed as generating a set approximation to the Bayes set and thus fall into the general category of set estimators . In contrast with the most studied tree-based algorithms , which utilize piecewise constant approximation on the generated partition [IEEE Trans . Inform . Theory 00 ( 0000 ) 0000-0000 ; Mach . Learn . 00 ( 0000 ) 000-000] , we consider decorated trees , which allow us to derive higher order methods . Convergence rates for these methods are derived in terms the parameter $\alpha$ of margin conditions and a rate $s$ of best approximation of the Bayes set by decorated adaptive partitions . They can also be expressed in terms of the Besov smoothness $\beta$ of the regression function that governs its approximability by piecewise polynomials on adaptive partition . The execution of the algorithms does not require knowledge of the smoothness or margin conditions . Besov smoothness conditions are weaker than the commonly used H\ " {o}lder conditions , which govern approximation by nonadaptive partitions , and therefore for a given regression function can result in a higher rate of convergence . This in turn mitigates the compatibility conflict between smoothness and margin parameters .
It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks . In this paper we present Steerable Convolutional Neural Networks , an efficient and flexible class of equivariant convolutional networks . We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark . The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types , each one associated with a particular kind of symmetry . We show how the parameter cost of a steerable filter bank depends on the types of the input and output features , and show how to use this knowledge to construct CNNs that utilize parameters effectively .
Domain specific languages ( DSLs ) allow domain experts to model parts of the system under development in a problem-oriented notation that is well-known in the respective domain . The introduction of a DSL is often accompanied the desire to transform its instances . Although the modeling language is domain specific , the transformation language used to describe modifications , such as model evolution or refactoring operations , on the underlying model , usually is a rather domain independent language nowadays . Most transformation languages use a generic notation of model patterns that is closely related to typed and attributed graphs or to object diagrams ( the abstract syntax ) . A notation that reflects the transformed elements of the original DSL in its own concrete syntax would be strongly preferable , because it would be more comprehensible and easier to learn for domain experts . In this paper we present a transformation language that reuses the concrete syntax of a textual modeling language for hierarchical automata , which allows domain experts to describe models as well as modifications of models in a convenient , yet precise manner . As an outlook , we illustrate a scenario where we generate transformation languages from existing textual languages .
This paper describes several new algorithms for estimating the parameters of a periodic bandlimited signal from samples corrupted by jitter ( timing noise ) and additive noise . Both classical ( non-random ) and Bayesian formulations are considered : an Expectation-Maximization ( EM ) algorithm is developed to compute the maximum likelihood ( ML ) estimator for the classical estimation framework , and two Gibbs samplers are proposed to approximate the Bayes least squares ( BLS ) estimate for parameters independently distributed according to a uniform prior . Simulations are performed to demonstrate the significant performance improvement achievable using these algorithms as compared to linear estimators . The ML estimator is also compared to the Cramer-Rao lower bound to determine the range of jitter for which the estimator is approximately efficient . These simulations provide evidence that the nonlinear algorithms derived here can tolerate 0 . 0-0 times more jitter than linear estimators , reducing on-chip ADC power consumption by 00-00 percent .
The emerging Web of Data utilizes the web infrastructure to represent and interrelate data . The foundational standards of the Web of Data include the Uniform Resource Identifier ( URI ) and the Resource Description Framework ( RDF ) . URIs are used to identify resources and RDF is used to relate resources . While RDF has been posited as a logic language designed specifically for knowledge representation and reasoning , it is more generally useful if it can conveniently support other models of computing . In order to realize the Web of Data as a general-purpose medium for storing and processing the world ' s data , it is necessary to separate RDF from its logic language legacy and frame it simply as a data model . Moreover , there is significant advantage in seeing the Semantic Web as a particular interpretation of the Web of Data that is focused specifically on knowledge representation and reasoning . By doing so , other interpretations of the Web of Data are exposed that realize RDF in different capacities and in support of different computing models .
The structure representation of data distribution plays an important role in understanding the underlying mechanism of generating data . In this paper , we propose nearest prime simplicial complex approaches ( NSC ) by utilizing persistent homology to capture such structures . Assuming that each class is represented with a prime simplicial complex , we classify unlabeled samples based on the nearest projection distances from the samples to the simplicial complexes . We also extend the extrapolation ability of these complexes with a projection constraint term . Experiments in simulated and practical datasets indicate that compared with several published algorithms , the proposed NSC approaches achieve promising performance without losing the structure representation .
Within the context of autonomous vehicles , classical model-based control methods suffer from the trade-off between model complexity and computational burden required for the online solution of expensive optimization or search problems at every short sampling time . These methods include sampling-based algorithms , lattice-based algorithms and algorithms based on model predictive control ( MPC ) . Recently , end-to-end trained deep neural networks were proposed to map camera images directly to steering control . These algorithms , however , a priori dismiss decades of vehicle dynamics modeling experience , which could be leveraged for control design . In this paper , a model-based reinforcement learning ( RL ) method is proposed for the training of feedforward controllers in the context of autonomous driving . Fundamental philosophy is to offline train on arbitrarily sophisticated models , while online cheaply evaluate a feedforward controller , thereby avoiding the need for online optimization . The contributions are , first , the discussion of two closed-loop control architectures , and , second , the proposition of a simple gradient-free algorithm for deep reinforcement learning using task separation with hill climbing ( TSHC ) . Therefore , a ) simultaneous training on separate deterministic tasks with the purpose of encoding motion primitives in a neural network , and b ) the employment of maximally sparse rewards in combinations with virtual actuator constraints on velocity in setpoint proximity are advocated . For feedforward controller parametrization , both fully connected ( FC ) and recurrent neural networks ( RNNs ) are used .
The concept of comprehensive triangular decomposition ( CTD ) was first introduced by Chen et al . in their CASC ' 0000 paper and could be viewed as an analogue of comprehensive Grobner systems for parametric polynomial systems . The first complete algorithm for computing CTD was also proposed in that paper and implemented in the RegularChains library in Maple . Following our previous work on generic regular decomposition for parametric polynomial systems , we introduce in this paper a so-called hierarchical strategy for computing CTDs . Roughly speaking , for a given parametric system , the parametric space is divided into several sub-spaces of different dimensions and we compute CTDs over those sub-spaces one by one . So , it is possible that , for some benchmarks , it is difficult to compute CTDs in reasonable time while this strategy can obtain some " partial " solutions over some parametric sub-spaces . The program based on this strategy has been tested on a number of benchmarks from the literature . Experimental results on these benchmarks with comparison to RegularChains are reported and may be valuable for developing more efficient triangularization tools .
To concisely and effectively demonstrate the capabilities of our program transformation system Loo . py , we examine a transformation path from two real-world Fortran subroutines as found in a weather model to a single high-performance computational kernel suitable for execution on modern GPU hardware . Along the transformation path , we encounter kernel fusion , vectorization , prefetch- ing , parallelization , and algorithmic changes achieved by mechanized conversion between imperative and functional/substitution- based code , among a number more . We conclude with performance results that demonstrate the effects and support the effectiveness of the applied transformations .
The sparse matrix-vector multiply ( SpMV ) operation is a key computational kernel in many simulations and linear solvers . The large communication requirements associated with a reference implementation of a parallel SpMV result in poor parallel scalability . The cost of communication depends on the physical locations of the send and receive processes : messages injected into the network are more costly than messages sent between processes on the same node . In this paper , a node aware parallel SpMV ( NAPSpMV ) is introduced to exploit knowledge of the system topology , specifically the node-processor layout , to reduce costs associated with communication . The values of the input vector are redistributed to minimize both the number and the size of messages that are injected into the network during a SpMV , leading to a reduction in communication costs . A variety of computational experiments that highlight the efficiency of this approach are presented .
Prevalence mapping in low resource settings is an increasingly important endeavor to guide policy making and to spatially and temporally characterize the burden of disease . We will focus our discussion on consideration of the complex design when analyzing survey data , and on spatial modeling . With respect to the former , we consider two approaches : direct use of the weights , and a model-based approach using a spatial model to acknowledge clustering . For the latter we consider continuously indexed Markovian Gaussian random field models .
We present a Bayesian sampling algorithm called adaptive importance sampling or Population Monte Carlo ( PMC ) , whose computational workload is easily parallelizable and thus has the potential to considerably reduce the wall-clock time required for sampling , along with providing other benefits . To assess the performance of the approach for cosmological problems , we use simulated and actual data consisting of CMB anisotropies , supernovae of type Ia , and weak cosmological lensing , and provide a comparison of results to those obtained using state-of-the-art Markov Chain Monte Carlo ( MCMC ) . For both types of data sets , we find comparable parameter estimates for PMC and MCMC , with the advantage of a significantly lower computational time for PMC . In the case of WMAP0 data , for example , the wall-clock time reduces from several days for MCMC to a few hours using PMC on a cluster of processors . Other benefits of the PMC approach , along with potential difficulties in using the approach , are analysed and discussed .
While humans are highly capable of recovering from external disturbances and uncertainties that result in large tracking errors , humanoid robots have yet to reliably mimic this level of robustness . Essential to this is the ability to combine traditional " ankle strategy " balancing with step timing and location adjustment techniques . In doing so , the robot is able to step quickly to the necessary location to continue walking . In this work , we present both a new swing speed up algorithm to adjust the step timing , allowing the robot to set the foot down more quickly to recover from errors in the direction of the current capture point dynamics , and a new algorithm to adjust the desired footstep , expanding the base of support to utilize the center of pressure ( CoP ) -based ankle strategy for balance . We then utilize the desired centroidal moment pivot ( CMP ) to calculate the momentum rate of change for our inverse-dynamics based whole-body controller . We present simulation and experimental results using this work , and discuss performance limitations and potential improvements .
Most of the deployed IEEE 000 . 00e Wireless Local Area Networks ( WLANs ) use infrastructure Basic Service Set ( BSS ) in which an Access Point ( AP ) serves as a gateway between wired and wireless domains . We present the unfairness problem between the uplink and the downlink flows of any Access Category ( AC ) in the 000 . 00e Enhanced Distributed Channel Access ( EDCA ) when the default settings of the EDCA parameters are used . We propose a simple analytical model to calculate the EDCA parameter settings that achieve weighted fair resource allocation for all uplink and downlink flows . We also propose a simple model-assisted measurement-based dynamic EDCA parameter adaptation algorithm . Moreover , our dynamic solution addresses the differences in the transport layer and the Medium Access Control ( MAC ) layer interactions of User Datagram Protocol ( UDP ) and Transmission Control Protocol ( TCP ) . We show that proposed Contention Window ( CW ) and Transmit Opportunity ( TXOP ) limit adaptation at the AP provides fair UDP and TCP access between uplink and downlink flows of the same AC while preserving prioritization among ACs .
The beta process has recently been widely used as a nonparametric prior for different models in machine learning , including latent feature models . In this paper , we prove the asymptotic consistency of the finite dimensional approximation of the beta process due to Paisley \& Carin ( 0000 ) . In addition , we derive an almost sure approximation of the beta process . This approximation provides a direct method to efficiently simulate the beta process . A simulated example , illustrating the work of the method and comparing its performance to several existing algorithms , is also included .
It is shown that the fiducial distribution in a group model , or more generally a quasigroup model , determines the optimal equivariant frequentist inference procedures . The proof does not rely on existence of invariant measures , and generalizes results corresponding to the choice of the right Haar measure as a Bayesian prior . Classical and more recent examples show that fiducial arguments can be used to give good candidates for exact or approximate confidence distributions . It is here suggested that the fiducial algorithm can be considered as an alternative to the Bayesian algorithm for the construction of good frequentist inference procedures more generally .
The work concerns formal verification of workflow-oriented software models using deductive approach . The formal correctness of a model ' s behaviour is considered . Manually building logical specifications , which are considered as a set of temporal logic formulas , seems to be the significant obstacle for an inexperienced user when applying the deductive approach . A system , and its architecture , for the deduction-based verification of workflow-oriented models is proposed . The process of inference is based on the semantic tableaux method which has some advantages when compared to traditional deduction strategies . The algorithm for an automatic generation of logical specifications is proposed . The generation procedure is based on the predefined workflow patterns for BPMN , which is a standard and dominant notation for the modeling of business processes . The main idea for the approach is to consider patterns , defined in terms of temporal logic , as a kind of ( logical ) primitives which enable the transformation of models to temporal logic formulas constituting a logical specification . Automation of the generation process is crucial for bridging the gap between intuitiveness of the deductive reasoning and the difficulty of its practical application in the case when logical specifications are built manually . This approach has gone some way towards supporting , hopefully enhancing our understanding of , the deduction-based formal verification of workflow-oriented models .
Quantum annealing is a generic solver of the optimization problem that uses fictitious quantum fluctuation . Its simulation in classical computing is often performed using the quantum Monte Carlo simulation via the Suzuki--Trotter decomposition . However , the negative sign problem sometimes emerges in the simulation of quantum annealing with an elaborate driver Hamiltonian , since it belongs to a class of non-stoquastic Hamiltonians . In the present study , we propose an alternative way to avoid the negative sign problem involved in a particular class of the non-stoquastic Hamiltonians . To check the validity of the method , we demonstrate our method by applying it to a simple problem that includes the anti-ferromagnetic XX interaction , which is a typical instance of the non-stoquastic Hamiltonians .
Video analytics requires operating with large amounts of data . Compressive sensing allows to reduce the number of measurements required to represent the video using the prior knowledge of sparsity of the original signal , but it imposes certain conditions on the design matrix . The Bayesian compressive sensing approach relaxes the limitations of the conventional approach using the probabilistic reasoning and allows to include different prior knowledge about the signal structure . This paper presents two Bayesian compressive sensing methods for autonomous object detection in a video sequence from a static camera . Their performance is compared on the real datasets with the non-Bayesian greedy algorithm . It is shown that the Bayesian methods can provide the same accuracy as the greedy algorithm but much faster ; or if the computational time is not critical they can provide more accurate results .
The paper proposes some robust estimators of the finite population mean . Such estimators are particularly suitable in the presence of some outlying observations . Included as special cases of our general result are robust versions of the ratio estimator and the Horvitz-Thompson estimator . The robust estimators are derived on the basis of certain predictive influence functions .
In this article , we focus on the problem of testing the equality of several high dimensional mean vectors with unequal covariance matrices . This is one of the most important problem in multivariate statistical analysis and there have been various tests proposed in the literature . Motivated by \citet{BaiS00E} and \cite{ChenQ00T} , a test statistic is introduced and the asymptomatic distributions under the null hypothesis as well as the alternative hypothesis are given . In addition , it is compared with a test statistic recently proposed by \cite{SrivastavaK00Ta} . It is shown that our test statistic performs much better especially in the large dimensional case .
In this paper we derive inferential results for a new index of inequality , specifically defined for capturing significant changes observed both in the left and in the right tail of the income distributions . The latter shifts are an apparent fact for many countries like US , Germany , UK , and France in the last decades , and are a concern for many policy makers . We propose two empirical estimators for the index , and show that they are asymptotically equivalent . Afterwards , we adopt one estimator and prove its consistency and asymptotic normality . Finally we introduce an empirical estimator for its variance and provide conditions to show its convergence to the finite theoretical value . An analysis of real data on net income from the Bank of Italy Survey of Income and Wealth is also presented , on the base of the obtained inferential results .
Learning and memory in the brain are implemented by complex , time-varying changes in neural circuitry . The computational rules according to which synaptic weights change over time are the subject of much research , and are not precisely understood . Until recently , limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale . However , as such data become available and these barriers are lifted , it becomes necessary to develop analysis techniques to validate plasticity models . Here , we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons . We treat synaptic weights as a ( potentially nonlinear ) dynamical system embedded in a fully-Bayesian generalized linear model ( GLM ) . In addition , we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the GLM and of the learning rules . Using this method , we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity ( STDP ) rule , where nonlinear effects play a substantial role . On synthetic data generated from the biophysical simulator NEURON , we show that we can recover the weight trajectories , the pattern of connectivity , and the underlying learning rules .
We study a new family of random variables , that each arise as the distribution of the maximum or minimum of a random number $N$ of i . i . d . ~random variables $X_0 , X_0 , \ldots , X_N$ , each distributed as a variable $X$ with support on $[0 , 0]$ . The general scheme is first outlined , and several special cases are studied in detail . Wherever appropriate , we find estimates of the parameter $\theta$ in the one-parameter family in question .
Urban intersections represent a complex environment for autonomous vehicles with many sources of uncertainty . The vehicle must plan in a stochastic environment with potentially rapid changes in driver behavior . Providing an efficient strategy to navigate through urban intersections is a difficult task . This paper frames the problem of navigating unsignalized intersections as a partially observable Markov decision process ( POMDP ) and solves it using a Monte Carlo sampling method . Empirical results in simulation show that the resulting policy outperforms a threshold-based heuristic strategy on several relevant metrics that measure both safety and efficiency .
Continuous Time Markov Chains ( CTMC ) have been used extensively to model reliability of storage systems . While the exponentially distributed sojourn time of Markov models is widely known to be unrealistic ( and it is necessary to consider Weibull-type models for components such as disks ) , recent work has also highlighted some additional infirmities with the CTMC model , such as the ability to handle repair times . Due to the memoryless property of these models , any failure or repair of one component resets the " clock " to zero with any partial repair or aging in some other subsystem forgotten . It has therefore been argued that simulation is the only accurate technique available for modelling the reliability of a storage system with multiple components . We show how both the above problematic aspects can be handled when we consider a careful set of approximations in a detailed model of the system . A detailed model has many states , and the transitions between them and the current state captures the " memory " of the various components . We model a non-exponential distribution using a sum of exponential distributions , along with the use of a CTMC solver in a probabilistic model checking tool that has support for reducing large state spaces . Furthermore , it is possible to get results close to what is obtained through simulation and at much lower cost .
Not all instances in a data set are equally beneficial for inducing a model of the data . Some instances ( such as outliers or noise ) can be detrimental . However , at least initially , the instances in a data set are generally considered equally in machine learning algorithms . Many current approaches for handling noisy and detrimental instances make a binary decision about whether an instance is detrimental or not . In this paper , we 0 ) extend this paradigm by weighting the instances on a continuous scale and 0 ) present a methodology for measuring how detrimental an instance may be for inducing a model of the data . We call our method of identifying and weighting detrimental instances reduced detrimental instance learning ( RDIL ) . We examine RIDL on a set of 00 data sets and 0 learning algorithms and compare RIDL with other weighting and filtering approaches . RDIL is especially useful for learning algorithms where every instance can affect the classification boundary and the training instances are considered individually , such as multilayer perceptrons trained with backpropagation ( MLPs ) . Our results also suggest that a more accurate estimate of which instances are detrimental can have a significant positive impact for handling them .
Let $Y\in\R^n$ be a random vector with mean $s$ and covariance matrix $\sigma^0P_n\tra{P_n}$ where $P_n$ is some known $n\times n$-matrix . We construct a statistical procedure to estimate $s$ as well as under moment condition on $Y$ or Gaussian hypothesis . Both cases are developed for known or unknown $\sigma^0$ . Our approach is free from any prior assumption on $s$ and is based on non-asymptotic model selection methods . Given some linear spaces collection $\{S_m , \ m\in\M\}$ , we consider , for any $m\in\M$ , the least-squares estimator $\hat{s}_m$ of $s$ in $S_m$ . Considering a penalty function that is not linear in the dimensions of the $S_m$ ' s , we select some $\hat{m}\in\M$ in order to get an estimator $\hat{s}_{\hat{m}}$ with a quadratic risk as close as possible to the minimal one among the risks of the $\hat{s}_m$ ' s . Non-asymptotic oracle-type inequalities and minimax convergence rates are proved for $\hat{s}_{\hat{m}}$ . A special attention is given to the estimation of a non-parametric component in additive models . Finally , we carry out a simulation study in order to illustrate the performances of our estimators in practice .
A fundamental problem in wireless sensor networks is to connect a given set of sensors while minimizing the \emph{receiver interference} . This is modeled as follows : each sensor node corresponds to a point in $\mathbb{R}^d$ and each \emph{transmission range} corresponds to a ball . The receiver interference of a sensor node is defined as the number of transmission ranges it lies in . Our goal is to choose transmission radii that minimize the maximum interference while maintaining a strongly connected asymmetric communication graph . For the two-dimensional case , we show that it is NP-complete to decide whether one can achieve a receiver interference of at most $0$ . In the one-dimensional case , we prove that there are optimal solutions with nontrivial structural properties . These properties can be exploited to obtain an exact algorithm that runs in quasi-polynomial time . This generalizes a result by Tan et al . to the asymmetric case .
This paper comments on " Asynchronous Logic Implementation Based on Factorized DIMS " [Jour . of Circuits , Systems , and Computers , vol . 00 , no . 0 , 0000000 , pages 0 , May 0000] with respect to two problematic issues : i ) the gate orphan problem implicit in the factorized DIMS approach discussed in the referenced article which affects the strong-indication , and ii ) how the enumeration of product terms to represent the synthesis cost is skewed in the referenced article because the logic expression contains sum of products and also product of sums . It is observed that the referenced article has not provided a general logic synthesis algorithm excepting only an example illustration involving a 0-input AND gate function . The absence of a general logic synthesis algorithm would make it difficult to reproduce the research described in the referenced article . Moreover , the example illustration in the referenced article describes an unsafe logic decomposition which is not suitable for strong-indication asynchronous circuit synthesis . Further , a logic synthesis method which safely decomposes the DIMS solution to synthesize strong-indication asynchronous circuits is already available in the existing literature , which was neither cited nor taken up for comparison in the referenced article , which is another drawback . Subsequently , it is concluded that the referenced article has not advanced existing knowledge in the field but has caused confusions . Hence , in the interest of avid readers , this paper additionally highlights some important and relevant literature which provide valuable information about robust asynchronous circuit synthesis techniques which employ delay-insensitive codes for data representation and processing and the 0-phase return-to-zero handshake protocol for data communication .
The emergence of Web 0 . 0 and simultaneously Library 0 . 0 platforms has helped the library and information professionals to outreach to new audiences beyond their physical boundaries . In a globalized society , information becomes very useful resource for socio-economic empowerment of marginalized communities , economic prosperity of common citizens , and knowledge enrichment of liberated minds . Scholarly information becomes both developmental and functional for researchers working towards advancement of knowledge . We must recognize a relay of information flow and information ecology while pursuing scholarly research . Published scholarly literatures we consult that help us in creation of new knowledge . Similarly , our published scholarly works should be outreached to future researchers for regeneration of next dimension of knowledge . Fortunately , present day research communicators have many freely available personalized digital tools to outreach to globalized research audiences having similar research interests . These tools and techniques , already adopted by many researchers in different subject areas across the world , should be enthusiastically utilized by LIS researchers in South Asia for global dissemination of their scholarly research works . This newly found enthusiasm will soon become integral part of the positive habits and cultural practices of research communicators in LIS domain .
This paper presents a new approach to select events of interest to a user in a social media setting where events are generated by the activities of the user ' s friends through their mobile devices . We argue that given the unique requirements of the social media setting , the problem is best viewed as an inductive learning problem , where the goal is to first generalize from the users ' expressed " likes " and " dislikes " of specific events , then to produce a program that can be manipulated by the system and distributed to the collection devices to collect only data of interest . The key contribution of this paper is a new algorithm that combines existing machine learning techniques with new program synthesis technology to learn users ' preferences . We show that when compared with the more standard approaches , our new algorithm provides up to order-of-magnitude reductions in model training time , and significantly higher prediction accuracies for our target application . The approach also improves on standard machine learning techniques in that it produces clear programs that can be manipulated to optimize data collection and filtering .
Rapid increase in the volume of sentiment rich social media on the web has resulted in an increased interest among researchers regarding Sentimental Analysis and opinion mining . However , with so much social media available on the web , sentiment analysis is now considered as a big data task . Hence the conventional sentiment analysis approaches fails to efficiently handle the vast amount of sentiment data available now a days . The main focus of the research was to find such a technique that can efficiently perform sentiment analysis on big data sets . A technique that can categorize the text as positive , negative and neutral in a fast and accurate manner . In the research , sentiment analysis was performed on a large data set of tweets using Hadoop and the performance of the technique was measured in form of speed and accuracy . The experimental results shows that the technique exhibits very good efficiency in handling big sentiment data sets .
This research aims to mine the relationship between demographic variables and brand associations , and study the relative importance of these variables . The study is conducted on fast-food restaurant brands chains in Jordan . The result ranks and evaluates the demographic variables in relation with the brand associations for the selected sample . Discovering brand associations according to demographic variables reveals many facts and linkages in the context of Jordanian culture . Suggestions are given accordingly for marketers to benefits from to build their strategies and direct their decisions . Also , data mining technique used in this study reflects a new trend for studying and analyzing marketing samples .
Separation Logic with inductive definitions is a well-known approach for deductive verification of programs that manipulate dynamic data structures . Deciding verification conditions in this context is usually based on user-provided lemmas relating the inductive definitions . We propose a novel approach for generating these lemmas automatically which is based on simple syntactic criteria and deterministic strategies for applying them . Our approach focuses on iterative programs , although it can be applied to recursive programs as well , and specifications that describe not only the shape of the data structures , but also their content or their size . Empirically , we find that our approach is powerful enough to deal with sophisticated benchmarks , e . g . , iterative procedures for searching , inserting , or deleting elements in sorted lists , binary search tress , red-black trees , and AVL trees , in a very efficient way .
Content based Document Classification is one of the biggest challenges in the context of free text mining . Current algorithms on document classifications mostly rely on cluster analysis based on bag-of-words approach . However that method is still being applied to many modern scientific dilemmas . It has established a strong presence in fields like economics and social science to merit serious attention from the researchers . In this paper we would like to propose and explore an alternative grounded more securely on the dictionary classification and correlatedness of words and phrases . It is expected that application of our existing knowledge about the underlying classification structure may lead to improvement of the classifier ' s performance .
Modeling correlation ( and covariance ) matrices is a challenging problem due to the large dimensionality and positive-definiteness constraint . In this paper , we propose a novel Bayesian framework based on modeling the correlations as products of vectors on unit spheres . The covariance matrix is then modeled by using its decomposition into correlation and variance matrices . This approach allows us to induce flexible prior distributions for covariance matrices ( that go beyond the commonly used inverse-Wishart prior ) by specifying a wide range of distributions on spheres ( e . g . the squared-Dirichlet distribution ) . For modeling real-life spatio-temporal processes with complex dependence structures , we extend our method to dynamic cases and introduce unit-vector Gaussian process priors in order to capture the evolution of correlation among multiple time series . To handle the intractability of the resulting posterior , we introduce the adaptive $\Delta$-Spherical Hamiltonian Monte Carlo . Using an example of Normal-Inverse-Wishart problem , a simulated periodic process , and an analysis of local field potential data ( collected from the hippocampus of rats performing a complex sequence memory task ) , we demonstrate the validity and effectiveness of our proposed framework for ( dynamic ) modeling covariance and correlation matrices .
Understanding the functional architecture of the brain in terms of networks is becoming increasingly common . In most fMRI applications functional networks are assumed to be stationary , resulting in a single network estimated for the entire time course . However recent results suggest that the connectivity between brain regions is highly non-stationary even at rest . As a result , there is a need for new brain imaging methodologies that comprehensively account for the dynamic ( i . e . , non-stationary ) nature of the fMRI data . In this work we propose the Smooth Incremental Graphical Lasso Estimation ( SINGLE ) algorithm which estimates dynamic brain networks from fMRI data . We apply the SINGLE algorithm to functional MRI data from 00 healthy patients performing a choice-response task to demonstrate the dynamic changes in network structure that accompany a simple but attentionally demanding cognitive task . Using graph theoretic measures we show that the Right Inferior Frontal Gyrus , frequently reported as playing an important role in cognitive control , dynamically changes with the task . Our results suggest that the Right Inferior Frontal Gyrus plays a fundamental role in the attention and executive function during cognitively demanding tasks and may play a key role in regulating the balance between other brain regions .
This work presents GROUSE ( Grassmanian Rank-One Update Subspace Estimation ) , an efficient online algorithm for tracking subspaces from highly incomplete observations . GROUSE requires only basic linear algebraic manipulations at each iteration , and each subspace update can be performed in linear time in the dimension of the subspace . The algorithm is derived by analyzing incremental gradient descent on the Grassmannian manifold of subspaces . With a slight modification , GROUSE can also be used as an online incremental algorithm for the matrix completion problem of imputing missing entries of a low-rank matrix . GROUSE performs exceptionally well in practice both in tracking subspaces and as an online algorithm for matrix completion .
Survey statisticians make use of the available auxiliary information to improve estimates . One important example is given by calibration estimation , that seeks for new weights that are close ( in some sense ) to the basic design weights and that , at the same time , match benchmark constraints on available auxiliary information . Recently , multiple frame surveys have gained much attention and became largely used by statistical agencies and private organizations to decrease sampling costs or to reduce frame undercoverage errors that could occur with the use of only a single sampling frame . Much attention has been devoted to the introduction of different ways of combining estimates coming from the different frames . We will extend the calibration paradigm , developed so far for one frame surveys , to the estimation of the total of a variable of interest in dual frame surveys as a general tool to include auxiliary information , also available at different levels . In fact , calibration allows us to handle different types of auxiliary information and can be shown to encompass as a special cases some of the methods already proposed in the literature . The theoretical properties of the proposed class of estimators are derived and discussed , a set of simulation studies is conducted to compare the efficiency of the procedure in presence of different sets of auxiliary variables . Finally , the proposed methodology is applied to data from the Barometer of Culture of Andalusia survey .
Background . This paper study statistical data gathered from wind turbines located on the territory of the Republic of Poland . The research is aimed to construct the stochastic model that predicts the change of wind speed with time . Purpose . The purpose of this work is to find the optimal distribution for the approximation of available statistical data on wind speed . Methods . We consider four distributions of a random variable : Log-Normal , Weibull , Gamma and Beta . In order to evaluate the parameters of distributions we use method of maximum likelihood . To assess the the results of approximation we use a quantile-quantile plot . Results . All the considered distributions properly approximate the available data . The Weibull distribution shows the best results for the extreme values of the wind speed . Conclusions . The results of the analysis are consistent with the common practice of using the Weibull distribution for wind speed modeling . In the future we plan to compare the results obtained with a much larger data set as well as to build a stochastic model of the evolution of the wind speed depending on time .
Big data are often contaminated by outliers and heavy-tailed errors , which makes many conventional methods inadequate . To address this challenge , we propose the adaptive Huber regression for robust estimation and inference . The key observation is that the robustification parameter should adapt to the sample size , dimension and moments for optimal tradeoff between bias and robustness . Our framework is able to handle heavy-tailed data with bounded $ ( 0 \ ! + \ ! \delta ) $-th moment for any $\delta\ ! >\ ! 0$ . We establish a sharp phase transition for robust estimation of regression parameters in both low and high dimensions : when $\delta \ ! \geq\ ! 0$ , the estimator admits a sub-Gaussian-type deviation bound without sub-Gaussian assumptions on the data , while only a slower rate is available in the regime $0 \ ! <\ ! \delta \ ! <\ ! 0$ and the transition is smooth and optimal . Moreover , a nonasymptotic Bahadur representation for finite-sample inference is derived when the variance is finite . Numerical experiments lend further support to our obtained theory .
Ensembles of forecasts are typically employed to account for the forecast uncertainties inherent in predictions of future weather states . However , biases and dispersion errors often present in forecast ensembles require statistical post-processing . Univariate post-processing models such as Bayesian model averaging ( BMA ) have been successfully applied for various weather quantities . Nonetheless , BMA and many other standard post-processing procedures are designed for a single weather variable , thus ignoring possible dependencies among weather quantities . In line with recently upcoming research to develop multivariate post-processing procedures , e . g . , BMA for bivariate wind vectors , or flexible procedures applicable for multiple weather quantities of different types , a bivariate BMA model for joint calibration of wind speed and temperature forecasts is proposed based on the bivariate truncated normal distribution . It extends the univariate truncated normal BMA model designed for post-processing ensemble forecast of wind speed by adding a normally distributed temperature component with a covariance structure representing the dependency among the two weather quantities . The method is applied to wind speed and temperature forecasts of the eight-member University of Washington mesoscale ensemble and of the eleven-member ALADIN-HUNEPS ensemble of the Hungarian Meteorological Service and its predictive performance is compared to that of the general Gaussian copula method . The results indicate improved calibration of probabilistic and accuracy of point forecasts in comparison to the raw ensemble and the overall performance of this model is able to keep up with that of the Gaussian copula method .
Sponsored search mechanisms have drawn much attention from both academic community and industry in recent years since the seminal papers of [00] and [00] . However , most of the existing literature concentrates on the mechanism design and analysis within the scope of only one search engine in the market . In this paper we propose a mathematical framework for modeling the interaction of publishers , advertisers and end users in a competitive market . We first consider the monopoly market model and provide optimal solutions for both ex ante and ex post cases , which represents the long-term and short-term revenues of search engines respectively . We then analyze the strategic behaviors of end users and advertisers under duopoly and prove the existence of equilibrium for both search engines to co-exist from ex-post perspective . To show the more general ex ante results , we carry out extensive simulations under different parameter settings . Our analysis and observation in this work can provide useful insight in regulating the sponsored search market and protecting the interests of advertisers and end users .
Given a set $P$ of $n$ points in $\mathbb{R}^0$ , we show that , for any $\varepsilon >0$ , there exists an $\varepsilon$-net of $P$ for halfspace ranges , of size $O ( 0/\varepsilon ) $ . We give five proofs of this result , which are arguably simpler than previous proofs \cite{msw-hnlls-00 , cv-iaags-00 , pr-nepen-00} . We also consider several related variants of this result , including the case of points and pseudo-disks in the plane .
First-order logic is known to have limited expressive power over finite structures . It enjoys in particular the locality property , which states that first-order formulae cannot have a global view of a structure . This limitation ensures on their low sequential computational complexity . We show that the locality impacts as well on their distributed computational complexity . We use first-order formulae to describe the properties of finite connected graphs , which are the topology of communication networks , on which the first-order formulae are also evaluated . We show that over bounded degree networks and planar networks , first-order properties can be frugally evaluated , that is , with only a bounded number of messages , of size logarithmic in the number of nodes , sent over each link . Moreover , we show that the result carries over for the extension of first-order logic with unary counting .
Non-parametric approaches for analyzing network data based on exchangeable graph models ( ExGM ) have recently gained interest . The key object that defines an ExGM is often referred to as a graphon . This non-parametric perspective on network modeling poses challenging questions on how to make inference on the graphon underlying observed network data . In this paper , we propose a computationally efficient procedure to estimate a graphon from a set of observed networks generated from it . This procedure is based on a stochastic blockmodel approximation ( SBA ) of the graphon . We show that , by approximating the graphon with a stochastic block model , the graphon can be consistently estimated , that is , the estimation error vanishes as the size of the graph approaches infinity .
The performance of a speaker recognition system decreases when the speaker is under stress or emotion . In this paper we explore and identify a mechanism that enables use of inherent stress-in-speech or speaking style information present in speech of a person as additional cues for speaker recognition . We quantify the the inherent stress present in the speech of a speaker mainly using 0 features , namely , pitch , amplitude and duration ( together called PAD ) We experimentally observe that the PAD vectors of similar phones in different words of a speaker are close to each other in the three dimensional ( PAD ) space confirming that the way a speaker stresses different syllables in their speech is unique to them , thus we propose the use of PAD based speaking style of a speaker as an additional feature for speaker recognition applications .
Discrete random probability measures and the exchangeable random partitions they induce are key tools for addressing a variety of estimation and prediction problems in Bayesian inference . Indeed , many popular nonparametric priors , such as the Dirichlet and the Pitman-Yor process priors , select discrete probability distributions almost surely and , therefore , automatically induce exchangeable random partitions . Here we focus on the family of Gibbs-type priors , a recent and elegant generalization of the Dirichlet and the Pitman-Yor process priors . These random probability measures share properties that are appealing both from a theoretical and an applied point of view : ( i ) they admit an intuitive characterization in terms of their predictive structure justifying their use in terms of a precise assumption on the learning mechanism ; ( ii ) they stand out in terms of mathematical tractability ; ( iii ) they include several interesting special cases besides the Dirichlet and the Pitman-Yor processes . The goal of our paper is to provide a systematic and unified treatment of Gibbs-type priors and highlight their implications for Bayesian nonparametric inference . We will deal with their distributional properties , the resulting estimators , frequentist asymptotic validation and the construction of time-dependent versions . Applications , mainly concerning hierarchical mixture models and species sampling , will serve to convey the main ideas . The intuition inherent to this class of priors and the neat results that can be deduced for it lead one to wonder whether it actually represents the most natural generalization of the Dirichlet process .
This paper extends a recently proposed model for combinatorial landscapes : Local Optima Networks ( LON ) , to incorporate a first-improvement ( greedy-ascent ) hill-climbing algorithm , instead of a best-improvement ( steepest-ascent ) one , for the definition and extraction of the basins of attraction of the landscape optima . A statistical analysis comparing best and first improvement network models for a set of NK landscapes , is presented and discussed . Our results suggest structural differences between the two models with respect to both the network connectivity , and the nature of the basins of attraction . The impact of these differences in the behavior of search heuristics based on first and best improvement local search is thoroughly discussed .
Due to the increasing complexity and heterogeneity of contemporary Command , Control , Communications , Computers , & Intelligence systems at all levels within military organizations , the adoption of the Service Oriented Architectures ( SOA ) principles and concepts is becoming essential . SOA provides flexibility and interoperability of services enabling the realization of efficient and modular information infrastructure for command and control systems . However , within a tactical domain , the presence of potentially highly mobile actors equipped with constrained communications media ( i . e . , unreliable radio networks with limited bandwidth ) limits the applicability of traditional SOA technologies . The TACTICS project aims at the definition and experimental demonstration of a Tactical Services Infrastructure enabling tactical radio networks ( without any modifications of the radio part of those networks ) to participate in SOA infrastructures and provide , as well as consume , services to and from the strategic domain independently of the user ' s location .
We consider drawings of graphs that contain dense subgraphs . We introduce intersection-link representations for such graphs , in which each vertex $u$ is represented by a geometric object $R ( u ) $ and in which each edge $ ( u , v ) $ is represented by the intersection between $R ( u ) $ and $R ( v ) $ if it belongs to a dense subgraph or by a curve connecting the boundaries of $R ( u ) $ and $R ( v ) $ otherwise . We study a notion of planarity , called Clique Planarity , for intersection-link representations of graphs in which the dense subgraphs are cliques .
Due to limited metering infrastructure , distribution grids are currently challenged by observability issues . On the other hand , smart meter data , including local voltage magnitudes and power injections , are communicated to the utility operator from grid buses with renewable generation and demand-response programs . This work employs grid data from metered buses towards inferring the underlying grid state . To this end , a coupled formulation of the power flow problem ( CPF ) is put forth . Exploiting the high variability of injections at metered buses , the controllability of solar inverters , and the relative time-invariance of conventional loads , the idea is to solve the non-linear power flow equations jointly over consecutive time instants . An intuitive and easily verifiable rule pertaining to the locations of metered and non-metered buses on the physical grid is shown to be a necessary and sufficient criterion for local observability in radial networks . To account for noisy smart meter readings , a coupled power system state estimation ( CPSSE ) problem is further developed . Both CPF and CPSSE tasks are tackled via augmented semi-definite program relaxations . The observability criterion along with the CPF and CPSSE solvers are numerically corroborated using synthetic and actual solar generation and load data on the IEEE 00-bus benchmark feeder .
Novel technologies in genomics allow creating data in exascale dimension with relatively minor effort of human and laboratory and thus monetary resources compared to capabilities only a decade ago . While the availability of this data salvage to find answers for research questions , which would not have been feasible before , maybe even not feasible to ask before , the amount of data creates new challenges , which obviously need new software and data management systems . Such new solutions have to consider integrative approaches , which are not only considering the effectiveness and efficiency of data processing but improve reusability , reproducibility and usability especially tailored to the target user communities of genomic big data . In our opinion , current solutions tackle part of the challenges and have each their strengths but lack to provide a complete solution . We present in this paper the key challenges and the characteristics cutting-edge developments should possess for fulfilling the needs of the user communities to allow for seamless sharing and data analysis on a large scale .
Gaussian graphical models are widely used to represent conditional dependence among random variables . In this paper , we propose a novel estimator for data arising from a group of Gaussian graphical models that are themselves dependent . A motivating example is that of modeling gene expression collected on multiple tissues from the same individual : here the multivariate outcome is affected by dependencies acting not only at the level of the specific tissues , but also at the level of the whole body ; existing methods that assume independence among graphs are not applicable in this case . To estimate multiple dependent graphs , we decompose the problem into two graphical layers : the systemic layer , which affects all outcomes and thereby induces cross- graph dependence , and the category-specific layer , which represents graph-specific variation . We propose a graphical EM technique that estimates both layers jointly , establish estimation consistency and selection sparsistency of the proposed estimator , and confirm by simulation that the EM method is superior to a simple one-step method . We apply our technique to mouse genomics data and obtain biologically plausible results .
The recent proliferation of increasingly capable mobile devices has given rise to mobile crowd sensing ( MCS ) systems that outsource the collection of sensory data to a crowd of participating workers that carry various mobile devices . Aware of the paramount importance of effectively incentivizing participation in such systems , the research community has proposed a wide variety of incentive mechanisms . However , different from most of these existing mechanisms which assume the existence of only one data requester , we consider MCS systems with multiple data requesters , which are actually more common in practice . Specifically , our incentive mechanism is based on double auction , and is able to stimulate the participation of both data requesters and workers . In real practice , the incentive mechanism is typically not an isolated module , but interacts with the data aggregation mechanism that aggregates workers ' data . For this reason , we propose CENTURION , a novel integrated framework for multi-requester MCS systems , consisting of the aforementioned incentive and data aggregation mechanism . CENTURION ' s incentive mechanism satisfies truthfulness , individual rationality , computational efficiency , as well as guaranteeing non-negative social welfare , and its data aggregation mechanism generates highly accurate aggregated results . The desirable properties of CENTURION are validated through both theoretical analysis and extensive simulations .
We study a class of games in which a finite number of agents each controls a quantity of flow to be routed through a network , and are able to split their own flow between multiple paths through the network . Recent work on this model has contrasted the social cost of Nash equilibria with the best possible social cost . Here we show that additional costs are incurred in situations where a selfish ``leader ' ' agent allocates his flow , and then commits to that choice so that other agents are compelled to minimise their own cost based on the first agent ' s choice . We find that even in simple networks , the leader can often improve his own cost at the expense of increased social cost . Focusing on the 0-player case , we give upper and lower bounds on the worst-case additional cost incurred .
Many man-made objects have intrinsic symmetries and Manhattan structure . By assuming an orthographic projection model , this paper addresses the estimation of 0D structures and camera projection using symmetry and/or Manhattan structure cues , which occur when the input is single- or multiple-image from the same category , e . g . , multiple different cars . Specifically , analysis on the single image case implies that Manhattan alone is sufficient to recover the camera projection , and then the 0D structure can be reconstructed uniquely exploiting symmetry . However , Manhattan structure can be difficult to observe from a single image due to occlusion . To this end , we extend to the multiple-image case which can also exploit symmetry but does not require Manhattan axes . We propose a novel rigid structure from motion method , exploiting symmetry and using multiple images from the same category as input . Experimental results on the Pascal0D+ dataset show that our method significantly outperforms baseline methods .
By lifting the ReLU function into a higher dimensional space , we develop a smooth multi-convex formulation for training feed-forward deep neural networks ( DNNs ) . This allows us to develop a block coordinate descent ( BCD ) training algorithm consisting of a sequence of numerically well-behaved convex optimizations . Using ideas from proximal point methods in convex analysis , we prove that this BCD algorithm will converge globally to a stationary point with R-linear convergence rate of order one . In experiments with the MNIST database , DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures trained via all the stochastic gradient descent ( SGD ) variants in the Caffe toolbox .
We prove that the inverse of a positive-definite matrix can be approximated by a weighted-sum of a small number of matrix exponentials . Combining this with a previous result [OSV00] , we establish an equivalence between matrix inversion and exponentiation up to polylogarithmic factors . In particular , this connection justifies the use of Laplacian solvers for designing fast semi-definite programming based algorithms for certain graph problems . The proof relies on the Euler-Maclaurin formula and certain bounds derived from the Riemann zeta function .
In this article , we consider Markov chain Monte Carlo ( MCMC ) algorithms for exploring the intractable posterior densities associated with Bayesian probit linear mixed models under both proper and improper priors on the regression coefficients and variance components . In particular , we construct two-block Gibbs samplers using the data augmentation ( DA ) techniques . Furthermore , we prove the geometric ergodicity of the Gibbs samplers , which is the foundation for building the central limit theorems for MCMC based estimators and subsequent inferences . Under proper priors , the Gibbs sampler chain is practically always geometrically ergodic . While under improper priors , the conditions for geometric convergence are similar to those guaranteeing posterior propriety . We also provide conditions for posterior propriety when the design matrices take commonly observed forms . The Haar parameter expansion for DA ( PX-DA ) algorithm is an improvement of the DA algorithm and it has been shown that it is theoretically at least as good as the DA algorithm . We propose corresponding Haar PX-DA algorithms , which have essentially the same computational cost as the two-block Gibbs samplers . An example is used to show the efficiency gain of the Haar PX-DA algorithm over the block Gibbs sampler .
Through the use of a system-building approach , an approach that includes finding common ground for the various philosophical paradigms within statistics , Erich L . Lehmann is responsible for much of the synthesis of classical statistical knowledge that developed from the Neyman--Pearson--Wald school . A biographical sketch and a brief summary of some of his many contributions are presented here . His complete bibliography is also included and the references present many other sources of information on his life and his work .
A method is proposed for estimating the potential function of a non-parametric estimator for stationary and isotropic pairwise interaction point process . The relation between a pair potential and the corresponding Papangelou conditional intensity is considered . Consistency and strong consistency of non-parametric estimate are proved in case of finite-range interaction potential .
Most businesses these days use the web services technology as a medium to allow interaction between a service provider and a service requestor . However , both the service provider and the requestor would be unable to achieve their business goals when there are miscommunications between their processes . This research focuses on the process incompatibility between the web services and the way to automatically resolve them by using a process mediator . This paper presents an overview of the behavioral incompatibility between web services and the overview of process mediation in order to resolve the complications faced due to the incompatibility . Several state-of the-art approaches have been selected and analyzed to understand the existing process mediation components . This paper aims to provide a valuable gap analysis that identifies the important research areas in process mediation that have yet to be fully explored .
We propose a new technique , Spectral Contextualization , to study political engagement on Facebook during the 0000 French presidential election . In particular , we examine the Facebook posts of the eight leading candidates and the comments beneath these posts . We find evidence of both ( i ) candidate-centered structure , where citizens primarily comment on the wall of one candidate and ( ii ) issue-centered structure ( i . e . on political topics ) , where citizens ' attention and expression is primarily directed towards a specific set of issues ( e . g . economics , immigration , etc ) . To discover issue-centered structure , we develop Spectral Contextualization , a novel approach to analyze a network with high-dimensional node covariates . This technique scales to hundreds of thousands of nodes and thousands of covariates . In the Facebook data , spectral clustering without any contextualizing information finds a mixture of ( i ) candidate and ( ii ) issue clusters . The contextualizing information with text data helps to separate these two structures . We conclude by showing that the novel methodology is consistent under a statistical model .
Collaborative filtering ( CF ) aims to predict users ' ratings on items according to historical user-item preference data . In many real-world applications , preference data are usually sparse , which would make models overfit and fail to give accurate predictions . Recently , several research works show that by transferring knowledge from some manually selected source domains , the data sparseness problem could be mitigated . However for most cases , parts of source domain data are not consistent with the observations in the target domain , which may misguide the target domain model building . In this paper , we propose a novel criterion based on empirical prediction error and its variance to better capture the consistency across domains in CF settings . Consequently , we embed this criterion into a boosting framework to perform selective knowledge transfer . Comparing to several state-of-the-art methods , we show that our proposed selective transfer learning framework can significantly improve the accuracy of rating prediction tasks on several real-world recommendation tasks .
In reinforcement learning , the TD ( $\lambda$ ) algorithm is a fundamental policy evaluation method with an efficient online implementation that is suitable for large-scale problems . One practical drawback of TD ( $\lambda$ ) is its sensitivity to the choice of the step-size . It is an empirically well-known fact that a large step-size leads to fast convergence , at the cost of higher variance and risk of instability . In this work , we introduce the implicit TD ( $\lambda$ ) algorithm which has the same function and computational cost as TD ( $\lambda$ ) , but is significantly more stable . We provide a theoretical explanation of this stability and an empirical evaluation of implicit TD ( $\lambda$ ) on typical benchmark tasks . Our results show that implicit TD ( $\lambda$ ) outperforms standard TD ( $\lambda$ ) and a state-of-the-art method that automatically tunes the step-size , and thus shows promise for wide applicability .
HTTP Adaptive Streaming ( HAS ) techniques are now the dominant solution for video delivery in mobile networks . Over the past few years , several HAS algorithms have been introduced in order to improve user quality-of-experience ( QoE ) by bit-rate adaptation . Their difference is mainly the required input information , ranging from network characteristics to application-layer parameters such as the playback buffer . Interestingly , despite the recent outburst in scientific papers on the topic , a comprehensive comparative study of the main algorithm classes is still missing . In this paper we provide such comparison by evaluating the performance of the state-of-the-art HAS algorithms per class , based on data from field measurements . We provide a systematic study of the main QoE factors and the impact of the target buffer level . We conclude that this target buffer level is a critical classifier for the studied HAS algorithms . While buffer-based algorithms show superior QoE in most of the cases , their performance may differ at the low target buffer levels of live streaming services . Overall , we believe that our findings provide valuable insight for the design and choice of HAS algorithms according to networks conditions and service requirements .
Advances in neural network based classifiers have transformed automatic feature learning from a pipe dream of stronger AI to a routine and expected property of practical systems . Since the emergence of AlexNet every winning submission of the ImageNet challenge has employed end-to-end representation learning , and due to the utility of good representations for transfer learning , representation learning has become as an important and distinct task from supervised learning . At present , this distinction is inconsequential , as supervised methods are state-of-the-art in learning transferable representations . But recent work has shown that generative models can also be powerful agents of representation learning . Will the representations learned from these generative methods ever rival the quality of those from their supervised competitors ? In this work , we argue in the affirmative , that from an information theoretic perspective , generative models have greater potential for representation learning . Based on several experimentally validated assumptions , we show that supervised learning is upper bounded in its capacity for representation learning in ways that certain generative models , such as Generative Adversarial Networks ( GANs ) are not . We hope that our analysis will provide a rigorous motivation for further exploration of generative representation learning .
When plaintiffs prevail in a discrimination case , a major component of the calculation of economic loss is the length of time they would have been in the higher position had they been treated fairly during the period in which the employer practiced discrimination . This problem is complicated by the fact that one ' s eligibility for promotion is subject to termination by retirement and both the promotion and retirement processes may be affected by discriminatory practices . This semi-competing risk setup is decomposed into a retirement process and a promotion process among the employees . Predictions for the purpose of compensation are made by utilizing the expected promotion and retirement probabilities of similarly qualified members of the nondiscriminated group . The restricted mean durations of three periods are estimated - the time an employee would be at the lower position , at the higher level and in retirement . The asymptotic properties of the estimators are presented and examined through simulation studies . The proposed restricted mean job duration estimators are shown to be robust in the presence of an independent frailty term . Data from the reverse discrimination case , Alexander v . Milwaukee , where White-male lieutenants were discriminated in promotion to captain are reanalyzed . While the appellate court upheld liability , it reversed the original damage calculations , which heavily depended on the time a plaintiff would have been in each position . The results obtained by the proposed method are compared to those made at the first trial . Substantial differences in both directions are observed .
Standard multivariate analysis methods aim to identify and summarize the main structures in large data sets containing the description of a number of observations by several variables . In many cases , spatial information is also available for each observation , so that a map can be associated to the multivariate data set . Two main objectives are relevant in the analysis of spatial multivariate data : summarizing covariation structures and identifying spatial patterns . In practice , achieving both goals simultaneously is a statistical challenge , and a range of methods have been developed that offer trade-offs between these two objectives . In an applied context , this methodological question has been and remains a major issue in community ecology , where species assemblages ( i . e . , covariation between species abundances ) are often driven by spatial processes ( and thus exhibit spatial patterns ) . In this paper we review a variety of methods developed in community ecology to investigate multivariate spatial patterns . We present different ways of incorporating spatial constraints in multivariate analysis and illustrate these different approaches using the famous data set on moral statistics in France published by Andr\ ' {e}-Michel Guerry in 0000 . We discuss and compare the properties of these different approaches both from a practical and theoretical viewpoint .
There has been a lot of work fitting Ising models to multivariate binary data in order to understand the conditional dependency relationships between the variables . However , additional covariates are frequently recorded together with the binary data , and may influence the dependence relationships . Motivated by such a dataset on genomic instability collected from tumor samples of several types , we propose a sparse covariate dependent Ising model to study both the conditional dependency within the binary data and its relationship with the additional covariates . This results in subject-specific Ising models , where the subject ' s covariates influence the strength of association between the genes . As in all exploratory data analysis , interpretability of results is important , and we use L0 penalties to induce sparsity in the fitted graphs and in the number of selected covariates . Two algorithms to fit the model are proposed and compared on a set of simulated data , and asymptotic results are established . The results on the tumor dataset and their biological significance are discussed in detail .
Non-reversible Markov chain Monte Carlo schemes based on piecewise deterministic Markov processes have been recently introduced in applied probability , automatic control , physics and statistics . Although these algorithms demonstrate experimentally good performance and are accordingly increasingly used in a wide range of applications , geometric ergodicity results for such schemes have only been established so far under very restrictive assumptions . We give here verifiable conditions on the target distribution under which the Bouncy Particle Sampler algorithm introduced in \cite{P_dW_00} is geometrically ergodic . This holds whenever the target satisfies a curvature condition and has tails decaying at least as fast as an exponential and at most as fast as a Gaussian distribution . This allows us to provide a central limit theorem for the associated ergodic averages . When the target has tails thinner than a Gaussian distribution , we propose an original modification of this scheme that is geometrically ergodic . For thick-tailed target distributions , such as $t$-distributions , we extend the idea pioneered in \cite{J_G_00} in a random walk Metropolis context . We apply a change of variable to obtain a transformed target satisfying the tail conditions for geometric ergodicity . By sampling the transformed target using the Bouncy Particle Sampler and mapping back the Markov process to the original parameterization , we obtain a geometrically ergodic algorithm .
Let y=A\beta+\epsilon , where y is an N\times0 vector of observations , \beta is a p\times0 vector of unknown regression coefficients , A is an N\times p design matrix and \epsilon is a spherically symmetric error term with unknown scale parameter \sigma . We consider estimation of \beta under general quadratic loss functions , and , in particular , extend the work of Strawderman [J . Amer . Statist . Assoc . 00 ( 0000 ) 000-000] and Casella [Ann . Statist . 0 ( 0000 ) 0000-0000 , J . Amer . Statist . Assoc . 00 ( 0000 ) 000-000] by finding adaptive minimax estimators ( which are , under the normality assumption , also generalized Bayes ) of \beta , which have greater numerical stability ( i . e . , smaller condition number ) than the usual least squares estimator . In particular , we give a subclass of such estimators which , surprisingly , has a very simple form . We also show that under certain conditions the generalized Bayes minimax estimators in the normal case are also generalized Bayes and minimax in the general case of spherically symmetric errors .
Simple , short , and compact hashtags cover a wide range of information on social networks . Although many works in the field of natural language processing ( NLP ) have demonstrated the importance of hashtag recommendation , hashtag recommendation for images has barely been studied . In this paper , we introduce the HARRISON dataset , a benchmark on hashtag recommendation for real world images in social networks . The HARRISON dataset is a realistic dataset , composed of 00 , 000 photos from Instagram and an average of 0 . 0 associated hashtags for each photo . To evaluate our dataset , we design a baseline framework consisting of visual feature extractor based on convolutional neural network ( CNN ) and multi-label classifier based on neural network . Based on this framework , two single feature-based models , object-based and scene-based model , and an integrated model of them are evaluated on the HARRISON dataset . Our dataset shows that hashtag recommendation task requires a wide and contextual understanding of the situation conveyed in the image . As far as we know , this work is the first vision-only attempt at hashtag recommendation for real world images in social networks . We expect this benchmark to accelerate the advancement of hashtag recommendation .
The present work focuses on geometrically exact finite elements for highly slender beams . It aims at the proposal of novel formulations of Kirchhoff-Love type , a detailed review of existing formulations of Kirchhoff-Love and Simo-Reissner type as well as a careful evaluation and comparison of the proposed and existing formulations . Two different rotation interpolation schemes with strong or weak Kirchhoff constraint enforcement , respectively , as well as two different choices of nodal triad parametrizations in terms of rotation or tangent vectors are proposed . The combination of these schemes leads to four novel finite element variants , all of them based on a C0-continuous Hermite interpolation of the beam centerline . Essential requirements such as representability of general 0D , large-deformation , dynamic problems involving slender beams with arbitrary initial curvatures and anisotropic cross-section shapes or preservation of objectivity and path-independence will be investigated analytically and verified numerically for the different formulations . It will be shown that the geometrically exact Kirchhoff-Love beam elements proposed in this work are the first ones of this type that fulfill all the considered requirements . On the contrary , Simo-Reissner type formulations fulfilling these requirements can be found in the literature very well . However , it will be argued that the shear-free Kirchhoff-Love formulations can provide considerable numerical advantages when applied to highly slender beams . Concretely , several representative numerical test cases confirm that the proposed Kirchhoff-Love formulations exhibit a lower discretization error level as well as a considerably improved nonlinear solver performance in the range of high beam slenderness ratios as compared to two representative Simo-Reissner element formulations from the literature .
Motivated by the fundamental problem of measuring species diversity , this paper introduces the concept of a cluster structure to define an exchangeable cluster probability function that governs the joint distribution of a random count and its exchangeable random partitions . A cluster structure , naturally arising from a completely random measure mixed Poisson process , allows the probability distribution of the random partitions of a subset of a sample to be dependent on the sample size , a distinct and motivated feature that differs it from a partition structure . A generalized negative binomial process model is proposed to generate a cluster structure , where in the prior the number of clusters is finite and Poisson distributed , and the cluster sizes follow a truncated negative binomial distribution . We construct a nonparametric Bayesian estimator of Simpson ' s index of diversity under the generalized negative binomial process . We illustrate our results through the analysis of two real sequencing count datasets .
This paper aims to review the fiercely discussed question of whether the ranking of Wikipedia articles in search engines is justified by the quality of the articles . After an overview of current research on information quality in Wikipedia , a summary of the extended discussion on the quality of encyclopedic entries in general is given . On this basis , a heuristic method for evaluating Wikipedia entries is developed and applied to Wikipedia articles that scored highly in a search engine retrieval effectiveness test and compared with the relevance judgment of jurors . In all search engines tested , Wikipedia results are unanimously judged better by the jurors than other results on the corresponding results position . Relevance judgments often roughly correspond with the results from the heuristic evaluation . Cases in which high relevance judgments are not in accordance with the comparatively low score from the heuristic evaluation are interpreted as an indicator of a high degree of trust in Wikipedia . One of the systemic shortcomings of Wikipedia lies in its necessarily incoherent user model . A further tuning of the suggested criteria catalogue , for instance the different weighing of the supplied criteria , could serve as a starting point for a user model differentiated evaluation of Wikipedia articles . Approved methods of quality evaluation of reference works are applied to Wikipedia articles and integrated with the question of search engine evaluation .
We enlarge the number of available functional depths by introducing the kernelized functional spatial depth ( KFSD ) . KFSD is a local-oriented and kernel-based version of the recently proposed functional spatial depth ( FSD ) that may be useful for studying functional samples that require an analysis at a local level . In addition , we consider supervised functional classification problems , focusing on cases in which the differences between groups are not extremely clear-cut or the data may contain outlying curves . We perform classification by means of some available robust methods that involve the use of a given functional depth , including FSD and KFSD , among others . We use the functional \textit{k}-nearest neighbor classifier as a benchmark procedure . The results of a simulation study indicate that the KFSD-based classification approach leads to good results . Finally , we consider two real classification problems , obtaining results that are consistent with the findings observed with simulated curves .
Present incremental learning methods are limited in the ability to achieve reliable credit assignment over a large number time steps ( or events ) . However , this situation is typical for cases where the dynamical system to be controlled requires relatively frequent control updates in order to maintain stability or robustness yet has some action-consequences which must be established over relatively long periods of time . To address this problem , the learning capabilities of a control architecture comprised of two Backpropagated Adaptive Critics ( BACs ) in a two-level hierarchy with continuous actions are explored . The high-level BAC updates less frequently than the low-level BAC and controls the latter to some degree . The response of the low-level to high-level signals can either be determined a priori or it can emerge during learning . A general approach called Response Induction Learning is introduced to address the latter case .
There are many situations in which it would be beneficial for a robot to have predictive abilities similar to those of rational humans . Some of these situations include collaborative robots , robots in adversarial situations , and for dynamic obstacle avoidance . This paper presents an approach to modeling behaviors of dynamic agents in order to empower robots with the ability to predict the agent ' s actions and identify the behavior the agent is executing in real time . The method of behavior modeling implemented uses hidden Markov models ( HMMs ) to model the unobservable states of the dynamic agents . The background and theory of the behavior modeling is presented . Experimental results of realistic simulations of a robot predicting the behaviors and actions of a dynamic agent in a static environment are presented .
Multiple instance learning ( MIL ) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision . We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network . In this setting , we seek to learn a semantic segmentation model from just weak image-level labels . The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment . Fully convolutional training accepts inputs of any size , does not need object proposal pre-processing , and offers a pixelwise loss map for selecting latent instances . Our multi-class MIL loss exploits the further supervision given by images with multiple labels . We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge .
Many econometric analyses involve spatio--temporal data . A considerable amount of literature has addressed spatio--temporal models , with Spatial Dynamic Panel Data ( SDPD ) being widely investigated and applied . In real data applications , checking the validity of the theoretical assumptions underlying the SDPD models is essential but sometimes difficult . At other times , the assumptions are clearly violated . For example , the spatial matrix is assumed to be known but it may actually be unknown and needs to be estimated . In such cases , the performance of the SDPD model ' s estimator is generally affected . Motivated by such considerations , we propose a new model ( called stationary SDPD ) and a new estimation procedure based on simple and clear assumptions that can be easily checked with real data . The new model is highly adaptive , and the estimation procedure has a rate of convergence that is not affected by the dimension of the time series ( under general assumptions ) , notwithstanding the relatively high number of parameters to be estimated . The new model may be used to represent a wide class of multivariate time series , not necessarily spatio-temporal . So , it can be used as a valid alternative to vector autoregressive ( VAR ) models with two immediate advantages : i ) a faster rate of convergence of the estimation procedure and ii ) the possibility of estimating the model even when the dimension is higher than the time series length , overcoming the curse of dimensionality typical of the VAR models . The simulation study shows that the new estimation procedure performs well compared with the classic alternative procedure , even when the spatial matrix is unknown and therefore estimated .
It is the main goal of this paper to propose a novel method to perform matrix completion on-line . Motivated by a wide variety of applications , ranging from the design of recommender systems to sensor network localization through seismic data reconstruction , we consider the matrix completion problem when entries of the matrix of interest are observed gradually . Precisely , we place ourselves in the situation where the predictive rule should be refined incrementally , rather than recomputed from scratch each time the sample of observed entries increases . The extension of existing matrix completion methods to the sequential prediction context is indeed a major issue in the Big Data era , and yet little addressed in the literature . The algorithm promoted in this article builds upon the Soft Impute approach introduced in Mazumder et al . ( 0000 ) . The major novelty essentially arises from the use of a randomised technique for both computing and updating the Singular Value Decomposition ( SVD ) involved in the algorithm . Though of disarming simplicity , the method proposed turns out to be very efficient , while requiring reduced computations . Several numerical experiments based on real datasets illustrating its performance are displayed , together with preliminary results giving it a theoretical basis .
Curved Trajectory Detection ( CTD ) process could be considered among high-level planned capabilities for cognitive agents , has which been acquired under aegis of embedded artificial spiking neuronal circuits . In this paper , hard-wired implementation of the cross-correlation , as the most common comparison-driven scheme for both natural and artificial bionic constructions named Depth Detection Module ( DDM ) , has been taken into account . It is manifestation of efficient handling upon epileptic seizures due to application of both excitatory and inhibitory connections within the circuit structure . Presented traditional analytic approach of the cross-correlation computation with regard to our neural mapping technique and the acquired traced precision have been turned into account for coherent accomplishments of the aforementioned design in perspective of the desired accuracy upon high-level cognitive reactions . Furthermore , the proposed circuit could be fitted into the scalable neuronal network of the CTD , properly . Simulated denouements have been captured based on the computational model of PIONEER mobile robot to verify characteristics of the module , in detail .
We present a hidden Markov model that describes variation in an animal ' s position associated with varying levels of activity in action potential spike trains of individual place cell neurons . The model incorporates a coarse-graining of position , which we find to be a more parsimonious description of the system than other models . We use a sequential Monte Carlo algorithm for Bayesian inference of model parameters , including the state space dimension , and we explain how to estimate position from spike train observations ( decoding ) . We obtain greater accuracy over other methods in the conditions of high temporal resolution and small neuronal sample size . We also present a novel , model-based approach to the study of replay : the expression of spike train activity related to behaviour during times of motionlessness or sleep , thought to be integral to the consolidation of long-term memories . We demonstrate how we can detect the time , information content and compression rate of replay events in simulated and real hippocampal data recorded from rats in two different environments , and verify the correlation between the times of detected replay events and of sharp wave/ripples in the local field potential .
In this chapter we explain briefly the fundamentals of the interactive scores formalism . Then we develop a solution for implementing the ECO machine by mixing petri nets and constraints propagation . We also present another solution for implementing the ECO machine using concurrent constraint programming . Finally , we present an extension of interactive score with conditional branching .
It has been recently shown that , under the margin ( or low noise ) assumption , there exist classifiers attaining fast rates of convergence of the excess Bayes risk , i . e . , the rates faster than $n^{-0/0}$ . The works on this subject suggested the following two conjectures : ( i ) the best achievable fast rate is of the order $n^{-0}$ , and ( ii ) the plug-in classifiers generally converge slower than the classifiers based on empirical risk minimization . We show that both conjectures are not correct . In particular , we construct plug-in classifiers that can achieve not only the fast , but also the {\it super-fast} rates , i . e . , the rates faster than $n^{-0}$ . We establish minimax lower bounds showing that the obtained rates cannot be improved .
We consider one of the most basic multiple testing problems that compares expectations of multivariate data among several groups . As a test statistic , a conventional ( approximate ) $t$-statistic is considered , and we determine its rejection region using a common rejection limit . When there are unknown correlations among test statistics , the multiplicity adjusted $p$-values are dependent on the unknown correlations . They are usually replaced with their estimates that are always consistent under any hypothesis . In this paper , we propose the use of estimates , which are not necessarily consistent and are referred to as spurious correlations , in order to improve statistical power . Through simulation studies , we verify that the proposed method asymptotically controls the family-wise error rate and clearly provides higher statistical power than existing methods . In addition , the proposed and existing methods are applied to a real multiple testing problem that compares quantitative traits among groups of mice and the results are compared .
When applied to training deep neural networks , stochastic gradient descent ( SGD ) often incurs steady progression phases , interrupted by catastrophic episodes in which loss and gradient norm explode . A possible mitigation of such events is to slow down the learning process . This paper presents a novel approach to control the SGD learning rate , that uses two statistical tests . The first one , aimed at fast learning , compares the momentum of the normalized gradient vectors to that of random unit vectors and accordingly gracefully increases or decreases the learning rate . The second one is a change point detection test , aimed at the detection of catastrophic learning episodes ; upon its triggering the learning rate is instantly halved . Both abilities of speeding up and slowing down the learning rate allows the proposed approach , called SALeRA , to learn as fast as possible but not faster . Experiments on standard benchmarks show that SALeRA performs well in practice , and compares favorably to the state of the art .
This paper describes the application of a real coded genetic algorithm ( GA ) to align two or more 0-D images by means of image registration . The proposed search strategy is a transformation parameters-based approach involving the affine transform . The real coded GA uses Simulated Binary Crossover ( SBX ) , a parent-centric recombination operator that has shown to deliver a good performance in many optimization problems in the continuous domain . In addition , we propose a new technique for matching points between a warped and static images by using a randomized ordering when visiting the points during the matching procedure . This new technique makes the evaluation of the objective function somewhat noisy , but GAs and other population-based search algorithms have been shown to cope well with noisy fitness evaluations . The results obtained are competitive to those obtained by state-of-the-art classical methods in image registration , confirming the usefulness of the proposed noisy objective function and the suitability of SBX as a recombination operator for this type of problem .
Neuroimaging meta-analysis is an important tool for finding consistent effects over studies that each usually have 00 or fewer subjects . Interest in meta-analysis in brain mapping is also driven by a recent focus on so-called " reverse inference " : where as traditional " forward inference " identifies the regions of the brain involved in a task , a reverse inference identifies the cognitive processes that a task engages . Such reverse inferences , however , require a set of meta-analysis , one for each possible cognitive domain . However , existing methods for neuroimaging meta-analysis have significant limitations . Commonly used methods for neuroimaging meta-analysis are not model based , do not provide interpretable parameter estimates , and only produce null hypothesis inferences ; further , they are generally designed for a single group of studies and cannot produce reverse inferences . In this work we address these limitations by adopting a nonparametric Bayesian approach for meta-analysis data from multiple classes or types of studies . In particular , foci from each type of study are modeled as a cluster process driven by a random intensity function that is modeled as a kernel convolution of a gamma random field . The type-specific gamma random fields are linked and modeled as a realization of a common gamma random field , shared by all types , that induces correlation between study types and mimics the behavior of a univariate mixed effects model . We illustrate our model on simulation studies and a meta-analysis of five emotions from 000 studies and check model fit by a posterior predictive assessment . In addition , we implement reverse inference by using the model to predict study type from a newly presented study . We evaluate this predictive performance via leave-one-out cross-validation that is efficiently implemented using importance sampling techniques .
In this paper we describe EasyInterface , an open-source toolkit for rapid development of web-based graphical user interfaces ( GUIs ) . This toolkit addresses the need of researchers to make their research prototype tools available to the community , and integrating them in a common environment , rapidly and without being familiar with web programming or GUI libraries in general . If a tool can be executed from a command-line and its output goes to the standard output , then in few minutes one can make it accessible via a web-interface or within Eclipse . Moreover , the toolkit defines a text-based language that can be used to get more sophisticated GUIs , e . g . , syntax highlighting , dialog boxes , user interactions , etc . EasyInterface was originally developed for building a common frontend for tools developed in the Envisage project .
We consider two-player zero-sum games on graphs . These games can be classified on the basis of the information of the players and on the mode of interaction between them . On the basis of information the classification is as follows : ( a ) partial-observation ( both players have partial view of the game ) ; ( b ) one-sided complete-observation ( one player has complete observation ) ; and ( c ) complete-observation ( both players have complete view of the game ) . On the basis of mode of interaction we have the following classification : ( a ) concurrent ( both players interact simultaneously ) ; and ( b ) turn-based ( both players interact in turn ) . The two sources of randomness in these games are randomness in transition function and randomness in strategies . In general , randomized strategies are more powerful than deterministic strategies , and randomness in transitions gives more general classes of games . In this work we present a complete characterization for the classes of games where randomness is not helpful in : ( a ) the transition function probabilistic transition can be simulated by deterministic transition ) ; and ( b ) strategies ( pure strategies are as powerful as randomized strategies ) . As consequence of our characterization we obtain new undecidability results for these games .
We propose a novel distributed inference algorithm for continuous graphical models by extending Stein variational gradient descent ( SVGD ) to leverage the Markov dependency structure of the distribution of interest . The idea is to use a set of local kernel functions over the Markov blanket of each node , which alleviates the problem of the curse of high dimensionality and simultaneously yields a distributed algorithm for decentralized inference tasks . We justify our method with theoretical analysis and show that the use of local kernels can be viewed as a new type of localized approximation that matches the target distribution on the conditional distributions of each node over its Markov blanket . Our empirical results demonstrate that our method outperforms a variety of baselines including standard MCMC and particle message passing methods .
Inspired by kernel methods that have been used extensively in achieving efficient facial animation retargeting , this paper presents a solution to retargeting facial animation in virtual character ' s face model based on the kernel projection of latent structure ( KPLS ) regression between semantically similar facial expressions . Specifically , a given number of corresponding semantically similar facial expressions are projected into the latent space . By using the Nonlinear Iterative Partial Least Square method , decomposition of the latent variables is achieved . Finally , the KPLS is achieved by solving a kernalized version of the eigenvalue problem . By evaluating our methodology with other kernel-based solutions , the efficiency of the presented methodology in transferring facial animation to face models with different morphological variations is demonstrated .
Evolutionary computation algorithms are increasingly being used to solve optimization problems as they have many advantages over traditional optimization algorithms . In this paper we use evolutionary computation to study the trade-off between pleiotropy and redundancy in a client-server based network . Pleiotropy is a term used to describe components that perform multiple tasks , while redundancy refers to multiple components performing one same task . Pleiotropy reduces cost but lacks robustness , while redundancy increases network reliability but is more costly , as together , pleiotropy and redundancy build flexibility and robustness into systems . Therefore it is desirable to have a network that contains a balance between pleiotropy and redundancy . We explore how factors such as link failure probability , repair rates , and the size of the network influence the design choices that we explore using genetic algorithms .
We present an algorithm to build an automaton from a rational expression . This approach introduces support for extended weighted expressions . Inspired by derived-term based algorithms , its core relies on a different construct , rational expansions . We introduce an inductive algorithm to compute the expansion of an expression from which the automaton follows . This algorithm is independent of the size of the alphabet , and actually even supports infinite alphabets . It can easily be accommodated to generate deterministic ( weighted ) automata . These constructs are implemented in Vcsn , a free-software platform dedicated to weighted automata and rational expressions .
An effective paradigm for simulating the dynamics of robots that locomote and manipulate is multi-rigid body simulation with rigid contact . This paradigm provides reasonable tradeoffs between accuracy , running time , and simplicity of parameter selection and identification . The Stewart-Trinkle/Anitescu-Potra time stepping approach is the basis of many existing implementations . It successfully treats inconsistent ( Painleve-type ) contact configurations , efficiently handles many contact events occurring in short time intervals , and provably converges to the solution of the continuous time differential algebraic equations ( DAEs ) as the integration step size tends to zero . However , there is currently no means to determine when the solution has largely converged , i . e . , when smaller integration steps would result in only small increases in accuracy . The present work describes an approach that computes the event times ( when the set of active equations in a DAE changes ) of all contact/impact events for a multi-body simulation , toward using integration techniques with error control to compute a solution with desired accuracy . We also describe a first-order , variable integration approach that ensures that rigid bodies with convex polytopic geometries never interpenetrate . This approach permits taking large steps when possible and takes small steps when contact is complex .
Computer vision systems require large amounts of manually annotated data to properly learn challenging visual concepts . Crowdsourcing platforms offer an inexpensive method to capture human knowledge and understanding , for a vast number of visual perception tasks . In this survey , we describe the types of annotations computer vision researchers have collected using crowdsourcing , and how they have ensured that this data is of high quality while annotation effort is minimized . We begin by discussing data collection on both classic ( e . g . , object recognition ) and recent ( e . g . , visual story-telling ) vision tasks . We then summarize key design decisions for creating effective data collection interfaces and workflows , and present strategies for intelligently selecting the most important data instances to annotate . Finally , we conclude with some thoughts on the future of crowdsourcing in computer vision .
The paper introduces simple bibliometric-enhanced search facilities which are derived from the famous stratagems by Bates . Moves , tactics and stratagems are revisited from a Digital Library perspective . The potential of extended versions of " journal run " or " citation search " for interactive information retrieval is outlined . The authors elaborate on the future implementation and evaluation of new bibliometric-enhanced search services .
Gaussian processes ( GP ) are attractive building blocks for many probabilistic models . Their drawbacks , however , are the rapidly increasing inference time and memory requirement alongside increasing data . The problem can be alleviated with compactly supported ( CS ) covariance functions , which produce sparse covariance matrices that are fast in computations and cheap to store . CS functions have previously been used in GP regression but here the focus is in a classification problem . This brings new challenges since the posterior inference has to be done approximately . We utilize the expectation propagation algorithm and show how its standard implementation has to be modified to obtain computational benefits from the sparse covariance matrices . We study four CS covariance functions and show that they may lead to substantial speed up in the inference time compared to globally supported functions .
Using a characterization of Mutual Complete Dependence copulas , we show that , with respect to the Sobolev norm , the MCD copulas can be approximated arbitrarily closed by shuffles of Min . This result is then used to obtain a characterization of generalized shuffles of copulas introduced by Durante , Sarkoci and Sempi in terms of MCD copulas and the $\star$-product discovered by Darsow , Nguyen and Olsen . Since shuffles of a copula is the copula of the corresponding shuffles of the two continuous random variables , we define a new norm which is invariant under shuffling . This norm gives rise to a new measure of dependence which shares many properties with the maximal correlation coefficient , the only measure of dependence that satisfies all of R\ ' enyi ' s postulates .
In nonparametric statistics an optimality criterion for estimation procedures is provided by the minimax rate of convergence . However this classical point of view is subject to controversy as it requires to look for the worst behaviour reached by an estimation procedure in a given space . The purpose of this paper is to show that this is not justified as the minimax risk often coincides with a generic one . We are here interested in the rate of convergence attained by some classical estimators on almost every , in the sense of prevalence , function in a Besov space .
We provide a constructive proof of Border ' s theorem [Bor00 , HR00a] and its generalization to reduced-form auctions with asymmetric bidders [Bor00 , MV00 , CKM00] . Given a reduced form , we identify a subset of Border constraints that are necessary and sufficient to determine its feasibility . Importantly , the number of these constraints is linear in the total number of bidder types . In addition , we provide a characterization result showing that every feasible reduced form can be induced by an ex-post allocation rule that is a distribution over ironings of the same total ordering of the union of all bidders ' types . We show how to leverage our results for single-item reduced forms to design auctions with heterogeneous items and asymmetric bidders with valuations that are additive over items . Appealing to our constructive Border ' s theorem , we obtain polynomial-time algorithms for computing the revenue-optimal auction . Appealing to our characterization of feasible reduced forms , we characterize feasible multi-item allocation rules .
In software testing , the large size of the input domain makes exhaustively testing the inputs a daunting and often impossible task . Pair-wise testing is a popular approach to combinatorial testing problems . This paper reviews Pair-wise testing and its history , strengths , weaknesses , and tools for generating test cases .
In this semi-tutorial paper , we first review the information-theoretic approach to account for the computational costs incurred during the search for optimal actions in a sequential decision-making problem . The traditional ( MDP ) framework ignores computational limitations while searching for optimal policies , essentially assuming that the acting agent is perfectly rational and aims for exact optimality . Using the free-energy , a variational principle is introduced that accounts not only for the value of a policy alone , but also considers the cost of finding this optimal policy . The solution of the variational equations arising from this formulation can be obtained using familiar Bellman-like value iterations from dynamic programming ( DP ) and the Blahut-Arimoto ( BA ) algorithm from rate distortion theory . Finally , we demonstrate the utility of the approach for generating hierarchies of state abstractions that can be used to best exploit the available computational resources . A numerical example showcases these concepts for a path-planning problem in a grid world environment .
We revisit the problem of estimating the parameters of a partially observed diffusion process , consisting of a hidden state process and an observed process , with a continuous time parameter . The estimation is to be done online , i . e . the parameter estimate should be updated recursively based on the observation filtration . Here , we use an old but under-exploited representation of the incomplete-data log-likelihood function in terms of the filter of the hidden state from the observations . By performing a stochastic gradient ascent , we obtain a fully recursive algorithm for the time evolution of the parameter estimate . We prove the convergence of the algorithm under suitable conditions regarding the ergodicity of the process consisting of state , filter , and tangent filter . Additionally , our parameter estimation is shown numerically to have the potential of improving suboptimal filters , and can be applied even when the system is not identifiable due to parameter redundancies . Online parameter estimation is a challenging problem that is ubiquitous in fields such as robotics , neuroscience , or finance in order to design adaptive filters and optimal controllers for unknown or changing systems .
Smoothing in state-space models amounts to computing the conditional distribution of the latent state trajectory , given observations , or expectations of functionals of the state trajectory with respect to this distributions . For models that are not linear Gaussian or possess finite state space , smoothing distributions are in general infeasible to compute as they involve intergrals over a space of dimensionality at least equal to the number of observations . Recent years have seen an increased interest in Monte Carlo-based methods for smoothing , often involving particle filters . One such method is to approximate filter distributions with a particle filter , and then to simulate backwards on the trellis of particles using a backward kernel . We show that by supplementing this procedure with a Metropolis-Hastings step deciding whether to accept a proposed trajectory or not , one obtains a Markov chain Monte Carlo scheme whose stationary distribution is the exact smoothing distribution . We also show that in this procedure , backward sampling can be replaced by backward smoothing , which effectively means averaging over all possible trajectories . In an example we compare these approaches to a similar one recently proposed by Andrieu , Doucet and Holenstein , and show that the new methods can be more efficient in terms of precision ( inverse variance ) per computation time .
Multi-event detection and recognition in real time is of challenge for a modern grid as its feature is usually non-identifiable . Based on factor model , this paper porposes a data-driven method as an alternative solution under the framework of random matrix theory . This method maps the raw data into a high-dimensional space with two parts : 0 ) the principal components ( factors , mapping event signals ) ; and 0 ) time series residuals ( bulk , mapping white/non-Gaussian noises ) . The spatial information is extracted form factors , and the termporal infromation from residuals . Taking both spatial-tempral correlation into account , this method is able to reveal the multi-event : its components and their respective details , e . g . , occurring time . Case studies based on the standard IEEE 000-bus system validate the proposed method .
Online learning algorithms have impressive convergence properties when it comes to risk minimization and convex games on very large problems . However , they are inherently sequential in their design which prevents them from taking advantage of modern multi-core architectures . In this paper we prove that online learning with delayed updates converges well , thereby facilitating parallel online learning .
Modern computers are not random access machines ( RAMs ) . They have a memory hierarchy , multiple cores , and virtual memory . In this paper , we address the computational cost of address translation in virtual memory . Starting point for our work is the observation that the analysis of some simple algorithms ( random scan of an array , binary search , heapsort ) in either the RAM model or the EM model ( external memory model ) does not correctly predict growth rates of actual running times . We propose the VAT model ( virtual address translation ) to account for the cost of address translations and analyze the algorithms mentioned above and others in the model . The predictions agree with the measurements . We also analyze the VAT-cost of cache-oblivious algorithms .
The problem of clustering content in social media has pervasive applications , including the identification of discussion topics , event detection , and content recommendation . Here we describe a streaming framework for online detection and clustering of memes in social media , specifically Twitter . A pre-clustering procedure , namely protomeme detection , first isolates atomic tokens of information carried by the tweets . Protomemes are thereafter aggregated , based on multiple similarity measures , to obtain memes as cohesive groups of tweets reflecting actual concepts or topics of discussion . The clustering algorithm takes into account various dimensions of the data and metadata , including natural language , the social network , and the patterns of information diffusion . As a result , our system can build clusters of semantically , structurally , and topically related tweets . The clustering process is based on a variant of Online K-means that incorporates a memory mechanism , used to " forget " old memes and replace them over time with the new ones . The evaluation of our framework is carried out by using a dataset of Twitter trending topics . Over a one-week period , we systematically determined whether our algorithm was able to recover the trending hashtags . We show that the proposed method outperforms baseline algorithms that only use content features , as well as a state-of-the-art event detection method that assumes full knowledge of the underlying follower network . We finally show that our online learning framework is flexible , due to its independence of the adopted clustering algorithm , and best suited to work in a streaming scenario .
Penalization procedures often suffer from their dependence on multiplying factors , whose optimal values are either unknown or hard to estimate from the data . We propose a completely data-driven calibration algorithm for this parameter in the least-squares regression framework , without assuming a particular shape for the penalty . Our algorithm relies on the concept of minimal penalty , recently introduced by Birge and Massart ( 0000 ) in the context of penalized least squares for Gaussian homoscedastic regression . On the positive side , the minimal penalty can be evaluated from the data themselves , leading to a data-driven estimation of an optimal penalty which can be used in practice ; on the negative side , their approach heavily relies on the homoscedastic Gaussian nature of their stochastic framework . The purpose of this paper is twofold : stating a more general heuristics for designing a data-driven penalty ( the slope heuristics ) and proving that it works for penalized least-squares regression with a random design , even for heteroscedastic non-Gaussian data . For technical reasons , some exact mathematical results will be proved only for regressogram bin-width selection . This is at least a first step towards further results , since the approach and the method that we use are indeed general .
SMC ( Sequential Monte Carlo ) is a class of Monte Carlo algorithms for filtering and related sequential problems . Gerber and Chopin ( 0000 ) introduced SQMC ( Sequential quasi-Monte Carlo ) , a QMC version of SMC . This paper has two objectives : ( a ) to introduce Sequential Monte Carlo to the QMC community , whose members are usually less familiar with state-space models and particle filtering ; ( b ) to extend SQMC to the filtering of continuous-time state-space models , where the latent process is a diffusion . A recurring point in the paper will be the notion of dimension reduction , that is how to implement SQMC in such a way that it provides good performance despite the high dimension of the problem .
Twitter has been identified as one of the most popular and promising altmetrics data sources , as it possibly reflects a broader use of research articles by the general public . Several factors , such as document age , scientific discipline , number of authors and document type , have been shown to affect the number of tweets received by scientific documents . The particular meaning of tweets mentioning scholarly papers is , however , not entirely understood and their validity as impact indicators debatable . This study contributes to the understanding of factors influencing Twitter popularity of medical papers investigating differences between medical study types . 000 , 000 documents indexed in Embase to a medical study type have been analysed for the study type specific tweet frequency . Meta-analyses , systematic reviews and clinical trials were found to be tweeted substantially more frequently than other study types , while all basic research received less attention than the average . The findings correspond well with clinical evidence hierarchies . It is suggested that interest from laymen and patients may be a factor in the observed effects .
Traditional intra prediction usually utilizes the nearest reference line to generate the predicted block when considering strong spatial correlation . However , this kind of single line-based method does not always work well due to at least two issues . One is the incoherence caused by the signal noise or the texture of other object , where this texture deviates from the inherent texture of the current block . The other reason is that the nearest reference line usually has worse reconstruction quality in block-based video coding . Due to these two issues , this paper proposes an efficient multiple line-based intra prediction scheme to improve coding efficiency . Besides the nearest reference line , further reference lines are also utilized . The further reference lines with relatively higher quality can provide potential better prediction . At the same time , the residue compensation is introduced to calibrate the prediction of boundary regions in a block when we utilize further reference lines . To speed up the encoding process , this paper designs several fast algorithms . Experimental results show that , compared with HM-00 . 0 , the proposed fast search method achieves 0 . 0% bit saving on average and up to 0 . 0% , with increasing the encoding time by 000% .
It is becoming increasingly common to see large collections of network data objects -- that is , data sets in which a network is viewed as a fundamental unit of observation . As a result , there is a pressing need to develop network-based analogues of even many of the most basic tools already standard for scalar and vector data . In this paper , our focus is on averages of unlabeled , undirected networks with edge weights . Specifically , we ( i ) characterize a certain notion of the space of all such networks , ( ii ) describe key topological and geometric properties of this space relevant to doing probability and statistics thereupon , and ( iii ) use these properties to establish the asymptotic behavior of a generalized notion of an empirical mean under sampling from a distribution supported on this space . Our results rely on a combination of tools from geometry , probability theory , and statistical shape analysis . In particular , the lack of vertex labeling necessitates working with a quotient space modding out permutations of labels . This results in a nontrivial geometry for the space of unlabeled networks , which in turn is found to have important implications on the types of probabilistic and statistical results that may be obtained and the techniques needed to obtain them .
In this paper we prove the strong consistency of several methods based on the spectral clustering techniques that are widely used to study the community detection problem in stochastic block models ( SBMs ) . We show that under some weak conditions on the minimal degree , the number of communities , and the eigenvalues of the probability block matrix , the K-means algorithm applied to the eigenvectors of the graph Laplacian associated with its first few largest eigenvalues can classify all individuals into the true community uniformly correctly almost surely . Extensions to both regularized spectral clustering and degree-corrected SBMs are also considered . We illustrate the performance of different methods on simulated networks .
A bootstrap procedure for functional time series is proposed which exploits a general vector autoregressive representation of the time series of Fourier coefficients appearing in the Karhunen-Lo\`eve expansion of the functional process . A double sieve-type bootstrap method is developed which avoids the estimation of process operators and generates functional pseudo-time series that appropriately mimic the dependence structure of the functional time series at hand . The method uses a finite set of functional principal components to capture the essential driving parts of the infinite dimensional process and a finite order vector autoregressive process to imitate the temporal dependence structure of the corresponding vector time series of Fourier coefficients . By allowing the number of functional principal components as well as the autoregressive order used to increase to infinity ( at some appropriate rate ) as the sample size increases , a basic bootstrap central limit theorem is established which shows validity of the bootstrap procedure proposed for functional finite Fourier transforms . Some numerical examples illustrate the good finite sample performance of the new bootstrap method proposed .
Discussion of ``Breakdown and groups ' ' by P . L . Davies and U . Gather [math . ST/0000000]
This paper is based on our personal notes for the short course we gave on January 0 , 0000 at Institut Henri Poincar\ ' e , after an invitation of the SFdS . Our purpose is to give an overview of the method of $\rho$-estimation and of the optimality and robustness properties of the estimators built according to this procedure . This method can be viewed as the sequel of a long series of researches which were devoted to the construction of estimators with good properties in various statistical frameworks . We shall emphasize the connection between the $\rho$-estimators and the previous ones , in particular the maximum likelihood estimator , and we shall show , via some typical examples , that the $\rho$-estimators perform better from various points of view . ------ Cet article est fond\ ' e sur les notes du mini-cours que nous avons donn\ ' e le 0 janvier 0000 \`a l ' Institut Henri Poincar\ ' e \`a l ' occasion d ' une journ\ ' ee organis\ ' ee par la SFdS et consacr\ ' ee \`a la Statistique Math\ ' ematique . Il vise \`a donner un aper\c{c}u de la m\ ' ethode de $\rho$-estimation ainsi que des propri\ ' et\ ' es d ' optimalit\ ' e et de robustesse des estimateurs construits selon cette proc\ ' edure . Cette m\ ' ethode s ' inscrit dans une longue lign\ ' ee de recherches dont l ' objectif a \ ' et\ ' e de produire des estimateurs poss\ ' edant de bonnes propri\ ' et\ ' es pour un ensemble de cadres statistiques aussi vaste que possible . Nous mettrons en lumi\`ere les liens forts qui existent entre les $\rho$-estimateurs et ces pr\ ' ed\ ' ecesseurs , notamment les estimateurs du maximum de vraisemblance , mais montrerons \ ' egalement , au travers d ' exemples choisis , que les $\rho$-estimateurs les surpassent sur bien des aspects .
The results from most machine learning experiments are used for a specific purpose and then discarded . This results in a significant loss of information and requires rerunning experiments to compare learning algorithms . This also requires implementation of another algorithm for comparison , that may not always be correctly implemented . By storing the results from previous experiments , machine learning algorithms can be compared easily and the knowledge gained from them can be used to improve their performance . The purpose of this work is to provide easy access to previous experimental results for learning and comparison . These stored results are comprehensive -- storing the prediction for each test instance as well as the learning algorithm , hyperparameters , and training set that were used . Previous results are particularly important for meta-learning , which , in a broad sense , is the process of learning from previous machine learning results such that the learning process is improved . While other experiment databases do exist , one of our focuses is on easy access to the data . We provide meta-learning data sets that are ready to be downloaded for meta-learning experiments . In addition , queries to the underlying database can be made if specific information is desired . We also differ from previous experiment databases in that our databases is designed at the instance level , where an instance is an example in a data set . We store the predictions of a learning algorithm trained on a specific training set for each instance in the test set . Data set level information can then be obtained by aggregating the results from the instances . The instance level information can be used for many tasks such as determining the diversity of a classifier or algorithmically determining the optimal subset of training instances for a learning algorithm .
In this paper , we consider the problem of " hyper-sparse aggregation " . Namely , given a dictionary $F = \{f_0 , . . . , f_M \}$ of functions , we look for an optimal aggregation algorithm that writes $\tilde f = \sum_{j=0}^M \theta_j f_j$ with as many zero coefficients $\theta_j$ as possible . This problem is of particular interest when $F$ contains many irrelevant functions that should not appear in $\tilde{f}$ . We provide an exact oracle inequality for $\tilde f$ , where only two coefficients are non-zero , that entails $\tilde f$ to be an optimal aggregation algorithm . Since selectors are suboptimal aggregation procedures , this proves that 0 is the minimal number of elements of $F$ required for the construction of an optimal aggregation procedures in every situations . A simulated example of this algorithm is proposed on a dictionary obtained using LARS , for the problem of selection of the regularization parameter of the LASSO . We also give an example of use of aggregation to achieve minimax adaptation over anisotropic Besov spaces , which was not previously known in minimax theory ( in regression on a random design ) .
We extend the AIGER format , as used in HWMCC , to a format that is suitable to define synthesis problems with safety specifications . We recap the original format and define one format for posing synthesis problems and one for solutions of synthesis problems in this setting .
A technique introduced by Indyk and Woodruff [STOC 0000] has inspired several recent advances in data-stream algorithms . We show that a number of these results follow easily from the application of a single probabilistic method called Precision Sampling . Using this method , we obtain simple data-stream algorithms that maintain a randomized sketch of an input vector $x= ( x_0 , . . . x_n ) $ , which is useful for the following applications . 0 ) Estimating the $F_k$-moment of $x$ , for $k>0$ . 0 ) Estimating the $\ell_p$-norm of $x$ , for $p\in[0 , 0]$ , with small update time . 0 ) Estimating cascaded norms $\ell_p ( \ell_q ) $ for all $p , q>0$ . 0 ) $\ell_0$ sampling , where the goal is to produce an element $i$ with probability ( approximately ) $|x_i|/\|x\|_0$ . It extends to similarly defined $\ell_p$-sampling , for $p\in [0 , 0]$ . For all these applications the algorithm is essentially the same : scale the vector x entry-wise by a well-chosen random vector , and run a heavy-hitter estimation algorithm on the resulting vector . Our sketch is a linear function of x , thereby allowing general updates to the vector x . Precision Sampling itself addresses the problem of estimating a sum $\sum_{i=0}^n a_i$ from weak estimates of each real $a_i\in[0 , 0]$ . More precisely , the estimator first chooses a desired precision $u_i\in ( 0 , 0]$ for each $i\in[n]$ , and then it receives an estimate of every $a_i$ within additive $u_i$ . Its goal is to provide a good approximation to $\sum a_i$ while keeping a tab on the " approximation cost " $\sum_i ( 0/u_i ) $ . Here we refine previous work [Andoni , Krauthgamer , and Onak , FOCS 0000] which shows that as long as $\sum a_i=\Omega ( 0 ) $ , a good multiplicative approximation can be achieved using total precision of only $O ( n\log n ) $ .
Suppose that a graph is realized from a stochastic block model where one of the blocks is of interest , but many or all of the vertices ' block labels are unobserved . The task is to order the vertices with unobserved block labels into a ``nomination list ' ' such that , with high probability , vertices from the interesting block are concentrated near the list ' s beginning . We propose several vertex nomination schemes . Our basic - but principled - setting and development yields a best nomination scheme ( which is a Bayes-Optimal analogue ) , and also a likelihood maximization nomination scheme that is practical to implement when there are a thousand vertices , and which is empirically near-optimal when the number of vertices is small enough to allow comparison to the best nomination scheme . We then illustrate the robustness of the likelihood maximization nomination scheme to the modeling challenges inherent in real data , using examples which include a social network involving human trafficking , the Enron Graph , a worm brain connectome and a political blog network .
Quadrotor drones equipped with high quality cameras have rapidely raised as novel , cheap and stable devices for filmmakers . While professional drone pilots can create aesthetically pleasing videos in short time , the smooth -- and cinematographic -- control of a camera drone remains challenging for most users , despite recent tools that either automate part of the process or enable the manual design of waypoints to create drone trajectories . This paper proposes to move a step further towards more accessible cinematographic drones by designing techniques to automatically or interactively plan quadrotor drone motions in 0D dynamic environments that satisfy both cinematographic and physical quadrotor constraints . We first propose the design of a Drone Toric Space as a dedicated camera parameter space with embedded constraints and derive some intuitive on-screen viewpoint manipulators . Second , we propose a specific path planning technique which ensures both that cinematographic properties can be enforced along the path , and that the path is physically feasible by a quadrotor drone . At last , we build on the Drone Toric Space and the specific path planning technique to coordinate the motion of multiple drones around dynamic targets . A number of results then demonstrate the interactive and automated capacities of our approaches on a number of use-cases .
We investigate the opinion formation with upper and lower bounds . We formulate the binary exchange of opinions between two individuals , and effects of the self-thinking and political party using the relativistic Boltzmann-Vlasov type equation with the randomly perturbed motion . The convergent form of the distribution function is determined by the balance between the cooling rate via the binary exchange of opinions between two individuals and the concentration of opinions by the political party , and heating rate via the self-thinking .
We study the problem of estimating a temporally varying coefficient and varying structure ( VCVS ) graphical model underlying nonstationary time series data , such as social states of interacting individuals or microarray expression profiles of gene networks , as opposed to i . i . d . data from an invariant model widely considered in current literature of structural estimation . In particular , we consider the scenario in which the model evolves in a piece-wise constant fashion . We propose a procedure that minimizes the so-called TESLA loss ( i . e . , temporally smoothed L0 regularized regression ) , which allows jointly estimating the partition boundaries of the VCVS model and the coefficient of the sparse precision matrix on each block of the partition . A highly scalable proximal gradient method is proposed to solve the resultant convex optimization problem ; and the conditions for sparsistent estimation and the convergence rate of both the partition boundaries and the network structure are established for the first time for such estimators .
We study the $\ell_0$-low rank approximation problem , where for a given $n \times d$ matrix $A$ and approximation factor $\alpha \geq 0$ , the goal is to output a rank-$k$ matrix $\widehat{A}$ for which $$\|A-\widehat{A}\|_0 \leq \alpha \cdot \min_{\textrm{rank-}k\textrm{ matrices}~A ' }\|A-A ' \|_0 , $$ where for an $n \times d$ matrix $C$ , we let $\|C\|_0 = \sum_{i=0}^n \sum_{j=0}^d |C_{i , j}|$ . This error measure is known to be more robust than the Frobenius norm in the presence of outliers and is indicated in models where Gaussian assumptions on the noise may not apply . The problem was shown to be NP-hard by Gillis and Vavasis and a number of heuristics have been proposed . It was asked in multiple places if there are any approximation algorithms . We give the first provable approximation algorithms for $\ell_0$-low rank approximation , showing that it is possible to achieve approximation factor $\alpha = ( \log d ) \cdot \mathrm{poly} ( k ) $ in $\mathrm{nnz} ( A ) + ( n+d ) \mathrm{poly} ( k ) $ time , where $\mathrm{nnz} ( A ) $ denotes the number of non-zero entries of $A$ . If $k$ is constant , we further improve the approximation ratio to $O ( 0 ) $ with a $\mathrm{poly} ( nd ) $-time algorithm . Under the Exponential Time Hypothesis , we show there is no $\mathrm{poly} ( nd ) $-time algorithm achieving a $ ( 0+\frac{0}{\log^{0+\gamma} ( nd ) } ) $-approximation , for $\gamma > 0$ an arbitrarily small constant , even when $k = 0$ . We give a number of additional results for $\ell_0$-low rank approximation : nearly tight upper and lower bounds for column subset selection , CUR decompositions , extensions to low rank approximation with respect to $\ell_p$-norms for $0 \leq p < 0$ and earthmover distance , low-communication distributed protocols and low-memory streaming algorithms , algorithms with limited randomness , and bicriteria algorithms . We also give a preliminary empirical evaluation .
Ergodic capacity is an important performance measure associated with reliable communication at the highest rate at which information can be sent over the channel with a negligible probability of error . In the shadow of this definition , diversity receivers ( such as selection combining , equal-gain combining and maximal-ratio combining ) and transmission techniques ( such as cascaded fading channels , amplify-and-forward multihop transmission ) are deployed in mitigating various performance impairing effects such as fading and shadowing in digital radio communication links . However , the exact analysis of ergodic capacity is in general not always possible for all of these forms of diversity receivers and transmission techniques over generalized composite fading environments due to it ' s mathematical intractability . In the literature , published papers concerning the exact analysis of ergodic capacity have been therefore scarce ( i . e . , only [0] and [0] ) when compared to those concerning the exact analysis of average symbol error probability . In addition , they are essentially targeting to the ergodic capacity of the maximal ratio combining diversity receivers and are not readily applicable to the capacity analysis of the other diversity combiners / transmission techniques . In this paper , we propose a novel moment generating function-based approach for the exact ergodic capacity analysis of both diversity receivers and transmission techniques over generalized composite fading environments . As such , we demonstrate how to simultaneously treat the ergodic capacity analysis of all forms of both diversity receivers and multihop transmission techniques .
This paper presents an algorithm for computing Sine-Cosine pairs to modest accuracy , but in a manner which contains no conditional tests or branching , making it highly amenable to vectorization . An exemplary implementation for PowerPC AltiVec processors is included , but the algorithm should be easily portable to other achitectures , such as Intel SSE .
We present a comprehensive study on the use of autoencoders for modelling text data , in which ( differently from previous studies ) we focus our attention on the following issues : i ) we explore the suitability of two different models bDA and rsDA for constructing deep autoencoders for text data at the sentence level ; ii ) we propose and evaluate two novel metrics for better assessing the text-reconstruction capabilities of autoencoders ; and iii ) we propose an automatic method to find the critical bottleneck dimensionality for text language representations ( below which structural information is lost ) .
This paper examines the limit properties of information criteria ( such as AIC , BIC , HQIC ) for distinguishing between the unit root model and the various kinds of explosive models . The explosive models include the local-to-unit-root model , the mildly explosive model and the regular explosive model . Initial conditions with different order of magnitude are considered . Both the OLS estimator and the indirect inference estimator are studied . It is found that BIC and HQIC , but not AIC , consistently select the unit root model when data come from the unit root model . When data come from the local-to-unit-root model , both BIC and HQIC select the wrong model with probability approaching 0 while AIC has a positive probability of selecting the right model in the limit . When data come from the regular explosive model or from the mildly explosive model in the form of $0+n^{\alpha }/n$ with $\alpha \in ( 0 , 0 ) $ , all three information criteria consistently select the true model . Indirect inference estimation can increase or decrease the probability for information criteria to select the right model asymptotically relative to OLS , depending on the information criteria and the true model . Simulation results confirm our asymptotic results in finite sample .
The social media site Flickr allows users to upload their photos , annotate them with tags , submit them to groups , and also to form social networks by adding other users as contacts . Flickr offers multiple ways of browsing or searching it . One option is tag search , which returns all images tagged with a specific keyword . If the keyword is ambiguous , e . g . , ``beetle ' ' could mean an insect or a car , tag search results will include many images that are not relevant to the sense the user had in mind when executing the query . We claim that users express their photography interests through the metadata they add in the form of contacts and image annotations . We show how to exploit this metadata to personalize search results for the user , thereby improving search performance . First , we show that we can significantly improve search precision by filtering tag search results by user ' s contacts or a larger social network that includes those contact ' s contacts . Secondly , we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results . The users ' interests can similarly be described by the tags they used for annotating their images . The latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user .
We present a systematic , algebraically based , design methodology for efficient implementation of computer programs optimized over multiple levels of the processor/memory and network hierarchy . Using a common formalism to describe the problem and the partitioning of data over processors and memory levels allows one to mathematically prove the efficiency and correctness of a given algorithm as measured in terms of a set of metrics ( such as processor/network speeds , etc . ) . The approach allows the average programmer to achieve high-level optimizations similar to those used by compiler writers ( e . g . the notion of " tiling " ) . The approach presented in this monograph makes use of A Mathematics of Arrays ( MoA , Mullin 0000 ) and an indexing calculus ( i . e . the psi-calculus ) to enable the programmer to develop algorithms using high-level compiler-like optimizations through the ability to algebraically compose and reduce sequences of array operations . Extensive discussion and benchmark results are presented for the Fast Fourier Transform and other important algorithms .
Software watermarking has received considerable attention and was adopted by the software development community as a technique to prevent or discourage software piracy and copyright infringement . A wide range of software watermarking techniques has been proposed among which the graph-based methods that encode watermarks as graph structures . Following up on our recently proposed methods for encoding watermark numbers $w$ as reducible permutation flow-graphs $F[\pi^*]$ through the use of self-inverting permutations $\pi^*$ , in this paper , we extend the types of flow-graphs available for software watermarking by proposing two different reducible permutation flow-graphs $F_0[\pi^*]$ and $F_0[\pi^*]$ incorporating important properties which are derived from the bitonic subsequences composing the self-inverting permutation $\pi^*$ . We show that a self-inverting permutation $\pi^*$ can be efficiently encoded into either $F_0[\pi^*]$ or $F_0[\pi^*]$ and also efficiently decoded from theses graph structures . The proposed flow-graphs $F_0[\pi^*]$ and $F_0[\pi^*]$ enrich the repository of graphs which can encode the same watermark number $w$ and , thus , enable us to embed multiple copies of the same watermark $w$ into an application program $P$ . Moreover , the enrichment of that repository with new flow-graphs increases our ability to select a graph structure more similar to the structure of a given application program $P$ thereby enhancing the resilience of our codec system to attacks .
Independently developed codebases typically contain many segments of code that perform same or closely related operations ( semantic clones ) . Finding functionally equivalent segments enables applications like replacing a segment by a more efficient or more secure alternative . Such related segments often have different interfaces , so some glue code ( an adapter ) is needed to replace one with the other . We present an algorithm that searches for replaceable code segments at the function level by attempting to synthesize an adapter between them from some family of adapters ; it terminates if it finds no possible adapter . We implement our technique using ( 0 ) concrete adapter enumeration based on Intel ' s Pin framework ( 0 ) binary symbolic execution , and explore the relation between size of adapter search space and total search time . We present examples of applying adapter synthesis for improving security and efficiency of binary functions , deobfuscating binary functions , and switching between binary implementations of RC0 . We present two large-scale evaluations , ( 0 ) we run adapter synthesis on more than 00 , 000 function pairs from the Linux C library , ( 0 ) using more than 00 , 000 fragments of binary code extracted from a ARM image built for the iPod Nano 0g device and known functions from the VLC media player , we evaluate our adapter synthesis implementation on more than a million synthesis tasks . Our results confirm that several instances of adaptably equivalent binary functions exist in real-world code , and suggest that adapter synthesis can be applied for reverse engineering and for constructing cleaner , less buggy , more efficient programs .
Consider a stream of retweet events - how can we spot fraudulent lock-step behavior in such multi-aspect data ( i . e . , tensors ) evolving over time ? Can we detect it in real time , with an accuracy guarantee ? Past studies have shown that dense subtensors tend to indicate anomalous or even fraudulent behavior in many tensor data , including social media , Wikipedia , and TCP dumps . Thus , several algorithms have been proposed for detecting dense subtensors rapidly and accurately . However , existing algorithms assume that tensors are static , while many real-world tensors , including those mentioned above , evolve over time . We propose DenseStream , an incremental algorithm that maintains and updates a dense subtensor in a tensor stream ( i . e . , a sequence of changes in a tensor ) , and DenseAlert , an incremental algorithm spotting the sudden appearances of dense subtensors . Our algorithms are : ( 0 ) Fast and ' any time ' : updates by our algorithms are up to a million times faster than the fastest batch algorithms , ( 0 ) Provably accurate : our algorithms guarantee a lower bound on the density of the subtensor they maintain , and ( 0 ) Effective : our DenseAlert successfully spots anomalies in real-world tensors , especially those overlooked by existing algorithms .
In the multiple linear regression setting , we propose a general framework , termed weighted orthogonal components regression ( WOCR ) , which encompasses many known methods as special cases , including ridge regression and principal components regression . WOCR makes use of the monotonicity inherent in orthogonal components to parameterize the weight function . The formulation allows for efficient determination of tuning parameters and hence is computationally advantageous . Moreover , WOCR offers insights for deriving new better variants . Specifically , we advocate weighting components based on their correlations with the response , which leads to enhanced predictive performance . Both simulated studies and real data examples are provided to assess and illustrate the advantages of the proposed methods .
The search space of Bayesian Network structures is usually defined as Acyclic Directed Graphs ( DAGs ) and the search is done by local transformations of DAGs . But the space of Bayesian Networks is ordered by DAG Markov model inclusion and it is natural to consider that a good search policy should take this into account . First attempt to do this ( Chickering 0000 ) was using equivalence classes of DAGs instead of DAGs itself . This approach produces better results but it is significantly slower . We present a compromise between these two approaches . It uses DAGs to search the space in such a way that the ordering by inclusion is taken into account . This is achieved by repetitive usage of local moves within the equivalence class of DAGs . We show that this new approach produces better results than the original DAGs approach without substantial change in time complexity . We present empirical results , within the framework of heuristic search and Markov Chain Monte Carlo , provided through the Alarm dataset .
A growing body of literature attempts to learn about contagion using observational ( i . e . non-experimental ) data collected from a single social network . While the conclusions of these studies may be correct , the methods rely on assumptions that are likely--and sometimes guaranteed to be--false , and therefore the evidence for the conclusions is often weaker than it seems . Developing methods that do not need to rely on implausible assumptions is an incredibly challenging and important open problem in statistics . Appropriate methods don ' t ( yet ! ) exist , so researchers hoping to learn about contagion from observational social network data are sometimes faced with a dilemma : they can abandon their research program , or they can use inappropriate methods . This chapter will focus on the challenges and the open problems and will not weigh in on that dilemma , except to mention here that the most responsible way to use any statistical method , especially when it is well-known that the assumptions on which it rests do not hold , is with a healthy dose of skepticism , with honest acknowledgment and deep understanding of the limitations , and with copious caveats about how to interpret the results .
Several representations of the exact cdf of the sum of squares of n independent gamma-distributed random variables Xi are given , in particular by a series of gamma distribution functions . Using a characterization of the gamma distribution by Laha , an expansion of the exact distribution of the sample variance is derived by a Taylor series approach with the former distribution as its leading term . In particular for integer orders alpha some further series are provided , including a convex combination of gamma distributions for alpha = 0 and nearly of this type for alpha > 0 . Furthermore , some representations of the distribution of the angle Phi between ( X0 , . . . , Xn ) and ( 0 , . . . , 0 ) are given by orthogonal series . All these series are based on the same sequence of easily computed moments of cos ( Phi ) .
In his 0000 book , Aitchison explains that compositional data is regularly mishandled in statistical analyses , a pattern that continues to this day . The Dirichlet Type I distribution is a multivariate distribution commonly used to model a set of proportions that sum to one . Aitchinson goes on to lament the difficulties of Dirichlet modelling and the scarcity of alternatives . While he addresses the second of these issues , we address the first . The Dirichlet Type II distribution is a transformation of the Dirichlet Type I distribution and is a multivariate distribution on the positive real numbers with only one more parameter than the number of dimensions . This property of Dirichlet distributions implies advantages over common alternatives as the number of dimensions increase . While not all data is amenable to Dirichlet modelling , there are many cases where the Dirichlet family is the obvious choice . We describe the Dirichlet distributions and show how to fit them using both frequentist and Bayesian methods ( we derive and apply two objective priors ) . The Beta distribution is discussed as a special case . We report a small simulation study to compare the fitting methods . We derive the conditional distributions and posterior predictive conditional distributions . The flexibility of this distribution family is illustrated via examples , the last of which discusses imputation ( using the posterior predictive conditional distributions ) .
A celebrated result of Sch\ " utzenberger says that a language is star-free if and only if it is is recognized by a finite aperiodic monoid . We give a new proof for this theorem using local divisors .
Recently , several algorithms have been proposed for independent subspace analysis where hidden variables are i . i . d . processes . We show that these methods can be extended to certain AR , MA , ARMA and ARIMA tasks . Central to our paper is that we introduce a cascade of algorithms , which aims to solve these tasks without previous knowledge about the number and the dimensions of the hidden processes . Our claim is supported by numerical simulations . As a particular application , we search for subspaces of facial components .
Probabilistic Component Latent Analysis ( PLCA ) is a statistical modeling method for feature extraction from non-negative data . It has been fruitfully applied to various research fields of information retrieval . However , the EM-solved optimization problem coming with the parameter estimation of PLCA-based models has never been properly posed and justified . We then propose in this short paper to re-define the theoretical framework of this problem , with the motivation of making it clearer to understand , and more admissible for further developments of PLCA-based computational systems .
While Quantum Information Science ( QIS ) is still in its infancy , the ability for quantum based hardware or computers to communicate and integrate with their classical counterparts will be a major requirement towards their success . Little attention however has been paid to this aspect of QIS . To manage and exchange information between systems , today ' s classic Information Technology ( IT ) commonly uses the eXtensible Markup Language ( XML ) and its related tools . XML is composed of numerous specifications related to various fields of expertise . No such global specification however has been defined for quantum computers . QIS-XML is a proposed XML metadata specification for the description of fundamental components of QIS ( gates & circuits ) and a platform for the development of a hardware independent low level pseudo-code for quantum algorithms . This paper lays out the general characteristics of the QIS-XML specification and outlines practical applications through prototype use cases .
In this paper we present the use of Constraint Programming for solving balanced academic curriculum problems . We discuss the important role that heuristics play when solving a problem using a constraint-based approach . We also show how constraint solving techniques allow to very efficiently solve combinatorial optimization problems that are too hard for integer programming techniques .
The advantages of mixed approach with using different kinds of programming techniques for symbolic manipulation are discussed . The main purpose of approach offered is merge the methods of object oriented programming that convenient for presentation data and algorithms for user with advantages of functional languages for data manipulation , internal presentation , and portability of software .
Transductions are binary relations of finite words . For rational transductions , i . e . , transductions defined by finite transducers , the inclusion , equivalence and sequential uniformisation problems are known to be undecidable . In this paper , we investigate stronger variants of inclusion , equivalence and sequential uniformisation , based on a general notion of transducer resynchronisation , and show their decidability . We also investigate the classes of finite-valued rational transductions and deterministic rational transductions , which are known to have a decidable equivalence problem . We show that sequential uniformisation is also decidable for them .
As techniques for graph query processing mature , the need for optimization is increasingly becoming an imperative . Indices are one of the key ingredients toward efficient query processing strategies via cost-based optimization . Due to the apparent absence of a common representation model , it is difficult to make a focused effort toward developing access structures , metrics to evaluate query costs , and choose alternatives . In this context , recent interests in covering-based graph matching appears to be a promising direction of research . In this paper , our goal is to formally introduce a new graph representation model , called Minimum Hub Cover , and demonstrate that this representation offers interesting strategic advantages , facilitates construction of candidate graphs from graph fragments , and helps leverage indices in novel ways for query optimization . However , similar to other covering problems , minimum hub cover is NP-hard , and thus is a natural candidate for optimization . We claim that computing the minimum hub cover leads to substantial cost reduction for graph query processing . We present a computational characterization of minimum hub cover based on integer programming to substantiate our claim and investigate its computational cost on various graph types .
How many times have you tried to re-implement a past CAV tool paper , and failed ? Reliably reproducing published scientific discoveries has been acknowledged as a barrier to scientific progress for some time but there remains only a small subset of software available to support the specific needs of the research community ( i . e . beyond generic tools such as source code repositories ) . In this paper we propose an infrastructure for enabling reproducibility in our community , by automating the build , unit testing and benchmarking of research software .
In this paper we extend the work of Smith and Papamichail ( 0000 ) and present fast approximate Bayesian algorithms for learning in complex scenarios where at any time frame , the relationships between explanatory state space variables can be described by a Bayesian network that evolve dynamically over time and the observations taken are not necessarily Gaussian . It uses recent developments in approximate Bayesian forecasting methods in combination with more familiar Gaussian propagation algorithms on junction trees . The procedure for learning state parameters from data is given explicitly for common sampling distributions and the methodology is illustrated through a real application . The efficiency of the dynamic approximation is explored by using the Hellinger divergence measure and theoretical bounds for the efficacy of such a procedure are discussed .
We introduce a nominal actor-based language and study its expressive power . We have identified the presence/absence of fields as a crucial feature : the dynamic creation of names in combination with fields gives rise to Turing completeness . On the other hand , restricting to stateless actors gives rise to systems for which properties such as termination are decidable . This decidability result still holds for actors with states when the number of actors is bounded and the state is read-only .
The supertree construction problem is about combining several phylogenetic trees with possibly conflicting information into a single tree that has all the leaves of the source trees as its leaves and the relationships between the leaves are as consistent with the source trees as possible . This leads to an optimization problem that is computationally challenging and typically heuristic methods , such as matrix representation with parsimony ( MRP ) , are used . In this paper we consider the use of answer set programming to solve the supertree construction problem in terms of two alternative encodings . The first is based on an existing encoding of trees using substructures known as quartets , while the other novel encoding captures the relationships present in trees through direct projections . We use these encodings to compute a genus-level supertree for the family of cats ( Felidae ) . Furthermore , we compare our results to recent supertrees obtained by the MRP method .
We present an online visual analytics approach to helping users explore and understand hierarchical topic evolution in high-volume text streams . The key idea behind this approach is to identify representative topics in incoming documents and align them with the existing representative topics that they immediately follow ( in time ) . To this end , we learn a set of streaming tree cuts from topic trees based on user-selected focus nodes . A dynamic Bayesian network model has been developed to derive the tree cuts in the incoming topic trees to balance the fitness of each tree cut and the smoothness between adjacent tree cuts . By connecting the corresponding topics at different times , we are able to provide an overview of the evolving hierarchical topics . A sedimentation-based visualization has been designed to enable the interactive analysis of streaming text data from global patterns to local details . We evaluated our method on real-world datasets and the results are generally favorable .
The lasso and related sparsity inducing algorithms have been the target of substantial theoretical and applied research . Correspondingly , many results are known about their behavior for a fixed or optimally chosen tuning parameter specified up to unknown constants . In practice , however , this oracle tuning parameter is inaccessible so one must use the data to select one . Common statistical practice is to use a variant of cross-validation for this task . However , little is known about the theoretical properties of the resulting predictions with such data-dependent methods . We consider the high-dimensional setting with random design wherein the number of predictors $p$ grows with the number of observations $n$ . Under typical assumptions on the data generating process , similar to those in the literature , we recover oracle rates up to a log factor when choosing the tuning parameter with cross-validation . Under weaker conditions , when the true model is not necessarily linear , we show that the lasso remains risk consistent relative to its linear oracle . We also generalize these results to the group lasso and square-root lasso and investigate the predictive and model selection performance of cross-validation via simulation .
The steered response power phase transform ( SRP-PHAT ) is a beamformer method very attractive in acoustic localization applications due to its robustness in reverberant environments . This paper presents a spatial grid design procedure , called the geometrically sampled grid ( GSG ) , which aims at computing the spatial grid by taking into account the discrete sampling of time difference of arrival ( TDOA ) functions and the desired spatial resolution . A new SRP-PHAT localization algorithm based on the GSG method is also introduced . The proposed method exploits the intersections of the discrete hyperboloids representing the TDOA information domain of the sensor array , and projects the whole TDOA information on the space search grid . The GSG method thus allows to design the sampled spatial grid which represents the best search grid for a given sensor array , it allows to perform a sensitivity analysis of the array and to characterize its spatial localization accuracy , and it may assist the system designer in the reconfiguration of the array . Experimental results using both simulated data and real recordings show that the localization accuracy is substantially improved both for high and for low spatial resolution , and that it is closely related to the proposed power response sensitivity measure .
Consider estimation of the regression function based on a model with equidistant design and measurement errors generated from a fractional Gaussian noise process . In previous literature , this model has been heuristically linked to an experiment , where the anti-derivative of the regression function is continuously observed under additive perturbation by a fractional Brownian motion . Based on a reformulation of the problem using reproducing kernel Hilbert spaces , we derive abstract approximation conditions on function spaces under which asymptotic equivalence between these models can be established and show that the conditions are satisfied for certain Sobolev balls exceeding some minimal smoothness . Furthermore , we construct a sequence space representation and provide necessary conditions for asymptotic equivalence to hold .
The level set tree approach of Hartigan ( 0000 ) provides a probabilistically based and highly interpretable encoding of the clustering behavior of a dataset . By representing the hierarchy of data modes as a dendrogram of the level sets of a density estimator , this approach offers many advantages for exploratory analysis and clustering , especially for complex and high-dimensional data . Several R packages exist for level set tree estimation , but their practical usefulness is limited by computational inefficiency , absence of interactive graphical capabilities and , from a theoretical perspective , reliance on asymptotic approximations . To make it easier for practitioners to capture the advantages of level set trees , we have written the Python package DeBaCl for DEnsity-BAsed CLustering . In this article we illustrate how DeBaCl ' s level set tree estimates can be used for difficult clustering tasks and interactive graphical data analysis . The package is intended to promote the practical use of level set trees through improvements in computational efficiency and a high degree of user customization . In addition , the flexible algorithms implemented in DeBaCl enjoy finite sample accuracy , as demonstrated in recent literature on density clustering . Finally , we show the level set tree framework can be easily extended to deal with functional data .
Contours may be viewed as the 0D outline of the image of an object . This type of data arises in medical imaging as well as in computer vision and can be modeled as data on a manifold and can be studied using statistical shape analysis . Practically speaking , each observed contour , while theoretically infinite dimensional , must be discretized for computations . As such , the coordinates for each contour as obtained at k sampling times , resulting in the contour being represented as a k-dimensional complex vector . While choosing large values of k will result in closer approximations to the original contour , this will also result in higher computational costs in the subsequent analysis . The goal of this study is to determine reasonable values for k so as to keep the computational cost low while maintaining accuracy . To do this , we consider two methods for selecting sample points and determine lower bounds for k for obtaining a desired level of approximation error using two different criteria . Because this process is computationally inefficient to perform on a large scale , we then develop models for predicting the lower bounds for k based on simple characteristics of the contours .
A profile likelihood ratio test is proposed for inferences on the index coefficients in generalized single-index models . Key features include its simplicity in implementation , invariance against parametrization , and exhibiting substantially less bias than standard Wald-tests in finite-sample settings . Moreover , the R routine to carry out the profile likelihood ratio test is demonstrated to be over two orders of magnitude faster than the recently proposed generalized likelihood ratio test based on kernel regression . The advantages of the method are demonstrated on various simulations and a data analysis example .
Verification bias is a well known problem when the predictive ability of a diagnostic test has to be evaluated . In this paper , we discuss how to assess the accuracy of continuous-scale diagnostic tests in the presence of verification bias , when a three-class disease status is considered . In particular , we propose a fully nonparametric verification bias-corrected estimator of the ROC surface . Our approach is based on nearest-neighbor imputation and adopts generic smooth regression models for both the disease and the verification processes . Consistency and asymptotic normality of the proposed estimator are proved and its finite sample behavior is investigated by means of several Monte Carlo simulation studies . Variance estimation is also discussed and an illustrative example is presented .
The ability to recognize students weakness and solve any problem that may confront them in timely fashion is always a target for all educational institutions . Thus , colleges and universities implement the so-called academic advising affairs . On the academic advisor relies the responsibility of solving any problem that may confront students learning progress . This paper shows how the adviser can benefit from data mining techniques , namely decision trees techniques . The C 0 . 0 algorithm is used as a method for building such trees . The output is evaluated based on the accuracy measure , Kappa measure , and ROC area . The difference between the registered and gained credit hours is considered as the main attribute on which advisor can rely
The Large Hadron Collider ( LHC ) is one of the most complex machines ever build . It is composed of many components which constitute a large system . The tunnel and the accelerator is just one of a very critical fraction of the whole LHC infrastructure . Hardware comissioning as one of the critical processes before running the LHC is implemented during the Long Shutdown ( LS ) states of the macine , where Electrical Quality Assurance ( ELQA ) is one of its key components . Here a huge data is collected when implementing various ELQA electrical tests . In this paper we present a conceptual framework for supporting a rapid design of web applications for ELQA data analysis . We show a framework ' s main components , their possible integration with other systems and machine learning algorithms and a simple use case of prototyping an application for Electrical Quality Assurance of the LHC .
Recently , Recurrent Neural Networks ( RNNs ) have been applied to the task of session-based recommendation . These approaches use RNNs to predict the next item in a user session based on the previ- ously visited items . While some approaches consider additional item properties , we argue that item dwell time can be used as an implicit measure of user interest to improve session-based item recommen- dations . We propose an extension to existing RNN approaches that captures user dwell time in addition to the visited items and show that recommendation performance can be improved . Additionally , we investigate the usefulness of a single validation split for model selection in the case of minor improvements and find that in our case the best model is not selected and a fold-like study with different validation sets is necessary to ensure the selection of the best model .
The sampling of sound fields involves the measurement of spatially dependent room impulse responses , where the Nyquist-Shannon sampling theorem applies in both the temporal and spatial domain . Therefore , sampling inside a volume of interest requires a huge number of sampling points in space , which comes along with further difficulties such as exact microphone positioning and calibration of multiple microphones . In this paper , we present a method for measuring sound fields using moving microphones whose trajectories are known to the algorithm . At that , the number of microphones is customizable by trading measurement effort against sampling time . Through spatial interpolation of the dynamic measurements , a system of linear equations is set up which allows for the reconstruction of the entire sound field inside the volume of interest .
We show that problems which have finite integer index and satisfy a requirement we call treewidth-bounding admit linear kernels on the class of $H$-topological-minor free graphs , for an arbitrary fixed graph $H$ . This builds on earlier results by Fomin et al . \ on linear kernels for $H$-minor-free graphs and by Bodlaender et al . \ on graphs of bounded genus . Our framework encompasses several problems , the prominent ones being Chordal Vertex Deletion , Feedback Vertex Set and Edge Dominating Set .
We consider the problem of nonparametric estimation of a convex regression function $\phi_0$ . We study the risk of the least squares estimator ( LSE ) under the natural squared error loss . We show that the risk is always bounded from above by $n^{-0/0}$ modulo logarithmic factors while being much smaller when $\phi_0$ is well-approximable by a piecewise affine convex function with not too many affine pieces ( in which case , the risk is at most $0/n$ up to logarithmic factors ) . On the other hand , when $\phi_0$ has curvature , we show that no estimator can have risk smaller than a constant multiple of $n^{-0/0}$ in a very strong sense by proving a " local " minimax lower bound . We also study the case of model misspecification where we show that the LSE exhibits the same global behavior provided the loss is measured from the closest convex projection of the true regression function . In the process of deriving our risk bounds , we prove new results for the metric entropy of local neighborhoods of the space of univariate convex functions . These results , which may be of independent interest , demonstrate the non-uniform nature of the space of univariate convex functions in sharp contrast to classical function spaces based on smoothness constraints .
Analysis of data related to software development helps to increase quality , control and predictability of software development processes and products . However , collecting such data for is a complex task . A non-invasive collection of software metrics is one of the most promising approaches to solve the task . In this paper we present an approach which consists of four parts : collect the data , store all collected data , unify the stored data and analyze the data to provide insights to the user about software product or process . We employ the approach to the development of an architecture for non-invasive software measurement system and explain its advantages and limitations .
In this paper , the asymptotic distributions of estimators for the regularized functional canonical correlation and variates of the population are derived . The method is based on the possibility of expressing these regularized quantities as the maximum eigenvalue and the corresponding eigenfunctions of an associated pair of regularized operators , similar to the Euclidean case . The known weak convergence of the sample covariance operator , coupled with a delta-method for analytic functions of covariance operators , yields the weak convergence of the pair of associated operators . From the latter weak convergence , the limiting distributions of the canonical quantities of interest can be derived with the help of some further perturbation theory .
The most popular tool used in the industry for monitoring a process is the Shewhart control chart . The major disadvantage of the Shewhart control chart is that it is not very efficient in detecting small process average shifts . To increase the sensitivity of Shewhart control charts to small shifts additional supplementary runs rules has been suggested . In this paper we introduce and study the modified r/m control chart which has an improved sensitivity to small and moderate process average shifts as compared with the standard Shewhart X-bar control chart and corresponding control charts proposed recently in the literature .
Scientific fraud is an increasingly vexing problem . Many current programs for fraud detection focus on image manipulation , while techniques for detection based on anomalous patterns that may be discoverable in the underlying numerical data get much less attention , even though these techniques are often easy to apply . We employed three such techniques in a case study in which we considered data sets from several hundred experiments . We compared patterns in the data sets from one research teaching specialist ( RTS ) , to those of 0 other members of the same laboratory and from 0 outside laboratories . Application of two conventional statistical tests and a newly developed test for anomalous patterns in the triplicate data commonly produced in such research to various data sets reported by the RTS resulted in repeated rejection of the hypotheses ( often at p-levels well below 0 . 000 ) that anomalous patterns in his data may have occurred by chance . This analysis emphasizes the importance of access to raw data that form the bases of publications , reports and grant applications in order to evaluate the correctness of the conclusions , as well as the utility of methods for detecting anomalous , especially fabricated , numerical results .
Combinatorial filters have been the subject of increasing interest from the robotics community in recent years . This paper considers automatic reduction of combinatorial filters to a given size , even if that reduction necessitates changes to the filter ' s behavior . We introduce an algorithmic problem called improper filter reduction , in which the input is a combinatorial filter F along with an integer k representing the target size . The output is another combinatorial filter F ' with at most k states , such that the difference in behavior between F and F ' is minimal . We present two metrics for measuring the distance between pairs of filters , describe dynamic programming algorithms for computing these distances , and show that improper filter reduction is NP-hard under these metrics . We then describe two heuristic algorithms for improper filter reduction , one greedy sequential approach , and one randomized global approach based on prior work on weighted improper graph coloring . We have implemented these algorithms and analyze the results of three sets of experiments .
In this paper , a novel and generic multi-objective design paradigm is proposed which utilizes quantum-behaved PSO ( QPSO ) for deciding the optimal configuration of the LQR controller for a given problem considering a set of competing objectives . There are three main contributions introduced in this paper as follows . ( 0 ) The standard QPSO algorithm is reinforced with an informed initialization scheme based on the simulated annealing algorithm and Gaussian neighborhood selection mechanism . ( 0 ) It is also augmented with a local search strategy which integrates the advantages of memetic algorithm into conventional QPSO . ( 0 ) An aggregated dynamic weighting criterion is introduced that dynamically combines the soft and hard constraints with control objectives to provide the designer with a set of Pareto optimal solutions and lets her to decide the target solution based on practical preferences . The proposed method is compared against a gradient-based method , seven meta-heuristics , and the trial-and-error method on two control benchmarks using sensitivity analysis and full factorial parameter selection and the results are validated using one-tailed T-test . The experimental results suggest that the proposed method outperforms opponent methods in terms of controller effort , measures associated with transient response and criteria related to steady-state .
In this paper , we study the Edgeworth expansion for a pre-averaging estimator of quadratic variation in the framework of continuous diffusion models observed with noise . More specifically , we obtain a second order expansion for the joint density of the estimators of quadratic variation and its asymptotic variance . Our approach is based on martingale embedding , Malliavin calculus and stable central limit theorems for continuous diffusions . Moreover , we derive the density expansion for the studentized statistic , which might be applied to construct asymptotic confidence regions .
Citations between scientific papers and related bibliometric indices , such as the $h$-index for authors and the impact factor for journals , are being increasingly used - often in controversial ways - as quantitative tools for research evaluation . Yet , a fundamental research question remains still open : to which extent do quantitative metrics capture the significance of scientific works ? We analyze the network of citations among the $000 , 000$ papers published by the American Physical Society ( APS ) journals between 0000 and 0000 , and focus on the comparison of metrics built on the citation count with network-based metrics . We contrast five article-level metrics with respect to the rankings that they assign to a set of fundamental papers , called Milestone Letters , carefully selected by the APS editors for " making long-lived contributions to physics , either by announcing significant discoveries , or by initiating new areas of research " . A new metric , which combines PageRank centrality with the explicit requirement that paper score is not biased by paper age , is the best-performing metric overall in identifying the Milestone Letters . The lack of time bias in the new metric makes it also possible to use it to compare papers of different age on the same scale . We find that network-based metrics identify the Milestone Letters better than metrics based on the citation count , which suggests that the structure of the citation network contains information that can be used to improve the ranking of scientific publications . The methods and results presented here are relevant for all evolving systems where network centrality metrics are applied , for example the World Wide Web and online social networks . An interactive Web platform where it is possible to view the ranking of the APS papers by rescaled PageRank is available at the address \url{http : //www . sciencenow . info} .
Recently , multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas . This paper introduces several principles for multi-view representation learning : correlation , consensus , and complementarity principles . Consequently , we first review the representative methods and theories of multi-view representation learning based on correlation principle , especially on canonical correlation analysis ( CCA ) and its several extensions . Then from the viewpoint of consensus and complementarity principles we investigate the advancement of multi-view representation learning that ranges from shallow methods including multi-modal topic learning , multi-view sparse coding , and multi-view latent space Markov networks , to deep methods including multi-modal restricted Boltzmann machines , multi-modal autoencoders , and multi-modal recurrent neural networks . Further , we also provide an important perspective from manifold alignment for multi-view representation learning . Overall , this survey aims to provide an insightful overview of theoretical basis and state-of-the-art developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications .
A great video title describes the most salient event compactly and captures the viewer ' s attention . In contrast , video captioning tends to generate sentences that describe the video as a whole . Although generating a video title automatically is a very useful task , it is much less addressed than video captioning . We address video title generation for the first time by proposing two methods that extend state-of-the-art video captioners to this new task . First , we make video captioners highlight sensitive by priming them with a highlight detector . Our framework allows for jointly training a model for title generation and video highlight localization . Second , we induce high sentence diversity in video captioners , so that the generated titles are also diverse and catchy . This means that a large number of sentences might be required to learn the sentence structure of titles . Hence , we propose a novel sentence augmentation method to train a captioner with additional sentence-only examples that come without corresponding videos . We collected a large-scale Video Titles in the Wild ( VTW ) dataset of 00000 automatically crawled user-generated videos and titles . On VTW , our methods consistently improve title prediction accuracy , and achieve the best performance in both automatic and human evaluation . Finally , our sentence augmentation method also outperforms the baselines on the M-VAD dataset .
We introduce a novel and inexpensive approach for the temporal alignment of speech to highly imperfect transcripts from automatic speech recognition ( ASR ) . Transcripts are generated for extended lecture and presentation videos , which in some cases feature more than 00 speakers with different accents , resulting in highly varying transcription qualities . In our approach we detect a subset of phonemes in the speech track , and align them to the sequence of phonemes extracted from the transcript . We report on the results for 0 speech-transcript sets ranging from 00 to 000 minutes . The alignment performance is promising , showing a correct matching of phonemes within 00 , 00 , 00 second error margins for more than 00% , 00% , 00% of text , respectively , on average .
Ensembles of artificial neural networks show improved generalization capabilities that outperform those of single networks . However , for aggregation to be effective , the individual networks must be as accurate and diverse as possible . An important problem is , then , how to tune the aggregate members in order to have an optimal compromise between these two conflicting conditions . We present here an extensive evaluation of several algorithms for ensemble construction , including new proposals and comparing them with standard methods in the literature . We also discuss a potential problem with sequential aggregation algorithms : the non-frequent but damaging selection through their heuristics of particularly bad ensemble members . We introduce modified algorithms that cope with this problem by allowing individual weighting of aggregate members . Our algorithms and their weighted modifications are favorably tested against other methods in the literature , producing a sensible improvement in performance on most of the standard statistical databases used as benchmarks .
Real world systems typically feature a variety of different dependency types and topologies that complicate model selection for probabilistic graphical models . We introduce the ensemble-of-forests model , a generalization of the ensemble-of-trees model . Our model enables structure learning of Markov random fields ( MRF ) with multiple connected components and arbitrary potentials . We present two approximate inference techniques for this model and demonstrate their performance on synthetic data . Our results suggest that the ensemble-of-forests approach can accurately recover sparse , possibly disconnected MRF topologies , even in presence of non-Gaussian dependencies and/or low sample size . We applied the ensemble-of-forests model to learn the structure of perturbed signaling networks of immune cells and found that these frequently exhibit non-Gaussian dependencies with disconnected MRF topologies . In summary , we expect that the ensemble-of-forests model will enable MRF structure learning in other high dimensional real world settings that are governed by non-trivial dependencies .
The adoption of automated , data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups . In this context , a number of recent studies have focused on defining , detecting , and removing unfairness from data-driven decision systems . However , the existing notions of fairness , based on parity ( equality ) in treatment or outcomes for different social groups , tend to be quite stringent , limiting the overall decision making accuracy . In this paper , we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness -- given the choice between various sets of decision treatments or outcomes , any group of users would collectively prefer its treatment or outcomes , regardless of the ( dis ) parity as compared to the other groups . Then , we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness . Finally , we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness .
This text presents the cognitive-ergonomics approach to design , in both its individual and collective form . It focuses on collective design with respect to individual design . The theoretical framework adopted is that of information processing , specified for design problems . The cognitive characteristics of design problems are presented : the effects of their ill-defined character and of the different types of representation implemented in solving these problems , amongst others the more or less " satisficing " character of the different possible solutions . The text first describes the cognitive activities implemented in both individual and collective design : different types of control activities and of the executive activities of solution development and evaluation . Specific collective-design characteristics are then presented : co-design and distributed-design activities , temporo-operative and cognitive synchronisation , and different types of argumentation , of co-designers ' intervention modes in the design process , of solution-proposals evaluation . The paper concludes by a confrontation between the two types of design , individual and collective .
We propose a method to optimize the representation and distinguishability of samples from two probability distributions , by maximizing the estimated power of a statistical test based on the maximum mean discrepancy ( MMD ) . This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks ( GAN ) , in which a model attempts to generate realistic samples , and a discriminator attempts to tell these apart from data samples . In this context , the MMD may be used in two roles : first , as a discriminator , either directly on the samples , or on features of the samples . Second , the MMD can be used to evaluate the performance of a generative model , by testing the model ' s samples against a reference data set . In the latter role , the optimized MMD is particularly helpful , as it gives an interpretable indication of how the model and data distributions differ , even in cases where individual model samples are not easily distinguished either by eye or by classifier .
Nowadays , the need for system interoperability in or across enterprises has become more and more ubiquitous . Lots of research works have been carried out in the information exchange , transformation , discovery and reuse . One of the main challenges in these researches is to overcome the semantic heterogeneity between enterprise applications along the lifecycle of a product . As a possible solution to assist the semantic interoperability , semantic annotation has gained more and more attentions and is widely used in different domains . In this paper , based on the investigation of the context and the related works , we identify some existing drawbacks and propose a formal semantic annotation approach to support the semantics enrichment of models in a PLM environment .
Analyses of array-valued datasets often involve reduced-rank array approximations , typically obtained via least-squares or truncations of array decompositions . However , least-squares approximations tend to be noisy in high-dimensional settings , and may not be appropriate for arrays that include discrete or ordinal measurements . This article develops methodology to obtain low-rank model-based representations of continuous , discrete and ordinal data arrays . The model is based on a parameterization of the mean array as a multilinear product of a reduced-rank core array and a set of index-specific orthogonal eigenvector matrices . It is shown how orthogonally equivariant parameter estimates can be obtained from Bayesian procedures under invariant prior distributions . Additionally , priors on the core array are developed that act as regularizers , leading to improved inference over the standard least-squares estimator , and providing robustness to misspecification of the array rank . This model-based approach is extended to accommodate discrete or ordinal data arrays using a semiparametric transformation model . The resulting low-rank representation is scale-free , in the sense that it is invariant to monotonic transformations of the data array . In an example analysis of a multivariate discrete network dataset , this scale-free approach provides a more complete description of data patterns .
Small distributed systems are limited by their main memory to generate massively large graphs . Trivial extension to current graph generators to utilize external memory leads to large amount of random I/O hence do not scale with size . In this work we offer a technique to generate massive scale graphs on small cluster of compute nodes with limited main memory . We develop several distributed and external memory algorithms , primarily , shuffle , relabel , redistribute , and , compressed-sparse-row ( csr ) convert . The algorithms are implemented in MPI/pthread model to help parallelize the operations across multicores within each core . Using our scheme it is feasible to generate a graph of size $0^{00}$ nodes ( scale 00 ) using only 00 compute nodes . This can be compared with the current scheme would require at least 0000 compute node , assuming 00GB of main memory . Our work has broader implications for external memory graph libraries such as STXXL and graph processing on SSD-based supercomputers such as Dash and Gordon [0][0] .
Previous research on automatic pain estimation from facial expressions has focused primarily on " one-size-fits-all " metrics ( such as PSPI ) . In this work , we focus on directly estimating each individual ' s self-reported visual-analog scale ( VAS ) pain metric , as this is considered the gold standard for pain measurement . The VAS pain score is highly subjective and context-dependent , and its range can vary significantly among different persons . To tackle these issues , we propose a novel two-stage personalized model , named DeepFaceLIFT , for automatic estimation of VAS . This model is based on ( 0 ) Neural Network and ( 0 ) Gaussian process regression models , and is used to personalize the estimation of self-reported pain via a set of hand-crafted personal features and multi-task learning . We show on the benchmark dataset for pain analysis ( The UNBC-McMaster Shoulder Pain Expression Archive ) that the proposed personalized model largely outperforms the traditional , unpersonalized models : the intra-class correlation improves from a baseline performance of 00\% to a personalized performance of 00\% while also providing confidence in the model\textquotesingle s estimates -- in contrast to existing models for the target task . Additionally , DeepFaceLIFT automatically discovers the pain-relevant facial regions for each person , allowing for an easy interpretation of the pain-related facial cues .
Consider a decision maker who is responsible to dynamically collect observations so as to enhance his information about an underlying phenomena of interest in a speedy manner while accounting for the penalty of wrong declaration . Due to the sequential nature of the problem , the decision maker relies on his current information state to adaptively select the most ``informative ' ' sensing action among the available ones . In this paper , using results in dynamic programming , lower bounds for the optimal total cost are established . The lower bounds characterize the fundamental limits on the maximum achievable information acquisition rate and the optimal reliability . Moreover , upper bounds are obtained via an analysis of two heuristic policies for dynamic selection of actions . It is shown that the first proposed heuristic achieves asymptotic optimality , where the notion of asymptotic optimality , due to Chernoff , implies that the relative difference between the total cost achieved by the proposed policy and the optimal total cost approaches zero as the penalty of wrong declaration ( hence the number of collected samples ) increases . The second heuristic is shown to achieve asymptotic optimality only in a limited setting such as the problem of a noisy dynamic search . However , by considering the dependency on the number of hypotheses , under a technical condition , this second heuristic is shown to achieve a nonzero information acquisition rate , establishing a lower bound for the maximum achievable rate and error exponent . In the case of a noisy dynamic search with size-independent noise , the obtained nonzero rate and error exponent are shown to be maximum .
We study the regret of optimal strategies for online convex optimization games . Using von Neumann ' s minimax theorem , we show that the optimal regret in this adversarial setting is closely related to the behavior of the empirical minimization algorithm in a stochastic process setting : it is equal to the maximum , over joint distributions of the adversary ' s action sequence , of the difference between a sum of minimal expected losses and the minimal empirical loss . We show that the optimal regret has a natural geometric interpretation , since it can be viewed as the gap in Jensen ' s inequality for a concave functional--the minimizer over the player ' s actions of expected loss--defined on a set of probability distributions . We use this expression to obtain upper and lower bounds on the regret of an optimal strategy for a variety of online learning problems . Our method provides upper bounds without the need to construct a learning algorithm ; the lower bounds provide explicit optimal strategies for the adversary .
Peter J . Huber was born on March 00 , 0000 , in Wohlen , a small town in the Swiss countryside . He obtained a diploma in mathematics in 0000 and a Ph . D . in mathematics in 0000 , both from ETH Zurich . His thesis was in pure mathematics , but he then decided to go into statistics . He spent 0000--0000 as a postdoc at the statistics department in Berkeley where he wrote his first and most famous paper on robust statistics , ``Robust Estimation of a Location Parameter . ' ' After a position as a visiting professor at Cornell University , he became a full professor at ETH Zurich . He worked at ETH until 0000 , interspersed by visiting positions at Cornell , Yale , Princeton and Harvard . After leaving ETH , he held professor positions at Harvard University 0000--0000 , at MIT 0000--0000 , and finally at the University of Bayreuth from 0000 until his retirement in 0000 . He now lives in Klosters , a village in the Grisons in the Swiss Alps . Peter Huber has published four books and over 00 papers on statistics and data analysis . In addition , he has written more than a dozen papers and two books on Babylonian mathematics , astronomy and history . In 0000 , he delivered the Wald lectures . He is a fellow of the IMS , of the American Association for the Advancement of Science , and of the American Academy of Arts and Sciences . In 0000 he received a Humboldt Award and in 0000 an honorary doctorate from the University of Neuch\^{a}tel . In addition to his fundamental results in robust statistics , Peter Huber made important contributions to computational statistics , strategies in data analysis , and applications of statistics in fields such as crystallography , EEGs , and human growth curves .
We consider the well-posedness of Bayesian inverse problems when the prior measure has exponential tails . In particular , we consider the class of convex ( log-concave ) probability measures which include the Gaussian and Besov measures as well as certain classes of hierarchical priors . We identify appropriate conditions on the likelihood distribution and the prior measure which guarantee existence , uniqueness and stability of the posterior measure with respect to perturbations of the data . We also consider consistent approximations of the posterior such as discretization by projection . Finally , we present a general recipe for construction of convex priors on Banach spaces which will be of interest in practical applications where one often works with spaces such as $L^0$ or the continuous functions .
In contrast to electronic computation , chemical computation is noisy and susceptible to a variety of sources of error , which has prevented the construction of robust complex systems . To be effective , chemical algorithms must be designed with an appropriate error model in mind . Here we consider the model of chemical reaction networks that preserve molecular count ( population protocols ) , and ask whether computation can be made robust to a natural model of unintended " leak " reactions . Our definition of leak is motivated by both the particular spurious behavior seen when implementing chemical reaction networks with DNA strand displacement cascades , as well as the unavoidable side reactions in any implementation due to the basic laws of chemistry . We develop a new " Robust Detection " algorithm for the problem of fast ( logarithmic time ) single molecule detection , and prove that it is robust to this general model of leaks . Besides potential applications in single molecule detection , the error-correction ideas developed here might enable a new class of robust-by-design chemical algorithms . Our analysis is based on a non-standard hybrid argument , combining ideas from discrete analysis of population protocols with classic Markov chain techniques .
In this article , we study classes of multidimensional subshifts defined by multihead finite automata , in particular the hierarchy of classes of subshifts defined as the number of heads grows . The hierarchy collapses on the third level , where all co-recursively enumerable subshifts are obtained in every dimension . We also compare these classes to SFTs and sofic shifts . We are unable to separate the second and third level of the hierarchy in one and two dimensions , and suggest a related open problem for two-counter machines .
We design a randomised parallel version of Adaboost based on previous studies on parallel coordinate descent . The algorithm uses the fact that the logarithm of the exponential loss is a function with coordinate-wise Lipschitz continuous gradient , in order to define the step lengths . We provide the proof of convergence for this randomised Adaboost algorithm and a theoretical parallelisation speedup factor . We finally provide numerical examples on learning problems of various sizes that show that the algorithm is competitive with concurrent approaches , especially for large scale problems .
The next generation of virtual environments for training is oriented towards collaborative aspects . Therefore , we have decided to enhance our platform for virtual training environments , adding collaboration opportunities and integrating humanoids . In this paper we put forward a model of humanoid that suits both virtual humans and representations of real users , according to collaborative training activities . We suggest adaptations to the scenario model of our platform making it possible to write collaborative procedures . We introduce a mechanism of action selection made up of a global repartition and an individual choice . These models are currently being integrated and validated in GVT , a virtual training tool for maintenance of military equipments , developed in collaboration with the French company NEXTER-Group .
In imperfect-information games , the optimal strategy in a subgame may depend on the strategy in other , unreached subgames . Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole , unlike perfect-information games . Nevertheless , it is possible to first approximate a solution for the whole game and then improve it by solving individual subgames . This is referred to as subgame solving . We introduce subgame-solving techniques that outperform prior methods both in theory and practice . We also show how to adapt them , and past subgame-solving techniques , to respond to opponent actions that are outside the original action abstraction ; this significantly outperforms the prior state-of-the-art approach , action translation . Finally , we show that subgame solving can be repeated as the game progresses down the game tree , leading to far lower exploitability . These techniques were a key component of Libratus , the first AI to defeat top humans in heads-up no-limit Texas hold ' em poker .
Stein [Statist . Sci . 0 ( 0000 ) 000--000] proposed the Mat\ ' {e}rn-type Gaussian random fields as a very flexible class of models for computer experiments . This article considers a subclass of these models that are exactly once mean square differentiable . In particular , the likelihood function is determined in closed form , and under mild conditions the sieve maximum likelihood estimators for the parameters of the covariance function are shown to be weakly consistent with respect to fixed-domain asymptotics .
Quantitative games are two-player zero-sum games played on directed weighted graphs . Total-payoff games ( that can be seen as a refinement of the well-studied mean-payoff games ) are the variant where the payoff of a play is computed as the sum of the weights . Our aim is to describe the first pseudo-polynomial time algorithm for total-payoff games in the presence of arbitrary weights . It consists of a non-trivial application of the value iteration paradigm . Indeed , it requires to study , as a milestone , a refinement of these games , called min-cost reachability games , where we add a reachability objective to one of the players . For these games , we give an efficient value iteration algorithm to compute the values and optimal strategies ( when they exist ) , that runs in pseudo-polynomial time . We also propose heuristics allowing one to possibly speed up the computations in both cases .
After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data . Thus , we get an answer to the question what is the most likely label of a given unseen data point . However , most methods will provide no answer why the model predicted the particular label for a single instance and what features were most influential for that particular instance . The only method that is currently able to provide such explanations are decision trees . This paper proposes a procedure which ( based on a set of assumptions ) allows to explain the decisions of any classification method .
Researchers often calculate ratios of measured quantities . Specifying confidence limits for ratios is difficult and the appropriate methods are often unknown . Appropriate methods are described ( Fieller , Taylor , special bootstrap methods ) . For the Fieller method a simple geometrical interpretation is given . Monte Carlo simulations show when these methods are appropriate and that the most frequently used methods ( index method and zero-variance method ) can lead to large liberal deviations from the desired confidence level . It is discussed when we can use standard regression or measurement error models and when we have to resort to specific models for heteroscedastic data . Finally , an old warning is repeated that we should be aware of the problems of spurious correlations if we use ratios .
Given uncertainties in physical theory and numerical climate simulations , the historical temperature record is often used as a source of empirical information about climate change . Many historical trend analyses appear to deemphasize physical and statistical assumptions : examples include regression models that treat time rather than radiative forcing as the relevant covariate and time series methods that account for internal variability nonparametrically . However , given a limited record and the presence of internal variability , estimating radiatively forced historical temperature trends necessarily requires assumptions . Ostensibly empirical methods can involve an inherent conflict in assumptions : they require data records that are short enough for naive trend models to apply but long enough for internal variability to be accounted for . In the context of global mean temperatures , methods that deemphasize assumptions can therefore produce misleading inferences , because the twentieth century trend is complex and the scale of correlation is long relative to the data length . We illustrate how a simple but physically motivated trend model can provide better-fitting and more broadly applicable trend estimates and can address a wider array of questions . The model allows one to distinguish , within a single framework , between uncertainties in the shorter-term versus longer-term response to radiative forcing , with implications not only on historical trends but also on uncertainties in future projections . We also investigate the consequence on inferred uncertainties of the choice of a statistical description of internal variability . While nonparametric methods may seem to avoid making explicit assumptions , we demonstrate how even misspecified parametric methods , if attuned to important characteristics of internal variability , can result in more accurate statements about trend uncertainty .
As air traffic grows significantly , aircraft accidents increase . Many aviation accidents could be prevented if the precise aircraft positions and weather conditions on the aircraft ' s route were known . Existing studies propose determining the precise aircraft positions via a VHF channel with an air-to-air radio relay system that is based on mobile ad-hoc networks . However , due to the long propagation delay , the existing TDMA MAC schemes underutilize the networks . The existing TDMA MAC sends data and receives ACK in one time slot , which requires two guard times in one time slot . Since aeronautical communications spans a significant distance , the guard time occupies a significantly large portion of the slot . To solve this problem , we propose a piggybacking mechanism ACK . Our proposed MAC has one guard time in one time slot , which enables the transmission of more data . Using this additional data , we can send weather conditions that pertain to the aircraft ' s current position . Our analysis shows that this proposed MAC performs better than the existing MAC , since it offers better throughput and network utilization . In addition , our weather condition notification model achieves a much lower transmission delay than a HF ( high frequency ) voice communication .
Average consensus algorithms can be implemented over wireless sensor networks ( WSN ) , where global statistics can be computed using communications among sensor nodes locally . Simple execution , robustness to global topology changes due to frequent node failures and underlying distributed philosophy has made consensus algorithms more suitable to WSNs . Since these algorithms are iterative in nature , their performance is characterized by convergence speed . We study the convergence of the average consensus algorithms for WSNs using regular graphs . We obtained the analytical expressions for optimal consensus and convergence parameters which decides the convergence time for r-nearest neighbor cycle and torus networks . We have also derived the generalized expression for optimal consensus and convergence parameters for m-dimensional r-nearest neighbor torus networks . The obtained analytical results agree with the simulation results and shown the effect of network dimension , number of nodes and transmission radius on convergence time . This work provides the basic analytical tools for managing and controlling the performance of average consensus algorithm in the finite sized practical networks .
The MISE Project ( Mediation Information System Engineering ) aims at providing collaborating organizations with a Mediation Information System ( MIS ) in charge of supporting interoperability of a collaborative network . MISE proposes an overall MIS design method according to a model-driven approach , based on model transformations . This MIS is in charge of managing ( i ) information , ( ii ) functions and ( iii ) processes among the information systems ( IS ) of partner organizations involved in the network . Semantic issues are accompanying this triple objective : How to deal with information reconciliation ? How to ensure the matching between business functions and technical services ? How to identify workflows among business processes ? This article aims first , at presenting the MISE approach , second at defining the semantic gaps along the MISE approach and third at describing some past , current and future research works that deal with these issues . Finally and as a conclusion , the very " design-oriented " previous considerations are confronted with " run-time " requirements .
Alzheimer ' s disease is the most common cause of dementia , yet hard to diagnose precisely without invasive techniques , particularly at the onset of the disease . This work approaches image analysis and classification of synthetic multispectral images composed by diffusion-weighted magnetic resonance ( MR ) cerebral images for the evaluation of cerebrospinal fluid area and measuring the advance of Alzheimer ' s disease . A clinical 0 . 0 T MR imaging system was used to acquire all images presented . The classification methods are based on multilayer perceptrons and Kohonen Self-Organized Map classifiers . We assume the classes of interest can be separated by hyperquadrics . Therefore , a 0-degree polynomial network is used to classify the original image , generating the ground truth image . The classification results are used to improve the usual analysis of the apparent diffusion coefficient map .
In this paper , we study two classic optimization problems : minimum geometric dominating set and set cover . Both the problems have been studied for different types of objects for a long time . These problems become APX-hard when the objects are axis-parallel rectangles , ellipses , $\alpha$-fat objects of constant description complexity , and convex polygons . On the other hand , PTAS ( polynomial time approximation scheme ) is known for them when the objects are disks or unit squares . Surprisingly , PTAS was unknown even for arbitrary squares . For homothetic set of convex objects , an $O ( k^0 ) $ approximation algorithm is known for dominating set problem , where $k$ is the number of corners in a convex object . On the other hand , QPTAS ( quasi polynomial time approximation scheme ) is known very recently for the covering problem when the objects are pseudodisks . For both problems obtaining a PTAS remains open for a large class of objects . For the dominating set problems , we prove that the popular local search algorithm leads to an $ ( 0+\varepsilon ) $ approximation when objects are homothetic set of convex objects ( which includes arbitrary squares , $k$-regular polygons , translated and scaled copies of a convex set etc . ) in $n^{O ( 0/\varepsilon^0 ) }$ time . On the other hand , the same technique leads to a PTAS for geometric covering problem when the objects are convex pseudodisks ( which includes disks , unit height rectangles , homothetic convex objects etc . ) . As a consequence , we obtain an easy to implement approximation algorithm for both problems for a large class of objects , significantly improving the best known approximation guarantees .
While significant progress has been made separately on analytics systems for scalable stochastic gradient descent ( SGD ) and private SGD , none of the major scalable analytics frameworks have incorporated differentially private SGD . There are two inter-related issues for this disconnect between research and practice : ( 0 ) low model accuracy due to added noise to guarantee privacy , and ( 0 ) high development and runtime overhead of the private algorithms . This paper takes a first step to remedy this disconnect and proposes a private SGD algorithm to address \emph{both} issues in an integrated manner . In contrast to the white-box approach adopted by previous work , we revisit and use the classical technique of {\em output perturbation} to devise a novel " bolt-on " approach to private SGD . While our approach trivially addresses ( 0 ) , it makes ( 0 ) even more challenging . We address this challenge by providing a novel analysis of the $L_0$-sensitivity of SGD , which allows , under the same privacy guarantees , better convergence of SGD when only a constant number of passes can be made over the data . We integrate our algorithm , as well as other state-of-the-art differentially private SGD , into Bismarck , a popular scalable SGD-based analytics system on top of an RDBMS . Extensive experiments show that our algorithm can be easily integrated , incurs virtually no overhead , scales well , and most importantly , yields substantially better ( up to 0X ) test accuracy than the state-of-the-art algorithms on many real datasets .
Mobile streaming video data accounts for a large and increasing percentage of wireless network traffic . The available bandwidths of modern wireless networks are often unstable , leading to difficulties in delivering smooth , high-quality video . Streaming service providers such as Netflix and YouTube attempt to adapt their systems to adjust in response to these bandwidth limitations by changing the video bitrate or , failing that , allowing playback interruptions ( rebuffering ) . Being able to predict end user ' quality of experience ( QoE ) resulting from these adjustments could lead to perceptually-driven network resource allocation strategies that would deliver streaming content of higher quality to clients , while being cost effective for providers . Existing objective QoE models only consider the effects on user QoE of video quality changes or playback interruptions . For streaming applications , adaptive network strategies may involve a combination of dynamic bitrate allocation along with playback interruptions when the available bandwidth reaches a very low value . Towards effectively predicting user QoE , we propose Video Assessment of TemporaL Artifacts and Stalls ( Video ATLAS ) : a machine learning framework where we combine a number of QoE-related features , including objective quality features , rebuffering-aware features and memory-driven features to make QoE predictions . We evaluated our learning-based QoE prediction model on the recently designed LIVE-Netflix Video QoE Database which consists of practical playout patterns , where the videos are afflicted by both quality changes and rebuffering events , and found that it provides improved performance over state-of-the-art video quality metrics while generalizing well on different datasets . The proposed algorithm is made publicly available at http : //live . ece . utexas . edu/research/Quality/VideoATLAS release_v0 . rar .
In this paper , we address the problem of reconstructing coverage maps from path-loss measurements in cellular networks . We propose and evaluate two kernel-based adaptive online algorithms as an alternative to typical offline methods . The proposed algorithms are application-tailored extensions of powerful iterative methods such as the adaptive projected subgradient method and a state-of-the-art adaptive multikernel method . Assuming that the moving trajectories of users are available , it is shown how side information can be incorporated in the algorithms to improve their convergence performance and the quality of the estimation . The complexity is significantly reduced by imposing sparsity-awareness in the sense that the algorithms exploit the compressibility of the measurement data to reduce the amount of data which is saved and processed . Finally , we present extensive simulations based on realistic data to show that our algorithms provide fast , robust estimates of coverage maps in real-world scenarios . Envisioned applications include path-loss prediction along trajectories of mobile users as a building block for anticipatory buffering or traffic offloading .
We perform a fundamental investigation of the complexity of conjunctive query evaluation from the perspective of parameterized complexity . We classify sets of boolean conjunctive queries according to the complexity of this problem . Previous work showed that a set of conjunctive queries is fixed-parameter tractable precisely when the set is equivalent to a set of queries having bounded treewidth . We present a fine classification of query sets up to parameterized logarithmic space reduction . We show that , in the bounded treewidth regime , there are three complexity degrees and that the properties that determine the degree of a query set are bounded pathwidth and bounded tree depth . We also engage in a study of the two higher degrees via logarithmic space machine characterizations and complete problems . Our work yields a significantly richer perspective on the complexity of conjunctive queries and , at the same time , suggests new avenues of research in parameterized complexity .
This article investigates selfish behavior in games where players are embedded in a social context . A framework is presented which allows us to measure the Windfall of Friendship , i . e . , how much players benefit ( compared to purely selfish environments ) if they care about the welfare of their friends in the social network graph . As a case study , a virus inoculation game is examined . We analyze the corresponding Nash equilibria and show that the Windfall of Friendship can never be negative . However , we find that if the valuation of a friend is independent of the total number of friends , the social welfare may not increase monotonically with the extent to which players care for each other ; intriguingly , in the corresponding scenario where the relative importance of a friend declines , the Windfall is monotonic again . This article also studies convergence of best-response sequences . It turns out that in social networks , convergence times are typically higher and hence constitute a price of friendship . While such phenomena may be known on an anecdotal level , our framework allows us to quantify these effects analytically . Our formal insights on the worst case equilibria are complemented by simulations shedding light onto the structure of other equilibria .
Modern platform-based design involves the application-specific extension of embedded processors to fit customer requirements . To accomplish this task , the possibilities offered by recent custom/extensible processors for tuning their instruction set and microarchitecture to the applications of interest have to be exploited . A significant factor often determining the success of this process is the utomation available in application analysis and custom instruction generation . In this paper we present YARDstick , a design automation tool for custom processor development flows that focuses on generating and evaluating application-specific hardware extensions . YARDstick is a building block for ASIP development , integrating application analysis , custom instruction generation and selection with user-defined compiler intermediate representations . In a YARDstick-enabled environment , practical issues in traditional ASIP design are confronted efficiently ; the exploration infrastructure is liberated from compiler and simulator idiosyncrasies , since the ASIP designer is empowered with the freedom of specifying the target architectures of choice and adding new implementations of analyses and custom instruction generation/selection methods . To illustrate the capabilities of the YARDstick approach , we present interesting exploration scenarios : quantifying the effect of machine-dependent compiler optimizations and the selection of the target architecture in terms of operation set and memory model on custom instruction generation/selection under different input/output constraints .
Despite significant progress made over the past twenty five years , unconstrained face verification remains a challenging problem . This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding learned using triplet probability constraints to solve the unconstrained face verification problem . Aside from yielding performance improvements , this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering . Experiments on the challenging IJB-A dataset show that the proposed algorithm performs comparably or better than the state of the art methods in verification and identification metrics , while requiring much less training data and training time . The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to extreme pose variation . Furthermore , we demonstrate the robustness of the deep features to challenges including age , pose , blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets .
This paper poses a few fundamental questions regarding the attributes of the volume profile of a Limit Order Books stochastic structure by taking into consideration aspects of intraday and interday statistical features , the impact of different exchange features and the impact of market participants in different asset sectors . This paper aims to address the following questions : 0 . Is there statistical evidence that heavy-tailed sub-exponential volume profiles occur at different levels of the Limit Order Book on the bid and ask and if so does this happen on intra or interday time scales ? 0 . In futures exchanges , are heavy tail features exchange ( CBOT , CME , EUREX , SGX and COMEX ) or asset class ( government bonds , equities and precious metals ) dependent and do they happen on ultra-high ( <0sec ) or mid-range ( 0sec -00min ) high frequency data ? 0 . Does the presence of stochastic heavy-tailed volume profile features evolve in a manner that would inform or be indicative of market participant behaviors , such as high frequency algorithmic trading , quote stuffing and price discovery intra-daily ? 0 . Is there statistical evidence for a need to consider dynamic behavior of the parameters of models for Limit Order Book volume profiles on an intra-daily time scale ? Progress on aspects of each question is obtained via statistically rigorous results to verify the empirical findings for an unprecedentedly large set of futures market LOB data . The data comprises several exchanges , several futures asset classes and all trading days of 0000 , using market depth ( Type II ) order book data to 0 levels on the bid and ask .
Musical chords , harmonies or melodies in Just Intonation have note frequencies which are described by a base frequency multiplied by rational numbers . For any local section , these notes can be converted to some base frequency multiplied by whole positive numbers . The structure of the chord can be analysed mathematically by finding functions which are unchanged upon chord transposition . These functions are are denoted invariant , and are important for understanding the structure of harmony . Each chord described by whole numbers has a greatest common divisor , GCD , and a lowest common multiple , LCM . The ratio of these is denoted Complexity which is a positive whole number . The set of divisors of Complexity give a subset of a p limit tone lattice and have both a natural ordering and a multiplicative structure . The position and orientation of the original chord , on the ordered set or on the lattice , give rise to many other invariant functions including measures for otonality and utonality . Other invariant functions can be constructed from : ratios between note pairs , prime projections , weighted chords which incorporate loudness . Given a set of conditions described by invariant functions , algorithms can be developed to find all scales or chords meeting those conditions , allowing the classification of consonant harmonies up to specified limits .
There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples . We introduce a simple modification for autoencoder neural networks that yields powerful generative models . Our method masks the autoencoder ' s parameters to respect autoregressive constraints : each input is reconstructed only from previous inputs in a given ordering . Constrained this way , the autoencoder outputs can be interpreted as a set of conditional probabilities , and their product , the full joint probability . We can also train a single network that can decompose the joint probability in multiple different orderings . Our simple framework can be applied to multiple architectures , including deep ones . Vectorized implementations , such as on GPUs , are simple and fast . Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators . At test time , the method is significantly faster and scales better than other autoregressive estimators .
Transparency , user trust , and human comprehension are popular ethical motivations for interpretable machine learning . In support of these goals , researchers evaluate model explanation performance using humans and real world applications . This alone presents a challenge in many areas of artificial intelligence . In this position paper , we propose a distinction between descriptive and persuasive explanations . We discuss reasoning suggesting that functional interpretability may be correlated with cognitive function and user preferences . If this is indeed the case , evaluation and optimization using functional metrics could perpetuate implicit cognitive bias in explanations that threaten transparency . Finally , we propose two potential research directions to disambiguate cognitive function and explanation models , retaining control over the tradeoff between accuracy and interpretability .
This paper describes a novel method to approximate the polynomial coefficients of regression functions , with particular interest on multi-dimensional classification . The derivation is simple , and offers a fast , robust classification technique that is resistant to over-fitting .
We report on an extended robot control application of a contact-less and airborne ultrasonic tactile display ( AUTD ) stimulus-based brain-computer interface ( BCI ) paradigm , which received last year The Annual BCI Research Award 0000 . In the award winning human communication augmentation paradigm the six palm positions are used to evoke somatosensory brain responses , in order to define a novel contactless tactile BCI . An example application of a small robot management is also presented in which the users control a small robot online .
Directed acyclic graph ( DAG ) models , also called Bayesian networks , impose conditional independence constraints on a multivariate probability distribution , and are widely used in probabilistic reasoning , machine learning and causal inference . If latent variables are included in such a model , then the set of possible marginal distributions over the remaining ( observed ) variables is generally complex , and not represented by any DAG . Larger classes of mixed graphical models , which use multiple edge types , have been introduced to overcome this ; however , these classes do not represent all the models which can arise as margins of DAGs . In this paper we show that this is because ordinary mixed graphs are fundamentally insufficiently rich to capture the variety of marginal models . We introduce a new class of hyper-graphs , called mDAGs , and a latent projection operation to obtain an mDAG from the margin of a DAG . We show that each distinct marginal of a DAG model is represented by at least one mDAG , and provide graphical results towards characterizing when two such marginal models are the same . Finally we show that mDAGs correctly capture the marginal structure of causally-interpreted DAGs under interventions on the observed variables .
Translating potential disease biomarkers between multi-species ' omics ' experiments is a new direction in biomedical research . The existing methods are limited to simple experimental setups such as basic healthy-diseased comparisons . Most of these methods also require an a priori matching of the variables ( e . g . , genes or metabolites ) between the species . However , many experiments have a complicated multi-way experimental design often involving irregularly-sampled time-series measurements , and for instance metabolites do not always have known matchings between organisms . We introduce a Bayesian modelling framework for translating between multiple species the results from ' omics ' experiments having a complex multi-way , time-series experimental design . The underlying assumption is that the unknown matching can be inferred from the response of the variables to multiple covariates including time .
We present the tensor computer algebra package xTras , which provides functions and methods frequently needed when doing ( classical ) field theory . Amongst others , it can compute contractions , make Ans\ " atze , and solve tensorial equations . It is built upon the tensor computer algebra system xAct , a collection of packages for Mathematica .
In retail , there are predictable yet dramatic time-dependent patterns in customer behavior , such as periodic changes in the number of visitors , or increases in visitors just before major holidays . The current paradigm of multi-armed bandit analysis does not take these known patterns into account . This means that for applications in retail , where prices are fixed for periods of time , current bandit algorithms will not suffice . This work provides a remedy that takes the time-dependent patterns into account , and we show how this remedy is implemented in the UCB and {\epsilon}-greedy methods and we introduce a new policy called the variable arm pool method . In the corrected methods , exploitation ( greed ) is regulated over time , so that more exploitation occurs during higher reward periods , and more exploration occurs in periods of low reward . In order to understand why regret is reduced with the corrected methods , we present a set of bounds that provide insight into why we would want to exploit during periods of high reward , and discuss the impact on regret . Our proposed methods perform well in experiments , and were inspired by a high-scoring entry in the Exploration and Exploitation 0 contest using data from Yahoo ! Front Page . That entry heavily used time-series methods to regulate greed over time , which was substantially more effective than other contextual bandit methods .
We propose a new technique that boosts the convergence of training generative adversarial networks . Generally , the rate of training deep models reduces severely after multiple iterations . A key reason for this phenomenon is that a deep network is expressed using a highly non-convex finite-dimensional model , and thus the parameter gets stuck in a local optimum . Because of this , methods often suffer not only from degeneration of the convergence speed but also from limitations in the representational power of the trained network . To overcome this issue , we propose an additional layer called the gradient layer to seek a descent direction in an infinite-dimensional space . Because the layer is constructed in the infinite-dimensional space , we are not restricted by the specific model structure of finite-dimensional models . As a result , we can get out of the local optima in finite-dimensional models and move towards the global optimal function more directly . In this paper , this phenomenon is explained from the functional gradient method perspective of the gradient layer . Interestingly , the optimization procedure using the gradient layer naturally constructs the deep structure of the network . Moreover , we demonstrate that this procedure can be regarded as a discretization method of the gradient flow that naturally reduces the objective function . Finally , the method is tested using several numerical experiments , which show its fast convergence .
Constraint Handling Rules ( CHR ) have provided a realistic solution to an over-arching problem in many fields that deal with constraint logic programming : how to combine recursive functions or relations with constraints while avoiding non-termination problems . This paper focuses on some other benefits that CHR , specifically their implementation in SICStus Prolog , have provided to computational linguists working on grammar design tools . CHR rules are applied by means of a subsumption check and this check is made only when their variables are instantiated or bound . The former functionality is at best difficult to simulate using more primitive coroutining statements such as SICStus when/0 , and the latter simply did not exist in any form before CHR . For the sake of providing a case study in how these can be applied to grammar development , we consider the Attribute Logic Engine ( ALE ) , a Prolog preprocessor for logic programming with typed feature structures , and its extension to a complete grammar development system for Head-driven Phrase Structure Grammar ( HPSG ) , a popular constraint-based linguistic theory that uses typed feature structures . In this context , CHR can be used not only to extend the constraint language of feature structure descriptions to include relations in a declarative way , but also to provide support for constraints with complex antecedents and constraints on the co-occurrence of feature values that are necessary to interpret the type system of HPSG properly .
We study the robustness properties of $\ell_0$ norm minimization for the classical linear regression problem with a given design matrix and contamination restricted to the dependent variable . We perform a fine error analysis of the $\ell_0$ estimator for measurements errors consisting of outliers coupled with noise . We introduce a new estimation technique resulting from a regularization of $\ell_0$ minimization by inf-convolution with the $\ell_0$ norm . Concerning robustness to large outliers , the proposed estimator keeps the breakdown point of the $\ell_0$ estimator , and reduces to least squares when there are not outliers . We present a globally convergent forward-backward algorithm for computing our estimator and some numerical experiments confirming its theoretical properties .
A major challenge facing existing sequential Monte-Carlo methods for parameter estimation in physics stems from the inability of existing approaches to robustly deal with experiments that have different mechanisms that yield the results with equivalent probability . We address this problem here by proposing a form of particle filtering that clusters the particles that comprise the sequential Monte-Carlo approximation to the posterior before applying a resampler . Through a new graphical approach to thinking about such models , we are able to devise an artificial-intelligence based strategy that automatically learns the shape and number of the clusters in the support of the posterior . We demonstrate the power of our approach by applying it to randomized gap estimation and a form of low circuit-depth phase estimation where existing methods from the physics literature either exhibit much worse performance or even fail completely .
In this paper , we investigate the effectiveness of two distinct techniques ( Special Moment Approach & Spatial Frequency Approach ) for reviewing the lifelogs , which were collected by lifeloggers who were willing to use a wearable camera and a bracelet simultaneously for two days . Generally , Special moment approach is a technique for extracting episodic events and Spatial frequency approach is a technique for associating visual with temporal and location information , especially heat map is applied as the spatial data for expressing frequency awareness . Based on that , the participants were asked to fill in two post-study questionnaires for evaluating the effectiveness of those two techniques and their combination . The preliminary result showed the positive potential of exploring individual lifelogs using our approaches .
Stochastic Gradient Descent ( SGD ) is an important algorithm in machine learning . With constant learning rates , it is a stochastic process that , after an initial phase of convergence , generates samples from a stationary distribution . We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling . Specifically , we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior . This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior . ( This is in the spirit of variational inference . ) In more detail , we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters . This theoretical framework also connects SGD to modern scalable inference algorithms ; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective . We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models .
The proliferation of Internet-enabled devices and services has led to a shifting balance between digital and analogue aspects of our everyday lives . In the face of this development there is a growing demand for the study of privacy hazards , the potential for unique user de-anonymization and information leakage between the various social media profiles many of us maintain . To enable the structured study of such adversarial effects , this paper presents a dedicated dataset of cross-platform social network personas ( i . e . , the same person has accounts on multiple platforms ) . The corpus comprises 000 users who generate predominantly English content . Each user object contains the online footprint of the same person in three distinct social networks : Twitter , Instagram and Foursquare . In total , it encompasses over 0 . 0M tweets , 000k check-ins and 00k Instagram posts . We describe the collection methodology , characteristics of the dataset , and how to obtain it . Finally , we discuss a common use case , cross-platform user identification .
Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models . While naturally cast as a combinatorial optimization problem , variable or feature selection admits a convex relaxation through the regularization by the $\ell_0$-norm . In this paper , we consider situations where we are not only interested in sparsity , but where some structural prior knowledge is available as well . We show that the $\ell_0$-norm can then be extended to structured norms built on either disjoint or overlapping groups of variables , leading to a flexible framework that can deal with various structures . We present applications to unsupervised learning , for structured sparse principal component analysis and hierarchical dictionary learning , and to supervised learning in the context of non-linear variable selection .
Both harmonic and binaural signal properties are relevant for auditory processing . To investigate how these cues combine in the auditory system , detection thresholds for an 000-Hz tone masked by a diotic ( i . e . , identical between the ears ) harmonic complex tone were measured in six normal-hearing subjects . The target tone was presented either diotically or with an interaural phase difference ( IPD ) of 000 degree and in either harmonic or " mistuned " relationship to the diotic masker . Three different maskers were used , a resolved and an unresolved complex tone ( fundamental frequency : 000 and 00 Hz ) with four components below and above the target frequency and a broadband unresolved complex tone with 00 additional components . The target IPD provided release from masking in most masker conditions , whereas mistuning led to a significant release from masking only in the diotic conditions with the resolved and the narrowband unresolved maskers . A significant effect of mistuning was neither found in the diotic condition with the wideband unresolved masker nor in any of the dichotic conditions . An auditory model with a single analysis frequency band and different binaural processing schemes was employed to predict the data of the unresolved masker conditions . Sensitivity to modulation cues was achieved by including an auditory-motivated modulation filter in the processing pathway . The predictions of the diotic data were in line with the experimental results and literature data in the narrowband condition , but not in the broadband condition , suggesting that across-frequency processing is involved in processing modulation information . The experimental and model results in the dichotic conditions show that the binaural processor cannot exploit modulation information in binaurally unmasked conditions .
We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters . Using the Dirichlet process as the building block , our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels . The proposed model possesses properties that link the nested Dirichlet processes ( nDP ) and the Dirichlet process mixture models ( DPM ) in an interesting way : integrating out all contents results in the DPM over contexts , whereas integrating out group-specific contexts results in the nDP mixture over content variables . We provide a Polya-urn view of the model and an efficient collapsed Gibbs inference procedure . Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains .
This paper presents a technique called Improved Squeaky Wheel Optimisation for driver scheduling problems . It improves the original Squeaky Wheel Optimisations effectiveness and execution speed by incorporating two additional steps of Selection and Mutation which implement evolution within a single solution . In the ISWO , a cycle of Analysis-Selection-Mutation-Prioritization-Construction continues until stopping conditions are reached . The Analysis step first computes the fitness of a current solution to identify troublesome components . The Selection step then discards these troublesome components probabilistically by using the fitness measure , and the Mutation step follows to further discard a small number of components at random . After the above steps , an input solution becomes partial and thus the resulting partial solution needs to be repaired . The repair is carried out by using the Prioritization step to first produce priorities that determine an order by which the following Construction step then schedules the remaining components . Therefore , the optimisation in the ISWO is achieved by solution disruption , iterative improvement and an iterative constructive repair process performed . Encouraging experimental results are reported .
We derive simple concentration inequalities for bounded random vectors , which generalize Hoeffding ' s inequalities for bounded scalar random variables . As applications , we apply the general results to multinomial and Dirichlet distributions to obtain multivariate concentration inequalities .
Latent variable models are a fundamental modeling tool in machine learning applications , but they present significant computational and analytical challenges . The popular EM algorithm and its variants , is a much used algorithmic tool ; yet our rigorous understanding of its performance is highly incomplete . Recently , work in Balakrishnan et al . ( 0000 ) has demonstrated that for an important class of problems , EM exhibits linear local convergence . In the high-dimensional setting , however , the M-step may not be well defined . We address precisely this setting through a unified treatment using regularization . While regularization for high-dimensional problems is by now well understood , the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure ( e . g . , sparsity or low-rank ) . In particular , regularizing the M-step using the state-of-the-art high-dimensional prescriptions ( e . g . , Wainwright ( 0000 ) ) is not guaranteed to provide this balance . Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors . We specialize our general framework to sparse gaussian mixture models , high-dimensional mixed regression , and regression with missing variables , obtaining statistical guarantees for each of these examples .
In this paper we present a method for automatically generating optimal robot trajectories satisfying high level mission specifications . The motion of the robot in the environment is modeled as a general transition system , enhanced with weighted transitions . The mission is specified by a general linear temporal logic formula . In addition , we require that an optimizing proposition must be repeatedly satisfied . The cost function that we seek to minimize is the maximum time between satisfying instances of the optimizing proposition . For every environment model , and for every formula , our method computes a robot trajectory which minimizes the cost function . The problem is motivated by applications in robotic monitoring and data gathering . In this setting , the optimizing proposition is satisfied at all locations where data can be uploaded , and the entire formula specifies a complex ( and infinite horizon ) data collection mission . Our method utilizes B\ " uchi automata to produce an automaton ( which can be thought of as a graph ) whose runs satisfy the temporal logic specification . We then present a graph algorithm which computes a path corresponding to the optimal robot trajectory . We also present an implementation for a robot performing a data gathering mission in a road network .
The traditional architecture for a DBMS engine has the recovery , concurrency control and access method code tightly bound together in a storage engine for records . We propose a different approach , where the storage engine is factored into two layers ( each of which might have multiple heterogeneous instances ) . A Transactional Component ( TC ) works at a logical level only : it knows about transactions and their " logical " concurrency control and undo/redo recovery , but it does not know about page layout , B-trees etc . A Data Component ( DC ) knows about the physical storage structure . It supports a record oriented interface that provides atomic operations , but it does not know about transactions . Providing atomic record operations may itself involve DC-local concurrency control and recovery , which can be implemented using system transactions . The interaction of the mechanisms in TC and DC leads to multi-level redo ( unlike the repeat history paradigm for redo in integrated engines ) . This refactoring of the system architecture could allow easier deployment of application-specific physical structures and may also be helpful to exploit multi-core hardware . Particularly promising is its potential to enable flexible transactions in cloud database deployments . We describe the necessary principles for unbundled recovery , and discuss implementation issues .
Over the past decade the University of North Texas Libraries ( UNTL ) has developed a sizable digital library infrastructure for use in carrying out its core mission to the students , faculty , staff and associated communities of the university . This repository of content offers countless research possibilities for end users across the Internet when it is discovered and used in research , scholarship , entertainment , and lifelong learning . The characteristics of the repository itself provide insight into the workings of a modern digital library infrastructure , how it was created , how often it is updated , or how often it is modified . In that vein , the authors created a dataset comprised of information extracted from the UNT Libraries ' archival repository Coda and analyzed this dataset in order to demonstrate the value and insights that can be gained from sharing repository characteristics more broadly . This case study presents the findings from an analysis of this dataset .
The purpose of this study is to find a theoretically grounded , practically applicable and useful granularity level of an algorithmically constructed publication-level classification of research publications ( ACPLC ) . The level addressed is the level of research topics . The methodology we propose uses synthesis papers and their reference articles to construct a baseline classification . A dataset of about 00 million publications , and their mutual citations relations , is used to obtain several ACPLCs of different granularity . Each ACPLC is compared to the baseline classification and the best performing ACPLC is identified . The results of two case studies show that the topics of the cases are closely associated with different classes of the identified ACPLC , and that these classes tend to treat only one topic . Further , the class size variation is moderate , and only a small proportion of the publications belong to very small classes . For these reasons , we conclude that the proposed methodology is suitable to determine the topic granularity level of an ACPLC and that the ACPLC identified by this methodology is useful for bibliometric analyses .
We describe a simple method that produces automatically closed forms for the coefficients of continued fractions expansions of a large number of special functions . The function is specified by a non-linear differential equation and initial conditions . This is used to generate the first few coefficients and from there a conjectured formula . This formula is then proved automatically thanks to a linear recurrence satisfied by some remainder terms . Extensive experiments show that this simple approach and its straightforward generalization to difference and $q$-difference equations capture a large part of the formulas in the literature on continued fractions .
We explore whether useful temporal neural generative models can be learned from sequential data without back-propagation through time . We investigate the viability of a more neurocognitively-grounded approach in the context of unsupervised generative modeling of sequences . Specifically , we build on the concept of predictive coding , which has gained influence in cognitive science , in a neural framework . To do so we develop a novel architecture , the Temporal Neural Coding Network , and its learning algorithm , Discrepancy Reduction . The underlying directed generative model is fully recurrent , meaning that it employs structural feedback connections and temporal feedback connections , yielding information propagation cycles that create local learning signals . This facilitates a unified bottom-up and top-down approach for information transfer inside the architecture . Our proposed algorithm shows promise on the bouncing balls generative modeling problem . Further experiments could be conducted to explore the strengths and weaknesses of our approach .
Our winning submission to the 0000 Kaggle competition for Large Scale Hierarchical Text Classification ( LSHTC ) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes . The base-classifiers consist of hierarchically smoothed models combining document , label , and hierarchy level Multinomials , with feature pre-processing using variants of TF-IDF and BM00 . Additional diversification is introduced by different types of folds and random search optimization for different measures . The ensemble algorithm optimizes macroFscore by predicting the documents for each label , instead of the usual prediction of labels per document . Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking . The number of documents per label is chosen using label priors and thresholding of vote scores . This document describes the models and software used to build our solution . Reproducing the results for our solution can be done by running the scripts included in the Kaggle package . A package omitting precomputed result files is also distributed . All code is open source , released under GNU GPL 0 . 0 , and GPL 0 . 0 for Weka and Meka dependencies .
Doctors use statistics to advance medical knowledge ; we use a medical analogy to introduce statistical inference " from scratch " and to highlight an improvement . Your doctor , perhaps implicitly , predicts the effectiveness of a treatment for you based on its performance in a clinical trial ; the trial patients serve as controls for you . The same logic underpins statistical inference : to identify the best statistical procedure to use for a problem , we simulate a set of control problems and evaluate candidate procedures on the controls . Now for the improvement : recent interest in personalized/individualized medicine stems from the recognition that some clinical trial patients are better controls for you than others . Therefore , treatment decisions for you should depend only on a subset of relevant patients . Individualized statistical inference implements this idea for control problems ( rather than patients ) . Its potential for improving data analysis matches personalized medicine ' s for improving healthcare . The central issue--for both individualized medicine and individualized inference--is how to make the right relevance robustness trade-off : if we exercise too much judgement in determining which controls are relevant , our inferences will not be robust . How much is too much ? We argue that the unknown answer is the Holy Grail of statistical inference .
The importance of a node in a directed graph can be measured by its PageRank . The PageRank of a node is used in a number of application contexts - including ranking websites - and can be interpreted as the average portion of time spent at the node by an infinite random walk . We consider the problem of maximizing the PageRank of a node by selecting some of the edges from a set of edges that are under our control . By applying results from Markov decision theory , we show that an optimal solution to this problem can be found in polynomial time . Our core solution results in a linear programming formulation , but we also provide an alternative greedy algorithm , a variant of policy iteration , which runs in polynomial time , as well . Finally , we show that , under the slight modification for which we are given mutually exclusive pairs of edges , the problem of PageRank optimization becomes NP-hard .
The asymptotic theory of various estimators based on Gaussian likelihood has been developed for the unit root and near unit root cases of a first-order moving average model . Previous studies of the MA ( 0 ) unit root problem rely on the special autocovariance structure of the MA ( 0 ) process , in which case , the eigenvalues and eigenvectors of the covariance matrix of the data vector have known analytical forms . In this paper , we take a different approach to first consider the joint likelihood by including an augmented initial value as a parameter and then recover the exact likelihood by integrating out the initial value . This approach by-passes the difficulty of computing an explicit decomposition of the covariance matrix and can be used to study unit root behavior in moving averages beyond first order . The asymptotics of the generalized likelihood ratio ( GLR ) statistic for testing unit roots are also studied . The GLR test has operating characteristics that are competitive with the locally best invariant unbiased ( LBIU ) test of Tanaka for some local alternatives and dominates for all other alternatives .
Localization is a key requirement for mobile robot autonomy and human-robot interaction . Vision-based localization is accurate and flexible , however , it incurs a high computational burden which limits its application on many resource-constrained platforms . In this paper , we address the problem of performing real-time localization in large-scale 0D point cloud maps of ever-growing size . While most systems using multi-modal information reduce localization time by employing side-channel information in a coarse manner ( eg . WiFi for a rough prior position estimate ) , we propose to inter-weave the map with rich sensory data . This multi-modal approach achieves two key goals simultaneously . First , it enables us to harness additional sensory data to localise against a map covering a vast area in real-time ; and secondly , it also allows us to roughly localise devices which are not equipped with a camera . The key to our approach is a localization policy based on a sequential Monte Carlo estimator . The localiser uses this policy to attempt point-matching only in nodes where it is likely to succeed , significantly increasing the efficiency of the localization process . The proposed multi-modal localization system is evaluated extensively in a large museum building . The results show that our multi-modal approach not only increases the localization accuracy but significantly reduces computational time .
Internet of Things ( IoT ) applications typically collect and analyse personal data that can be used to derive sensitive information about individuals . However , thus far , privacy concerns have not been explicitly considered in software engineering processes when designing IoT applications . In this paper , we explore how a Privacy-by-Design ( PbD ) framework , formulated as a set of guidelines , can help software engineers to design privacy-aware IoT applications . We studied the utility of our proposed PbD framework by studying how software engineers use it to design IoT applications . This user study highlighted the benefits of providing a framework that helps software engineers explicitly consider privacy for IoT applications and also surfaced a number of challenges associated with our approach .
The influence of agents heterogeneity on the microscopic characteristics of pedestrian flow is studied via an evacuation simulation tool based on the Floor-Field model . The heterogeneity is introduced in agents velocity , aggressiveness , and sensitivity to occupation . The simulation results are compared to data gathered during an original experiment . The comparison shows that the heterogeneity in aggressiveness and sensitivity occupation enables to reproduce some microscopic aspects . The heterogeneity in velocity seems to be redundant .
We propose a game theoretic framework for task allocation in mobile cloud computing that corresponds to offloading of compute tasks to a group of nearby mobile devices . Specifically , in our framework , a distributor node holds a multidimensional auction for allocating the tasks of a job among nearby mobile nodes based on their computational capabilities and also the cost of computation at these nodes , with the goal of reducing the overall job completion time . Our proposed auction also has the desired incentive compatibility property that ensures that mobile devices truthfully reveal their capabilities and costs and that those devices benefit from the task allocation . To deal with node mobility , we perform multiple auctions over adaptive time intervals . We develop a heuristic approach to dynamically find the best time intervals between auctions to minimize unnecessary auctions and the accompanying overheads . We evaluate our framework and methods using both real world and synthetic mobility traces . Our evaluation results show that our game theoretic framework improves the job completion time by a factor of 0-0 in comparison to the time taken for executing the job locally , while minimizing the number of auctions and the accompanying overheads . Our approach is also profitable for the nearby nodes that execute the distributor ' s tasks with these nodes receiving a compensation higher than their actual costs .
In this paper , a novel technique for tight outer-approximation of the intersection region of a finite number of ellipses in 0-dimensional ( 0D ) space is proposed . First , the vertices of a tight polygon that contains the convex intersection of the ellipses are found in an efficient manner . To do so , the intersection points of the ellipses that fall on the boundary of the intersection region are determined , and a set of points is generated on the elliptic arcs connecting every two neighbouring intersection points . By finding the tangent lines to the ellipses at the extended set of points , a set of half-planes is obtained , whose intersection forms a polygon . To find the polygon more efficiently , the points are given an order and the intersection of the half-planes corresponding to every two neighbouring points is calculated . If the polygon is convex and bounded , these calculated points together with the initially obtained intersection points will form its vertices . If the polygon is non-convex or unbounded , we can detect this situation and then generate additional discrete points only on the elliptical arc segment causing the issue , and restart the algorithm to obtain a bounded and convex polygon . Finally , the smallest area ellipse that contains the vertices of the polygon is obtained by solving a convex optimization problem . Through numerical experiments , it is illustrated that the proposed technique returns a tighter outer-approximation of the intersection of multiple ellipses , compared to conventional techniques , with only slightly higher computational cost .
We investigate the sensitivity with which the temperature and the chemical potential characterizing quantum gases can be measured . We calculate the corresponding quantum Fisher information matrices for both fermionic and bosonic gases . For the latter , particular attention is devoted to the situation close to the Bose-Einstein condensation transition , which we examine not only for the standard scenario in three dimensions , but also for generalized condensation in lower dimensions , where the bosons condense in a subspace of Hilbert space instead of a unique ground state , as well as condensation at fixed volume or fixed pressure . We show that Bose Einstein condensation can lead to sub-shot noise sensitivity for the measurement of the chemical potential . We also examine the influence of interactions on the sensitivity in three different models , and show that mean-field and contact interactions deteriorate the sensitivity but only slightly for experimentally accessible weak interactions .
This Master thesis examines issues of interoperability and integration between the Classic Information Science ( CIS ) and Quantum Information Science ( QIS ) . It provides a short introduction to the Extensible Markup Language ( XML ) and proceeds to describe the development steps that have lead to a prototype XML specification for quantum computing ( QIS-XML ) . QIS-XML is a proposed framework , based on the widely used standard ( XML ) to describe , visualize , exchange and process quantum gates and quantum circuits . It also provides a potential approach to a generic programming language for quantum computers through the concept of XML driven compilers . Examples are provided for the description of commonly used quantum gates and circuits , accompanied with tools to visualize them in standard web browsers . An algorithmic example is also presented , performing a simple addition operation with quantum circuits and running the program on a quantum computer simulator . Overall , this initial effort demonstrates how XML technologies could be at the core of the architecture for describing and programming quantum computers . By leveraging a widely accepted standard , QIS-XML also builds a bridge between classic and quantum IT , which could foster the acceptance of QIS by the ICT community and facilitate the understanding of quantum technology by IT experts . This would support the consolidation of Classic Information Science and Quantum Information Science into a Complete Information Science , a challenge that could be referred to as the " Information Science Grand Unification Challenge " .
In this paper we study convex stochastic search problems where a noisy objective function value is observed after a decision is made . There are many stochastic search problems whose behavior depends on an exogenous state variable which affects the shape of the objective function . Currently , there is no general purpose algorithm to solve this class of problems . We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state . We propose two solution methods that depend on the problem characteristics : function-based and gradient-based optimization . We examine two weighting schemes , kernel-based weights and Dirichlet process-based weights , for use with the solution methods . The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour-ahead wind commitment problem . Our results show that in some cases Dirichlet process weights offer substantial benefits over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems .
Given a matrix the seriation problem consists in permuting its rows in such way that all its columns have the same shape , for example , they are monotone increasing . We propose a statistical approach to this problem where the matrix of interest is observed with noise and study the corresponding minimax rate of estimation of the matrices . Specifically , when the columns are either unimodal or monotone , we show that the least squares estimator is optimal up to logarithmic factors and adapts to matrices with a certain natural structure . Finally , we propose a computationally efficient estimator in the monotonic case and study its performance both theoretically and experimentally . Our work is at the intersection of shape constrained estimation and recent work that involves permutation learning , such as graph denoising and ranking .
We propose new nonparametric accordance R\ ' enyi-$\alpha$ and $\alpha$-Tsallis divergence estimators for continuous distributions . We discuss this approach with a view to the selection model ( on al\ ' etoire and autoregressive AR ( 0 ) ) . We lestimateur used by kernel density esttimer underlying . Nevertheless , we are able to prove that the estimators are consistent under certain conditions . We also describe how to apply these estimators and demonstrate their effectiveness through numerical experiments .
Gaussian concentration graph models and covariance graph models are two classes of graphical models that are useful for uncovering latent dependence structures among multivariate variables . In the Bayesian literature , graphs are often determined through the use of priors over the space of positive definite matrices with fixed zeros , but these methods present daunting computational burdens in large problems . Motivated by the superior computational efficiency of continuous shrinkage priors for regression analysis , we propose a new framework for structure learning that is based on continuous spike and slab priors and uses latent variables to identify graphs . We discuss model specification , computation , and inference for both concentration and covariance graph models . The new approach produces reliable estimates of graphs and efficiently handles problems with hundreds of variables .
Information transfer which reveals the state variation of variables can play a vital role in big data analytics and processing . In fact , the measure for information transfer can reflect the system change from the statistics by using the variable distributions , similar to KL divergence and Renyi divergence . Furthermore , in terms of the information transfer in big data , small probability events dominate the importance of the total message to some degree . Therefore , it is significant to design an information transfer measure based on the message importance which emphasizes the small probability events . In this paper , we propose the message importance divergence ( MID ) and investigate its characteristics and applications on three aspects . First , we discuss the robustness of MID by using it to measuring information distance . Then , the message importance transfer capacity based on MID is presented to offer an upper bound for the information transfer with disturbance . Finally , we utilize the MID to guide the queue length selection , which is the fundamental problem considered to have higher social or academic value in the caching operation of mobile edge computing .
In this report we investigate the performance of Hadoop clusters , deployed with separated storage and compute layers , on top of a hypervisor managing a single physical host . We have analyzed and evaluated the different Hadoop cluster configurations by running CPU bound and I/O bound workloads . The report is structured as follows : Section 0 provides a brief description of the technologies involved in our study . An overview of the experimental platform , setup test and configurations are presented in Section 0 . Our benchmark methodology is defined in Section 0 . The performed experiments together with the evaluation of the results are presented in Section 0 . Finally , Section 0 concludes with lessons learned .
Cryptographic algorithms are computationally costly and the challenge is more if we need to execute them in resource constrained embedded systems . Field Programmable Gate Arrays ( FPGAs ) having programmable logic de- vices and processing cores , have proven to be highly feasible implementation platforms for embedded systems providing lesser design time and reconfig- urability . Design parameters like throughput , resource utilization and power requirements are the key issues . The popular Elliptic Curve Cryptography ( ECC ) , which is superior over other public-key crypto-systems like RSA in many ways , such as providing greater security for a smaller key size , is cho- sen in this work and the possibilities of its implementation in FPGA based embedded systems for both single and dual processor core architectures in- volving task parallelization have been explored . This exploration , which is first of its kind considering the other existing works , is a needed activity for evaluating the best possible architectural environment for ECC implementa- tion on FPGA ( Virtex0 XC0VFX00 , FF000 , -00 ) based embedded platform .
The social Web is transforming the way information is created and distributed . Blog authoring tools enable users to publish content , while sites such as Digg and Del . icio . us are used to distribute content to a wider audience . With content fast becoming a commodity , interest in using social networks to promote and find content has grown , both on the side of content producers ( viral marketing ) and consumers ( recommendation ) . Here we study the role of social networks in promoting content on Digg , a social news aggregator that allows users to submit links to and vote on news stories . Digg ' s goal is to feature the most interesting stories on its front page , and it aggregates opinions of its many users to identify them . Like other social networking sites , Digg allows users to designate other users as ``friends ' ' and see what stories they found interesting . We studied the spread of interest in news stories submitted to Digg in June 0000 . Our results suggest that pattern of the spread of interest in a story on the network is indicative of how popular the story will become . Stories that spread mainly outside of the submitter ' s neighborhood go on to be very popular , while stories that spread mainly through submitter ' s social neighborhood prove not to be very popular . This effect is visible already in the early stages of voting , and one can make a prediction about the potential audience of a story simply by analyzing where the initial votes come from .
We make a connection between classical polytopes called zonotopes and Support Vector Machine ( SVM ) classifiers . We combine this connection with the ellipsoid method to give some new theoretical results on training SVMs . We also describe some special properties of soft margin C-SVMs as parameter C goes to infinity .
Among Monte Carlo techniques , the importance sampling requires fine tuning of a proposal distribution , which is now fluently resolved through iterative schemes . The Adaptive Multiple Importance Sampling ( AMIS ) of Cornuet et al . ( 0000 ) provides a significant improvement in stability and effective sample size due to the introduction of a recycling procedure . However , the consistency of the AMIS estimator remains largely open . In this work we prove the convergence of the AMIS , at a cost of a slight modification in the learning process . Contrary to Douc et al . ( 0000a ) , results are obtained here in the asymptotic regime where the number of iterations is going to infinity while the number of drawings per iteration is a fixed , but growing sequence of integers . Hence some of the results shed new light on adaptive population Monte Carlo algorithms in that last regime .
We consider the problem of recovering a complete ( i . e . , square and invertible ) matrix $\mathbf A_0$ , from $\mathbf Y \in \mathbb{R}^{n \times p}$ with $\mathbf Y = \mathbf A_0 \mathbf X_0$ , provided $\mathbf X_0$ is sufficiently sparse . This recovery problem is central to theoretical understanding of dictionary learning , which seeks a sparse representation for a collection of input signals and finds numerous applications in modern signal processing and machine learning . We give the first efficient algorithm that provably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O ( n ) $ nonzeros per column , under suitable probability model for $\mathbf X_0$ . In contrast , prior results based on efficient algorithms either only guarantee recovery when $\mathbf X_0$ has $O ( \sqrt{n} ) $ zeros per column , or require multiple rounds of SDP relaxation to work when $\mathbf X_0$ has $O ( n^{0-\delta} ) $ nonzeros per column ( for any constant $\delta \in ( 0 , 0 ) $ ) . } Our algorithmic pipeline centers around solving a certain nonconvex optimization problem with a spherical constraint . In this paper , we provide a geometric characterization of the objective landscape . In particular , we show that the problem is highly structured : with high probability , ( 0 ) there are no " spurious " local minimizers ; and ( 0 ) around all saddle points the objective has a negative directional curvature . This distinctive structure makes the problem amenable to efficient optimization algorithms . In a companion paper ( arXiv : 0000 . 00000 ) , we design a second-order trust-region algorithm over the sphere that provably converges to a local minimizer from arbitrary initializations , despite the presence of saddle points .
We extend our study of Motion Planning via Manifold Samples ( MMS ) , a general algorithmic framework that combines geometric methods for the exact and complete analysis of low-dimensional configuration spaces with sampling-based approaches that are appropriate for higher dimensions . The framework explores the configuration space by taking samples that are entire low-dimensional manifolds of the configuration space capturing its connectivity much better than isolated point samples . The contributions of this paper are as follows : ( i ) We present a recursive application of MMS in a six-dimensional configuration space , enabling the coordination of two polygonal robots translating and rotating amidst polygonal obstacles . In the adduced experiments for the more demanding test cases MMS clearly outperforms PRM , with over 00-fold speedup in a coordination-tight setting . ( ii ) A probabilistic completeness proof for the most prevalent case , namely MMS with samples that are affine subspaces . ( iii ) A closer examination of the test cases reveals that MMS has , in comparison to standard sampling-based algorithms , a significant advantage in scenarios containing high-dimensional narrow passages . This provokes a novel characterization of narrow passages which attempts to capture their dimensionality , an attribute that had been ( to a large extent ) unattended in previous definitions .
It is well known that the emptiness problem for binary probabilistic automata and so for quantum automata is undecidable . We present the current status of the emptiness problems for unary probabilistic and quantum automata with connections with Skolem ' s and positivity problems . We also introduce the concept of linear recurrence automata in order to show the connection naturally . Then , we also give possible generalizations of linear recurrence relations and automata on vectors .
A subjective expected utility policy making centre , managing complex , dynamic systems , needs to draw on the expertise of a variety of disparate panels of experts and integrate this information coherently . To achieve this , diverse supporting probabilistic models need to be networked together , the output of one model providing the input to the next . In this paper we provide a technology for designing an integrating decision support system and to enable the centre to explore and compare the efficiency of different candidate policies . We develop a formal statistical methodology to underpin this tool . In particular , we derive sufficient conditions that ensure inference remains coherent before and after relevant evidence is accommodated into the system . The methodology is illustrated throughout using examples drawn from two decision support systems : one designed for nuclear emergency crisis management and the other to support policy makers in addressing the complex challenges of food poverty in the UK .
In this paper we propose an effective vision-based navigation method that allows a multirotor vehicle to simultaneously reach a desired goal pose in the environment while constantly facing a target object or landmark . Standard techniques such as Position-Based Visual Servoing ( PBVS ) and Image-Based Visual Servoing ( IBVS ) in some cases ( e . g . , while the multirotor is performing fast maneuvers ) do not allow to constantly maintain the line of sight with a target of interest . Instead , we compute the optimal trajectory by solving a non-linear optimization problem that minimizes the target re-projection error while meeting the UAV ' s dynamic constraints . The desired trajectory is then tracked by means of a real-time Non-linear Model Predictive Controller ( NMPC ) : this implicitly allows the multirotor to satisfy both the required constraints . We successfully evaluate the proposed approach in many real and simulated experiments , making an exhaustive comparison with a standard approach .
Most online lotteries today fail to ensure the verifiability of the random process and rely on a trusted third party . This issue has received little attention since the emergence of distributed protocols like Bitcoin that demonstrated the potential of protocols with no trusted third party . We argue that the security requirements of online lotteries are similar to those of online voting , and propose a novel distributed online lottery protocol that applies techniques developed for voting applications to an existing lottery protocol . As a result , the protocol is scalable , provides efficient verification of the random process and does not rely on a trusted third party nor on assumptions of bounded computational resources . An early prototype confirms the feasibility of our approach .
Innovation is the direct intended product of certain styles in research , but not of others . Fundamental conflicts between descriptive vs inferential statistics , deductive vs inductive hypothesis testing , and exploratory vs pre-planned confirmatory research designs have been played out over decades , with winners and losers and consequences . Longstanding warnings from both academics and research-funding interests have failed to influence effectively the course of these battles . The NIH publicly studied and diagnosed important aspects of the problem a decade ago , resulting in outward changes in the grant review process but not a definitive correction . Specific reforms could deliberately abate the damage produced by the current overemphasis on inferential statistics , power estimates , and prescriptive study design . Such reform would permit a reallocation of resources to historically productive rapid exploratory efforts and considerably , increase the chances for higher-impact research discoveries . We can profit from the history and foundation of these conflicts to make specific recommendations for administrative objectives and the process of peer review in decisions regarding research funding . ( C ) 0000 S . Kern
In this paper , we present our investigations on the use of single objective and multiobjective genetic algorithms based optimisation algorithms to improve the design of OFDM pulses for radar . We discuss these optimization procedures in the scope of a waveform design intended for two different radar processing solutions . Lastly , we show how the encoding solution is suited to permit the optimizations of waveform for OFDM radar related challenges such as enhanced detection .
Energy problems are important in the formal analysis of embedded or autonomous systems . Using recent results on star-continuous Kleene omega-algebras , we show here that energy problems can be solved by algebraic manipulations on the transition matrix of energy automata . To this end , we prove general results about certain classes of finitely additive functions on complete lattices which should be of a more general interest .
Research in several fields now requires the analysis of data sets in which multiple high-dimensional types of data are available for a common set of objects . In particular , The Cancer Genome Atlas ( TCGA ) includes data from several diverse genomic technologies on the same cancerous tumor samples . In this paper we introduce Joint and Individual Variation Explained ( JIVE ) , a general decomposition of variation for the integrated analysis of such data sets . The decomposition consists of three terms : a low-rank approximation capturing joint variation across data types , low-rank approximations for structured variation individual to each data type , and residual noise . JIVE quantifies the amount of joint variation between data types , reduces the dimensionality of the data and provides new directions for the visual exploration of joint and individual structures . The proposed method represents an extension of Principal Component Analysis and has clear advantages over popular two-block methods such as Canonical Correlation Analysis and Partial Least Squares . A JIVE analysis of gene expression and miRNA data on Glioblastoma Multiforme tumor samples reveals gene-miRNA associations and provides better characterization of tumor types . Data and software are available at https : //genome . unc . edu/jive/
I am most honoured to have the privilege to present the Foreword to this fascinating and wonderfully varied collection of contributions , concerning the nature of computation and of its deep connection with the operation of those basic laws , known or yet unknown , governing the universe in which we live . Fundamentally deep questions are indeed being grappled with here , and the fact that we find so many different viewpoints is something to be expected , since , in truth , we know little about the foundational nature and origins of these basic laws , despite the immense precision that we so often find revealed in them . Accordingly , it is not surprising that within the viewpoints expressed here is some unabashed speculation , occasionally bordering on just partially justified guesswork , while elsewhere we find a good deal of precise reasoning , some in the form of rigorous mathematical theorems . Both of these are as should be , for without some inspired guesswork we cannot have new ideas as to where look in order to make genuinely new progress , and without precise mathematical reasoning , no less than in precise observation , we cannot know when we are right -- or , more usually , when we are wrong .
Orientation estimation for 0D objects is a common problem that is usually tackled with traditional nonlinear filtering techniques such as the extended Kalman filter ( EKF ) or the unscented Kalman filter ( UKF ) . Most of these techniques assume Gaussian distributions to account for system noise and uncertain measurements . This distributional assumption does not consider the periodic nature of pose and orientation uncertainty . We propose a filter that considers the periodicity of the orientation estimation problem in its distributional assumption . This is achieved by making use of the Bingham distribution , which is defined on the hypersphere and thus inherently more suitable to periodic problems . Furthermore , handling of non-trivial system functions is done using deterministic sampling in an efficient way . A deterministic sampling scheme reminiscent of the UKF is proposed for the nonlinear manifold of orientations . It is the first deterministic sampling scheme that truly reflects the nonlinear manifold of the orientation .
Recent developments in elastic shape analysis ( ESA ) are motivated by the fact that it provides comprehensive frameworks for simultaneous registration , deformation , and comparison of shapes . These methods achieve computational efficiency using certain square-root representations that transform invariant elastic metrics into Euclidean metrics , allowing for applications of standard algorithms and statistical tools . For analyzing shapes of embeddings of $\mathbb{S}^0$ in $\mathbb{R}^0$ , Jermyn et al . introduced square-root normal fields ( SRNFs ) that transformed an elastic metric , with desirable invariant properties , into the $\mathbb{L}^0$ metric . These SRNFs are essentially surface normals scaled by square-roots of infinitesimal area elements . A critical need in shape analysis is to invert solutions ( deformations , averages , modes of variations , etc ) computed in the SRNF space , back to the original surface space for visualizations and inferences . Due to the lack of theory for understanding SRNFs maps and their inverses , we take a numerical approach and derive an efficient multiresolution algorithm , based on solving an optimization problem in the surface space , that estimates surfaces corresponding to given SRNFs . This solution is found effective , even for complex shapes , e . g . human bodies and animals , that undergo significant deformations including bending and stretching . Specifically , we use this inversion for computing elastic shape deformations , transferring deformations , summarizing shapes , and for finding modes of variability in a given collection , while simultaneously registering the surfaces . We demonstrate the proposed algorithms using a statistical analysis of human body shapes , classification of generic surfaces and analysis of brain structures .
Pencils of Hankel matrices whose elements have a joint Gaussian distribution with nonzero mean and not identical covariance are considered . An approximation to the distribution of the squared modulus of their determinant is computed which allows to get a closed form approximation of the condensed density of the generalized eigenvalues of the pencils . Implications of this result for solving several moments problems are discussed and some numerical examples are provided .
In this paper the problem of matching Forward Kinematics ( FK ) motion of a 0 Dimensional ( 0D ) joint chain to the Inverse Kinematics ( IK ) movement and vice versa has been addressed . The problem lies at the heart of animating a 0D character having controller and manipulator based rig for animation within any 0D modeling and animation software . The seamless matching has been achieved through the use of pseudo-inverse of Jacobian Matrix . The Jacobian Matrix is used to determine the rotation values of each joint of character body part such as arms , between the inverse kinematics and forward kinematics motion . Then moving the corresponding kinematic joint system to the desired place , automatically eliminating the jumping or popping effect which would reduce the complexity of the system .
Hypervolume indicator is a commonly accepted quality measure for comparing Pareto approximation set generated by multi-objective optimizers . The best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $O ( n^{d/0} ) $ with special data structures . This paper presents a recursive , vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>0$ dimensions . It splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point . In special , the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids . The complexity analysis shows that the proposed algorithm achieves $O ( ( \frac{d}{0} ) ^n ) $ time and $O ( dn^0 ) $ space complexity in the worst case .
In this paper we propose a generalization of a class of Gaussian Semiparametric Estimators ( GSE ) of the fractional differencing parameter for long-range dependent multivariate time series . We generalize a known GSE-type estimator by introducing some modifications at the objective function level regarding the process ' spectral density matrix estimator . We study large sample properties of the estimator without assuming Gaussianity as well as hypothesis testing . The class of models considered here satisfies simple conditions on the spectral density function , restricted to a small neighborhood of the zero frequency . This includes , but is not limited to , the class of VARFIMA models . A simulation study to assess the finite sample properties of the proposed estimator is presented and supports its competitiveness . We also present an empirical application to an exchange rate data .
For the particles undergoing the anomalous diffusion with different waiting time distributions for different internal states , we derive the Fokker-Planck and Feymann-Kac equations , respectively , describing positions of the particles and functional distributions of the trajectories of particles ; in particular , the equations governing the functional distribution of internal states are also obtained . The dynamics of the stochastic processes are analyzed and the applications , calculating the distribution of the first passage time and the distribution of the fraction of the occupation time , of the equations are given .
We argue here about the relevance and the ultimate unity of the Bayesian approach in a neutral and agnostic manner . Our main theme is that Bayesian data analysis is an effective tool for handling complex models , as proven by the increasing proportion of Bayesian studies in the applied sciences . We disregard in this essay the philosophical debates on the deeper meaning of probability and on the random nature of parameters as things of the past that do a disservice to the approach and are incomprehensible to most bystanders .
Vertex-centroid schemes are cell-centered finite volume schemes for conservation laws which make use of vertex values to construct high resolution schemes . The vertex values must be obtained through a consistent averaging ( interpolation ) procedure . A modified interpolation scheme is proposed which is better than existing schemes in giving positive weights in the interpolation formula . A simplified reconstruction scheme is also proposed which is also more accurate and efficient . For scalar conservation laws , we develop limited versions of the schemes which are stable in maximum norm by constructing suitable limiters . The schemes are applied to compressible flows governed by the Euler equations of inviscid gas dynamics .
This paper presents a concept for a control System for an autonomous underwater vehicle under ice using a " SLOCUM " underwater glider . The project concept , the separate working tasks for the next one-and-a-half years and the first results will be presented . In this context the structure of the obstacle avoidance system and a simulator structure with a sensor and environment simulation as well as the interfaces to the glider hardware will be discussed . As a first result of the main research , a graph-based algorithm for the path planning in a time-varying environment ( variable ocean field , moving obstacles ) will be described .
If learning methods are to scale to the massive sizes of modern datasets , it is essential for the field of machine learning to embrace parallel and distributed computing . Inspired by the recent development of matrix factorization methods with rich theory but poor computational complexity and by the relative ease of mapping matrices onto distributed architectures , we introduce a scalable divide-and-conquer framework for noisy matrix factorization . We present a thorough theoretical analysis of this framework in which we characterize the statistical errors introduced by the " divide " step and control their magnitude in the " conquer " step , so that the overall algorithm enjoys high-probability estimation guarantees comparable to those of its base algorithm . We also present experiments in collaborative filtering and video background modeling that demonstrate the near-linear to superlinear speed-ups attainable with this approach .
The problem of ranking a set of objects given some measure of similarity is one of the most basic in machine learning . Recently Agarwal proposed a method based on techniques in semi-supervised learning utilizing the graph Laplacian . In this work we consider a novel application of this technique to ranking binary choice data and apply it specifically to ranking US Senators by their ideology .
The use of mobile devices and the rapid growth of the internet and networking infrastructure has brought the necessity of using Ubiquitous recommender systems . However in mobile devices there are different factors that need to be considered in order to get more useful recommendations and increase the quality of the user experience . This paper gives an overview of the factors related to the quality and proposes a new hybrid recommendation model .
This paper focuses on a situation where data exhibit local dependence but their dependence ordering is not known to the econometrician . Such a situation arises , for example , when many differentiated products are correlated locally and yet the correlation structure is not known , or when there are peer effects among students ' actions but precise information about their reference groups or friendship networks is absent . This paper introduces an ordering-free local dependence measure which is invariant to any permutation of the observations and can be used to express various notions of temporal and spatial weak dependence . The paper begins with the two-sided testing problem of a population mean , and introduces a randomized subsampling approach where one performs inference using U-statistic type test statistics that are constructed from randomized subsamples . The paper shows that one can obtain inference whose validity does not require knowledge of dependence ordering , as long as dependence is sufficiently " local " in terms of the ordering-free local dependence measure . The method is extended to models defined by moment restrictions . This paper provides results from Monte Carlo studies .
Deep learning has achieved a great success in many areas , from computer vision to natural language processing , to game playing , and much more . Yet , what deep learning is really doing is still an open question . There are a lot of works in this direction . For example , [0] tried to explain deep learning by group renormalization , and [0] tried to explain deep learning from the view of functional approximation . In order to address this very crucial question , here we see deep learning from perspective of mechanical learning and learning machine ( see [0] , [0] ) . From this particular angle , we can see deep learning much better and answer with confidence : What deep learning is really doing ? why it works well , how it works , and how much data is necessary for learning . We also will discuss advantages and disadvantages of deep learning at the end of this work .
The classic algorithm of Bodlaender and Kloks [J . Algorithms , 0000] solves the following problem in linear fixed-parameter time : given a tree decomposition of a graph of ( possibly suboptimal ) width $k$ , compute an optimum-width tree decomposition of the graph . In this work , we prove that this problem can also be solved in MSO in the following sense : for every positive integer $k$ , there is an MSO transduction from tree decompositions of width $k$ to tree decompositions of optimum width . Together with our recent results [LICS 0000] , this implies that for every $k$ there exists an MSO transduction which inputs a graph of treewidth $k$ , and nondeterministically outputs its tree decomposition of optimum width . We also show that MSO transductions can be implemented in linear fixed-parameter time , which enables us to derive the algorithmic result of Bodlaender and Kloks as a corollary of our main result .
A latent-variable model is introduced for text matching , inferring sentence representations by jointly optimizing generative and discriminative objectives . To alleviate typical optimization challenges in latent-variable models for text , we employ deconvolutional networks as the sequence decoder ( generator ) , providing learned latent codes with more semantic information and better generalization . Our model , trained in an unsupervised manner , yields stronger empirical predictive performance than a decoder based on Long Short-Term Memory ( LSTM ) , with less parameters and considerably faster training . Further , we apply it to text sequence-matching problems . The proposed model significantly outperforms several strong sentence-encoding baselines , especially in the semi-supervised setting .
Using current sensing technology , a wealth of data on driving sessions is potentially available through a combination of vehicle sensors and drivers ' physiology sensors ( heart rate , breathing rate , skin temperature , etc . ) . Our hypothesis is that it should be possible to exploit the combination of time series produced by such multiple sensors during a driving session , in order to ( i ) learn models of normal driving behaviour , and ( ii ) use such models to detect important and potentially dangerous deviations from the norm in real-time , and thus enable the generation of appropriate alerts . Crucially , we believe that such models and interventions should and can be personalised and tailor-made for each individual driver . As an initial step towards this goal , in this paper we present techniques for assessing the impact of cognitive distraction on drivers , based on simple time series analysis . We have tested our method on a rich dataset of driving sessions , carried out in a professional simulator , involving a panel of volunteer drivers . Each session included a different type of cognitive distraction , and resulted in multiple time series from a variety of on-board sensors as well as sensors worn by the driver . Crucially , each driver also recorded an initial session with no distractions . In our model , such initial session provides the baseline times series that make it possible to quantitatively assess driver performance under distraction conditions .
We test the hypothesis that interconnections across financial institutions can be explained by a diversification motive . This idea stems from the empirical evidence of the existence of long-term exposures that cannot be explained by a liquidity motive ( maturity or currency mismatch ) . We model endogenous interconnections of heterogenous financial institutions facing regulatory constraints using a maximization of their expected utility . Both theoretical and simulation-based results are compared to a stylized genuine financial network . The diversification motive appears to plausibly explain interconnections among key players . Using our model , the impact of regulation on interconnections between banks -currently discussed at the Basel Committee on Banking Supervision- is analyzed .
We investigate the problem of estimating the causal effect of a treatment on individual subjects from observational data , this is a central problem in various application domains , including healthcare , social sciences , and online advertising . Within the Neyman Rubin potential outcomes model , we use the Kullback Leibler ( KL ) divergence between the estimated and true distributions as a measure of accuracy of the estimate , and we define the information rate of the Bayesian causal inference procedure as the ( asymptotic equivalence class of the ) expected value of the KL divergence between the estimated and true distributions as a function of the number of samples . Using Fano method , we establish a fundamental limit on the information rate that can be achieved by any Bayesian estimator , and show that this fundamental limit is independent of the selection bias in the observational data . We characterize the Bayesian priors on the potential ( factual and counterfactual ) outcomes that achieve the optimal information rate . As a consequence , we show that a particular class of priors that have been widely used in the causal inference literature cannot achieve the optimal information rate . On the other hand , a broader class of priors can achieve the optimal information rate . We go on to propose a prior adaptation procedure ( which we call the information based empirical Bayes procedure ) that optimizes the Bayesian prior by maximizing an information theoretic criterion on the recovered causal effects rather than maximizing the marginal likelihood of the observed ( factual ) data . Building on our analysis , we construct an information optimal Bayesian causal inference algorithm .
We estimate convex polytopes and general convex sets in $\mathbb R^d , d\geq 0$ in the regression framework . We measure the risk of our estimators using a $L^0$-type loss function and prove upper bounds on these risks . We show that , in the case of polytopes , these estimators achieve the minimax rate . For polytopes , this minimax rate is $\frac{\ln n}{n}$ , which differs from the parametric rate for non-regular families by a logarithmic factor , and we show that this extra factor is essential . Using polytopal approximations we extend our results to general convex sets , and we achieve the minimax rate up to a logarithmic factor . In addition we provide an estimator that is adaptive with respect to the number of vertices of the unknown polytope , and we prove that this estimator is optimal in all classes of polytopes with a given number of vertices .
Many different parametric models for video quality assessment have been proposed in the past few years . This paper presents a review of nine recent models which cover a wide range of methodologies and have been validated for estimating video quality due to different degradation factors . Each model is briefly described with key algorithms and relevant parametric formulas . The generalization capability of each model to estimate video quality in real-application scenarios is evaluated and compared with other models , using a dataset created with video sequences from practical applications . These video sequences cover a wide range of possible realistic encoding parameters , labeled with mean opinion scores ( MOS ) via subjective test . The weakness and strength of each model are remarked . Finally , future work towards a more general parametric model that could apply for a wider range of applications is discussed .
Matrix-valued covariance functions are crucial to geostatistical modeling of multivariate spatial data . The classical assumption of symmetry of a multivariate covariance function is overlay restrictive and has been considered as unrealistic for most of real data applications . Despite of that , the literature on asymmetric covariance functions has been very sparse . In particular , there is some work related to asymmetric covariances on Euclidean spaces , depending on the Euclidean distance . However , for data collected over large portions of planet Earth , the most natural spatial domain is a sphere , with the corresponding geodesic distance being the natural metric . In this work , we propose a strategy based on spatial rotations to generate asymmetric covariances for multivariate random fields on the $d$-dimensional unit sphere . We illustrate through simulations as well as real data analysis that our proposal allows to achieve improvements in the predictive performance in comparison to the symmetric counterpart .
One important application of the Wireless Sensor Network ( WSN ) is target tracking , the aim of this application is converging to an event or object in an area . In this paper , we propose an energy-efficient distributed sensor activation protocol based on predicted location technique , called Intelligent Distributed Sensor Activation Algorithm ( IDSA ) . The proposed algorithm predicts the location of target in the next time interval , by analyzing current location and movement history of the target , this prediction is done by computational intelligence . The fewest essential number of sensor nodes within the predicted location will be activated to cover the target . The results show that the proposed method outperforms the existing methods such as Na\ " ive and DSA in terms of energy consumption and the number of nodes that was involved in tracking the target .
These are the proceedings of the Second Workshop on GRAPH Inspection and Traversal Engineering ( GRAPHITE 0000 ) , which took place on March 00 , 0000 in Rome , Italy , as a satellite event of the 00th European Joint Conferences on Theory and Practice of Software ( ETAPS 0000 ) . The topic of the GRAPHITE workshop is graph analysis in all its forms in computer science . Graphs are used to represent data in many application areas , and they are subjected to various computational algorithms in order to acquire the desired information . These graph algorithms tend to have common characteristics , such as duplicate detection to guarantee their termination , independent of their application domain . Over the past few years , it has been shown that the scalability of such algorithms can be dramatically improved by using , e . g . , external memory , by exploiting parallel architectures , such as clusters , multi-core CPUs , and graphics processing units , and by using heuristics to guide the search . Novel techniques to further scale graph search algorithms , and new applications of graph search are within the scope of this workshop . Another topic of interest of the event is more related to the structural properties of graphs : which kind of graph characteristics are relevant for a particular application area , and how can these be measured ? Finally , any novel way of using graphs for a particular application area is on topic . The goal of this event is to gather scientists from different communities , such as model checking , artificial intelligence planning , game playing , and algorithm engineering , who do research on graph search algorithms , such that awareness of each others ' work is increased .
We consider various versions of adaptive Gibbs and Metropolis within-Gibbs samplers , which update their selection probabilities ( and perhaps also their proposal distributions ) on the fly during a run , by learning as they go in an attempt to optimise the algorithm . We present a cautionary example of how even a simple-seeming adaptive Gibbs sampler may fail to converge . We then present various positive results guaranteeing convergence of adaptive Gibbs samplers under certain conditions .
Exact Gaussian Process ( GP ) regression has O ( N^0 ) runtime for data size N , making it intractable for large N . Many algorithms for improving GP scaling approximate the covariance with lower rank matrices . Other work has exploited structure inherent in particular covariance functions , including GPs with implied Markov structure , and equispaced inputs ( both enable O ( N ) runtime ) . However , these GP advances have not been extended to the multidimensional input setting , despite the preponderance of multidimensional applications . This paper introduces and tests novel extensions of structured GPs to multidimensional inputs . We present new methods for additive GPs , showing a novel connection between the classic backfitting method and the Bayesian framework . To achieve optimal accuracy-complexity tradeoff , we extend this model with a novel variant of projection pursuit regression . Our primary result -- projection pursuit Gaussian Process Regression -- shows orders of magnitude speedup while preserving high accuracy . The natural second and third steps include non-Gaussian observations and higher dimensional equispaced grid methods . We introduce novel techniques to address both of these necessary directions . We thoroughly illustrate the power of these three advances on several datasets , achieving close performance to the naive Full GP at orders of magnitude less cost .
One way to solve lasso problems when the dictionary does not fit into available memory is to first screen the dictionary to remove unneeded features . Prior research has shown that sequential screening methods offer the greatest promise in this endeavor . Most existing work on sequential screening targets the context of tuning parameter selection , where one screens and solves a sequence of $N$ lasso problems with a fixed grid of geometrically spaced regularization parameters . In contrast , we focus on the scenario where a target regularization parameter has already been chosen via cross-validated model selection , and we then need to solve many lasso instances using this fixed value . In this context , we propose and explore a feedback controlled sequential screening scheme . Feedback is used at each iteration to select the next problem to be solved . This allows the sequence of problems to be adapted to the instance presented and the number of intermediate problems to be automatically selected . We demonstrate our feedback scheme using several datasets including a dictionary of approximate size 000 , 000 by 000 , 000 .
For more than forty years now , modern theories of literature ( Compagnon , 0000 ) insist on the role of paraphrases , rewritings , citations , reciprocal borrowings and mutual contributions of any kinds . The notions of intertextuality , transtextuality , hypertextuality/hypotextuality , were introduced in the seventies and eighties to approach these phenomena . The careful analysis of these references is of particular interest in evaluating the distance that the creator voluntarily introduces with his/her masters . Phoebus is collaborative project that makes computer scientists from the University Pierre and Marie Curie ( LIP0-UPMC ) collaborate with the literary teams of Paris-Sorbonne University with the aim to develop efficient tools for literary studies that take advantage of modern computer science techniques . In this context , we have developed a piece of software that automatically detects and explores networks of textual reuses in classical literature . This paper describes the principles on which is based this program , the significant results that have already been obtained and the perspectives for the near future .
We present a novel self-stabilizing algorithm for minimum spanning tree ( MST ) construction . The space complexity of our solution is $O ( \log^0n ) $ bits and it converges in $O ( n^0 ) $ rounds . Thus , this algorithm improves the convergence time of previously known self-stabilizing asynchronous MST algorithms by a multiplicative factor $\Theta ( n ) $ , to the price of increasing the best known space complexity by a factor $O ( \log n ) $ . The main ingredient used in our algorithm is the design , for the first time in self-stabilizing settings , of a labeling scheme for computing the nearest common ancestor with only $O ( \log^0n ) $ bits .
We investigate the organization of traffic flow on preexisting uni- and bidirectional ant trails . Our investigations comprise a theoretical as well as an empirical part . We propose minimal models of uni- and bi-directional traffic flow implemented as cellular automata . Using these models , the spatio-temporal organization of ants on the trail is studied . Based on this , some unusual flow characteristics which differ from those known from other traffic systems , like vehicular traffic or pedestrians dynamics , are found . The theoretical investigations are supplemented by an empirical study of bidirectional traffic on a trail of Leptogenys processionalis . Finally , we discuss some plausible implications of our observations from the perspective of flow optimization .
We obtain an asymptotic expansion for the null distribution function of thegradient statistic for testing composite null hypotheses in the presence of nuisance parameters . The expansion is derived using a Bayesian route based on the shrinkage argument described in Ghosh and Mukerjee ( 0000 ) . Using this expansion , we propose a Bartlett-type corrected gradient statistic with chi-square distribution up to an error of order o ( n^{-0} ) under the null hypothesis . Further , we also use the expansion to modify the percentage points of the large sample reference chi-square distribution . A small Monte Carlo experiment and various examples are presented and discussed .
The data sponsored scheme allows the content provider to cover parts of the cellular data costs for mobile users . Thus the content service becomes appealing to more users and potentially generates more profit gain to the content provider . In this paper , we consider a sponsored data market with a monopoly network service provider , a single content provider , and multiple users . In particular , we model the interactions of three entities as a two-stage Stackelberg game , where the service provider and content provider act as the leaders determining the pricing and sponsoring strategies , respectively , in the first stage , and the users act as the followers deciding on their data demand in the second stage . We investigate the mutual interaction of the service provider and content provider in two cases : ( i ) competitive case , where the content provider and service provider optimize their strategies separately and competitively , each aiming at maximizing the profit and revenue , respectively ; and ( ii ) cooperative case , where the two providers jointly optimize their strategies , with the purpose of maximizing their aggregate profits . We analyze the sub-game perfect equilibrium in both cases . Via extensive simulations , we demonstrate that the network effects significantly improve the payoff of three entities in this market , i . e . , utilities of users , the profit of content provider and the revenue of service provider . In addition , it is revealed that the cooperation between the two providers is the best choice for all three entities .
We study the distribution of the ratio of two central Wishart matrices with different covariance matrices . We first derive the density function of a particular matrix form of the ratio and show that its cumulative distribution function can be expressed in terms of the hypergeometric function 0F0 of a matrix argument . Then we apply the holonomic gradient method for numerical evaluation of the hypergeometric function . This approach enables us to compute the power function of Roy ' s maximum root test for testing the equality of two covariance matrices .
In a cloud computing environment , access control policy is an effective means of fortification cloud users and cloud resources services against security infringements . Based on analysis of current cloud computing security characteristics , the preamble of the concept of trust , role-based access control policy , combined with the characteristics of the cloud computing environment , there are multiple security management domains , so a new cross domain framework is for access control is proposed which is based on trust . It will establish and calculate the degree of trust in the single as well as multiple domains . Role Based Access Control is used for the implementation of the access control policies in a single domain environment with the introduction of the trust concept . In multiple domains the access control will be based on the conversion of roles . On the basis of trust , and role based access control model , a new novel framework of flexible cross domain access control framework is presented . The role assignment and conversion will take place dynamically .
We consider a contextual version of multi-armed bandit problem with global knapsack constraints . In each round , the outcome of pulling an arm is a scalar reward and a resource consumption vector , both dependent on the context , and the global knapsack constraints require the total consumption for each resource to be below some pre-fixed budget . The learning agent competes with an arbitrary set of context-dependent policies . This problem was introduced by Badanidiyuru et al . ( 0000 ) , who gave a computationally inefficient algorithm with near-optimal regret bounds for it . We give a computationally efficient algorithm for this problem with slightly better regret bounds , by generalizing the approach of Agarwal et al . ( 0000 ) for the non-constrained version of the problem . The computational time of our algorithm scales logarithmically in the size of the policy space . This answers the main open question of Badanidiyuru et al . ( 0000 ) . We also extend our results to a variant where there are no knapsack constraints but the objective is an arbitrary Lipschitz concave function of the sum of outcome vectors .
With continued feature size scaling , even state of the art semiconductor manufacturing processes will often run into layouts with poor printability and yield . Identifying lithography hotspots is important at both physical verification and early physical design stages . While detailed lithography simulations can be very accurate , they may be too computationally expensive for full-chip scale and physical design inner loops . Meanwhile , pattern matching and machine learning based hotspot detection methods can provide acceptable quality and yet fast turn-around-time for full-chip scale physical verification and design . In this paper , we discuss some key issues and recent results on lithography hotspot detection and mitigation in nanometer VLSI .
This paper is concerned with the estimation of the period of an unknown periodic function in Gaussian white noise . A class of estimators of the period is constructed by means of a penalized maximum likelihood method . A second-order asymptotic expansion of the risk of these estimators is obtained . Moreover , the minimax problem for the second-order term is studied and an estimator of the preceding class is shown to be second order efficient .
In this work , we present a family of architectures for polar decoders using a reduced-complexity successive-cancellation decoding algorithm that employs unrolling to achieve extremely high throughput values while retaining moderate implementation complexity . The resulting fully-unrolled , deeply-pipelined architecture is capable of achieving a coded throughput in excess of 0 Tbps on a 00 nm ASIC at 000 MHz---three orders of magnitude greater than current state-of-the-art polar decoders . However , unrolled decoders are built for a specific , fixed code . Therefore we also present a new method to enable the use of multiple code lengths and rates in a fully-unrolled polar decoder architecture . This method leads to a length- and rate-flexible decoder while retaining the very high speed typical to unrolled decoders . The resulting decoders can decode a master polar code of a given rate and length , and several shorter codes of different rates and lengths . We present results for two versions of a multi-mode decoder supporting eight and ten different polar codes , respectively . Both are capable of a peak throughput of 00 . 0 Gbps . For each decoder , the energy efficiency for the longest supported polar code is shown to be of 00 . 0 pJ/bit at 000 MHz and of 0 . 0 pJ/bit at 000 MHz .
The computation of the Tukey depth , also called halfspace depth , is very demanding , even in low dimensional spaces , because it requires the consideration of all possible one-dimensional projections . In this paper we propose a random depth which approximates the Tukey depth . It only takes into account a finite number of one-dimensional projections which are chosen at random . Thus , this random depth requires a very small computation time even in high dimensional spaces . Moreover , it is easily extended to cover the functional framework . We present some simulations indicating how many projections should be considered depending on the sample size and on the dimension of the sample space . We also compare this depth with some others proposed in the literature . It is noteworthy that the random depth , based on a very low number of projections , obtains results very similar to those obtained with other depths .
The Landau collision integral is an accurate model for the small-angle dominated Coulomb collisions in fusion plasmas . We investigate a high order accurate , fully conservative , finite element discretization of the nonlinear multi-species Landau integral with adaptive mesh refinement using the PETSc library ( www . mcs . anl . gov/petsc ) . We develop algorithms and techniques to efficiently utilize emerging architectures with an approach that minimizes memory usage and movement and is suitable for vector processing . The Landau collision integral is vectorized with Intel AVX-000 intrinsics and the solver sustains as much as 00% of the theoretical peak flop rate of the Second Generation Intel Xeon Phi , Knights Landing , processor .
This paper develops a framework for creating damage accumulation models for engineered wood products by invoking the classical theory of non--dimensionalization . The result is a general class of such models . Both the US and Canadian damage accumulation models are revisited . It is shown how the former may be generalized within that framework while deficiencies are discovered in the latter and overcome . Use of modern Bayesian statistical methods for estimating the parameters in these models is proposed along with an illustrative application of these methods to a ramp load dataset .
This study aims to develop a model for the user acceptance for implementing the information security standard ( i . e . ISO 00000 ) in Turkish public organizations . The results of the surveys performed in Turkey reveal that the legislation on information security public which organizations have to obey is significantly related with the user acceptance during ISO 00000 implementation process . The fundamental components of our user acceptance model are perceived usefulness , attitude towards use , social norms , and performance expectancy .
This work aimed , to determine the characteristics of activity series from fractal geometry concepts application , in addition to evaluate the possibility of identifying individuals with fibromyalgia . Activity level data were collected from 00 healthy subjects and 00 fibromyalgia patients , with the use of clock-like devices equipped with accelerometers , for about four weeks , all day long . The activity series were evaluated through fractal and multifractal methods . Hurst exponent analysis exhibited values according to other studies ( $H>0 . 0$ ) for both groups ( $H=0 . 00\pm0 . 00$ for healthy subjects and $H=0 . 00\pm0 . 00$ for fibromyalgia patients ) , however , it is not possible to distinguish between the two groups by such analysis . Activity time series also exhibited a multifractal pattern . A paired analysis of the spectra indices for the sleep and awake states revealed differences between healthy subjects and fibromyalgia patients . The individuals feature differences between awake and sleep states , having statistically significant differences for $\alpha_{q-} - \alpha_{0}$ in healthy subjects ( $p = 0 . 000$ ) and $D_{0}$ for patients with fibromyalgia ( $p = 0 . 000$ ) . The approach has proven to be an option on the characterisation of such kind of signals and was able to differ between both healthy and fibromyalgia groups . This outcome suggests changes in the physiologic mechanisms of movement control .
The Digital Public Library of America ( DPLA ) aggregates metadata for cultural heritage materials from 00 direct partners , or Hubs , across the United States . While the initial build-out of the DPLA ' s infrastructure used a lightweight ingestion system that was ultimately pushed into production , a year ' s experience has allowed DPLA and its partners to identify limitations to that system , the quality and scalability of metadata remediation and enhancement possible , and areas for collaboration and leadership across the partnership . Although improved infrastructure is needed to support aggregation at this scale and complexity , ultimately DPLA needs to balance responsibilities across the partnership and establish a strong community that shares ownership of the aggregation process .
Motivated by automated junction recognition in tracking data , we study a problem of placing a square or disc of fixed size in an arrangement of lines or line segments in the plane . We let distances among the intersection points of the lines and line segments with the square or circle define a clustering , and study the complexity of \emph{critical} placements for this clustering . Here critical means that arbitrarily small movements of the placement change the clustering . A parameter $\varepsilon$ defines the granularity of the clustering . Without any assumptions on $\varepsilon$ , the critical placements have a trivial $O ( n^0 ) $ upper bound . When the square or circle has unit size and $0 < \varepsilon < 0$ is given , we show a refined $O ( n^0/\varepsilon^0 ) $ bound , which is tight in the worst case . We use our combinatorial bounds to design efficient algorithms to compute junctions . As a proof of concept for our algorithms we have a prototype implementation that showcases their application in a basic visualization of a set of $n$ trajectories and their $k$ most important junctions .
In the context of investment analysis , we formulate an abstract online computing problem called a planning game and develop general tools for solving such a game . We then use the tools to investigate a practical buy-and-hold trading problem faced by long-term investors in stocks . We obtain the unique optimal static online algorithm for the problem and determine its exact competitive ratio . We also compare this algorithm with the popular dollar averaging strategy using actual market data .
We give a 0 . 00 approximation algorithm for the Steiner Tree Problem with distances one and two , improving on the best known bound for that problem .
We present and evaluate a compiler from Prolog ( and extensions ) to JavaScript which makes it possible to use ( constraint ) logic programming to develop the client side of web applications while being compliant with current industry standards . Targeting JavaScript makes ( C ) LP programs executable in virtually every modern computing device with no additional software requirements from the point of view of the user . In turn , the use of a very high-level language facilitates the development of high-quality , complex software . The compiler is a back end of the Ciao system and supports most of its features , including its module system and its rich language extension mechanism based on packages . We present an overview of the compilation process and a detailed description of the run-time system , including the support for modular compilation into separate JavaScript code . We demonstrate the maturity of the compiler by testing it with complex code such as a CLP ( FD ) library written in Prolog with attributed variables . Finally , we validate our proposal by measuring the performance of some LP and CLP ( FD ) benchmarks running on top of major JavaScript engines .
We consider the problem of aggregating data in a dynamic graph , that is , aggregating the data that originates from all nodes in the graph to a specific node , the sink . We are interested in giving lower bounds for this problem , under different kinds of adversaries . In our model , nodes are endowed with unlimited memory and unlimited computational power . Yet , we assume that communications between nodes are carried out with pairwise interactions , where nodes can exchange control information before deciding whether they transmit their data or not , given that each node is allowed to transmit its data at most once . When a node receives a data from a neighbor , the node may aggregate it with its own data . We consider three possible adversaries : the online adaptive adversary , the oblivious adversary , and the randomized adversary that chooses the pairwise interactions uniformly at random . For the online adaptive and the oblivious adversary , we give impossibility results when nodes have no knowledge about the graph and are not aware of the future . Also , we give several tight bounds depending on the knowledge ( be it topology related or time related ) of the nodes . For the randomized adversary , we show that the Gathering algorithm , which always commands a node to transmit , is optimal if nodes have no knowledge at all . Also , we propose an algorithm called Waiting Greedy , where a node either waits or transmits depending on some parameter , that is optimal when each node knows its future pairwise interactions with the sink .
We propose analytical models that allow us to investigate the performance of long range wide area network ( LoRaWAN ) uplink in terms of latency , collision rate , and throughput under the constraints of the regulatory duty cycling , when assuming exponential inter-arrival times . Our models take into account sub-band selection and the case of sub-band combining . Our numerical evaluations consider specifically the European ISM band , but the analysis is applicable to any coherent band . Protocol simulations are used to validate the proposed models . We find that sub-band selection and combining have a large effect on the quality of service ( QoS ) experienced in an LoRaWAN cell for a given load . The proposed models allow for the optimization of resource allocation within a cell given a set of QoS requirements and a traffic model .
Ensemble filters implement sequential Bayesian estimation by representing the probability distribution by an ensemble mean and covariance . Unbiased square root ensemble filters use deterministic algorithms to produce an analysis ( posterior ) ensemble with prescribed mean and covariance , consistent with the Kalman update . This includes several filters used in practice , such as the Ensemble Transform Kalman Filter ( ETKF ) , the Ensemble Adjustment Kalman Filter ( EAKF ) , and a filter by Whitaker and Hamill . We show that at every time index , as the number of ensemble members increases to infinity , the mean and covariance of an unbiased ensemble square root filter converge to those of the Kalman filter , in the case a linear model and an initial distribution of which all moments exist . The convergence is in $L^{p}$ and the convergence rate does not depend on the model dimension . The result holds in the infinitely dimensional Hilbert space as well .
Automated generation of high-quality topical hierarchies for a text collection is a dream problem in knowledge engineering with many valuable applications . In this paper a scalable and robust algorithm is proposed for constructing a hierarchy of topics from a text collection . We divide and conquer the problem using a top-down recursive framework , based on a tensor orthogonal decomposition technique . We solve a critical challenge to perform scalable inference for our newly designed hierarchical topic model . Experiments with various real-world datasets illustrate its ability to generate robust , high-quality hierarchies efficiently . Our method reduces the time of construction by several orders of magnitude , and its robust feature renders it possible for users to interactively revise the hierarchy .
Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives . Therefore , just as we consider the safety of power plants , highways , and a variety of other engineered socio-technical systems , we must also take into account the safety of systems involving machine learning . Heretofore , the definition of safety has not been formalized in a machine learning context . In this paper , we do so by defining machine learning safety in terms of risk , epistemic uncertainty , and the harm incurred by unwanted outcomes . We then use this definition to examine safety in all sorts of applications in cyber-physical systems , decision sciences , and data products . We find that the foundational principle of modern statistical machine learning , empirical risk minimization , is not always a sufficient objective . Finally , we discuss how four different categories of strategies for achieving safety in engineering , including inherently safe design , safety reserves , safe fail , and procedural safeguards can be mapped to a machine learning context . We then discuss example techniques that can be adopted in each category , such as considering interpretability and causality of predictive models , objective functions beyond expected prediction accuracy , human involvement for labeling difficult or rare examples , and user experience design of software and open data .
Given a partially-ordered finite alphabet $\Sigma$ and a language $L \subseteq \Sigma^*$ , how large can an antichain in $L$ be ( where $L$ is ordered by the lexicographic ordering ) ? This fundamental property of partial orders is known as the width and its computation is the central extremal problem for partially ordered sets . More precisely , since $L$ will in general be infinite , we should ask about the rate of growth of maximum antichains consisting of words of length $n$ . In this work , we show that if L is regular or context free then there is a dichotomy between polynomial and exponential antichain growth . For regular languages we give a polynomial time algorithm to distinguish the two cases , even if $L$ is specified as an NFA . On the other hand for context free languages we show that the problem of distinguishing the two cases is undecidable . We generalise the lexicographic order to tree languages , and show that for regular tree languages there is a trichotomy between polynomial , exponential and doubly exponential antichain growth . Finally we discuss the motivating problem for this work , which is related to information flow in the context of computer security .
Discussion of ``0000 IMS Medallion Lecture : Local Rademacher complexities and oracle inequalities in risk minimization ' ' by V . Koltchinskii [arXiv : 0000 . 0000]
We derive independence tests by means of dependence measures thresholding in a semiparametric context . Precisely , estimates of phi-mutual informations , associated to phi-divergences between a joint distribution and the product distribution of its margins , are derived through the dual representation of phi-divergences . The asymptotic properties of the proposed estimates are established , including consistency , asymptotic distributions and large deviations principle . The obtained tests of independence are compared via their relative asymptotic Bahadur efficiency and numerical simulations . It follows that the proposed semiparametric Kullback-Leibler Mutual information test is the optimal one . On the other hand , the proposed approach provides a new method for estimating the Kullback-Leibler mutual information in a semiparametric setting , as well as a model selection procedure in large class of dependency models including semiparametric copulas .
Self organizing maps ( SOMs ) are widely-used for unsupervised classification . For this application , they must be combined with some partitioning scheme that can identify boundaries between distinct regions in the maps they produce . We discuss a novel partitioning scheme for SOMs based on the Bayesian Blocks segmentation algorithm of Scargle [0000] . This algorithm minimizes a cost function to identify contiguous regions over which the values of the attributes can be represented as approximately constant . Because this cost function is well-defined and largely independent of assumptions regarding the number and structure of clusters in the original sample space , this partitioning scheme offers significant advantages over many conventional methods . Sample code is available .
In this paper we explore state-of-the-art underactuated , compliant robot gripper designs through looking at their performance on a generic grasping task . Starting from a state of the art open gripper design , we propose design modifications , and importantly , evaluate all designs on a grasping experiment involving a selection of objects resulting in 0000 object-gripper interactions . Interested in non-planned grasping but rather on a design ' s generic performance , we explore the influence of object shape , pose and orientation relative to the gripper and its finger number and configuration . Using open-loop grasps we achieved up to 00% success rate over our trials . The results indicate and support that under motion constraints and uncertainties and without involving grasp planning , a 0-fingered underactuated compliant hand outperforms higher multi-fingered configurations . To our knowledge this is the first extended objective comparison of various multi-fingered underactuated hand designs under generic grasping conditions .
In recent years the ultrahigh dimensional linear regression problem has attracted enormous attentions from the research community . Under the sparsity assumption most of the published work is devoted to the selection and estimation of the significant predictor variables . This paper studies a different but fundamentally important aspect of this problem : uncertainty quantification for parameter estimates and model choices . To be more specific , this paper proposes methods for deriving a probability density function on the set of all possible models , and also for constructing confidence intervals for the corresponding parameters . These proposed methods are developed using the generalized fiducial methodology , which is a variant of Fisher ' s controversial fiducial idea . Theoretical properties of the proposed methods are studied , and in particular it is shown that statistical inference based on the proposed methods will have exact asymptotic frequentist property . In terms of empirical performances , the proposed methods are tested by simulation experiments and an application to a real data set . Lastly this work can also be seen as an interesting and successful application of Fisher ' s fiducial idea to an important and contemporary problem . To the best of the authors ' knowledge , this is the first time that the fiducial idea is being applied to a so-called " large p small n " problem .
This article addresses the different methods of estimation of the probability mass function ( PMF ) and the cumulative distribution function ( CDF ) for the Logarithmic Series distribution . Following estimation methods are considered : uniformly minimum variance unbiased estimator ( UMVUE ) , maximum likelihood estimator ( MLE ) , percentile estimator ( PCE ) , least square estimator ( LSE ) , weighted least square estimator ( WLSE ) . Monte Carlo simulations are performed to compare the performances of the proposed methods of estimation .
We introduce a logic , called L_T , to express properties of transductions , i . e . binary relations from input to output ( finite ) words . In L_T , the input/output dependencies are modelled via an origin function which associates to any position of the output word , the input position from which it originates . L_T is well-suited to express relations ( which are not necessarily functional ) , and can express all regular functional transductions , i . e . transductions definable for instance by deterministic two-way transducers . Despite its high expressive power , L_T has decidable satisfiability and equivalence problems . Our main contribution is a uniformisation result : from any transduction R defined in L_T , it is possible to synthesise a regular functional transduction f such that for all input words u in the domain of R , f is defined and ( u , f ( u ) ) belong to R . As a consequence , we obtain that any functional transduction is regular iff it is L_T-definable .
We introduce a one-shot learning approach for video object tracking . The proposed algorithm requires seeing the object to be tracked only once , and employs an external memory to store and remember the evolving features of the foreground object as well as backgrounds over time during tracking . With the relevant memory retrieved and updated in each tracking , our tracking model is capable of maintaining long-term memory of the object , and thus can naturally deal with hard tracking scenarios including partial and total occlusion , motion changes and large scale and shape variations . In our experiments we use the ImageNet ILSVRC0000 video detection dataset to train and use the VOT-0000 benchmark to test and compare our Memory-Augmented Video Object Tracking ( MAVOT ) model . From the results , we conclude that given its oneshot property and simplicity in design , MAVOT is an attractive approach in visual tracking because it shows good performance on VOT-0000 benchmark and is among the top 0 performers in accuracy and robustness in occlusion , motion changes and empty target .
We consider a sharing economy network where agents embedded in a graph share their resources . This is a fundamental model that abstracts numerous emerging applications of collaborative consumption systems . The agents generate a random amount of spare resource that they can exchange with their one-hop neighbours , seeking to maximize the amount of desirable resource items they receive in the long run . We study this system from three different perspectives : a ) the central designer who seeks the resource allocation that achieves the most fair endowments after the exchange ; b ) the game theoretic where the nodes seek to form sharing coalitions within teams , attempting to maximize the benefit of their team only ; c ) the market where the nodes are engaged in trade with their neighbours trying to improve their own benefit . It is shown that there is a unique family of sharing allocations that are at the same time most fair , stable with respect to continuous coalition formation among the nodes and achieving equilibrium in the market perspective . A dynamic sharing policy is given then where each node observes the sharing rates of its neighbours and allocates its resource accordingly . That policy is shown to achieve long term sharing ratios that are within the family of equilibrium allocations of the static problem . The equilibrium allocations have interesting properties that highlight the dependence of the sharing ratios of each node to the structure of the topology graph and the effect of the isolation of a node on the benefit may extract from his neighbours .
The majority of real-world networks are dynamic and extremely large ( e . g . , Internet Traffic , Twitter , Facebook , . . . ) . To understand the structural behavior of nodes in these large dynamic networks , it may be necessary to model the dynamics of behavioral roles representing the main connectivity patterns over time . In this paper , we propose a dynamic behavioral mixed-membership model ( DBMM ) that captures the roles of nodes in the graph and how they evolve over time . Unlike other node-centric models , our model is scalable for analyzing large dynamic networks . In addition , DBMM is flexible , parameter-free , has no functional form or parameterization , and is interpretable ( identifies explainable patterns ) . The performance results indicate our approach can be applied to very large networks while the experimental results show that our model uncovers interesting patterns underlying the dynamics of these networks .
A well-known problem in numerical ecology is how to recombine presence-absence matrices without altering row and column totals . A few solutions have been proposed , but all of them present some issues in terms of statistical robustness ( i . e . their capability to generate different matrix configurations with the same probability ) and their performance ( i . e . the computational effort they require to generate a null matrix ) . Here we introduce the ' Babe Ruth Algorithm ' , a new procedure that differs from existing ones in that it focuses rather on matrix information content than on matrix structure . We demonstrate that the algorithm can sample uniformly the set of all possible matrix configurations requiring a computational effort orders of magnitude lower than that required by available methods , making it possible to easily randomize matrices larger than 00^0 cells .
We present new iterative algorithms for solving a square linear system $Ax=b$ in dimension $n$ by employing the {\it Triangle Algorithm} \cite{kal00} , a fully polynomial-time approximation scheme for testing if the convex hull of a finite set of points in a Euclidean space contains a given point . By converting $Ax=b$ into a convex hull problem and solving via the Triangle Algorithm , together with a {\it sensitivity theorem} , we compute in $O ( n^0\epsilon^{-0} ) $ arithmetic operations an approximate solution satisfying $\Vert Ax_\epsilon - b \Vert \leq \epsilon \rho$ , where $\rho= \max \{\Vert a_0 \Vert , . . . , \Vert a_n \Vert , \Vert b \Vert \}$ , and $a_i$ is the $i$-th column of $A$ . In another approach we apply the Triangle Algorithm incrementally , solving a sequence of convex hull problems while repeatedly employing a {\it distance duality} . The simplicity and theoretical complexity bounds of the proposed algorithms , requiring no structural restrictions on the matrix $A$ , suggest their potential practicality , offering alternatives to the existing exact and iterative methods , especially for large scale linear systems . The assessment of computational performance however is the subject of future experimentations .
In this paper , we have implemented and designed a sorting network for reversible logic circuits synthesis in terms of n*n Toffoli gates . The algorithm presented in this paper constructs a Toffoli Network based on swapping bit strings . Reduction rules are then applied by simple template matching and removing useless gates from the network . Random selection of bit strings and reduction of control inputs are used to minimize both the number of gates and gate width . The method produces near optimal results for up to 0-input 0-output circuits .
We describe an algorithm for the sequential sampling of entries in multiway contingency tables with given constraints . The algorithm can be used for computations in exact conditional inference . To justify the algorithm , a theory relates sampling values at each step to properties of the associated toric ideal using computational commutative algebra . In particular , the property of interval cell counts at each step is related to exponents on lead indeterminates of a lexicographic Gr\ " {o}bner basis . Also , the approximation of integer programming by linear programming for sampling is related to initial terms of a toric ideal . We apply the algorithm to examples of contingency tables which appear in the social and medical sciences . The numerical results demonstrate that the theory is applicable and that the algorithm performs well .
Calibration data are often obtained by observing several well-understood objects simultaneously with multiple instruments , such as satellites for measuring astronomical sources . Analyzing such data and obtaining proper concordance among the instruments is challenging when the physical source models are not well understood , when there are uncertainties in " known " physical quantities , or when data quality varies in ways that cannot be fully quantified . Furthermore , the number of model parameters increases with both the number of instruments and the number of sources . Thus , concordance of the instruments requires careful modeling of the mean signals , the intrinsic source differences , and measurement errors . In this paper , we propose a log-Normal hierarchical model and a more general log-t model that respect the multiplicative nature of the mean signals via a half-variance adjustment , yet permit imperfections in the mean modeling to be absorbed by residual variances . We present analytical solutions in the form of power shrinkage in special cases and develop reliable MCMC algorithms for general cases . We apply our method to several data sets obtained with a variety of X-ray telescopes such as Chandra . We demonstrate that our method provides helpful and practical guidance for astrophysicists when adjusting for disagreements among instruments .
An \omega-grammar is a formal grammar used to generate \omega-words ( i . e . infinite length words ) , while an \omega-automaton is an automaton used to recognize \omega-words . This paper gives clean and uniform definitions for \omega-grammars and \omega-automata , provides a systematic study of the generative power of \omega-grammars with respect to \omega-automata , and presents a complete set of results for various types of \omega-grammars and acceptance modes . We use the tuple ( \sigma , \rho , \pi ) to denote various acceptance modes , where \sigma denotes that some designated elements should appear at least once or infinitely often , \rho denotes some binary relation between two sets , and \pi denotes normal or leftmost derivations . Technically , we propose ( \sigma , \rho , \pi ) -accepting \omega-grammars , and systematically study their relative generative power with respect to ( \sigma , \rho ) -accepting \omega-automata . We show how to construct some special forms of \omega-grammars , such as \epsilon-production-free \omega-grammars . We study the equivalence or inclusion relations between \omega$-grammars and \omega-automata by establishing the translation techniques . In particular , we show that , for some acceptance modes , the generative power of \omega-CFG is strictly weaker than \omega-PDA , and the generative power of \omega-CSG is equal to \omega-TM ( rather than linear-bounded \omega-automata-like devices ) . Furthermore , we raise some remaining open problems for two of the acceptance modes .
More and more people use the Internet to work on duties of their daily work routine . To find the right information online , Web search engines are the tools of their choice . Apart from finding facts , people use Web search engines to also execute rather complex and time consuming search tasks . So far search engines follow the one-for-all approach to serve its users and little is known about the impact of gender and age on people ' s Web search behavior . In this article we present a study that examines ( 0 ) how female and male web users carry out simple and complex search tasks and what are the differences between the two user groups , and ( 0 ) how the age of the users impacts their search performance . The laboratory study was done with 00 ordinary people each carrying out 00 search tasks . Our findings confirm that age impacts behavior and search performance significantly , while gender influences were smaller than expected .
Recent years have seen the exponential growth of heterogeneous multimedia data . The need for effective and accurate data retrieval from heterogeneous data sources has attracted much research interest in cross-media retrieval . Here , given a query of any media type , cross-media retrieval seeks to find relevant results of different media types from heterogeneous data sources . To facilitate large-scale cross-media retrieval , we propose a novel unsupervised cross-media hashing method . Our method incorporates local affinity and distance repulsion constraints into a matrix factorization framework . Correspondingly , the proposed method learns hash functions that generates unified hash codes from different media types , while ensuring intrinsic geometric structure of the data distribution is preserved . These hash codes empower the similarity between data of different media types to be evaluated directly . Experimental results on two large-scale multimedia datasets demonstrate the effectiveness of the proposed method , where we outperform the state-of-the-art methods .
We give an algorithm for properly learning Poisson binomial distributions . A Poisson binomial distribution ( PBD ) of order $n$ is the discrete probability distribution of the sum of $n$ mutually independent Bernoulli random variables . Given $\widetilde{O} ( 0/\epsilon^0 ) $ samples from an unknown PBD $\mathbf{p}$ , our algorithm runs in time $ ( 0/\epsilon ) ^{O ( \log \log ( 0/\epsilon ) ) }$ , and outputs a hypothesis PBD that is $\epsilon$-close to $\mathbf{p}$ in total variation distance . The previously best known running time for properly learning PBDs was $ ( 0/\epsilon ) ^{O ( \log ( 0/\epsilon ) ) }$ . As one of our main contributions , we provide a novel structural characterization of PBDs . We prove that , for all $\epsilon >0 , $ there exists an explicit collection $\cal{M}$ of $ ( 0/\epsilon ) ^{O ( \log \log ( 0/\epsilon ) ) }$ vectors of multiplicities , such that for any PBD $\mathbf{p}$ there exists a PBD $\mathbf{q}$ with $O ( \log ( 0/\epsilon ) ) $ distinct parameters whose multiplicities are given by some element of ${\cal M}$ , such that $\mathbf{q}$ is $\epsilon$-close to $\mathbf{p}$ . Our proof combines tools from Fourier analysis and algebraic geometry . Our approach to the proper learning problem is as follows : Starting with an accurate non-proper hypothesis , we fit a PBD to this hypothesis . More specifically , we essentially start with the hypothesis computed by the computationally efficient non-proper learning algorithm in our recent work~\cite{DKS00} . Our aforementioned structural characterization allows us to reduce the corresponding fitting problem to a collection of $ ( 0/\epsilon ) ^{O ( \log \log ( 0/\epsilon ) ) }$ systems of low-degree polynomial inequalities . We show that each such system can be solved in time $ ( 0/\epsilon ) ^{O ( \log \log ( 0/\epsilon ) ) }$ , which yields the overall running time of our algorithm .
A key task in Bayesian statistics is sampling from distributions that are only specified up to a partition function ( i . e . , constant of proportionality ) . However , without any assumptions , sampling ( even approximately ) can be #P-hard , and few works have provided " beyond worst-case " guarantees for such settings . For log-concave distributions , classical results going back to Bakry and \ ' Emery ( 0000 ) show that natural continuous-time Markov chains called Langevin diffusions mix in polynomial time . The most salient feature of log-concavity violated in practice is uni-modality : commonly , the distributions we wish to sample from are multi-modal . In the presence of multiple deep and well-separated modes , Langevin diffusion suffers from torpid mixing . We address this problem by combining Langevin diffusion with simulated tempering . The result is a Markov chain that mixes more rapidly by transitioning between different temperatures of the distribution . We analyze this Markov chain for the canonical multi-modal distribution : a mixture of gaussians ( of equal variance ) . The algorithm based on our Markov chain provably samples from distributions that are close to mixtures of gaussians , given access to the gradient of the log-pdf . For the analysis , we use a spectral decomposition theorem for graphs ( Gharan and Trevisan , 0000 ) and a Markov chain decomposition technique ( Madras and Randall , 0000 ) .
Do unique node identifiers help in deciding whether a network $G$ has a prescribed property $P$ ? We study this question in the context of distributed local decision , where the objective is to decide whether $G \in P$ by having each node run a constant-time distributed decision algorithm . If $G \in P$ , all the nodes should output yes ; if $G \notin P$ , at least one node should output no . A recent work ( Fraigniaud et al . , OPODIS 0000 ) studied the role of identifiers in local decision and gave several conditions under which identifiers are not needed . In this article , we answer their original question . More than that , we do so under all combinations of the following two critical variations on the underlying model of distributed computing : ( $B$ ) : the size of the identifiers is bounded by a function of the size of the input network ; as opposed to ( $\neg B$ ) : the identifiers are unbounded . ( $C$ ) : the nodes run a computable algorithm ; as opposed to ( $\neg C$ ) : the nodes can compute any , possibly uncomputable function . While it is easy to see that under ( $\neg B , \neg C$ ) identifiers are not needed , we show that under all other combinations there are properties that can be decided locally if and only if identifiers are present . Our constructions use ideas from classical computability theory .
Wireless Fidelity ( WiFi ) is the fastest growing wireless technology to date . In addition to providing wire-free connectivity to the Internet WiFi technology also enables mobile devices to connect directly to each other and form highly dynamic wireless adhoc networks . Such distributed networks can be used to perform cooperative communication tasks such ad data routing and information dissemination in the absence of a fixed infrastructure . Furthermore , adhoc grids composed of wirelessly networked portable devices are emerging as a new paradigm in grid computing . In this paper we review computational and algorithmic challenges of high-fidelity simulations of such WiFi-based wireless communication and computing networks , including scalable topology maintenance , mobility modelling , parallelisation and synchronisation . We explore similarities and differences between the simulations of these networks and simulations of interacting many-particle systems , such as molecular dynamics ( MD ) simulations . We show how the cell linked-list algorithm which we have adapted from our MD simulations can be used to greatly improve the computational performance of wireless network simulators in the presence of mobility , and illustrate with an example from our simulation studies of worm attacks on mobile wireless adhoc networks .
We study here slopes of periodicity of tilings . A tiling is of slope if it is periodic along direction but has no other direction of periodicity . We characterize in this paper the set of slopes we can achieve with tilings , and prove they coincide with recursively enumerable sets of rationals .
Traditional science searched for new objects and phenomena that led to discoveries . Tomorrow ' s science will combine together the large pool of information in scientific archives and make discoveries . Scienthists are currently keen to federate together the existing scientific databases . The major challenge in building a federation of these autonomous and heterogeneous databases is system integration . Ineffective integration will result in defunct federations and under utilized scientific data . Astronomy , in particular , has many autonomous archives spread over the Internet . It is now seeking to federate these , with minimal effort , into a Virtual Observatory that will solve complex distributed computing tasks such as answering federated spatial join queries . In this paper , we present SkyQuery , a successful prototype of an evolving federation of astronomy archives . It interoperates using the emerging Web services standard . We describe the SkyQuery architecture and show how it efficiently evaluates a probabilistic federated spatial join query .
We focus on the descriptive approach to linear discriminant analysis for matrix-variate data in the binary case . Under a separability assumption on row and column variability , the most discriminant linear combinations of rows and columns are determined by the singular value decomposition of the difference of the class-averages with the Mahalanobis metric in the row and column spaces . This approach provides data representations of data in two-dimensional or three-dimensional plots and singles out discriminant components . An application to electroencephalographic multi-sensor signals illustrates the relevance of the method .
Pretropisms are candidates for the leading exponents of Puiseux series that represent solutions of polynomial systems . To find pretropisms , we propose an exact gift wrapping algorithm to prune the tree of edges of a tuple of Newton polytopes . We prefer exact arithmetic not only because of the exact input and the degrees of the output , but because of the often unpredictable growth of the coordinates in the face normals , even for polytopes in generic position . We provide experimental results with our preliminary implementation in Sage that compare favorably with the pruning method that relies only on cone intersections .
This paper develops automated testing and debugging techniques for answer set solver development . We describe a flexible grammar-based black-box ASP fuzz testing tool which is able to reveal various defects such as unsound and incomplete behavior , i . e . invalid answer sets and inability to find existing solutions , in state-of-the-art answer set solver implementations . Moreover , we develop delta debugging techniques for shrinking failure-inducing inputs on which solvers exhibit defective behavior . In particular , we develop a delta debugging algorithm in the context of answer set solving , and evaluate two different elimination strategies for the algorithm .
This paper introduces a statistical method to decide whether two blocks in a pair of of images match reliably . The method ensures that the selected block matches are unlikely to have occurred " just by chance . " The new approach is based on the definition of a simple but faithful statistical " background model " for image blocks learned from the image itself . A theorem guarantees that under this model not more than a fixed number of wrong matches occurs ( on average ) for the whole image . This fixed number ( the number of false alarms ) is the only method parameter . Furthermore , the number of false alarms associated with each match measures its reliability . This " a contrario " block-matching method , however , cannot rule out false matches due to the presence of periodic objects in the images . But it is successfully complemented by a parameterless " self-similarity threshold . " Experimental evidence shows that the proposed method also detects occlusions and incoherent motions due to vehicles and pedestrians in non simultaneous stereo .
We present a formalization of the OSGi component framework . Our formalization is intended to be used as a basis for describing behavior of OSGi based systems . Furthermore , we describe specification formalisms for describing properties of OSGi based systems . One application is its use for behavioral types . Potential uses comprise the derivation of runtime monitors , checking compatibility of component composition , discovering components using brokerage services and checking the compatibility of implementation artifacts towards a specification .
Information geometry provides a geometric approach to families of statistical models . The key geometric structures are the Fisher quadratic form and the Amari-Chentsov tensor . In statistics , the notion of sufficient statistic expresses the criterion for passing from one model to another without loss of information . This leads to the question how the geometric structures behave under such sufficient statistics . While this is well studied in the finite sample size case , in the infinite case , we encounter technical problems concerning the appropriate topologies . Here , we introduce notions of parametrized measure models and tensor fields on them that exhibit the right behavior under statistical transformations . Within this framework , we can then handle the topological issues and show that the Fisher metric and the Amari-Chentsov tensor on statistical models in the class of symmetric 0-tensor fields and 0-tensor fields can be uniquely ( up to a constant ) characterized by their invariance under sufficient statistics , thereby achieving a full generalization of the original result of Chentsov to infinite sample sizes . More generally , we decompose Markov morphisms between statistical models in terms of statistics . In particular , a monotonicity result for the Fisher information naturally follows .
We treat all the bivariate lack-of-memory ( BLM ) distributions in a unified approach and develop some new general properties of the BLM distributions , including joint moment generating function , product moments and dependence structure . Necessary and sufficient conditions for the survival functions of BLM distributions to be totally positive of order two are given . Some previous results about specific BLM distributions are improved . In particular , we show that both the Marshall--Olkin survival copula and survival function are totally positive of all orders , regardless of parameters . Besides , we point out that Slepian ' s inequality also holds true for BLM distributions .
We propose a method for obtaining joint probabilistic projections of migration rates for all countries , broken down by age and sex . Joint trajectories for all countries are constrained to satisfy the requirement of zero global net migration . We evaluate our model using out-of-sample validation and compare point projections to the projected migration rates from a persistence model similar to the method used in the United Nations ' World Population Prospects , and also to a state of the art gravity model . We also resolve an apparently paradoxical discrepancy between growth trends in the proportion of the world population migrating and the average absolute migration rate across countries .
Nearly all estimators in statistical prediction come with an associated tuning parameter , in one way or another . Common practice , given data , is to choose the tuning parameter value that minimizes a constructed estimate of the prediction error of the estimator ; we focus on Stein ' s unbiased risk estimator , or SURE ( Stein , 0000 ; Efron , 0000 ) which forms an unbiased estimate of the prediction error by augmenting the observed training error with an estimate of the degrees of freedom of the estimator . Parameter tuning via SURE minimization has been advocated by many authors , in a wide variety of problem settings , and in general , it is natural to ask : what is the prediction error of the SURE-tuned estimator ? An obvious strategy would be simply use the apparent error estimate as reported by SURE , i . e . , the value of the SURE criterion at its minimum , to estimate the prediction error of the SURE-tuned estimator . But this is no longer unbiased ; in fact , we would expect that the minimum of the SURE criterion is systematically biased downwards for the true prediction error . In this paper , we formally describe and study this bias .
In this paper , a new goodness-of-fit test for a location-scale family based on progressively Type-II censored order statistics is proposed . Using Monte Carlo simulation studies , the present researchers have observed that the proposed test for normality is consistent and quite powerful in comparison with existing goodness-of-fit tests based on progressively Type-II censored data . Also , the new test statistic for a real data set is used and the results show that our new test statistic performs well .
Electronic and remote voting has become a large field of research and brought forth a multiplicity of schemes , systems , cryptographic primitives as well as formal definitions and requirements for electronic elections . In this survey we try to give a brief and precise overview and summary of the current situation .
The VC-dimension plays an important role for the algorithmic problem of guarding art galleries efficiently . We prove that inside a simple polygon at most $0$ points can be shattered by $L_0$-visibility polygons and give an example where 0 points are shattered . The VC-dimension is exactly $0$ . The proof idea for the upper bound is different from previous approaches . Keywords : Art gallery , VC-dimension , $L_0$-visibility , polygons
Inference and Estimation in Missing Information ( MI ) scenarios are important topics in Statistical Learning Theory and Machine Learning ( ML ) . In ML literature , attempts have been made to enhance prediction through precise feature selection methods . In sparse linear models , LASSO is well-known in extracting the desired support of the signal and resisting against noisy systems . When sparse models are also suffering from MI , the sparse recovery and inference of the missing models are taken into account simultaneously . In this paper , we will introduce an approach which enjoys sparse regression and covariance matrix estimation to improve matrix completion accuracy , and as a result enhancing feature selection preciseness which leads to reduction in prediction Mean Squared Error ( MSE ) . We will compare the effect of employing covariance matrix in enhancing estimation accuracy to the case it is not used in feature selection . Simulations show the improvement in the performance as compared to the case where the covariance matrix estimation is not used .
Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors . Due to the multidimensional character of tensors in describing complex datasets , tensor completion algorithms and their applications have received wide attention and achievement in data mining , computer vision , signal processing , and neuroscience , etc . In this survey , we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety , large volume , and high velocity . Towards a better comprehension and comparison of vast existing advances , we summarize and categorize them into four groups including general tensor completion algorithms , tensor completion with auxiliary information ( variety ) , scalable tensor completion algorithms ( volume ) and dynamic tensor completion algorithms ( velocity ) . Besides , we introduce their applications on real-world data-driven problems and present an open-source package covering several widely used tensor decomposition and completion algorithms . Our goal is to summarize these popular methods and introduce them to researchers for promoting the research process in this field and give an available repository for practitioners . In the end , we also discuss some challenges and promising research directions in this community for future explorations .
We show that the three-dimensional layers-of-maxima problem can be solved in $o ( n\log n ) $ time in the word RAM model . Our algorithm runs in $O ( n ( \log \log n ) ^0 ) $ deterministic time or $O ( n ( \log\log n ) ^0 ) $ expected time and uses O ( n ) space . We also describe an algorithm that uses optimal O ( n ) space and solves the three-dimensional layers-of-maxima problem in $O ( n\log n ) $ time in the pointer machine model .
PayPal is an account-based system that allows anyone with an email address to send and receive online payment s . This service is easy to use for customers . Members can instantaneously send money to anyone . Recipients are informed by email that they have received a payment . PayPal is also available to people in 00 countries . This paper starts with introduction to the company and its services . The information about the history and the current company situation are covered . Later some interesting and different technical issues are discussed . The Paper ends with analysis of the company and several future recommendations .
This work provides simple algorithms for multi-class ( and multi-label ) prediction in settings where both the number of examples n and the data dimension d are relatively large . These robust and parameter free algorithms are essentially iterative least-squares updates and very versatile both in theory and in practice . On the theoretical front , we present several variants with convergence guarantees . Owing to their effective use of second-order structure , these algorithms are substantially better than first-order methods in many practical scenarios . On the empirical side , we present a scalable stagewise variant of our approach , which achieves dramatic computational speedups over popular optimization packages such as Liblinear and Vowpal Wabbit on standard datasets ( MNIST and CIFAR-00 ) , while attaining state-of-the-art accuracies .
The idea of synthesis , i . e . , the process of automatically computing implementations from their specifications , has recently gained a lot of momentum in the contexts of software engineering and reactive system design . While it is widely believed that , due to complexity/undecidability issues , synthesis cannot completely replace manual engineering , it can assist the process of designing the intricate pieces of code that most programmers find challenging , or help with orchestrating tasks in reactive environments . The SYNT workshop aims to bring together researchers interested in synthesis to discuss and present ongoing and mature work on all aspects of automated synthesis and its applications . The third iteration of the workshop took place in Vienna , Austria , and was co-located with the 00th International Conference on Computer Aided Verification , held in the context of the Vienna Summer of Logic in July 0000 . The workshop included eight contributed talks and four invited talks . In addition , it featured a special session about the Syntax-Guided Synthesis Competition ( SyGuS ) and the SyntComp Synthesis competition .
Analysis of stochastic models of networks is quite important in light of the huge influx of network data in social , information and bio sciences , but a proper statistical analysis of features of different stochastic models of networks is still underway . We propose bootstrap subsampling methods for finding empirical distribution of count features or ``moments ' ' ( Bickel , Chen and Levina [Ann . Statist . 00 ( 0000 ) 0000-0000] ) and smooth functions of these features for the networks . Using these methods , we cannot only estimate the variance of count features but also get good estimates of such feature counts , which are usually expensive to compute numerically in large networks . In our paper , we prove theoretical properties of the bootstrap estimates of variance of the count features as well as show their efficacy through simulation . We also use the method on some real network data for estimation of variance and expectation of some count features .
A graphical model encodes conditional independence relations via the Markov properties . For an undirected graph these conditional independence relations can be represented by a simple polytope known as the graph associahedron , which can be constructed as a Minkowski sum of standard simplices . There is an analogous polytope for conditional independence relations coming from a regular Gaussian model , and it can be defined using multiinformation or relative entropy . For directed acyclic graphical models and also for mixed graphical models containing undirected , directed and bidirected edges , we give a construction of this polytope , up to equivalence of normal fans , as a Minkowski sum of matroid polytopes . Finally , we apply this geometric insight to construct a new ordering-based search algorithm for causal inference via directed acyclic graphical models .
Datalog+/- is a family of ontology languages that combine good computational properties with high expressive power . Datalog+/- languages are provably able to capture the most relevant Semantic Web languages . In this paper we consider the class of weakly-sticky ( WS ) Datalog+/- programs , which allow for certain useful forms of joins in rule bodies as well as extending the well-known class of weakly-acyclic TGDs . So far , only non-deterministic algorithms were known for answering queries on WS Datalog+/- programs . We present novel deterministic query answering algorithms under WS Datalog+/- . In particular , we propose : ( 0 ) a bottom-up grounding algorithm based on a query-driven chase , and ( 0 ) a hybrid approach based on transforming a WS program into a so-called sticky one , for which query rewriting techniques are known . We discuss how our algorithms can be optimized and effectively applied for query answering in real-world scenarios .
This paper presents a new model-based algorithm that computes predictive optimal controls on-line and in closed loop for traditionally challenging nonlinear systems . Examples demonstrate the same algorithm controlling hybrid impulsive , underactuated , and constrained systems using only high-level models and trajectory goals . Rather than iteratively optimize finite horizon control sequences to minimize an objective , this paper derives a closed-form expression for individual control actions , i . e . , control values that can be applied for short duration , that optimally improve a tracking objective over a long time horizon . Under mild assumptions , actions become linear feedback laws near equilibria that permit stability analysis and performance-based parameter selection . Globally , optimal actions are guaranteed existence and uniqueness . By sequencing these actions on-line , in receding horizon fashion , the proposed controller provides a min-max constrained response to state that avoids the overhead typically required to impose control constraints . Benchmark examples show the approach can avoid local minima and outperform nonlinear optimal controllers and recent , case-specific methods in terms of tracking performance , and at speeds orders of magnitude faster than traditionally achievable .
This paper extends the analogies employed in the development of quantum-inspired evolutionary algorithms by proposing quantum-inspired Hadamard walks , called QHW . A novel quantum-inspired evolutionary algorithm , called HQEA , for solving combinatorial optimization problems , is also proposed . The novelty of HQEA lies in it ' s incorporation of QHW Remote Search and QHW Local Search - the quantum equivalents of classical mutation and local search , that this paper defines . The intuitive reasoning behind this approach , and the exploration-exploitation balance thus occurring is explained . From the results of the experiments carried out on the 0 , 0-knapsack problem , HQEA performs significantly better than a conventional genetic algorithm , CGA , and two quantum-inspired evolutionary algorithms - QEA and NQEA , in terms of convergence speed and accuracy .
Probability models are only useful at explaining the uncertainty of what we do not know , and should never be used to say what we already know . Probability and statistical models are useless at discerning cause . Classical statistical procedures , in both their frequentist and Bayesian implementations are , falsely imply they can speak about cause . No hypothesis test , or Bayes factor , should ever be used again . Even assuming we know the cause or partial cause for some set of observations , reporting via relative risk exagerates the certainty we have in the future , often by a lot . This over-certainty is made much worse when parametetric and not predictive methods are used . Unfortunately , predictive methods are rarely used ; and even when they are , cause must still be an assumption , meaning ( again ) certainty in our scientific pronouncements is too high .
In this paper we study the inefficiency ratio of stable equilibria in load balancing games introduced by Asadpour and Saberi [0] . We prove tighter lower and upper bounds of 0/0 and 0/0 , respectively . This improves over the best known bounds in problem ( 00/00 and 0/0 , respectively ) . Equivalently , the results apply to the question of how well the optimum for the $L_0$ -norm can approximate the $L_{\infty}$-norm ( makespan ) in identical machines scheduling .
We present new algorithms for computing the log-determinant of symmetric , diagonally dominant matrices . Existing algorithms run with cubic complexity with respect to the size of the matrix in the worst case . Our algorithm computes an approximation of the log-determinant in time near-linear with respect to the number of non-zero entries and with high probability . This algorithm builds upon the utra-sparsifiers introduced by Spielman and Teng for Laplacian matrices and ultimately uses their refined versions introduced by Koutis , Miller and Peng in the context of solving linear systems . We also present simpler algorithms that compute upper and lower bounds and that may be of more immediate practical interest .
On Board Data Handling ( OBDH ) has functions to monitor , control , acquire , analyze , take a decision , and execute the command . OBDH should organize the task between sub system . OBDH like a heart which has a vital function . Because the function is seriously important therefore designing and implementing the OBDH should be carefully , in order to have a good reliability . Many OBDHs have been made to support the satellite mission using primitive programming . In handling the data from various input , OBDH should always be available to all sub systems , when the tasks are many , it is not easy to program using primitive programming . Sometimes the data become corrupt because the data which come to the OBDH is in the same time . Therefore it is required to have a way to handle the data safely and also easy in programming perspective . In this research , OBDH is programmed using multi tasking programming perspective has been created . The Operating System ( OS ) has been implemented so that can run the tasks simultaneously . The OS is prepared by configuring the Linux Kernel for the specific processor , creating Root File System ( RFS ) , installing the BusyBox . In order to do the above method , preparing the environment in our machine has been done , they are installing the Cross Tool Chain , U-Boot , GNU-Linux Kernel Source etc . After that , programming using c code with multitasking programming can be implemented . By using above method , it is found that programming is easier and the corruption data because of reentrancy can be minimized . Keywords- Operating System , PC-000 , Kernel , C Programming
In this paper nonparametric methods to assess the multivariate L\ ' {e}vy measure are introduced . Starting from high-frequency observations of a L\ ' {e}vy process $\mathbf{X}$ , we construct estimators for its tail integrals and the Pareto-L\ ' {e}vy copula and prove weak convergence of these estimators in certain function spaces . Given n observations of increments over intervals of length $\Delta_n$ , the rate of convergence is $k_n^{-0/0}$ for $k_n=n\Delta_n$ which is natural concerning inference on the L\ ' {e}vy measure . Besides extensions to nonequidistant sampling schemes analytic properties of the Pareto-L\ ' {e}vy copula which , to the best of our knowledge , have not been mentioned before in the literature are provided as well . We conclude with a short simulation study on the performance of our estimators and apply them to real data .
We consider the problem of approximating optimal in the Minimum Mean Squared Error ( MMSE ) sense nonlinear filters in a discrete time setting , exploiting properties of stochastically convergent state process approximations . More specifically , we consider a class of nonlinear , partially observable stochastic systems , comprised by a ( possibly nonstationary ) hidden stochastic process ( the state ) , observed through another conditionally Gaussian stochastic process ( the observations ) . Under general assumptions , we show that , given an approximating process which , for each time step , is stochastically convergent to the state process , an approximate filtering operator can be defined , which converges to the true optimal nonlinear filter of the state in a strong and well defined sense . In particular , the convergence is compact in time and uniform in a completely characterized measurable set of probability measure almost unity , also providing a purely quantitative justification of Egoroff ' s Theorem for the problem at hand . The results presented in this paper can form a common basis for the analysis and characterization of a number of heuristic approaches for approximating optimal nonlinear filters , such as approximate grid based techniques , known to perform well in a variety of applications .
This paper addresses the issue of detecting change-points in multivariate time series . The proposed approach differs from existing counterparts by making only weak assumptions on both the change-points structure across series , and the statistical signal distributions . Specifically change-points are not assumed to occur at simultaneous time instants across series , and no specific distribution is assumed on the individual signals . It relies on the combination of a local robust statistical test acting on individual time segments , with a global Bayesian framework able to optimize configurations from multiple local statistics ( from segments of a unique time series or multiple time series ) . Using an extensive experimental set-up , our algorithm is shown to perform well on Gaussian data , with the same results in term of recall and precision as classical approaches , such as the fused lasso and the Bernoulli Gaussian model . Furthermore , it outperforms the reference models in the case of non normal data with outliers . The control of the False Discovery Rate by an acceptance level is confirmed . In the case of multivariate data , the probabilities that simultaneous change-points are shared by some specific time series are learned . We finally illustrate our algorithm with real datasets from energy monitoring and genomic . Segmentations are compared to state-of-the-art approaches based on fused lasso and group fused lasso .
This paper presents a demo of our Security Toolbox to detect novel malware in Android apps . This Toolbox is developed through our recent research project funded by the DARPA Automated Program Analysis for Cybersecurity ( APAC ) project . The adversarial challenge ( " Red " ) teams in the DARPA APAC program are tasked with designing sophisticated malware to test the bounds of malware detection technology being developed by the research and development ( " Blue " ) teams . Our research group , a Blue team in the DARPA APAC program , proposed a " human-in-the-loop program analysis " approach to detect malware given the source or Java bytecode for an Android app . Our malware detection apparatus consists of two components : a general-purpose program analysis platform called Atlas , and a Security Toolbox built on the Atlas platform . This paper describes the major design goals , the Toolbox components to achieve the goals , and the workflow for auditing Android apps . The accompanying video ( http : //youtu . be/WhcoAX0HiNU ) illustrates features of the Toolbox through a live audit .
Restricted Boltzmann machines ( RBMs ) are energy-based neural-networks which are commonly used as the building blocks for deep architectures neural architectures . In this work , we derive a deterministic framework for the training , evaluation , and use of RBMs based upon the Thouless-Anderson-Palmer ( TAP ) mean-field approximation of widely-connected systems with weak interactions coming from spin-glass theory . While the TAP approach has been extensively studied for fully-visible binary spin systems , our construction is generalized to latent-variable models , as well as to arbitrarily distributed real-valued spin systems with bounded support . In our numerical experiments , we demonstrate the effective deterministic training of our proposed models and are able to show interesting features of unsupervised learning which could not be directly observed with sampling . Additionally , we demonstrate how to utilize our TAP-based framework for leveraging trained RBMs as joint priors in denoising problems .
In this paper , we propose a StochAstic Recursive grAdient algoritHm ( SARAH ) , as well as its practical variant SARAH+ , as a novel approach to the finite-sum minimization problems . Different from the vanilla SGD and other modern stochastic methods such as SVRG , S0GD , SAG and SAGA , SARAH admits a simple recursive framework for updating stochastic gradient estimates ; when comparing to SAG/SAGA , SARAH does not require a storage of past gradients . The linear convergence rate of SARAH is proven under strong convexity assumption . We also prove a linear convergence rate ( in the strongly convex case ) for an inner loop of SARAH , the property that SVRG does not possess . Numerical experiments demonstrate the efficiency of our algorithm .
Within the last decade , running has become one of the most popular physical activities in the world . Although the benefits of running are numerous , there is a risk of Running Related Injuries ( RRI ) of the lower extremities . Electromyography ( EMG ) techniques have previously been used to study causes of RRIs , but the complexity of this technology limits its use to a laboratory setting . As running is primarily an outdoors activity , this lack of technology acts as a barrier to the study of RRIs in natural environments . This study presents a minimally invasive wearable muscle sensing device consisting of jogging leggings with embroidered surface EMG ( sEMG ) electrodes capable of recording muscle activity data of the quadriceps group . To test the use of the device , a proof of concept study consisting of $N=0$ runners performing a set of $0km$ running trials is presented in which the effect of running surfaces on muscle fatigue , a potential cause of RRIs , is evaluated . Results show that muscle fatigue can be analysed from the sEMG data obtained through the wearable device , and that running on soft surfaces ( such as sand ) may increase the likelihood of suffering from RRIs .
The classical multi-set split feasibility problem seeks a point in the intersection of finitely many closed convex domain constraints , whose image under a linear mapping also lies in the intersection of finitely many closed convex range constraints . Split feasibility generalizes important inverse problems including convex feasibility , linear complementarity , and regression with constraint sets . When a feasible point does not exist , solution methods that proceed by minimizing a proximity function can be used to obtain optimal approximate solutions to the problem . We present an extension of the proximity function approach that generalizes the linear split feasibility problem to allow for non-linear mappings . Our algorithm is based on the principle of majorization-minimization , is amenable to quasi-Newton acceleration , and comes complete with convergence guarantees under mild assumptions . Furthermore , we show that the Euclidean norm appearing in the proximity function of the non-linear split feasibility problem can be replaced by arbitrary Bregman divergences . We explore several examples illustrating the merits of non-linear formulations over the linear case , with a focus on optimization for intensity-modulated radiation therapy .
We consider kernel estimation of marginal densities and regression functions of stationary processes . It is shown that for a wide class of time series , with proper centering and scaling , the maximum deviations of kernel density and regression estimates are asymptotically Gumbel . Our results substantially generalize earlier ones which were obtained under independence or beta mixing assumptions . The asymptotic results can be applied to assess patterns of marginal densities or regression functions via the construction of simultaneous confidence bands for which one can perform goodness-of-fit tests . As an application , we construct simultaneous confidence bands for drift and volatility functions in a dynamic short-term rate model for the U . S . Treasury yield curve rates data .
We are interested in estimating individual labels given only coarse , aggregated signal over the data points . In our setting , we receive sets ( " bags " ) of unlabeled instances with constraints on label proportions . We relax the unrealistic assumption of known label proportions , made in previous work ; instead , we assume only to have upper and lower bounds , and constraints on bag differences . We motivate the problem , propose an intuitive formulation and algorithm , and apply our methods to real-world scenarios . Across several domains , we show how using only proportion constraints and no labeled examples , we can achieve surprisingly high accuracy . In particular , we demonstrate how to predict income level using rough stereotypes and how to perform sentiment analysis using very little information . We also apply our method to guide exploratory analysis , recovering geographical differences in twitter dialect .
We present a variant of ATL with distributed knowledge operators based on a synchronous and perfect recall semantics . The coalition modalities in this logic are based on partial observation of the full history , and incorporate a form of cooperation between members of the coalition in which agents issue their actions based on the distributed knowledge , for that coalition , of the system history . We show that model-checking is decidable for this logic . The technique utilizes two variants of games with imperfect information and partially observable objectives , as well as a subset construction for identifying states whose histories are indistinguishable to the considered coalition .
We consider the Lasso for a noiseless experiment where one has observations $X \beta^0$ and uses the penalized version of basis pursuit . We compute for some special designs the compatibility constant , a quantity closely related to the restricted eigenvalue . We moreover show the dependence of the ( penalized ) prediction error on this compatibility constant . This exercise illustrates that compatibility is necessarily entering into the bounds for the ( penalized ) prediction error and that the bounds in the literature therefore are - up to constants - tight . We also give conditions that show that in the noisy case the dominating term for the prediction error is given by the prediction error of the noiseless case .
Starting from the basic ideas of mathematica , we give a detailed description about the way of linking of external programs with mathematica through proper mathlink commands . This article may be quite helpful for the beginners to start with and write programs in mathematica . In the first part , we illustrate how to use a mathemtica notebook and write a complete program in the notebook . Following with this , we also mention elaborately about the utility of the local and global variables those are very essential for writing a program in mathematica . All the commands needed for doing different mathematical operations can be found with some proper examples in the mathematica book written by Stephen Wolfram \cite{wolfram} . In the rest of this article , we concentrate our study on the most significant issue which is the process of linking of {\em external programs} with mathematica , so-called the mathlink operation . By using proper mathlink commands one can run very tedious jobs efficiently and the operations become extremely fast .
The global sensitivity analysis of a complex numerical model often calls for the estimation of variance-based importance measures , named Sobol ' indices . Metamodel-based techniques have been developed in order to replace the cpu time-expensive computer code with an inexpensive mathematical function , which predicts the computer code output . The common metamodel-based sensitivity analysis methods are well-suited for computer codes with scalar outputs . However , in the environmental domain , as in many areas of application , the numerical model outputs are often spatial maps , which may also vary with time . In this paper , we introduce an innovative method to obtain a spatial map of Sobol ' indices with a minimal number of numerical model computations . It is based upon the functional decomposition of the spatial output onto a wavelet basis and the metamodeling of the wavelet coefficients by the Gaussian process . An analytical example is presented to clarify the various steps of our methodology . This technique is then applied to a real hydrogeological case : for each model input variable , a spatial map of Sobol ' indices is thus obtained .
We consider a multivariate linear response regression in which the number of responses and predictors is large and comparable with the number of observations , and the rank of the matrix of regression coefficients is assumed to be small . We study the distribution of singular values for the matrix of regression coefficients and for the matrix of predicted responses . For both matrices , it is found that the limit distribution of the largest singular value is a rescaling of the Tracy-Widom distribution . Based on this result , we suggest algorithms for the model rank selection and compare them with the algorithm suggested by Bunea , She and Wegkamp . Next , we design two consistent estimators for the singular values of the coefficient matrix , compare them , and derive the asymptotic distribution for one of these estimators . .
In river flow analysis and forecasting there are some key elements to consider in order to obtain reliable results . For example , seasonality is often accounted for in statistical models because climatic oscillations occurring every year have an obvious impact on river flow . Further sources of alteration could be caused by changes in reservoir management , instrumentation or even unexpected shifts in climatic conditions . When these changes are ignored the statistical results can be strongly misleading . This paper develops an automatic procedure to estimate number and locations of changepoints in Periodic AutoRegressive models . These latter have been extensively used for modelling seasonality in hydrology , climatology , economics and electrical engineering , but there are very few papers devoted also to changepoints detection , moreover being limited to changes in mean or variance . In our proposal we allow the model structure as a whole to change , and estimation is performed by optimizing an objective function derived from the Information Criterion using Genetic Algorithms . The proposed methodology is brought out through the example of three river flows , for which we built models with possible changepoints and evaluated their forecasting accuracy by means of Root Mean Square Error , Mean Absolute Error and Mean Absolute Percentage Error . The last years of data sets have been omitted from the selection and estimation procedure and were then used to forecast . Comparisons with literature on river flow forecasting confirms the efficiency of our proposal .
As scientific discovery becomes increasingly data-driven , software platforms are needed to efficiently organize and disseminate data from disparate sources . This is certainly the case in the field of materials science . For example , Materials Project has generated computational data on over 00 , 000 chemical compounds and has made that data available through a web portal and REST interface . However , such portals must seek to incorporate community submissions to expand the scope of scientific data sharing . In this paper , we describe MPContribs , a computing/software infrastructure to integrate and organize contributions of simulated or measured materials data from users . Our solution supports complex submissions and provides interfaces that allow contributors to share analyses and graphs . A RESTful API exposes mechanisms for book-keeping , retrieval and aggregation of submitted entries , as well as persistent URIs or DOIs that can be used to reference the data in publications . Our approach isolates contributed data from a host project ' s quality-controlled core data and yet enables analyses across the entire dataset , programmatically or through customized web apps . We expect the developed framework to enhance collaborative determination of material properties and to maximize the impact of each contributor ' s dataset . In the long-term , MPContribs seeks to make Materials Project an institutional , and thus community-wide , memory for computational and experimental materials science .
In this paper we investigate the ability of generative adversarial networks ( GANs ) to synthesize spoofing attacks on modern speaker recognition systems . We first show that samples generated with SampleRNN and WaveNet are unable to fool a CNN-based speaker recognition system . We propose a modification of the Wasserstein GAN objective function to make use of data that is real but not from the class being learned . Our semi-supervised learning method is able to perform both targeted and untargeted attacks , raising questions related to security in speaker authentication systems .
We consider the estimation of large covariance and precision matrices from high-dimensional sub-Gaussian or heavier-tailed observations with slowly decaying temporal dependence . The temporal dependence is allowed to be long-range so with longer memory than those considered in the current literature . We show that several commonly used methods for independent observations can be applied to the temporally dependent data . In particular , the rates of convergence are obtained for the generalized thresholding estimation of covariance and correlation matrices , and for the constrained $\ell_0$ minimization and the $\ell_0$ penalized likelihood estimation of precision matrix . Properties of sparsistency and sign-consistency are also established . A gap-block cross-validation method is proposed for the tuning parameter selection , which performs well in simulations . As a motivating example , we study the brain functional connectivity using resting-state fMRI time series data with long-range temporal dependence .
Given a sequence of random variables ${\bf X}=X_0 , X_0 , \ldots$ suppose the aim is to maximize one ' s return by picking a `favorable ' $X_i$ . Obviously , the expected payoff crucially depends on the information at hand . An optimally informed person knows all the values $X_i=x_i$ and thus receives $E ( \sup X_i ) $ . We will compare this return to the expected payoffs of a number of observers having less information , in particular $\sup_i ( EX_i ) $ , the value of the sequence to a person who only knows the first moments of the random variables . In general , there is a stochastic environment ( i . e . a class of random variables $\cal C$ ) , and several levels of information . Given some ${\bf X} \in {\cal C}$ , an observer possessing information $j$ obtains $r_j ( {\bf X} ) $ . We are going to study `information sets ' of the form $$ R_{\cal C}^{j , k} = \{ ( x , y ) | x = r_j ( {\bf X} ) , y=r_k ( {\bf X} ) , {\bf X} \in {\cal C} \} , $$ characterizing the advantage of $k$ relative to $j$ . Since such a set measures the additional payoff by virtue of increased information , its analysis yields a number of interesting results , in particular `prophet-type ' inequalities .
The phenomenal growth in the healthcare data has inspired us in investigating robust and scalable models for data mining . For classification problems Information Gain ( IG ) based Decision Tree is one of the popular choices . However , depending upon the nature of the dataset , IG based Decision Tree may not always perform well as it prefers the attribute with more number of distinct values as the splitting attribute . Healthcare datasets generally have many attributes and each attribute generally has many distinct values . In this paper , we have tried to focus on this characteristics of the datasets while analysing the performance of our proposed approach which is a variant of Decision Tree model and uses the concept of Correlation Ratio ( CR ) . Unlike IG based approach , this CR based approach has no biasness towards the attribute with more number of distinct values . We have applied our model on some benchmark healthcare datasets to show the effectiveness of the proposed technique .
We provide a condition for f-ergodicity of strong Markov processes at a subgeometric rate . This condition is couched in terms of a supermartingale property for a functional of the Markov process . Equivalent formulations in terms of a drift inequality on the extended generator and on the resolvent kernel are given . Results related to ( f , r ) -regularity and to moderate deviation principle for integral ( bounded ) functional are also derived . Applications to specific processes are considered , including elliptic stochastic differential equation , Langevin diffusions , hypoelliptic stochastic damping Hamiltonian system and storage models .
Covariance is used as an inner product on a formal vector space built on n random variables to define measures of correlation Md across a set of vectors in a d-dimensional space . For d = 0 , one has the diameter ; for d = 0 , one has an area . These concepts are directly applied to correlation studies in climate science .
The problem of estimating the mean of random functions based on discretely sampled data arises naturally in functional data analysis . In this paper , we study optimal estimation of the mean function under both common and independent designs . Minimax rates of convergence are established and easily implementable rate-optimal estimators are introduced . The analysis reveals interesting and different phase transition phenomena in the two cases . Under the common design , the sampling frequency solely determines the optimal rate of convergence when it is relatively small and the sampling frequency has no effect on the optimal rate when it is large . On the other hand , under the independent design , the optimal rate of convergence is determined jointly by the sampling frequency and the number of curves when the sampling frequency is relatively small . When it is large , the sampling frequency has no effect on the optimal rate . Another interesting contrast between the two settings is that smoothing is necessary under the independent design , while , somewhat surprisingly , it is not essential under the common design .
We demonstrate a polynomial approach to express the decision version of the directed Hamiltonian Cycle Problem ( HCP ) , which is NP-Complete , as the Solvability of a Polynomial Equation with a constant number of variables , within a bounded real space . We first introduce four new Theorems for a set of periodic Functions with irrational periods , based on which we then use a trigonometric substitution , to show how the HCP can be expressed as the Solvability of a single polynomial Equation with a constant number of variables . The feasible solution of each of these variables is bounded within two real numbers . We point out what future work is necessary to prove that P=NP .
Consider a linear regression model . Fan and Li ( 0000 ) describe the smoothly clipped absolute deviation ( SCAD ) point estimator of the regression parameter vector . To gain insight into the properties of this estimator , they consider an orthonormal design matrix and focus on the estimation of a specified component of this vector . They show that the SCAD point estimator has three attractive properties . We answer the question : To what extent can an interval estimator , centred on the SCAD estimator , have similar attractive properties ?
Approximations of Laplace-Beltrami operators on manifolds through graph Lapla-cians have become popular tools in data analysis and machine learning . These discretized operators usually depend on bandwidth parameters whose tuning remains a theoretical and practical problem . In this paper , we address this problem for the unnormalized graph Laplacian by establishing an oracle inequality that opens the door to a well-founded data-driven procedure for the bandwidth selection . Our approach relies on recent results by Lacour and Massart [LM00] on the so-called Lepski ' s method .
This paper is devoted to the convergence analysis of stochastic approximation algorithms of the form $\theta\_{n+0} = \theta\_n + \gamma\_{n+0} H\_{\theta\_n} ( X\_{n+0} ) $ where $\{\theta\_nn , n \geq 0\}$ is a $R^d$-valued sequence , $\{\gamma , n \geq 0\}$ is a deterministic step-size sequence and $\{X\_n , n \geq 0\}$ is a controlled Markov chain . We study the convergence under weak assumptions on smoothness-in-$\theta$ of the function $\theta \mapsto H\_{\theta} ( x ) $ . It is usually assumed that this function is continuous for any $x$ ; in this work , we relax this condition . Our results are illustrated by considering stochastic approximation algorithms for ( adaptive ) quantile estimation and a penalized version of the vector quantization .
We present and formalize h , a core ( or " plank " ) calculus that can serve as the foundation for several compiler specification languages , notably CRSX ( Combinatory Reductions Systems with eXtensions ) , HACS ( Higher-order Attribute Contraction Schemes ) , and TransScript . We discuss how the h typing and formation rules introduce the necessary restrictions to ensure that rewriting is well-defined , even in the presence of h ' s powerful extensions for manipulating free variables and environments as first class elements ( including in pattern matching ) .
Assume that one aims to simulate an event of unknown probability $s\in ( 0 , 0 ) $ which is uniquely determined , however only its approximations can be obtained using a finite computational effort . Such settings are often encountered in statistical simulations . We consider two specific examples . First , the exact simulation of non-linear diffusions , second , the celebrated Bernoulli factory problem of generating an $f ( p ) -$coin given a sequence $X_0 , X_0 , . . . $ of independent tosses of a $p-$coin ( with known $f$ and unknown $p$ ) . We describe a general framework and provide algorithms where this kind of problems can be fitted and solved . The algorithms are straightforward to implement and thus allow for effective simulation of desired events of probability $s . $ In the case of diffusions , we obtain the algorithm of \cite{BeskosRobertsEA0} as a specific instance of the generic framework developed here . In the case of the Bernoulli factory , our work offers a statistical understanding of the Nacu-Peres algorithm for $f ( p ) = \min\{0p , 0-0\varepsilon\}$ ( which is central to the general question ) and allows for its immediate implementation that avoids algorithmic difficulties of the original version .
When an item goes out of stock , sales transaction data no longer reflect the original customer demand , since some customers leave with no purchase while others substitute alternative products for the one that was out of stock . Here we develop a Bayesian hierarchical model for inferring the underlying customer arrival rate and choice model from sales transaction data and the corresponding stock levels . The model uses a nonhomogeneous Poisson process to allow the arrival rate to vary throughout the day , and allows for a variety of choice models . Model parameters are inferred using a stochastic gradient MCMC algorithm that can scale to large transaction databases . We fit the model to data from a local bakery and show that it is able to make accurate out-of-sample predictions , and to provide actionable insight into lost cookie sales .
We present a novel approach for generalizing the IC0 algorithm for invariant checking from finite-state to infinite-state transition systems , expressed over some background theories . The procedure is based on a tight integration of IC0 with Implicit ( predicate ) Abstraction , a technique that expresses abstract tran- sitions without computing explicitly the abstract system and is incremental with respect to the addition of predicates . In this scenario , IC0 operates only at the Boolean level of the abstract state space , discovering inductive clauses over the abstraction predicates . Theory reasoning is confined within the underlying SMT solver , and applied transparently when performing satisfiability checks . When the current abstraction allows for a spurious counterexample , it is refined by discov- ering and adding a sufficient set of new predicates . Importantly , this can be done in a completely incremental manner , without discarding the clauses found in the previous search . The proposed approach has two key advantages . First , unlike current SMT gener- alizations of IC0 , it allows to handle a wide range of background theories without relying on ad-hoc extensions , such as quantifier elimination or theory-specific clause generalization procedures , which might not always be available , and can moreover be inefficient . Second , compared to a direct exploration of the concrete transition system , the use of abstraction gives a significant performance improve- ment , as our experiments demonstrate .
Theoretical developments on cross validation ( CV ) have mainly focused on selecting one among a list of finite-dimensional models ( e . g . , subset or order selection in linear regression ) or selecting a smoothing parameter ( e . g . , bandwidth for kernel smoothing ) . However , little is known about consistency of cross validation when applied to compare between parametric and nonparametric methods or within nonparametric methods . We show that under some conditions , with an appropriate choice of data splitting ratio , cross validation is consistent in the sense of selecting the better procedure with probability approaching 0 . Our results reveal interesting behavior of cross validation . When comparing two models ( procedures ) converging at the same nonparametric rate , in contrast to the parametric case , it turns out that the proportion of data used for evaluation in CV does not need to be dominating in size . Furthermore , it can even be of a smaller order than the proportion for estimation while not affecting the consistency property .
In this paper , we consider an estimation problem concerning the matrix of correlation coefficients in context of high dimensional data settings . In particular , we revisit some results in Li and Rolsalsky [Li , D . and Rolsalsky , A . ( 0000 ) . Some strong limit theorems for the largest entries of sample correlation matrices , The Annals of Applied Probability , 00 , 0 , 000-000] . Four of the main theorems of Li and Rolsalsky ( 0000 ) are established in their full generalities and we simplify substantially some proofs of the quoted paper . Further , we generalize a theorem which is useful in deriving the existence of the pth moment as well as in studying the convergence rates in law of large numbers .
The major function of this model is to access the UCI Wisconsin Breast Can- cer data-set[0] and classify the data items into two categories , which are normal and anomalous . This kind of classifi cation can be referred as anomaly detection , which discriminates anomalous behaviour from normal behaviour in computer systems . One popular solution for anomaly detection is Artifi cial Immune Sys- tems ( AIS ) . AIS are adaptive systems inspired by theoretical immunology and observed immune functions , principles and models which are applied to prob- lem solving . The Dendritic Cell Algorithm ( DCA ) [0] is an AIS algorithm that is developed specifi cally for anomaly detection . It has been successfully applied to intrusion detection in computer security . It is believed that agent-based mod- elling is an ideal approach for implementing AIS , as intelligent agents could be the perfect representations of immune entities in AIS . This model evaluates the feasibility of re-implementing the DCA in an agent-based simulation environ- ment called AnyLogic , where the immune entities in the DCA are represented by intelligent agents . If this model can be successfully implemented , it makes it possible to implement more complicated and adaptive AIS models in the agent-based simulation environment .
A simple digraph is semi-complete if for any two of its vertices $u$ and $v$ , at least one of the arcs $ ( u , v ) $ and $ ( v , u ) $ is present . We study the complexity of computing two layout parameters of semi-complete digraphs : cutwidth and optimal linear arrangement ( OLA ) . We prove that : ( 0 ) Both parameters are $\mathsf{NP}$-hard to compute and the known exact and parameterized algorithms for them have essentially optimal running times , assuming the Exponential Time Hypothesis ; ( 0 ) The cutwidth parameter admits a quadratic Turing kernel , whereas it does not admit any polynomial kernel unless $\mathsf{NP}\subseteq \mathsf{coNP}/\textrm{poly}$ . By contrast , OLA admits a linear kernel . These results essentially complete the complexity analysis of computing cutwidth and OLA on semi-complete digraphs . Our techniques can be also used to analyze the sizes of minimal obstructions for having small cutwidth under the induced subdigraph relation .
Astrophysicists are interested in recovering the 0D gas emissivity of a galaxy cluster from a 0D image taken by a telescope . A blurring phenomenon and presence of point sources make this inverse problem even harder to solve . The current state-of-the-art technique is two step : first identify the location of potential point sources , then mask these locations and deproject the data . We instead model the data as a Poisson generalized linear model ( involving blurring , Abel and wavelets operators ) regularized by two lasso penalties to induce sparse wavelet representation and sparse point sources . The amount of sparsity is controlled by two quantile universal thresholds . As a result , our method outperforms the existing one .
This paper introduces a nonparametric copula-based approach for detecting the strength and uniquely , the monotonicity structure of linear and nonlinear statistical dependence between pairs of random variables or stochastic signals , termed CIM . CIM satisfies the data processing inequality and is , consequently , a self-equitable metric . Simulation results using synthetic datasets reveal that the CIM compares favorably to other state-of-the-art measures of association that satisfy the data processing inequality , including the estimators of mutual information based on k-nearest neighbors , k-NN , adaptive partitioning , AP , and the von-Mises expansion , vME . Additionally , CIM performs similarly to other state-of-the-art statistical dependency metrics , including the Maximal Information Coefficient ( MIC ) , Randomized Dependency Coefficient ( RDC ) , distance correlation ( dCor ) , copula correlation ( Ccor ) , and the Copula Statistic ( CoS ) in both statistical power and sample size requirements . Simulations using real world data highlight the importance of understanding the monotonicity structure of the dependence .
We present two new hybrid techniques that replace the synchronized product used in the automata-theoretic approach for LTL model checking . The proposed products are explicit graphs of aggregates ( symbolic sets of states ) that can be interpreted as B\ " uchi automata . These hybrid approaches allow on the one hand to use classical emptiness-check algorithms and build the graph on-the-fly , and on the other hand , to have a compact encoding of the state space thanks to the symbolic representation of the aggregates . The Symbolic Observation Product assumes a globally stuttering property ( e . g . , LTL \ X ) to aggregate states . The Self-Loop Aggregation Product} does not require the property to be globally stuttering ( i . e . , it can tackle full LTL ) , but dynamically detects and exploits a form of stuttering where possible . Our experiments show that these two variants , while incomparable with each other , can outperform other existing approaches .
Comparisons between observed and predicted strong lensing properties of galaxy clusters have been routinely used to claim either tension or consistency with $\Lambda$CDM cosmology . However , standard approaches to such cosmological tests are unable to quantify the preference for one cosmology over another . We advocate approximating the relevant Bayes factor using a marginal likelihood that is based on the following summary statistic : the posterior probability distribution function for the parameters of the scaling relation between Einstein radii and cluster mass , $\alpha$ and $\beta$ . We demonstrate , for the first time , a method of estimating the marginal likelihood using the X-ray selected $z>0 . 0$ MACS clusters as a case in point and employing both N-body and hydrodynamic simulations of clusters . We investigate the uncertainty in this estimate and consequential ability to compare competing cosmologies , that arises from incomplete descriptions of baryonic processes , discrepancies in cluster selection criteria , redshift distribution , and dynamical state . The relation between triaxial cluster masses at various overdensities provide a promising alternative to the strong lensing test .
Motivated by the problem of nonparametric inference in high level digital image analysis , we introduce a general extrinsic approach for data analysis on Hilbert manifolds with a focus on means of probability distributions on such sample spaces . To perform inference on these means , we appeal to the concept of neighborhood hypotheses from functional data analysis and derive a one-sample test . We then consider analysis of shapes of contours lying in the plane . By embedding the corresponding sample space of such shapes , which is a Hilbert manifold , into a space of Hilbert-Schmidt operators , we can define extrinsic mean shapes of planar contours and their sample analogues . We apply the general methods to this problem while considering the computational restrictions faced when utilizing digital imaging data . Comparisons of computational cost are provided to another method for analyzing shapes of contours .
Studies of functional MRI data are increasingly concerned with the estimation of differences in spatio-temporal networks across groups of subjects or experimental conditions . Unsupervised clustering and independent component analysis ( ICA ) have been used to identify such spatio-temporal networks . While these approaches have been useful for estimating these networks at the subject-level , comparisons over groups or experimental conditions require further methodological development . In this paper , we tackle this problem by showing how self-organizing maps ( SOMs ) can be compared within a Frechean inferential framework . Here , we summarize the mean SOM in each group as a Frechet mean with respect to a metric on the space of SOMs . We consider the use of different metrics , and introduce two extensions of the classical sum of minimum distance ( SMD ) between two SOMs , which take into account the spatio-temporal pattern of the fMRI data . The validity of these methods is illustrated on synthetic data . Through these simulations , we show that the three metrics of interest behave as expected , in the sense that the ones capturing temporal , spatial and spatio-temporal aspects of the SOMs are more likely to reach significance under simulated scenarios characterized by temporal , spatial and spatio-temporal differences , respectively . In addition , a re-analysis of a classical experiment on visually-triggered emotions demonstrates the usefulness of this methodology . In this study , the multivariate functional patterns typical of the subjects exposed to pleasant and unpleasant stimuli are found to be more similar than the ones of the subjects exposed to emotionally neutral stimuli . Taken together , these results indicate that our proposed methods can cast new light on existing data by adopting a global analytical perspective on functional MRI paradigms .
We propose generalized additive partial linear models for complex data which allow one to capture nonlinear patterns of some covariates , in the presence of linear components . The proposed method improves estimation efficiency and increases statistical power for correlated data through incorporating the correlation information . A unique feature of the proposed method is its capability of handling model selection in cases where it is difficult to specify the likelihood function . We derive the quadratic inference function-based estimators for the linear coefficients and the nonparametric functions when the dimension of covariates diverges , and establish asymptotic normality for the linear coefficient estimators and the rates of convergence for the nonparametric functions estimators for both finite and high-dimensional cases . The proposed method and theoretical development are quite challenging since the numbers of linear covariates and nonlinear components both increase as the sample size increases . We also propose a doubly penalized procedure for variable selection which can simultaneously identify nonzero linear and nonparametric components , and which has an asymptotic oracle property . Extensive Monte Carlo studies have been conducted and show that the proposed procedure works effectively even with moderate sample sizes . A pharmacokinetics study on renal cancer data is illustrated using the proposed method .
A formula is derived for the log quantile difference of the temporal aggregation of some types of stable moving average processes , MA ( q ) . The shape of the log quantile difference as a function of the aggregation level is examined and shown to be dependent on the parameters of the moving average process but not the quantile levels . The classes of invertible , stable MA ( 0 ) and MA ( 0 ) processes are examined in more detail .
The Macaulay0 package GraphicalModels contains algorithms for the algebraic study of graphical models associated to undirected , directed and mixed graphs , and associated collections of conditional independence statements . Among the algorithms implemented are procedures for computing the vanishing ideal of graphical models , for generating conditional independence ideals of families of independence statements associated to graphs , and for checking for identifiable parameters in Gaussian mixed graph models . These procedures can be used to study fundamental problems about graphical models .
Recently Balakrishnan and Iliopoulos [Ann . Inst . Statist . Math . 00 ( 0000 ) ] gave sufficient conditions under which maximum likelihood estimator ( MLE ) is stochastically increasing . In this paper we study test plans which are not considered there and we prove that MLEs for those plans are also stochastic ordered . We also give some applications to the estimation of reliability .
Microservice Architectures ( MA ) have the potential to increase the agility of software development . In an era where businesses require software applications to evolve to support software emerging requirements , particularly for Internet of Things ( IoT ) applications , we examine the issue of microservice granularity and explore its effect upon application latency . Two approaches to microservice deployment are simulated ; the first with microservices in a single container , and the second with microservices partitioned across separate containers . We observed a neglibible increase in service latency for the multiple container deployment over a single container .
We present Deep Voice , a production-quality text-to-speech system constructed entirely from deep neural networks . Deep Voice lays the groundwork for truly end-to-end neural speech synthesis . The system comprises five major building blocks : a segmentation model for locating phoneme boundaries , a grapheme-to-phoneme conversion model , a phoneme duration prediction model , a fundamental frequency prediction model , and an audio synthesis model . For the segmentation model , we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification ( CTC ) loss . For the audio synthesis model , we implement a variant of WaveNet that requires fewer parameters and trains faster than the original . By using a neural network for each component , our system is simpler and more flexible than traditional text-to-speech systems , where each component requires laborious feature engineering and extensive domain expertise . Finally , we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 000x speedups over existing implementations .
This paper presents a parallel genetic algorithm for three dimensional bin packing with heterogeneous bins using Hadoop Map-Reduce framework . The most common three dimensional bin packing problem which packs given set of boxes into minimum number of equal sized bins is proven to be NP Hard . The variation of three dimensional bin packing problem that allows heterogeneous bin sizes and rotation of boxes is computationally more harder than common three dimensional bin packing problem . The proposed Map-Reduce implementation helps to run the genetic algorithm for three dimensional bin packing with heterogeneous bins on multiple machines parallely and computes the solution in relatively short time .
Discussion of ``EQUI-energy sampler ' ' by Kou , Zhou and Wong [math . ST/0000000]
Recent advances with in-memory columnar database techniques have increased the performance of analytical queries on very large databases and data warehouses . At the same time , advances in artificial intelligence ( AI ) algorithms have increased the ability to analyze data . We use the term AI to encompass both Deep Learning ( DL or neural network ) and Machine Learning ( ML aka Big Data analytics ) . Our exploration of the AI full stack has led us to a cross-stack columnar database innovation that efficiently creates features for AI analytics . The innovation is to create Augmented Dictionary Values ( ADVs ) to add to existing columnar database dictionaries in order to increase the efficiency of featurization by minimizing data movement and data duplication . We show how various forms of featurization ( feature selection , feature extraction , and feature creation ) can be efficiently calculated in a columnar database . The full stack AI investigation has also led us to propose an integrated columnar database and AI architecture . This architecture has information flows and feedback loops to improve the whole analytics cycle during multiple iterations of extracting data from the data sources , featurization , and analysis .
Computing practice today depends on visual output to drive almost all user interaction . Other senses , such as audition , may be totally neglected , or used tangentially , or used in highly restricted specialized ways . We have excellent audio rendering through D-A conversion , but we lack rich general facilities for modeling and manipulating sound comparable in quality and flexibility to graphics . We need co-ordinated research in several disciplines to improve the use of sound as an interactive information channel . Incremental and separate improvements in synthesis , analysis , speech processing , audiology , acoustics , music , etc . will not alone produce the radical progress that we seek in sonic practice . We also need to create a new central topic of study in digital audio research . The new topic will assimilate the contributions of different disciplines on a common foundation . The key central concept that we lack is sound as a general-purpose information channel . We must investigate the structure of this information channel , which is driven by the co-operative development of auditory perception and physical sound production . Particular audible encodings , such as speech and music , illuminate sonic information by example , but they are no more sufficient for a characterization than typography is sufficient for a characterization of visual information .
Although the traditional permute matrix coming along with Hopfield is able to describe many common problems , it seems to have limitation in solving more complicated problem with more constrains , like resource leveling which is actually a NP problem . This paper tries to find a better solution for it by using neural network . In order to give the neural network description of resource leveling problem , a new description method called Augmented permute matrix is proposed by expending the ability of the traditional one . An Embedded Hybrid Model combining Hopfield model and SA are put forward to improve the optimization in essence in which Hopfield servers as State Generator for the SA . The experiment results show that Augmented permute matrix is able to completely and appropriately describe the application . The energy function and hybrid model given in this study are also highly efficient in solving resource leveling problem .
We present two approaches for next step linear prediction of long memory time series . The first is based on the truncation of the Wiener-Kolmogorov predictor by restricting the observations to the last $k$ terms , which are the only available values in practice . Part of the mean squared prediction error comes from the truncation , and another part comes from the parametric estimation of the parameters of the predictor . By contrast , the second approach is non-parametric . An AR ( $k$ ) model is fitted to the long memory time series and we study the error made with this misspecified model .
Target tracking faces the challenge in coping with large volumes of data which requires efficient methods for real time applications . The complexity considered in this paper is when there is a large number of measurements which are required to be processed at each time step . Sequential Markov chain Monte Carlo ( MCMC ) has been shown to be a promising approach to target tracking in complex environments , especially when dealing with clutter . However , a large number of measurements usually results in large processing requirements . This paper goes beyond the current state-of-the-art and presents a novel Sequential MCMC approach that can overcome this challenge through adaptively subsampling the set of measurements . Instead of using the whole large volume of available data , the proposed algorithm performs a trade off between the number of measurements to be used and the desired accuracy of the estimates to be obtained in the presence of clutter . We show results with large improvements in processing time , more than 00% with a negligible loss in tracking performance , compared with the solution without subsampling .
We extend the common mixtures-of-Gaussians density estimation approach to account for a known sample incompleteness by simultaneous imputation from the current model . The method called GMMis generalizes existing Expectation-Maximization techniques for truncated data to arbitrary truncation geometries and probabilistic rejection . It can incorporate an uniform background distribution as well as independent multivariate normal measurement errors for each of the observed samples , and recovers an estimate of the error-free distribution from which both observed and unobserved samples are drawn . We compare GMMis to the standard Gaussian mixture model for simple test cases with different types of incompleteness , and apply it to observational data from the NASA Chandra X-ray telescope . The python code is capable of performing density estimation with millions of samples and thousands of model components and is released as an open-source package at https : //github . com/pmelchior/pyGMMis
Under-determined systems of linear equations with sparse solutions have been the subject of an extensive research in last several years above all due to results of \cite{CRT , CanRomTao00 , DonohoPol} . In this paper we will consider \emph{noisy} under-determined linear systems . In a breakthrough \cite{CanRomTao00} it was established that in \emph{noisy} systems for any linear level of under-determinedness there is a linear sparsity that can be \emph{approximately} recovered through an SOCP ( second order cone programming ) optimization algorithm so that the approximate solution vector is ( in an $\ell_0$-norm sense ) guaranteed to be no further from the sparse unknown vector than a constant times the noise . In our recent work \cite{StojnicGenSocp00} we established an alternative framework that can be used for statistical performance analysis of the SOCP algorithms . To demonstrate how the framework works we then showed in \cite{StojnicGenSocp00} how one can use it to precisely characterize the \emph{generic} ( worst-case ) performance of the SOCP . In this paper we present a different set of results that can be obtained through the framework of \cite{StojnicGenSocp00} . The results will relate to \emph{problem dependent} performance analysis of SOCP ' s . We will consider specific types of unknown sparse vectors and characterize the SOCP performance when used for recovery of such vectors . We will also show that our theoretical predictions are in a solid agreement with the results one can get through numerical simulations .
We present two algorithms for learning the structure of a Markov network from data : GSMN* and GSIMN . Both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these tests . Until very recently , algorithms for structure learning were based on maximum likelihood estimation , which has been proved to be NP-hard for Markov networks due to the difficulty of estimating the parameters of the network , needed for the computation of the data likelihood . The independence-based approach does not require the computation of the likelihood , and thus both GSMN* and GSIMN can compute the structure efficiently ( as shown in our experiments ) . GSMN* is an adaptation of the Grow-Shrink algorithm of Margaritis and Thrun for learning the structure of Bayesian networks . GSIMN extends GSMN* by additionally exploiting Pearls well-known properties of the conditional independence relation to infer novel independences from known ones , thus avoiding the performance of statistical tests to estimate them . To accomplish this efficiently GSIMN uses the Triangle theorem , also introduced in this work , which is a simplified version of the set of Markov axioms . Experimental comparisons on artificial and real-world data sets show GSIMN can yield significant savings with respect to GSMN* , while generating a Markov network with comparable or in some cases improved quality . We also compare GSIMN to a forward-chaining implementation , called GSIMN-FCH , that produces all possible conditional independences resulting from repeatedly applying Pearls theorems on the known conditional independence tests . The results of this comparison show that GSIMN , by the sole use of the Triangle theorem , is nearly optimal in terms of the set of independences tests that it infers .
Over the recent years the importance of numerical experiments has gradually been more recognized . Nonetheless , sufficient documentation of how computational results have been obtained is often not available . Especially in the scientific computing and applied mathematics domain this is crucial , since numerical experiments are usually employed to verify the proposed hypothesis in a publication . This work aims to propose standards and best practices for the setup and publication of numerical experiments . Naturally , this amounts to a guideline for development , maintenance , and publication of numerical research software . Such a primer will enable the replicability and reproducibility of computer-based experiments and published results and also promote the reusability of the associated software .
Subspace clustering is the problem of clustering data points into a union of low-dimensional linear/affine subspaces . It is the mathematical abstraction of many important problems in computer vision , image processing and machine learning . A line of recent work ( 0 , 00 , 00 , 00 ) provided strong theoretical guarantee for sparse subspace clustering ( 0 ) , the state-of-the-art algorithm for subspace clustering , on both noiseless and noisy data sets . It was shown that under mild conditions , with high probability no two points from different subspaces are clustered together . Such guarantee , however , is not sufficient for the clustering to be correct , due to the notorious " graph connectivity problem " ( 00 ) . In this paper , we investigate the graph connectivity problem for noisy sparse subspace clustering and show that a simple post-processing procedure is capable of delivering consistent clustering under certain " general position " or " restricted eigenvalue " assumptions . We also show that our condition is almost tight with adversarial noise perturbation by constructing a counter-example . These results provide the first exact clustering guarantee of noisy SSC for subspaces of dimension greater then 0 .
The asynchronous systems are the models of the asynchronous circuits from the digital electrical engineering and non-anticipation is one of the most important properties in systems theory . Our present purpose is to introduce several concepts of non-anticipation of the asynchronous systems .
The Ricker model was introduced in the context of managing fishing stocks . It is a discrete non-linear iterative model given by $N ( t+0 ) =rN ( t ) \exp ( -N ( t ) ) $ where $N ( t ) $ is the population at time $t$ . The model treated in this paper includes a random component $N ( t+0 ) =rN ( t ) \exp ( -N ( t ) +\varepsilon ( t+0 ) ) $ and what is observed at time $t$ is a Poisson random variable with parameter $\varphi N ( t ) $ . Such a model has been analysed using `synthetic likelihood ' and ABC ( Approximate Bayesian Computation ) . In contrast this paper takes a non-likelihood approach and treats the model in a consistent manner as an approximation . The goal is to specify those parameter values if any which are consistent with the data .
The widely used genetic pleiotropic analysis of multiple phenotypes are often designed for examining the relationship between common variants and a few phenotypes . They are not suited for both high dimensional phenotypes and high dimensional genotype ( next-generation sequencing ) data . To overcome these limitations , we develop sparse structural equation models ( SEMs ) as a general framework for a new paradigm of genetic analysis of multiple phenotypes . To incorporate both common and rare variants into the analysis , we extend the traditional multivariate SEMs to sparse functional SEMs . To deal with high dimensional phenotype and genotype data , we employ functional data analysis and the alternative direction methods of multiplier ( ADMM ) techniques to reduce data dimension and improve computational efficiency . Using large scale simulations we showed that the proposed methods have higher power to detect true causal genetic pleiotropic structure than other existing methods . Simulations also demonstrate that the gene-based pleiotropic analysis has higher power than the single variant-based pleiotropic analysis . The proposed method is applied to exome sequence data from the NHLBI Exome Sequencing Project ( ESP ) with 00 phenotypes , which identifies a network with 000 genes connected to 00 phenotypes and 000 edges . Among them , 000 genes showed pleiotropic genetic effects and 00 genes were reported to be associated with phenotypes in the analysis or other cardiovascular disease ( CVD ) related phenotypes in the literature .
P0P system rely on voluntary allocation of resources by its members due to absence of any central controlling authority . This resource allocation can be viewed as classical control problem where feedback is the amount of resource received , which controls the output i . e . the amount of resources shared back to the network by the node . The motivation behind the use of control system in resource allocation is to exploit already existing tools in control theory to improve the overall allocation process and thereby solving the problem of freeriding and whitewashing in the network . At the outset , we have derived the transfer function to model the P0P system . Subsequently , through the simulation results we have shown that transfer function was able to provide optimal value of resource sharing for the peers during the normal as well as high degree of overloading in the network . Thereafter we verified the accuracy of the transfer function derived by comparing its output with the simulated P0P network . To demonstrate how control system reduces free riding it has been shown through simulations how the control systems penalizes the nodes indulging in different levels of freeriding . Our proposed control system shows considerable gain over existing state of art algorithm . This improvement is achieved through PI action of controller . Since low reputation peers usually subvert reputation system by whitewashing . We propose and substantiate a technique modifying transfer function such that systems ' sluggishness becomes adaptive in such a way that it encourage genuine new comers to enter network and discourages member peers to whitewash .
We construct optimal designs for group testing experiments where the goal is to estimate the prevalence of a trait by using a test with uncertain sensitivity and specificity . Using optimal design theory for approximate designs , we show that the most efficient design for simultaneously estimating the prevalence , sensitivity and specificity requires three different group sizes with equal frequencies . However , if estimating prevalence as accurately as possible is the only focus , the optimal strategy is to have three group sizes with unequal frequencies . On the basis of a chlamydia study in the U . S . A . , we compare performances of competing designs and provide insights into how the unknown sensitivity and specificity of the test affect the performance of the prevalence estimator . We demonstrate that the locally D- and Ds-optimal designs proposed have high efficiencies even when the prespecified values of the parameters are moderately misspecified .
We consider a stationary regularly varying time series which can be expressed as a function of a geometrically ergodic Markov chain . We obtain practical conditions for the weak convergence of weighted versions of the multivariate tail empirical process . These conditions include the so-called geometric drift or Foster-Lyapunov condition and can be easily checked for most usual time series models with a Markovian structure . We illustrate these conditions on several models and statistical applications .
Combined inference for heterogeneous high-dimensional data is critical in modern biology , where clinical and various kinds of molecular data may be available from a single study . Classical genetic association studies regress a single clinical outcome on many genetic variants one by one , but there is an increasing demand for joint analysis of many molecular outcomes and genetic variants in order to unravel functional interactions . Unfortunately , most existing approaches to joint modelling are either too simplistic to be powerful or are impracticable for computational reasons . Inspired by Richardson et al . ( 0000 , Bayesian Statistics 0 ) , we consider a sparse multivariate regression model that allows simultaneous selection of predictors and associated responses . As Markov chain Monte Carlo ( MCMC ) inference on such models can be prohibitively slow when the number of genetic variants exceeds a few thousand , we propose a variational inference approach which produces posterior information very close to that of MCMC inference , at a much reduced computational cost . Extensive numerical experiments show that our approach outperforms popular variable selection methods and tailored Bayesian procedures , dealing within hours with problems involving hundreds of thousands of genetic variants and tens to hundreds of clinical or molecular outcomes .
The new era of the Web is known as the semantic Web or the Web of data . The semantic Web depends on ontologies that are seen as one of its pillars . The bigger these ontologies , the greater their exploitation . However , when these ontologies become too big other problems may appear , such as the complexity to charge big files in memory , the time it needs to download such files and especially the time it needs to make reasoning on them . We discuss in this paper approaches for segmenting such big Web ontologies as well as its usefulness . The segmentation method extracts from an existing ontology a segment that represents a layer or a generation in the existing ontology ; i . e . a horizontally extraction . The extracted segment should be itself an ontology .
Extending generalized estimating equations ( GEE ) to ordinal response data requires a conversion of the ordinal response to a vector of binary category indicators . That leads to a rather complicated association structure , and the introduction of large matrices when the number of categories and dimension of the cluster are large . To allow a richer specification of working correlation assumptions , we adopt the weighted scores method which is essentially an extension of the GEE approach , since it can also be applied to families that are not in the GLM class . The weighted scores method stems from the lack of a theoretically sound methodology for analyzing multivariate discrete data based only on moments up to second order and it is robust to dependence and nearly as efficient as maximum likelihood . There is no need to convert the ordinal response to binary indicators , thus the weight matrices have smaller dimensions and it is not necessary to guess the correlations of indicator variables for different categories . We focus on important issues that would interest the data analyst , such as choice of the structure of the correlation matrix and of explanatory variables , comparison of results obtained from our methods versus GEE , and insights provided by our method that would be missed with the GEE method . Our modelling framework is implemented in the package weightedScores within the open source statistical environment R .
Stochastic dual coordinate ascent ( SDCA ) is an effective technique for solving regularized loss minimization problems in machine learning . This paper considers an extension of SDCA under the mini-batch setting that is often used in practice . Our main contribution is to introduce an accelerated mini-batch version of SDCA and prove a fast convergence rate for this method . We discuss an implementation of our method over a parallel computing system , and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of \cite{nesterov0000gradient} .
Recently , there has been a growing interest in the problem of learning rich implicit models - those from which we can sample , but can not evaluate their density . These models apply some parametric function , such as a deep network , to a base measure , and are learned end-to-end using stochastic optimization . One strategy of devising a loss function is through the statistics of two sample tests - if we can fool a statistical test , the learned distribution should be a good model of the true data . However , not all tests can easily fit into this framework , as they might not be differentiable with respect to the data points , and hence with respect to the parameters of the implicit model . Motivated by this problem , in this paper we show how two such classical tests , the Friedman-Rafsky and k-nearest neighbour tests , can be effectively smoothed using ideas from undirected graphical models - the matrix tree theorem and cardinality potentials . Moreover , as we show experimentally , smoothing can significantly increase the power of the test , which might of of independent interest . Finally , we apply our method to learn implicit models .
Graphical models with change-points are computationally challenging to fit , particularly in cases where the number of observation points and the number of nodes in the graph are large . Focusing on Gaussian graphical models , we introduce an approximate majorize-minimize ( MM ) algorithm that can be useful for computing change-points in large graphical models . The proposed algorithm is an order of magnitude faster than a brute force search . Under some regularity conditions on the data generating process , we show that with high probability , the algorithm converges to a value that is within statistical error of the true change-point . A fast implementation of the algorithm using Markov Chain Monte Carlo is also introduced . The performances of the proposed algorithms are evaluated on synthetic data sets and the algorithm is also used to analyze structural changes in the S&P 000 over the period 0000-0000 .
A dimension reduction-based adaptive-to-model test is proposed for significance of a subset of covariates in the context of a nonparametric regression model . Unlike existing local smoothing significance tests , the new test behaves like a local smoothing test as if the number of covariates were just that under the null hypothesis and it can detect local alternatives distinct from the null at the rate that is only related to the number of covariates under the null hypothesis . Thus , the curse of dimensionality is largely alleviated when nonparametric estimation is inevitably required . In the cases where there are many insignificant covariates , the improvement of the new test is very significant over existing local smoothing tests on the significance level maintenance and power enhancement . Simulation studies and a real data analysis are conducted to examine the finite sample performance of the proposed test .
Social tagging systems have established themselves as an important part in today ' s web and have attracted the interest from our research community in a variety of investigations . The overall vision of our community is that simply through interactions with the system , i . e . , through tagging and sharing of resources , users would contribute to building useful semantic structures as well as resource indexes using uncontrolled vocabulary not only due to the easy-to-use mechanics . Henceforth , a variety of assumptions about social tagging systems have emerged , yet testing them has been difficult due to the absence of suitable data . In this work we thoroughly investigate three available assumptions - e . g . , is a tagging system really social ? - by examining live log data gathered from the real-world public social tagging system BibSonomy . Our empirical results indicate that while some of these assumptions hold to a certain extent , other assumptions need to be reflected and viewed in a very critical light . Our observations have implications for the design of future search and other algorithms to better reflect the actual user behavior .
This paper provides a probabilistic and statistical comparison of the log-GARCH and EGARCH models , which both rely on multiplicative volatility dynamics without positivity constraints . We compare the main probabilistic properties ( strict stationarity , existence of moments , tails ) of the EGARCH model , which are already known , with those of an asymmetric version of the log-GARCH . The quasi-maximum likelihood estimation of the log-GARCH parameters is shown to be strongly consistent and asymptotically normal . Similar estimation results are only available for particular EGARCH models , and under much stronger assumptions . The comparison is pursued via simulation experiments and estimation on real data .
Today , full-texts of scientific articles are often stored in different locations than the used datasets . Dataset registries aim at a closer integration by making datasets citable but authors typically refer to datasets using inconsistent abbreviations and heterogeneous metadata ( e . g . title , publication year ) . It is thus hard to reproduce research results , to access datasets for further analysis , and to determine the impact of a dataset . Manually detecting references to datasets in scientific articles is time-consuming and requires expert knowledge in the underlying research domain . We propose and evaluate a semi-automatic three-step approach for finding explicit references to datasets in social sciences articles . We first extract pre-defined special features from dataset titles in the da|ra registry , then detect references to datasets using the extracted features , and finally match the references found with corresponding dataset titles . The approach does not require a corpus of articles ( avoiding the cold start problem ) and performs well on a test corpus . We achieved an F-measure of 0 . 00 for detecting references in full-texts and an F-measure of 0 . 00 for finding correct matches of detected references in the da|ra dataset registry .
This report describes the computation of gradients by algorithmic differentiation for statistically optimum beamforming operations . Especially the derivation of complex-valued functions is a key component of this approach . Therefore the real-valued algorithmic differentiation is extended via the complex-valued chain rule . In addition to the basic mathematic operations the derivative of the eigenvalue problem with complex-valued eigenvectors is one of the key results of this report . The potential of this approach is shown with experimental results on the CHiME-0 challenge database . There , the beamforming task is used as a front-end for an ASR system . With the developed derivatives a joint optimization of a speech enhancement and speech recognition system w . r . t . the recognition optimization criterion is possible .
We propose a framework for indexing of grain and sub-grain structures in electron backscatter diffraction ( EBSD ) images of polycrystalline materials . The framework is based on a previously introduced physics-based forward model by Callahan and De Graef ( 0000 ) relating measured patterns to grain orientations ( Euler angle ) . The forward model is tuned to the microscope and the sample symmetry group . We discretize the domain of the forward model onto a dense grid of Euler angles and for each measured pattern we identify the most similar patterns in the dictionary . These patterns are used to identify boundaries , detect anomalies , and index crystal orientations . The statistical distribution of these closest matches is used in an unsupervised binary decision tree ( DT ) classifier to identify grain boundaries and anomalous regions . The DT classifies a pattern as an anomaly if it has an abnormally low similarity to any pattern in the dictionary . It classifies a pixel as being near a grain boundary if the highly ranked patterns in the dictionary differ significantly over the pixels 0x0 neighborhood . Indexing is accomplished by computing the mean orientation of the closest dictionary matches to each pattern . The mean orientation is estimated using a maximum likelihood approach that models the orientation distribution as a mixture of Von Mises-Fisher distributions over the quaternionic 0-sphere . The proposed dictionary matching approach permits segmentation , anomaly detection , and indexing to be performed in a unified manner with the additional benefit of uncertainty quantification . We demonstrate the proposed dictionary-based approach on a Ni-base IN000 alloy .
This paper studies reachability , coverability and inclusion problems for Integer Vector Addition Systems with States ( ZVASS ) and extensions and restrictions thereof . A ZVASS comprises a finite-state controller with a finite number of counters ranging over the integers . Although it is folklore that reachability in ZVASS is NP-complete , it turns out that despite their naturalness , from a complexity point of view this class has received little attention in the literature . We fill this gap by providing an in-depth analysis of the computational complexity of the aforementioned decision problems . Most interestingly , it turns out that while the addition of reset operations to ordinary VASS leads to undecidability and Ackermann-hardness of reachability and coverability , respectively , they can be added to ZVASS while retaining NP-completness of both coverability and reachability .
One of the fundamental problems in crowdsourcing is the trade-off between the number of the workers needed for high-accuracy aggregation and the budget to pay . For saving budget , it is important to ensure high quality of the crowd-sourced labels , hence the total cost on label collection will be reduced . Since the self-confidence of the workers often has a close relationship with their abilities , a possible way for quality control is to request the workers to return the labels only when they feel confident , by means of providing unsure option to them . On the other hand , allowing workers to choose unsure option also leads to the potential danger of budget waste . In this work , we propose the analysis towards understanding when providing the unsure option indeed leads to significant cost reduction , as well as how the confidence threshold is set . We also propose an online mechanism , which is alternative for threshold selection when the estimation of the crowd ability distribution is difficult .
The current and future developments of electric power systems are pushing the boundaries of reliability assessment to consider distribution networks with renewable generators . Given the stochastic features of these elements , most modeling approaches rely on Monte Carlo simulation . The computational costs associated to the simulation approach force to treating mostly small-sized systems , i . e . with a limited number of lumped components of a given renewable technology ( e . g . wind or solar , etc . ) whose behavior is described by a binary state , working or failed . In this paper , we propose an analytical multi-state modeling approach for the reliability assessment of distributed generation ( DG ) . The approach allows looking to a number of diverse energy generation technologies distributed on the system . Multiple states are used to describe the randomness in the generation units , due to the stochastic nature of the generation sources and of the mechanical degradation/failure behavior of the generation systems . The universal generating function ( UGF ) technique is used for the individual component multi-state modeling . A multiplication-type composition operator is introduced to combine the UGFs for the mechanical degradation and renewable generation source states into the UGF of the renewable generator power output . The overall multi-state DG system UGF is then constructed and classical reliability indices ( e . g . loss of load expectation ( LOLE ) , expected energy not supplied ( EENS ) ) are computed from the DG system generation and load UGFs . An application of the model is shown on a DG system adapted from the IEEE 00 nodes distribution test feeder .
The notion of delays arises naturally in many computational models , such as , in the design of circuits , control systems , and dataflow languages . In this work , we introduce \emph{automata with delay blocks} ( ADBs ) , extending finite state automata with variable time delay blocks , for deferring individual transition output symbols , in a discrete-time setting . We show that the ADB languages strictly subsume the regular languages , and are incomparable in expressive power to the context-free languages . We show that ADBs are closed under union , concatenation and Kleene star , and under intersection with regular languages , but not closed under complementation and intersection with other ADB languages . We show that the emptiness and the membership problems are decidable in polynomial time for ADBs , whereas the universality problem is undecidable . Finally we consider the linear-time model checking problem , i . e . , whether the language of an ADB is contained in a regular language , and show that the model checking problem is PSPACE-complete .
In general dimension , there is no known total polynomial algorithm for either convex hull or vertex enumeration , i . e . an algorithm whose complexity depends polynomially on the input and output sizes . It is thus important to identify problems ( and polytope representations ) for which total polynomial-time algorithms can be obtained . We offer the first total polynomial-time algorithm for computing the edge-skeleton ( including vertex enumeration ) of a polytope given by an optimization or separation oracle , where we are also given a superset of its edge directions . We also offer a space-efficient variant of our algorithm by employing reverse search . All complexity bounds refer to the ( oracle ) Turing machine model . There is a number of polytope classes naturally defined by oracles ; for some of them neither vertex nor facet representation is obvious . We consider two main applications , where we obtain ( weakly ) total polynomial-time algorithms : Signed Minkowski sums of convex polytopes , where polytopes can be subtracted provided the signed sum is a convex polytope , and computation of secondary , resultant , and discriminant polytopes . Further applications include convex combinatorial optimization and convex integer programming , where we offer a new approach , thus removing the complexity ' s exponential dependence in the dimension .
We prove that every Condorcet-consistent voting rule can be manipulated by a voter who completely reverses their preference ranking , assuming that there are at least 0 alternatives . This corrects an error and improves a result of [Sanver , M . R . and Zwicker , W . S . ( 0000 ) . One-way monotonicity as a form of strategy-proofness . Int J Game Theory 00 ( 0 ) , 000-000 . ] For the case of precisely 0 alternatives , we exactly characterise the number of voters for which this impossibility result can be proven . We also show analogues of our result for irresolute voting rules . We then leverage our result to state a strong form of the Gibbard-Satterthwaite Theorem .
This volume contains a selection of the papers presented at the 0nd International Workshop on Linearity ( LINEARITY ' 0000 ) , which took place 0 April 0000 in Tallinn , Estonia . The workshop was a one-day satellite event of ETAPS 0000 , the 00th European Joint Conference on Theory and Practice of Software . The aim of this workshop was to bring together researchers who are currently developing theory and applications of linear calculi , in order to foster their interaction , to provide a forum for presenting new ideas and work in progress , and to enable newcomers to learn about current activities in this area .
We propose a non-parametric method to cluster mixed data containing both continuous and discrete random variables . The product space of continuous and categorical sample spaces is approximated locally by analyzing neighborhoods with cluster patterns . Detection of cluster patterns on the product space is determined by using a modified Chi-square test . The proposed method does not impose a global distance function which could be difficult to specify in practice . Results from simulation studies have shown that our proposed methods out-performed the benchmark method , AutoClass , for various settings .
This article proposes a systematic methodological review and objective criticism of existing methods enabling the derivation of time-varying Granger-causality statistics in neuroscience . The increasing interest and the huge number of publications related to this topic calls for this systematic review which describes the very complex methodological aspects . The capacity to describe the causal links between signals recorded at different brain locations during a neuroscience experiment is of primary interest for neuroscientists , who often have very precise prior hypotheses about the relationships between recorded brain signals that arise at a specific time and in a specific frequency band . The ability to compute a time-varying frequency-specific causality statistic is therefore essential . Two steps are necessary to achieve this : the first consists of finding a statistic that can be interpreted and that directly answers the question of interest . The second concerns the model that underlies the causality statistic and that has this time-frequency specific causality interpretation . In this article , we will review Granger-causality statistics with their spectral and time-varying extensions .
This paper presents experimental results from real-time parameter estimation of a system model and subsequent trajectory optimization for a dynamic task using the Baxter Research Robot from Rethink Robotics . An active estimator maximizing Fisher information is used in real-time with a closed-loop , non-linear control technique known as Sequential Action Control . Baxter is tasked with estimating the length of a string connected to a load suspended from the gripper with a load cell providing the single source of feedback to the estimator . Following the active estimation , a trajectory is generated using the trep software package that controls Baxter to dynamically swing a suspended load into a box . Several trials are presented with varying initial estimates showing that estimation is required to obtain adequate open-loop trajectories to complete the prescribed task . The result of one trial with and without the active estimation is also shown in the accompanying video .
Projection theorems of divergences enable us to find reverse projection of a divergence on a specific statistical model as a forward projection of the divergence on a different but rather " simpler " statistical model , which , in turn , results in solving a system of linear equations . Reverse projection of divergences are closely related to various estimation methods such as the maximum likelihood estimation or its variants in robust statistics . We consider projection theorems of three parametric families of divergences that are widely used in robust statistics , namely the R\ ' enyi divergences ( or the Cressie-Reed power divergences ) , density power divergences , and the relative $\alpha$-entropy ( or the logarithmic density power divergences ) . We explore these projection theorems from the usual likelihood maximization approach and from the principle of sufficiency . In particular , we show the equivalence of solving the estimation problems by the projection theorems of the respective divergences and by directly solving the corresponding estimating equations . We also derive the projection theorem for the density power divergences .
In many applications it will be useful to know those patterns that occur with a balanced interval , e . g . , a certain combination of phone numbers are called almost every Friday or a group of products are sold a lot on Tuesday and Thursday . In previous work we proposed a new measure of support ( the number of occurrences of a pattern in a dataset ) , where we count the number of times a pattern occurs ( nearly ) in the middle between two other occurrences . If the number of non-occurrences between two occurrences of a pattern stays almost the same then we call the pattern balanced . It was noticed that some very frequent patterns obviously also occur with a balanced interval , meaning in every transaction . However more interesting patterns might occur , e . g . , every three transactions . Here we discuss a solution using standard deviation and average . Furthermore we propose a simpler approach for pruning patterns with a balanced interval , making estimating the pruning threshold more intuitive .
In the area of mobile ad-hoc networks and wireless mesh networks , sequence numbers are often used in routing protocols to avoid routing loops . It is commonly stated in protocol specifications that sequence numbers are sufficient to guarantee loop freedom if they are monotonically increased over time . A classical example for the use of sequence numbers is the popular Ad hoc On-Demand Distance Vector ( AODV ) routing protocol . The loop freedom of AODV is not only a common belief , it has been claimed in the abstract of its RFC and at least two proofs have been proposed . AODV-based protocols such as AODVv0 ( DYMO ) and HWMP also claim loop freedom due to the same use of sequence numbers . In this paper we show that AODV is not a priori loop free ; by this we counter the proposed proofs in the literature . In fact , loop freedom hinges on non-evident assumptions to be made when resolving ambiguities occurring in the RFC . Thus , monotonically increasing sequence numbers , by themselves , do not guarantee loop freedom .
Head gesture is a natural means of face-to-face communication between people but the recognition of head gestures in the context of virtual reality and use of head gesture as an interface for interacting with virtual avatars and virtual environments have been rarely investigated . In the current study , we present an approach for real-time head gesture recognition on head-mounted displays using Cascaded Hidden Markov Models . We conducted two experiments to evaluate our proposed approach . In experiment 0 , we trained the Cascaded Hidden Markov Models and assessed the offline classification performance using collected head motion data . In experiment 0 , we characterized the real-time performance of the approach by estimating the latency to recognize a head gesture with recorded real-time classification data . Our results show that the proposed approach is effective in recognizing head gestures . The method can be integrated into a virtual reality system as a head gesture interface for interacting with virtual worlds .
Social influence among users ( e . g . , collaboration on a project ) creates bursty behavior in the underlying high performance computing ( HPC ) workloads . Using representative HPC and cluster workload logs , this paper identifies , analyzes , and quantifies the level of social influence across HPC users . We show the existence of a social graph that is characterized by a pattern of dominant users and followers . This pattern also follows a power-law distribution , which is consistent with those observed in mainstream social networks . Given its potential impact on HPC workloads prediction and scheduling , we propose a fast-converging , computationally-efficient online learning algorithm for identifying social groups . Extensive evaluation shows that our online algorithm can ( 0 ) quickly identify the social relationships by using a small portion of incoming jobs and ( 0 ) can efficiently track group evolution over time .
We investigate the power of non-determinism in purely functional programming languages with higher-order types . Specifically , we set out to characterise the hierarchy NP $\subseteq$ NEXP $\subseteq$ NEXP$^{ ( 0 ) }$ $\subseteq \cdots \subseteq$ NEXP$^{ ( k ) }$ $\subseteq \cdots$ solely in terms of higher-typed , purely functional programs . Although the work is incomplete , we present an initial approach using cons-free programs with immutable functions .
We observe a $N\times M$ matrix $Y_{ij}=s_{ij}+\xi_{ij}$ with $\xi_{ij}\sim {\mathcal {N}} ( 0 , 0 ) $ i . i . d . in $i , j$ , and $s_{ij}\in \mathbb {R}$ . We test the null hypothesis $s_{ij}=0$ for all $i , j$ against the alternative that there exists some submatrix of size $n\times m$ with significant elements in the sense that $s_{ij}\ge a>0$ . We propose a test procedure and compute the asymptotical detection boundary $a$ so that the maximal testing risk tends to 0 as $M\to\infty$ , $N\to\infty$ , $p=n/N\to0$ , $q=m/M\to0$ . We prove that this boundary is asymptotically sharp minimax under some additional constraints . Relations with other testing problems are discussed . We propose a testing procedure which adapts to unknown $ ( n , m ) $ within some given set and compute the adaptive sharp rates . The implementation of our test procedure on synthetic data shows excellent behavior for sparse , not necessarily squared matrices . We extend our sharp minimax results in different directions : first , to Gaussian matrices with unknown variance , next , to matrices of random variables having a distribution from an exponential family ( non-Gaussian ) and , finally , to a two-sided alternative for matrices with Gaussian elements .
Julian Besag was an outstanding statistical scientist , distinguished for his pioneering work on the statistical theory and analysis of spatial processes , especially conditional lattice systems . His work has been seminal in statistical developments over the last several decades ranging from image analysis to Markov chain Monte Carlo methods . He clarified the role of auto-logistic and auto-normal models as instances of Markov random fields and paved the way for their use in diverse applications . Later work included investigations into the efficacy of nearest neighbour models to accommodate spatial dependence in the analysis of data from agricultural field trials , image restoration from noisy data , and texture generation using lattice models .
An Electronic Health Record ( EHR ) is designed to store diverse data accurately from a range of health care providers and to capture the status of a patient by a range of health care providers across time . Realising the numerous benefits of the system , EHR adoption is growing globally and many countries invest heavily in electronic health systems . In Australia , the Government invested $000 million to build key components of the Personally Controlled Electronic Health Record ( PCEHR ) system in July 0000 . However , in the last three years , the uptake from individuals and health care providers has not been satisfactory . Unauthorised access of the PCEHR was one of the major barriers . We propose an improved access control model for the PCEHR system to resolve the unauthorised access issue . We discuss the unauthorised access issue with real examples and present a potential solution to overcome the issue to make the PCEHR system a success in Australia .
The relation between two Morse functions defined on a common domain can be studied in terms of their Jacobi set . The Jacobi set contains points in the domain where the gradients of the functions are aligned . Both the Jacobi set itself as well as the segmentation of the domain it induces have shown to be useful in various applications . Unfortunately , in practice functions often contain noise and discretization artifacts causing their Jacobi set to become unmanageably large and complex . While there exist techniques to simplify Jacobi sets , these are unsuitable for most applications as they lack fine-grained control over the process and heavily restrict the type of simplifications possible . In this paper , we introduce a new framework that generalizes critical point cancellations in scalar functions to Jacobi sets in two dimensions . We focus on simplifications that can be realized by smooth approximations of the corresponding functions and show how this implies simultaneously simplifying contiguous subsets of the Jacobi set . These extended cancellations form the atomic operations in our framework , and we introduce an algorithm to successively cancel subsets of the Jacobi set with minimal modifications according to some user-defined metric . We prove that the algorithm is correct and terminates only once no more local , smooth and consistent simplifications are possible . We disprove a previous claim on the minimal Jacobi set for manifolds with arbitrary genus and show that for simply connected domains , our algorithm reduces a given Jacobi set to its simplest configuration .
Mixture models with Gamma and or inverse-Gamma distributed mixture components are useful for medical image tissue segmentation or as post-hoc models for regression coefficients obtained from linear regression within a Generalised Linear Modeling framework ( GLM ) , used in this case to separate stochastic ( Gaussian ) noise from some kind of positive or negative " activation " ( modeled as Gamma or inverse-Gamma distributed ) . To date , the most common choice in this context it is Gaussian/Gamma mixture models learned through a maximum likelihood ( ML ) approach ; we recently extended such algorithm for mixture models with inverse-Gamma components . Here , we introduce a fully analytical Variational Bayes ( VB ) learning framework for both Gamma and/or inverse-Gamma components . We use synthetic and resting state fMRI data to compare the performance of the ML and VB algorithms in terms of area under the curve and computational cost . We observed that the ML Gaussian/Gamma model is very expensive specially when considering high resolution images ; furthermore , these solutions are highly variable and they occasionally can overestimate the activations severely . The Bayesian Gauss-Gamma is in general the fastest algorithm but provides too dense solutions . The maximum likelihood Gaussian/inverse-Gamma is also very fast but provides in general very sparse solutions . The variational Gaussian/inverse-Gamma mixture model is the most robust and its cost is acceptable even for high resolution images . Further , the presented methodology represents an essential building block that can be directly used in more complex inference tasks , specially designed to analyse MRI-fMRI data ; such models include for example analytical variational mixture models with adaptive spatial regularization or better source models for new spatial blind source separation approaches .
We consider a finite mixture of Gaussian regression model for high- dimensional data , where the number of covariates may be much larger than the sample size . We propose to estimate the unknown conditional mixture density by a maximum likelihood estimator , restricted on relevant variables selected by an 0-penalized maximum likelihood estimator . We get an oracle inequality satisfied by this estimator with a Jensen-Kullback-Leibler type loss . Our oracle inequality is deduced from a general model selection theorem for maximum likelihood estimators with a random model collection . We can derive the penalty shape of the criterion , which depends on the complexity of the random model collection .
Hidden Markov models and their variants are the predominant sequential classification method in such domains as speech recognition , bioinformatics and natural language processing . Being generative rather than discriminative models , however , their classification performance is a drawback . In this paper we apply ideas from the field of density ratio estimation to bypass the difficult step of learning likelihood functions in HMMs . By reformulating inference and model fitting in terms of density ratios and applying a fast kernel-based estimation method , we show that it is possible to obtain a striking increase in discriminative performance while retaining the probabilistic qualities of the HMM . We demonstrate experimentally that this formulation makes more efficient use of training data than alternative approaches .
Multivariate GARCH models are important tools to describe the dynamics of multivariate times series of financial returns . Nevertheless , these models have been much less used in practice due to the lack of reliable software . This paper describes the {\tt R} package {\bf BayesDccGarch} which was developed to implement recently proposed inference procedures to estimate and compare multivariate GARCH models allowing for asymmetric and heavy tailed distributions .
We propose a novel estimator of the autocorrelation function in presence of missing observations . We establish the consistency , the asymptotic normality , and we derive deviation bounds for various classes of weakly dependent stationary time series , including causal or non causal models . In addition , we introduce a modified version periodogram defined from these autocorrelation estimators and derive asymptotic distribution of linear functionals of this estimator .
This volume contains the proceedings of the Combined 00th International Workshop on Expressiveness in Concurrency and the 00th Workshop on Structural Operational Semantics ( EXPRESS/SOS 0000 ) which was held on 00th August , 0000 in Buenos Aires , Argentina , as an affiliated workshop of CONCUR 0000 , the 00th International Conference on Concurrency Theory . The EXPRESS workshops aim at bringing together researchers interested in the expressiveness of various formal systems and semantic notions , particularly in the field of concurrency . Their focus has traditionally been on the comparison between programming concepts ( such as concurrent , functional , imperative , logic and object-oriented programming ) and between mathematical models of computation ( such as process algebras , Petri nets , event structures , modal logics , and rewrite systems ) on the basis of their relative expressive power . The EXPRESS workshop series has run successfully since 0000 and over the years this focus has become broadly construed . The SOS workshops aim at being a forum for researchers , students and practitioners interested in new developments , and directions for future investigation , in the field of structural operational semantics . One of the specific goals of the SOS workshop series is to establish synergies between the concurrency and programming language communities working on the theory and practice of SOS . Reports on applications of SOS to other fields are also most welcome , including : modelling and analysis of biological systems , security of computer systems programming , modelling and analysis of embedded systems , specification of middle-ware and coordination languages , programming language semantics and implementation , static analysis software and hardware verification , and semantics for domain-specific languages and model-based engineering .
This paper introduces Gene-Machine , an efficient and new search heuristic algorithm , based in the building-block hypothesis . It is inspired by natural evolution , but does not use some of the concepts present in genetic algorithms like population , mutation and generation . This heuristic exhibits good performance in comparison with genetic algorithms , and can be used to generate useful solutions to optimization and search problems .
In this paper we study the following problem : we are given a set of imprecise points modeled as parallel line segments , and we wish to place a point on each line segment such that the resulting point set maximizes/minimizes the size of the largest/smallest area $k$-gon . We first study the problem for the case $k=0$ . We show that for a given set of parallel line segments of equal length the largest possible area triangle can be found in $O ( n \log n ) $ time , and for line segments of arbitrary length the problem can be solved in $O ( n^0 ) $ time . Also , we show that the smallest largest-area triangle can be found in $O ( n^0 \log n ) $ time . As for finding smallest-area triangles , we show that finding the largest smallest-area triangle is NP-hard , but that the smallest possible area triangle for a set of arbitrary length parallel line segments can be found in $O ( n^0 ) $ time . Finally , we discuss to what extent our results can be generalized to larger values of $k$ .
Randomized experiments on a network often involve interference between connected units ; i . e . , a situation in which an individual ' s treatment can affect the response of another individual . Current approaches to deal with interference , in theory and in practice , often make restrictive assumptions on its structure---for instance , assuming that interference is local---even when using otherwise nonparametric inference strategies . This reliance on explicit restrictions on the interference mechanism suggests a shared intuition that inference is impossible without any assumptions on the interference structure . In this paper , we begin by formalizing this intuition in the context of a classical nonparametric approach to inference , referred to as design-based inference of causal effects . Next , we show how , always in the context of design-based inference , even parametric structural assumptions that allow the existence of unbiased estimators , cannot guarantee a decreasing variance even in the large sample limit . This lack of concentration in large samples is often observed empirically , in randomized experiments in which interference of some form is expected to be present . This result has direct consequences for the design and analysis of large experiments---for instance , in online social platforms---where the belief is that large sample sizes automatically guarantee small variance . More broadly , our results suggest that although strategies for causal inference in the presence of interference borrow their formalism and main concepts from the traditional causal inference literature , much of the intuition from the no-interference case do not easily transfer to the interference setting .
Self-Organising Maps ( SOM ) are Artificial Neural Networks used in Pattern Recognition tasks . Their major advantage over other architectures is human readability of a model . However , they often gain poorer accuracy . Mostly used metric in SOM is the Euclidean distance , which is not the best approach to some problems . In this paper , we study an impact of the metric change on the SOM ' s performance in classification problems . In order to change the metric of the SOM we applied a distance metric learning method , so-called ' Large Margin Nearest Neighbour ' . It computes the Mahalanobis matrix , which assures small distance between nearest neighbour points from the same class and separation of points belonging to different classes by large margin . Results are presented on several real data sets , containing for example recognition of written digits , spoken letters or faces .
A generative model based on training deep architectures is proposed . The model consists of K networks that are trained together to learn the underlying distribution of a given data set . The process starts with dividing the input data into K clusters and feeding each of them into a separate network . After few iterations of training networks separately , we use an EM-like algorithm to train the networks together and update the clusters of the data . We call this model Mixture of Networks . The provided model is a platform that can be used for any deep structure and be trained by any conventional objective function for distribution modeling . As the components of the model are neural networks , it has high capability in characterizing complicated data distributions as well as clustering data . We apply the algorithm on MNIST hand-written digits and Yale face datasets . We also demonstrate the clustering ability of the model using some real-world and toy examples .
This paper studies convergence properties of multivariate distributions constructed by endowing empirical margins with a copula . This setting includes Latin Hypercube Sampling with dependence , also known as the Iman--Conover method . The primary question addressed here is the convergence of the component sum , which is relevant to risk aggregation in insurance and finance . This paper shows that a CLT for the aggregated risk distribution is not available , so that the underlying mathematical problem goes beyond classic functional CLTs for empirical copulas . This issue is relevant to Monte-Carlo based risk aggregation in all multivariate models generated by plugging empirical margins into a copula . Instead of a functional CLT , this paper establishes strong uniform consistency of the estimated sum distribution function and provides a sufficient criterion for the convergence rate $O ( n^{-0/0} ) $ in probability . These convergence results hold for all copulas with bounded densities . Examples with unbounded densities include bivariate Clayton and Gauss copulas . The convergence results are not specific to the component sum and hold also for any other componentwise non-decreasing aggregation function . On the other hand , convergence of estimates for the joint distribution is much easier to prove , including CLTs . Beyond Iman--Conover estimates , the results of this paper apply to multivariate distributions obtained by plugging empirical margins into an exact copula or by plugging exact margins into an empirical copula .
Learning an algorithm from examples is a fundamental problem that has been widely studied . Recently it has been addressed using neural networks , in particular by Neural Turing Machines ( NTMs ) . These are fully differentiable computers that use backpropagation to learn their own programming . Despite their appeal NTMs have a weakness that is caused by their sequential nature : they are not parallel and are are hard to train due to their large depth when unfolded . We present a neural network architecture to address this problem : the Neural GPU . It is based on a type of convolutional gated recurrent unit and , like the NTM , is computationally universal . Unlike the NTM , the Neural GPU is highly parallel which makes it easier to train and efficient to run . An essential property of algorithms is their ability to handle inputs of arbitrary size . We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances . We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary . We train the Neural GPU on numbers with upto 00 bits and observe no errors whatsoever while testing it , even on much longer numbers . To achieve these results we introduce a technique for training deep recurrent networks : parameter sharing relaxation . We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization .
We consider the asymptotic behaviour of the marginal maximum likelihood empirical Bayes posterior distribution in general setting . First we characterize the set where the maximum marginal likelihood estimator is located with high probability . Then we provide oracle type of upper and lower bounds for the contraction rates of the empirical Bayes posterior . We also show that the hierarchical Bayes posterior achieves the same contraction rate as the maximum marginal likelihood empirical Bayes posterior . We demonstrate the applicability of our general results for various models and prior distributions by deriving upper and lower bounds for the contraction rates of the corresponding empirical and hierarchical Bayes posterior distributions .
Large-scale quantum computation is likely to require massive quantum error correction ( QEC ) . QEC codes and circuits are described via the stabilizer formalism , which represents stabilizer states by keeping track of the operators that preserve them . Such states are obtained by stabilizer circuits ( consisting of CNOT , Hadamard and Phase gates ) and can be represented compactly on conventional computers using $O ( n^0 ) $ bits , where $n$ is the number of qubits . As an additional application , the work by Aaronson and Gottesman suggests the use of superpositions of stabilizer states to represent arbitrary quantum states . To aid in such applications and improve our understanding of stabilizer states , we characterize and count nearest-neighbor stabilizer states , quantify the distribution of angles between pairs of stabilizer states , study succinct stabilizer superpositions and stabilizer bivectors , explore the approximation of non-stabilizer states by single stabilizer states and short linear combinations of stabilizer states , develop an improved inner-product computation for stabilizer states via synthesis of compact canonical stabilizer circuits , propose an orthogonalization procedure for stabilizer states , and evaluate several of these algorithms empirically .
We discuss the Bayesian emulation approach to computational solution of multi-step portfolio studies in financial time series . " Bayesian emulation for decisions " involves mapping the technical structure of a decision analysis problem to that of Bayesian inference in a purely synthetic " emulating " statistical model . This provides access to standard posterior analytic , simulation and optimization methods that yield indirect solutions of the decision problem . We develop this in time series portfolio analysis using classes of economically and psychologically relevant multi-step ahead portfolio utility functions . Studies with multivariate currency , commodity and stock index time series illustrate the approach and show some of the practical utility and benefits of the Bayesian emulation methodology .
One of the central problems in computer vision is the detection of semantically important objects and the estimation of their pose . Most of the work in object detection has been based on single image processing and its performance is limited by occlusions and ambiguity in appearance and geometry . This paper proposes an active approach to object detection by controlling the point of view of a mobile depth camera . When an initial static detection phase identifies an object of interest , several hypotheses are made about its class and orientation . The sensor then plans a sequence of views , which balances the amount of energy used to move with the chance of identifying the correct hypothesis . We formulate an active hypothesis testing problem , which includes sensor mobility , and solve it using a point-based approximate POMDP algorithm . The validity of our approach is verified through simulation and real-world experiments with the PR0 robot . The results suggest that our approach outperforms the widely-used greedy view point selection and provides a significant improvement over static object detection .
Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from bottom up for unsupervised nonlinear dimensionality reduction . Each layer of the network is a group of k-centers clusterings . Each clustering uses randomly sampled data points with randomly selected features as its centers , and learns a one-of-k encoding by one-nearest-neighbor optimization . Thanks to the binarized encoding , the similarity of two data points is measured by the number of the nearest centers they share in common , which is an adaptive similarity metric in the discrete space that needs no model assumption and parameter tuning . Thanks to the network structure , larger and larger local variations of data are gradually reduced from bottom up . The information loss caused by the binarized encoding is proportional to the correlation of the clusterings , both of which are reduced by the randomization steps .
We study signal recovery on graphs based on two sampling strategies : random sampling and experimentally designed sampling . We propose a new class of smooth graph signals , called approximately bandlimited , which generalizes the bandlimited class and is similar to the globally smooth class . We then propose two recovery strategies based on random sampling and experimentally designed sampling . The proposed recovery strategy based on experimentally designed sampling is similar to the leverage scores used in the matrix approximation . We show that while both strategies are unbiased estimators for the low-frequency components , the convergence rate of experimentally designed sampling is much faster than that of random sampling when a graph is irregular . We validate the proposed recovery strategies on three specific graphs : a ring graph , an Erd\H{o}s-R\ ' enyi graph , and a star graph . The simulation results support the theoretical analysis .
Conditional extreme value models have been introduced by Heffernan and Resnick ( 0000 ) to describe the asymptotic behavior of a random vector as one specific component becomes extreme . Obviously , this class of models is related to classical multivariate extreme value theory which describes the behavior of a random vector as its norm ( and therefore at least one of its components ) becomes extreme . However , it turns out that this relationship is rather subtle and sometimes contrary to intuition . We clarify the differences between the two approaches with the help of several illuminative ( counter ) examples . Furthermore , we discuss marginal standardization , which is a useful tool in classical multivariate extreme value theory but , as we point out , much less straightforward and sometimes even obscuring in conditional extreme value models . Finally , we indicate how , in some situations , a more comprehensive characterization of the asymptotic behavior can be obtained if the conditions of conditional extreme value models are relaxed so that the limit is no longer unique .
The number of component classifiers chosen for an ensemble has a great impact on its prediction ability . In this paper , we use a geometric framework for a priori determining the ensemble size , applicable to most of the existing batch and online ensemble classifiers . There are only a limited number of studies on the ensemble size considering Majority Voting ( MV ) and Weighted Majority Voting ( WMV ) . Almost all of them are designed for batch-mode , barely addressing online environments . The big data dimensions and resource limitations in terms of time and memory make the determination of the ensemble size crucial , especially for online environments . Our framework proves , for the MV aggregation rule , that the more strong components we can add to the ensemble the more accurate predictions we can achieve . On the other hand , for the WMV aggregation rule , we prove the existence of an ideal number of components equal to the number of class labels , with the premise that components are completely independent of each other and strong enough . While giving the exact definition for a strong and independent classifier in the context of an ensemble is a challenging task , our proposed geometric framework provides a theoretical explanation of diversity and its impact on the accuracy of predictions . We conduct an experimental evaluation with two different scenarios to show the practical value of our theorems .
In this report we study the problem of determining three-dimensional orientations for noisy projections of randomly oriented identical particles . The problem is of central importance in the tomographic reconstruction of the density map of macromolecular complexes from electron microscope images and it has been studied intensively for more than 00 years . We analyze the computational complexity of the orientation problem and show that while several variants of the problem are $NP$-hard , inapproximable and fixed-parameter intractable , some restrictions are polynomial-time approximable within a constant factor or even solvable in logarithmic space . The orientation search problem is formalized as a constrained line arrangement problem that is of independent interest . The negative complexity results give a partial justification for the heuristic methods used in orientation search , and the positive complexity results on the orientation search have some positive implications also to the problem of finding functionally analogous genes . A preliminary version ``The Computational Complexity of Orientation Search in Cryo-Electron Microscopy ' ' appeared in Proc . ICCS 0000 , LNCS 0000 , pp . 000--000 . Springer-Verlag 0000 .
In the setting of high-dimensional linear regression models , we propose two frameworks for constructing pointwise and group confidence sets for penalized estimators which incorporate prior knowledge about the organization of the non-zero coefficients . This is done by desparsifying the estimator as in van de Geer et al . [00] and van de Geer and Stucky [00] , then using an appropriate estimator for the precision matrix $\Theta$ . In order to estimate the precision matrix a corresponding structured matrix norm penalty has to be introduced . After normalization the result is an asymptotic pivot . The asymptotic behavior is studied and simulations are added to study the differences between the two schemes .
Classical approaches for estimating optical flow have achieved rapid progress in the last decade . However , most of them are too slow to be applied in real-time video analysis . Due to the great success of deep learning , recent work has focused on using CNNs to solve such dense prediction problems . In this paper , we investigate a new deep architecture , Densely Connected Convolutional Networks ( DenseNet ) , to learn optical flow . This specific architecture is ideal for the problem at hand as it provides shortcut connections throughout the network , which leads to implicit deep supervision . We extend current DenseNet to a fully convolutional network to learn motion estimation in an unsupervised manner . Evaluation results on three standard benchmarks demonstrate that DenseNet is a better fit than other widely adopted CNN architectures for optical flow estimation .
