In this paper we discuss a novel framework for multiclass learning , defined by a suitable coding/decoding strategy , namely the simplex coding , that allows to generalize to multiple classes a relaxation approach commonly used in binary classification . In this framework , a relaxation error analysis can be developed avoiding constraints on the considered hypotheses class . Moreover , we show that in this setting it is possible to derive the first provably consistent regularized method with training/tuning complexity which is independent to the number of classes . Tools from convex analysis are introduced that can be used beyond the scope of this paper .
A system is presented that segments , clusters and predicts musical audio in an unsupervised manner , adjusting the number of ( timbre ) clusters instantaneously to the audio input . A sequence learning algorithm adapts its structure to a dynamically changing clustering tree . The flow of the system is as follows : 0 ) segmentation by onset detection , 0 ) timbre representation of each segment by Mel frequency cepstrum coefficients , 0 ) discretization by incremental clustering , yielding a tree of different sound classes ( e . g . instruments ) that can grow or shrink on the fly driven by the instantaneous sound events , resulting in a discrete symbol sequence , 0 ) extraction of statistical regularities of the symbol sequence , using hierarchical N-grams and the newly introduced conceptual Boltzmann machine , and 0 ) prediction of the next sound event in the sequence . The system ' s robustness is assessed with respect to complexity and noisiness of the signal . Clustering in isolation yields an adjusted Rand index ( ARI ) of 00 . 0% / 00 . 0% for data sets of singing voice and drums . Onset detection jointly with clustering achieve an ARI of 00 . 0% / 00 . 0% and the prediction of the entire system yields an ARI of 00 . 0% / 00 . 0% .
Neuroscience is undergoing faster changes than ever before . Over 000 years our field qualitatively described and invasively manipulated single or few organisms to gain anatomical , physiological , and pharmacological insights . In the last 00 years neuroscience spawned quantitative big-sample datasets on microanatomy , synaptic connections , optogenetic brain-behavior assays , and high-level cognition . While growing data availability and information granularity have been amply discussed , we direct attention to a routinely neglected question : How will the unprecedented data richness shape data analysis practices ? Statistical reasoning is becoming more central to distill neurobiological knowledge from healthy and pathological brain recordings . We believe that large-scale data analysis will use more models that are non-parametric , generative , mixing frequentist and Bayesian aspects , and grounded in different statistical inferences .
Non-convex regularizers usually improve the performance of sparse estimation in practice . To prove this fact , we study the conditions of sparse estimations for the sharp concave regularizers which are a general family of non-convex regularizers including many existing regularizers . For the global solutions of the regularized regression , our sparse eigenvalue based conditions are weaker than that of L0-regularization for parameter estimation and sparseness estimation . For the approximate global and approximate stationary ( AGAS ) solutions , almost the same conditions are also enough . We show that the desired AGAS solutions can be obtained by coordinate descent ( CD ) based methods . Finally , we perform some experiments to show the performance of CD methods on giving AGAS solutions and the degree of weakness of the estimation conditions required by the sharp concave regularizers .
We propose a new stochastic dual coordinate ascent technique that can be applied to a wide range of regularized learning problems . Our method is based on Alternating Direction Multiplier Method ( ADMM ) to deal with complex regularization functions such as structured regularizations . Although the original ADMM is a batch method , the proposed method offers a stochastic update rule where each iteration requires only one or few sample observations . Moreover , our method can naturally afford mini-batch update and it gives speed up of convergence . We show that , under mild assumptions , our method converges exponentially . The numerical experiments show that our method actually performs efficiently .
Approximate Bayesian computation ( ABC ) is now an established technique for statistical inference used in cases where the likelihood function is computationally expensive or not available . It relies on the use of a model that is specified in the form of a simulator , and approximates the likelihood at a parameter $\theta$ by simulating auxiliary data sets $x$ and evaluating the distance of $x$ from the true data $y$ . However , ABC is not computationally feasible in cases where using the simulator for each $\theta$ is very expensive . This paper investigates this situation in cases where a cheap , but approximate , simulator is available . The approach is to employ delayed acceptance Markov chain Monte Carlo ( MCMC ) within an ABC sequential Monte Carlo ( SMC ) sampler in order to , in a first stage of the kernel , use the cheap simulator to rule out parts of the parameter space that are not worth exploring , so that the " true " simulator is only run ( in the second stage of the kernel ) where there is a reasonable chance of accepting proposed values of $\theta$ . We show that this approach can be used quite automatically , with the only tuning parameter choice additional to ABC-SMC being the number of particles we wish to carry through to the second stage of the kernel . Applications to stochastic differential equation models and latent doubly intractable distributions are presented .
Recent progress in artificial intelligence ( AI ) has renewed interest in building systems that learn and think like people . Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition , video games , and board games , achieving performance that equals or even beats humans in some respects . Despite their biological inspiration and performance achievements , these systems differ from human intelligence in crucial ways . We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn , and how they learn it . Specifically , we argue that these machines should ( a ) build causal models of the world that support explanation and understanding , rather than merely solving pattern recognition problems ; ( b ) ground learning in intuitive theories of physics and psychology , to support and enrich the knowledge that is learned ; and ( c ) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations . We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models .
Recently manifold learning algorithm for dimensionality reduction attracts more and more interests , and various linear and nonlinear , global and local algorithms are proposed . The key step of manifold learning algorithm is the neighboring region selection . However , so far for the references we know , few of which propose a generally accepted algorithm to well select the neighboring region . So in this paper , we propose an adaptive neighboring selection algorithm , which successfully applies the LLE and ISOMAP algorithms in the test . It is an algorithm that can find the optimal K nearest neighbors of the data points on the manifold . And the theoretical basis of the algorithm is the approximated curvature of the data point on the manifold . Based on Riemann Geometry , Jacob matrix is a proper mathematical concept to predict the approximated curvature . By verifying the proposed algorithm on embedding Swiss roll from R0 to R0 based on LLE and ISOMAP algorithm , the simulation results show that the proposed adaptive neighboring selection algorithm is feasible and able to find the optimal value of K , making the residual variance relatively small and better visualization of the results . By quantitative analysis , the embedding quality measured by residual variance is increased 00 . 00% after using the proposed algorithm in LLE .
Feature selection , identifying a subset of variables that are relevant for predicting a response , is an important and challenging component of many methods in statistics and machine learning . Feature selection is especially difficult and computationally intensive when the number of variables approaches or exceeds the number of samples , as is often the case for many genomic datasets . Here , we introduce a new approach -- the Bayesian Ising Approximation ( BIA ) -- to rapidly calculate posterior probabilities for feature relevance in L0 penalized linear regression . In the regime where the regression problem is strongly regularized by the prior , we show that computing the marginal posterior probabilities for features is equivalent to computing the magnetizations of an Ising model . Using a mean field approximation , we show it is possible to rapidly compute the feature selection path described by the posterior probabilities as a function of the L0 penalty . We present simulations and analytical results illustrating the accuracy of the BIA on some simple regression problems . Finally , we demonstrate the applicability of the BIA to high dimensional regression by analyzing a gene expression dataset with nearly 00 , 000 features .
Support Vector Data Description ( SVDD ) is a popular outlier detection technique which constructs a flexible description of the input data . SVDD computation time is high for large training datasets which limits its use in big-data process-monitoring applications . We propose a new iterative sampling-based method for SVDD training . The method incrementally learns the training data description at each iteration by computing SVDD on an independent random sample selected with replacement from the training data set . The experimental results indicate that the proposed method is extremely fast and provides a good data description .
We provide finite-sample analysis of a general framework for using k-nearest neighbor statistics to estimate functionals of a nonparametric continuous probability density , including entropies and divergences . Rather than plugging a consistent density estimate ( which requires $k \to \infty$ as the sample size $n \to \infty$ ) into the functional of interest , the estimators we consider fix k and perform a bias correction . This is more efficient computationally , and , as we show in certain cases , statistically , leading to faster convergence rates . Our framework unifies several previous estimators , for most of which ours are the first finite sample guarantees .
Despite our extensive knowledge of biophysical properties of neurons , there is no commonly accepted algorithmic theory of neuronal function . Here we explore the hypothesis that single-layer neuronal networks perform online symmetric nonnegative matrix factorization ( SNMF ) of the similarity matrix of the streamed data . By starting with the SNMF cost function we derive an online algorithm , which can be implemented by a biologically plausible network with local learning rules . We demonstrate that such network performs soft clustering of the data as well as sparse feature discovery . The derived algorithm replicates many known aspects of sensory anatomy and biophysical properties of neurons including unipolar nature of neuronal activity and synaptic weights , local synaptic plasticity rules and the dependence of learning rate on cumulative neuronal activity . Thus , we make a step towards an algorithmic theory of neuronal function , which should facilitate large-scale neural circuit simulations and biologically inspired artificial intelligence .
We propose a general technique for improving alternating optimization ( AO ) of nonconvex functions . Starting from the solution given by AO , we conduct another sequence of searches over subspaces that are both meaningful to the optimization problem at hand and different from those used by AO . To demonstrate the utility of our approach , we apply it to the matrix factorization ( MF ) algorithm for recommender systems and the coordinate descent algorithm for penalized regression ( PR ) , and show meaningful improvements using both real-world ( for MF ) and simulated ( for PR ) data sets . Moreover , we demonstrate for MF that , by constructing search spaces customized to the given data set , we can significantly increase the convergence rate of our technique .
In this paper , we investigate the sample size requirement for a general class of nuclear norm minimization methods for higher order tensor completion . We introduce a class of tensor norms by allowing for different levels of coherence , which allows us to leverage the incoherence of a tensor . In particular , we show that a $k$th order tensor of rank $r$ and dimension $d\times\cdots\times d$ can be recovered perfectly from as few as $O ( ( r^{ ( k-0 ) /0}d^{0/0}+r^{k-0}d ) ( \log ( d ) ) ^0 ) $ uniformly sampled entries through an appropriate incoherent nuclear norm minimization . Our results demonstrate some key differences between completing a matrix and a higher order tensor : They not only point to potential room for improvement over the usual nuclear norm minimization but also highlight the importance of explicitly accounting for incoherence , when dealing with higher order tensors .
In this paper , we consider linear state-space models with compressible innovations and convergent transition matrices in order to model spatiotemporally sparse transient events . We perform parameter and state estimation using a dynamic compressed sensing framework and develop an efficient solution consisting of two nested Expectation-Maximization ( EM ) algorithms . Under suitable sparsity assumptions on the innovations , we prove recovery guarantees and derive confidence bounds for the state estimates . We provide simulation studies as well as application to spike deconvolution from calcium imaging data which verify our theoretical results and show significant improvement over existing algorithms .
Context : One of the black arts of data mining is learning the magic parameters which control the learners . In software analytics , at least for defect prediction , several methods , like grid search and differential evolution ( DE ) , have been proposed to learn these parameters , which has been proved to be able to improve the performance scores of learners . Objective : We want to evaluate which method can find better parameters in terms of performance score and runtime cost . Methods : This paper compares grid search to differential evolution , which is an evolutionary algorithm that makes extensive use of stochastic jumps around the search space . Results : We find that the seemingly complete approach of grid search does no better , and sometimes worse , than the stochastic search . When repeated 00 times to check for conclusion validity , DE was over 000 times faster than grid search to tune Random Forests on 00 testing data sets with F-Measure Conclusions : These results are puzzling : why does a quick partial search be just as effective as a much slower , and much more , extensive search ? To answer that question , we turned to the theoretical optimization literature . Bergstra and Bengio conjecture that grid search is not more effective than more randomized searchers if the underlying search space is inherently low dimensional . This is significant since recent results show that defect prediction exhibits very low intrinsic dimensionality-- an observation that explains why a fast method like DE may work as well as a seemingly more thorough grid search . This suggests , as a future research direction , that it might be possible to peek at data sets before doing any optimization in order to match the optimization algorithm to the problem at hand .
Designing a covariance function that represents the underlying correlation is a crucial step in modeling complex natural systems , such as climate models . Geospatial datasets at a global scale usually suffer from non-stationarity and non-uniformly smooth spatial boundaries . A Gaussian process regression using a non-stationary covariance function has shown promise for this task , as this covariance function adapts to the variable correlation structure of the underlying distribution . In this paper , we generalize the non-stationary covariance function to address the aforementioned global scale geospatial issues . We define this generalized covariance function as an intrinsic non-stationary covariance function , because it uses intrinsic statistics of the symmetric positive definite matrices to represent the characteristic length scale and , thereby , models the local stochastic process . Experiments on a synthetic and real dataset of relative sea level changes across the world demonstrate improvements in the error metrics for the regression estimates using our newly proposed approach .
Conditional modeling x \to y is a central problem in machine learning . A substantial research effort is devoted to such modeling when x is high dimensional . We consider , instead , the case of a high dimensional y , where x is either low dimensional or high dimensional . Our approach is based on selecting a small subset y_L of the dimensions of y , and proceed by modeling ( i ) x \to y_L and ( ii ) y_L \to y . Composing these two models , we obtain a conditional model x \to y that possesses convenient statistical properties . Multi-label classification and multivariate regression experiments on several datasets show that this model outperforms the one vs . all approach as well as several sophisticated multiple output prediction methods .
Matching pursuit ( MP ) methods are a promising class of feature construction algorithms for value function approximation . Yet existing MP methods require creating a pool of potential features , mandating expert knowledge or enumeration of a large feature pool , both of which hinder scalability . This paper introduces batch incremental feature dependency discovery ( Batch-iFDD ) as an MP method that inherits a provable convergence property . Additionally , Batch-iFDD does not require a large pool of features , leading to lower computational complexity . Empirical policy evaluation results across three domains with up to one million states highlight the scalability of Batch-iFDD over the previous state of the art MP algorithm .
We characterize the class of exchangeable feature allocations assigning probability $V_{n , k}\prod_{l=0}^{k}W_{m_{l}}U_{n-m_{l}}$ to a feature allocation of $n$ individuals , displaying $k$ features with counts $ ( m_{0} , \ldots , m_{k} ) $ for these features . Each element of this class is parametrized by a countable matrix $V$ and two sequences $U$ and $W$ of non-negative weights . Moreover , a consistency condition is imposed to guarantee that the distribution for feature allocations of $n-0$ individuals is recovered from that of $n$ individuals , when the last individual is integrated out . In Theorem 0 . 0 , we prove that the only members of this class satisfying the consistency condition are mixtures of the Indian Buffet Process over its mass parameter $\gamma$ and mixtures of the Beta--Bernoulli model over its dimensionality parameter $N$ . Hence , we provide a characterization of these two models as the only , up to randomization of the parameters , consistent exchangeable feature allocations having the required product form .
We present a new algorithm for approximate inference in probabilistic programs , based on a stochastic gradient for variational programs . This method is efficient without restrictions on the probabilistic program ; it is particularly practical for distributions which are not analytically tractable , including highly structured distributions that arise in probabilistic programs . We show how to automatically derive mean-field probabilistic programs and optimize them , and demonstrate that our perspective improves inference efficiency over other algorithms .
Stochastic Gradient Descent ( SGD ) is one of the most widely used techniques for online optimization in machine learning . In this work , we accelerate SGD by adaptively learning how to sample the most useful training examples at each time step . First , we show that SGD can be used to learn the best possible sampling distribution of an importance sampling estimator . Second , we show that the sampling distribution of an SGD algorithm can be estimated online by incrementally minimizing the variance of the gradient . The resulting algorithm - called Adaptive Weighted SGD ( AW-SGD ) - maintains a set of parameters to optimize , as well as a set of parameters to sample learning examples . We show that AWSGD yields faster convergence in three different applications : ( i ) image classification with deep features , where the sampling of images depends on their labels , ( ii ) matrix factorization , where rows and columns are not sampled uniformly , and ( iii ) reinforcement learning , where the optimized and exploration policies are estimated at the same time , where our approach corresponds to an off-policy gradient algorithm .
This work addresses the problem of regret minimization in non-stochastic multi-armed bandit problems , focusing on performance guarantees that hold with high probability . Such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard , more intuitive algorithms that come only with guarantees that hold on expectation . One of these modifications is forcing the learner to sample arms from the uniform distribution at least $\Omega ( \sqrt{T} ) $ times over $T$ rounds , which can adversely affect performance if many of the arms are suboptimal . While it is widely conjectured that this property is essential for proving high-probability regret bounds , we show in this paper that it is possible to achieve such strong results without this undesirable exploration component . Our result relies on a simple and intuitive loss-estimation strategy called Implicit eXploration ( IX ) that allows a remarkably clean analysis . To demonstrate the flexibility of our technique , we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework . Finally , we conduct a simple experiment that illustrates the robustness of our implicit exploration technique .
In this paper , we explore ordinal classification ( in the context of deep neural networks ) through a simple modification of the squared error loss which not only allows it to not only be sensitive to class ordering , but also allows the possibility of having a discrete probability distribution over the classes . Our formulation is based on the use of a softmax hidden layer , which has received relatively little attention in the literature . We empirically evaluate its performance on the Kaggle diabetic retinopathy dataset , an ordinal and high-resolution dataset and show that it outperforms all of the baselines employed .
Art historians and archaeologists have long grappled with the regional classification of ancient Near Eastern ivory carvings . Based on the visual similarity of sculptures , individuals within these fields have proposed object assemblages linked to hypothesized regional production centers . Using quantitative rather than visual methods , we here approach this classification task by exploiting computational methods from machine learning currently used with success in a variety of statistical problems in science and engineering . We first construct a prediction function using 00 categorical features as inputs and regional style as output . The model assigns regional style group ( RSG ) , with 00 percent prediction accuracy . We then rank these features by their mutual information with RSG , quantifying single-feature predictive power . Using the highest- ranking features in combination with nomographic visualization , we have found previously unknown relationships that may aid in the regional classification of these ivories and their interpretation in art historical context .
We consider the problem of minimizing the sum of two convex functions : one is the average of a large number of smooth component functions , and the other is a general convex function that admits a simple proximal mapping . We assume the whole objective function is strongly convex . Such problems often arise in machine learning , known as regularized empirical risk minimization . We propose and analyze a new proximal stochastic gradient method , which uses a multi-stage scheme to progressively reduce the variance of the stochastic gradient . While each iteration of this algorithm has similar cost as the classical stochastic gradient method ( or incremental gradient method ) , we show that the expected objective value converges to the optimum at a geometric rate . The overall complexity of this method is much lower than both the proximal full gradient method and the standard proximal stochastic gradient method .
Hypothesis tests in models whose dimension far exceeds the sample size can be formulated much like the classical studentized tests only after the initial bias of estimation is removed successfully . The theory of debiased estimators can be developed in the context of quantile regression models for a fixed quantile value . However , it is frequently desirable to formulate tests based on the quantile regression process , as this leads to more robust tests and more stable confidence sets . Additionally , inference in quantile regression requires estimation of the so called sparsity function , which depends on the unknown density of the error . In this paper we consider a debiasing approach for the uniform testing problem . We develop high-dimensional regression rank scores and show how to use them to estimate the sparsity function , as well as how to adapt them for inference involving the quantile regression process . Furthermore , we develop a Kolmogorov-Smirnov test in a location-shift high-dimensional models and confidence sets that are uniformly valid for many quantile values . The main technical result are the development of a Bahadur representation of the debiasing estimator that is uniform over a range of quantiles and uniform convergence of the quantile process to the Brownian bridge process , which are of independent interest . Simulation studies illustrate finite sample properties of our procedure .
The seemingly stochastic transient dynamics of neocortical circuits observed in vivo have been hypothesized to represent a signature of ongoing stochastic inference . In vitro neurons , on the other hand , exhibit a highly deterministic response to various types of stimulation . We show that an ensemble of deterministic leaky integrate-and-fire neurons embedded in a spiking noisy environment can attain the correct firing statistics in order to sample from a well-defined target distribution . We provide an analytical derivation of the activation function on the single cell level ; for recurrent networks , we examine convergence towards stationarity in computer simulations and demonstrate sample-based Bayesian inference in a mixed graphical model . This establishes a rigorous link between deterministic neuron models and functional stochastic dynamics on the network level .
In [00] , a general , inexact , efficient proximal quasi-Newton algorithm for composite optimization problems has been proposed and a sublinear global convergence rate has been established . In this paper , we analyze the convergence properties of this method , both in the exact and inexact setting , in the case when the objective function is strongly convex . We also investigate a practical variant of this method by establishing a simple stopping criterion for the subproblem optimization . Furthermore , we consider an accelerated variant , based on FISTA [0] , to the proximal quasi-Newton algorithm . A similar accelerated method has been considered in [0] , where the convergence rate analysis relies on very strong impractical assumptions . We present a modified analysis while relaxing these assumptions and perform a practical comparison of the accelerated proximal quasi- Newton algorithm and the regular one . Our analysis and computational results show that acceleration may not bring any benefit in the quasi-Newton setting .
We present a model of a basic recurrent neural network ( or bRNN ) that includes a separate linear term with a slightly " stable " fixed matrix to guarantee bounded solutions and fast dynamic response . We formulate a state space viewpoint and adapt the constrained optimization Lagrange Multiplier ( CLM ) technique and the vector Calculus of Variations ( CoV ) to derive the ( stochastic ) gradient descent . In this process , one avoids the commonly used re-application of the circular chain-rule and identifies the error back-propagation with the co-state backward dynamic equations . We assert that this bRNN can successfully perform regression tracking of time-series . Moreover , the " vanishing and exploding " gradients are explicitly quantified and explained through the co-state dynamics and the update laws . The adapted CoV framework , in addition , can correctly and principally integrate new loss functions in the network on any variable and for varied goals , e . g . , for supervised learning on the outputs and unsupervised learning on the internal ( hidden ) states .
In this paper , we have derived a set of sufficient conditions for reliable clustering of data produced by Bernoulli Mixture Models ( BMM ) , when the number of clusters is unknown . A BMM refers to a random binary vector whose components are independent Bernoulli trials with cluster-specific frequencies . The problem of clustering BMM data arises in many real-world applications , most notably in population genetics where researchers aim at inferring the population structure from multilocus genotype data . Our findings stipulate a minimum dataset size and a minimum number of Bernoulli trials ( or genotyped loci ) per sample , such that the existence of a clustering algorithm with a sufficient accuracy is guaranteed . Moreover , the mathematical intuitions and tools behind our work can help researchers in designing more effective and theoretically-plausible heuristic methods for similar problems .
In the classic sparsity-driven problems , the fundamental L-0 penalty method has been shown to have good performance in reconstructing signals for a wide range of problems . However this performance relies on a good choice of penalty weight which is often found from empirical experiments . We propose an algorithm called the Laplacian variational automatic relevance determination ( Lap-VARD ) that takes this penalty weight as a parameter of a prior Laplace distribution . Optimization of this parameter using an automatic relevance determination framework results in a balance between the sparsity and accuracy of signal reconstruction . Our algorithm is implemented in a transmission tomography model with sparsity constraint in wavelet domain .
In this paper , we provide a theoretical analysis of the nuclear-norm regularized least squares for full-rank matrix completion . Although similar formulations have been examined by previous studies , their results are unsatisfactory because only additive upper bounds are provided . Under the assumption that the top eigenspaces of the target matrix are incoherent , we derive a relative upper bound for recovering the best low-rank approximation of the unknown matrix . Our relative upper bound is tighter than previous additive bounds of other methods if the mass of the target matrix is concentrated on its top eigenspaces , and also implies perfect recovery if it is low-rank . The analysis is built upon the optimality condition of the regularized formulation and existing guarantees for low-rank matrix completion . To the best of our knowledge , this is first time such a relative bound is proved for the regularized formulation of matrix completion .
The elastic net was introduced as a heuristic algorithm for combinatorial optimisation and has been applied , among other problems , to biological modelling . It has an energy function which trades off a fitness term against a tension term . In the original formulation of the algorithm the tension term was implicitly based on a first-order derivative . In this paper we generalise the elastic net model to an arbitrary quadratic tension term , e . g . derived from a discretised differential operator , and give an efficient learning algorithm . We refer to these as generalised elastic nets ( GENs ) . We give a theoretical analysis of the tension term for 0D nets with periodic boundary conditions , and show that the model is sensitive to the choice of finite difference scheme that represents the discretised derivative . We illustrate some of these issues in the context of cortical map models , by relating the choice of tension term to a cortical interaction function . In particular , we prove that this interaction takes the form of a Mexican hat for the original elastic net , and of progressively more oscillatory Mexican hats for higher-order derivatives . The results apply not only to generalised elastic nets but also to other methods using discrete differential penalties , and are expected to be useful in other areas , such as data analysis , computer graphics and optimisation problems .
The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence . Neural networks are not , in general , capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models . We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time . Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks . We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 0000 games sequentially .
We consider a Bayesian method for learning the Bayesian network structure from complete data . Recently , Koivisto and Sood ( 0000 ) presented an algorithm that for any single edge computes its marginal posterior probability in O ( n 0^n ) time , where n is the number of attributes ; the number of parents per attribute is bounded by a constant . In this paper we show that the posterior probabilities for all the n ( n - 0 ) potential edges can be computed in O ( n 0^n ) total time . This result is achieved by a forward-backward technique and fast Moebius transform algorithms , which are of independent interest . The resulting speedup by a factor of about n^0 allows us to experimentally study the statistical power of learning moderate-size networks . We report results from a simulation study that covers data sets with 00 to 00 , 000 records over 0 to 00 discrete attributes
Deep neural networks ( DNNs ) are now a central component of nearly all state-of-the-art speech recognition systems . Building neural network acoustic models requires several design decisions including network architecture , size , and training loss function . This paper offers an empirical investigation on which aspects of DNN acoustic model design are most important for speech recognition system performance . We report DNN classifier performance and final speech recognizer word error rates , and compare DNNs using several metrics to quantify factors influencing differences in task performance . Our first set of experiments use the standard Switchboard benchmark corpus , which contains approximately 000 hours of conversational telephone speech . We compare standard DNNs to convolutional networks , and present the first experiments using locally-connected , untied neural networks for acoustic modeling . We additionally build systems on a corpus of 0 , 000 hours of training data by combining the Switchboard and Fisher corpora . This larger corpus allows us to more thoroughly examine performance of large DNN models -- with up to ten times more parameters than those typically used in speech recognition systems . Our results suggest that a relatively simple DNN architecture and optimization technique produces strong results . These findings , along with previous work , help establish a set of best practices for building DNN hybrid speech recognition systems with maximum likelihood training . Our experiments in DNN optimization additionally serve as a case study for training DNNs with discriminative loss functions for speech tasks , as well as DNN classifiers more generally .
This paper proposed a new regression model called $l_0$-regularized outlier isolation and regression ( LOIRE ) and a fast algorithm based on block coordinate descent to solve this model . Besides , assuming outliers are gross errors following a Bernoulli process , this paper also presented a Bernoulli estimate model which , in theory , should be very accurate and robust due to its complete elimination of affections caused by outliers . Though this Bernoulli estimate is hard to solve , it could be approximately achieved through a process which takes LOIRE as an important intermediate step . As a result , the approximate Bernoulli estimate is a good combination of Bernoulli estimate ' s accuracy and LOIRE regression ' s efficiency with several simulations conducted to strongly verify this point . Moreover , LOIRE can be further extended to realize robust rank factorization which is powerful in recovering low-rank component from massive corruptions . Extensive experimental results showed that the proposed method outperforms state-of-the-art methods like RPCA and GoDec in the aspect of computation speed with a competitive performance .
Rank aggregation based on pairwise comparisons over a set of items has a wide range of applications . Although considerable research has been devoted to the development of rank aggregation algorithms , one basic question is how to efficiently collect a large amount of high-quality pairwise comparisons for the ranking purpose . Because of the advent of many crowdsourcing services , a crowd of workers are often hired to conduct pairwise comparisons with a small monetary reward for each pair they compare . Since different workers have different levels of reliability and different pairs have different levels of ambiguity , it is desirable to wisely allocate the limited budget for comparisons among the pairs of items and workers so that the global ranking can be accurately inferred from the comparison results . To this end , we model the active sampling problem in crowdsourced ranking as a Bayesian Markov decision process , which dynamically selects item pairs and workers to improve the ranking accuracy under a budget constraint . We further develop a computationally efficient sampling policy based on knowledge gradient as well as a moment matching technique for posterior approximation . Experimental evaluations on both synthetic and real data show that the proposed policy achieves high ranking accuracy with a lower labeling cost .
This paper presents a novel multitask multiple kernel learning framework that efficiently learns the kernel weights leveraging the relationship across multiple tasks . The idea is to automatically infer this task relationship in the \textit{RKHS} space corresponding to the given base kernels . The problem is formulated as a regularization-based approach called \textit{Multi-Task Multiple Kernel Relationship Learning} ( \textit{MK-MTRL} ) , which models the task relationship matrix from the weights learned from latent feature spaces of task-specific base kernels . Unlike in previous work , the proposed formulation allows one to incorporate prior knowledge for simultaneously learning several related tasks . We propose an alternating minimization algorithm to learn the model parameters , kernel weights and task relationship matrix . In order to tackle large-scale problems , we further propose a two-stage \textit{MK-MTRL} online learning algorithm and show that it significantly reduces the computational time , and also achieves performance comparable to that of the joint learning framework . Experimental results on benchmark datasets show that the proposed formulations outperform several state-of-the-art multitask learning methods .
We study a generalization of the multi-armed bandit problem with multiple plays where there is a cost associated with pulling each arm and the agent has a budget at each time that dictates how much she can expect to spend . We derive an asymptotic regret lower bound for any uniformly efficient algorithm in our setting . We then study a variant of Thompson sampling for Bernoulli rewards and a variant of KL-UCB for both single-parameter exponential families and bounded , finitely supported rewards . We show these algorithms are asymptotically optimal , both in rate and leading problem-dependent constants , including in the thick margin setting where multiple arms fall on the decision boundary .
Convolutional neural networks ( CNNs ) can be applied to graph similarity matching , in which case they are called graph CNNs . Graph CNNs are attracting increasing attention due to their effectiveness and efficiency . However , the existing convolution approaches focus only on regular data forms and require the transfer of the graph or key node neighborhoods of the graph into the same fixed form . During this transfer process , structural information of the graph can be lost , and some redundant information can be incorporated . To overcome this problem , we propose the disordered graph convolutional neural network ( DGCNN ) based on the mixed Gaussian model , which extends the CNN by adding a preprocessing layer called the disordered graph convolutional layer ( DGCL ) . The DGCL uses a mixed Gaussian function to realize the mapping between the convolution kernel and the nodes in the neighborhood of the graph . The output of the DGCL is the input of the CNN . We further implement a backward-propagation optimization process of the convolutional layer by which we incorporate the feature-learning model of the irregular node neighborhood structure into the network . Thereafter , the optimization of the convolution kernel becomes part of the neural network learning process . The DGCNN can accept arbitrary scaled and disordered neighborhood graph structures as the receptive fields of CNNs , which reduces information loss during graph transformation . Finally , we perform experiments on multiple standard graph datasets . The results show that the proposed method outperforms the state-of-the-art methods in graph classification and retrieval .
The collection and analysis of user data drives improvements in the app and web ecosystems , but comes with risks to privacy . This paper examines discrete distribution estimation under local privacy , a setting wherein service providers can learn the distribution of a categorical statistic of interest without collecting the underlying data . We present new mechanisms , including hashed K-ary Randomized Response ( KRR ) , that empirically meet or exceed the utility of existing mechanisms at all privacy levels . New theoretical results demonstrate the order-optimality of KRR and the existing RAPPOR mechanism at different privacy regimes .
This paper provides lower bounds on the convergence rate of Derivative Free Optimization ( DFO ) with noisy function evaluations , exposing a fundamental and unavoidable gap between the performance of algorithms with access to gradients and those with access to only function evaluations . However , there are situations in which DFO is unavoidable , and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions . A distinctive feature of the algorithm is that it uses only Boolean-valued function comparisons , rather than function evaluations . This makes the algorithm useful in an even wider range of applications , such as optimization based on paired comparisons from human subjects , for example . We also show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons , the convergence rate is the same .
We present a new , efficient method for automatically detecting severe conflicts `edit wars ' in Wikipedia and evaluate this method on six different language WPs . We discuss how the number of edits , reverts , the length of discussions , the burstiness of edits and reverts deviate in such pages from those following the general workflow , and argue that earlier work has significantly over-estimated the contentiousness of the Wikipedia editing process .
A Bayesian factor graph reduced to normal form consists in the interconnection of diverter units ( or equal constraint units ) and Single-Input/Single-Output ( SISO ) blocks . In this framework localized adaptation rules are explicitly derived from a constrained maximum likelihood ( ML ) formulation and from a minimum KL-divergence criterion using KKT conditions . The learning algorithms are compared with two other updating equations based on a Viterbi-like and on a variational approximation respectively . The performance of the various algorithm is verified on synthetic data sets for various architectures . The objective of this paper is to provide the programmer with explicit algorithms for rapid deployment of Bayesian graphs in the applications .
We investigate statistical properties of a clustering algorithm that receives level set estimates from a kernel density estimator and then estimates the first split in the density level cluster tree if such a split is present or detects the absence of such a split . Key aspects of our analysis include finite sample guarantees , consistency , rates of convergence , and an adaptive data-driven strategy for chosing the kernel bandwidth . For the rates and the adaptivity we do not need continuity assumptions on the density such as H\ " older continuity , but only require intuitive geometric assumptions of non-parametric nature .
Computer vision is hard because of a large variability in lighting , shape , and texture ; in addition the image signal is non-additive due to occlusion . Generative models promised to account for this variability by accurately modelling the image formation process as a function of latent variables with prior beliefs . Bayesian posterior inference could then , in principle , explain the observation . While intuitively appealing , generative models for computer vision have largely failed to deliver on that promise due to the difficulty of posterior inference . As a result the community has favoured efficient discriminative approaches . We still believe in the usefulness of generative models in computer vision , but argue that we need to leverage existing discriminative or even heuristic computer vision methods . We implement this idea in a principled way with an " informed sampler " and in careful experiments demonstrate it on challenging generative models which contain renderer programs as their components . We concentrate on the problem of inverting an existing graphics rendering engine , an approach that can be understood as " Inverse Graphics " . The informed sampler , using simple discriminative proposals based on existing computer vision technology , achieves significant improvements of inference .
Subspace learning and matrix factorization problems have a great many applications in science and engineering , and efficient algorithms are critical as dataset sizes continue to grow . Many relevant problem formulations are non-convex , and in a variety of contexts it has been observed that solving the non-convex problem directly is not only efficient but reliably accurate . We discuss convergence theory for a particular method : first order incremental gradient descent constrained to the Grassmannian . The output of the algorithm is an orthonormal basis for a $d$-dimensional subspace spanned by an input streaming data matrix . We study two sampling cases : where each data vector of the streaming matrix is fully sampled , or where it is undersampled by a sampling matrix $A_t\in \R^{m\times n}$ with $m\ll n$ . We propose an adaptive stepsize scheme that depends only on the sampled data and algorithm outputs . We prove that with fully sampled data , the stepsize scheme maximizes the improvement of our convergence metric at each iteration , and this method converges from any random initialization to the true subspace , despite the non-convex formulation and orthogonality constraints . For the case of undersampled data , we establish monotonic improvement on the defined convergence metric for each iteration with high probability .
Many efforts have been devoted to training generative latent variable models with autoregressive decoders , such as recurrent neural networks ( RNN ) . Stochastic recurrent models have been successful in capturing the variability observed in natural sequential data such as speech . We unify successful ideas from recently proposed architectures into a stochastic recurrent model : each step in the sequence is associated with a latent variable that is used to condition the recurrent dynamics for future steps . Training is performed with amortized variational inference where the approximate posterior is augmented with a RNN that runs backward through the sequence . In addition to maximizing the variational lower bound , we ease training of the latent variables by adding an auxiliary cost which forces them to reconstruct the state of the backward recurrent network . This provides the latent variables with a task-independent objective that enhances the performance of the overall model . We found this strategy to perform better than alternative approaches such as KL annealing . Although being conceptually simple , our model achieves state-of-the-art results on standard speech benchmarks such as TIMIT and Blizzard and competitive performance on sequential MNIST . Finally , we apply our model to language modeling on the IMDB dataset where the auxiliary cost helps in learning interpretable latent variables . Source Code : \url{https : //github . com/anirudh0000/zforcing_nips00}
Online media provides opportunities for marketers through which they can deliver effective brand messages to a wide range of audiences . Advertising technology platforms enable advertisers to reach their target audience by delivering ad impressions to online users in real time . In order to identify the best marketing message for a user and to purchase impressions at the right price , we rely heavily on bid prediction and optimization models . Even though the bid prediction models are well studied in the literature , the equally important subject of model evaluation is usually overlooked . Effective and reliable evaluation of an online bidding model is crucial for making faster model improvements as well as for utilizing the marketing budgets more efficiently . In this paper , we present an experimentation framework for bid prediction models where our focus is on the practical aspects of model evaluation . Specifically , we outline the unique challenges we encounter in our platform due to a variety of factors such as heterogeneous goal definitions , varying budget requirements across different campaigns , high seasonality and the auction-based environment for inventory purchasing . Then , we introduce return on investment ( ROI ) as a unified model performance ( i . e . , success ) metric and explain its merits over more traditional metrics such as click-through rate ( CTR ) or conversion rate ( CVR ) . Most importantly , we discuss commonly used evaluation and metric summarization approaches in detail and propose a more accurate method for online evaluation of new experimental models against the baseline . Our meta-analysis-based approach addresses various shortcomings of other methods and yields statistically robust conclusions that allow us to conclude experiments more quickly in a reliable manner . We demonstrate the effectiveness of our evaluation strategy on real campaign data through some experiments .
In many machine learning applications , it is important to explain the predictions of a black-box classifier . For example , why does a deep neural network assign an image to a particular class ? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints . By extending ideas from Badanidiyuru et al . [0000] , we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function . This is the first such theoretical guarantee for this general class of functions , and we also show that no such algorithm exists for a worst case stream order . Our algorithm obtains similar explanations of Inception V0 predictions $00$ times faster than the state-of-the-art LIME framework of Ribeiro et al . [0000] .
Growing interest in automatic speaker verification ( ASV ) systems has lead to significant quality improvement of spoofing attackson them . Many research works confirm that despite the low equal er-ror rate ( EER ) ASV systems are still vulnerable to spoofing attacks . Inthis work we overview different acoustic feature spaces and classifiersto determine reliable and robust countermeasures against spoofing at-tacks . We compared several spoofing detection systems , presented so far , on the development and evaluation datasets of the Automatic SpeakerVerification Spoofing and Countermeasures ( ASVspoof ) Challenge 0000 . Experimental results presented in this paper demonstrate that the useof magnitude and phase information combination provides a substantialinput into the efficiency of the spoofing detection systems . Also wavelet-based features show impressive results in terms of equal error rate . Inour overview we compare spoofing performance for systems based on dif-ferent classifiers . Comparison results demonstrate that the linear SVMclassifier outperforms the conventional GMM approach . However , manyresearchers inspired by the great success of deep neural networks ( DNN ) approaches in the automatic speech recognition , applied DNN in thespoofing detection task and obtained quite low EER for known and un-known type of spoofing attacks .
In this paper , auto-associative models are proposed as candidates to the generalization of Principal Component Analysis . We show that these models are dedicated to the approximation of the dataset by a manifold . Here , the word " manifold " refers to the topology properties of the structure . The approximating manifold is built by a projection pursuit algorithm . At each step of the algorithm , the dimension of the manifold is incremented . Some theoretical properties are provided . In particular , we can show that , at each step of the algorithm , the mean residuals norm is not increased . Moreover , it is also established that the algorithm converges in a finite number of steps . Some particular auto-associative models are exhibited and compared to the classical PCA and some neural networks models . Implementation aspects are discussed . We show that , in numerous cases , no optimization procedure is required . Some illustrations on simulated and real data are presented .
The ability to perform effective off-policy learning would revolutionize the process of building better interactive systems , such as search engines and recommendation systems for e-commerce , computational advertising and news . Recent approaches for off-policy evaluation and learning in these settings appear promising . With this paper , we provide real-world data and a standardized test-bed to systematically investigate these algorithms using data from display advertising . In particular , we consider the problem of filling a banner ad with an aggregate of multiple products the user may want to purchase . This paper presents our test-bed , the sanity checks we ran to ensure its validity , and shows results comparing state-of-the-art off-policy learning methods like doubly robust optimization , POEM , and reductions to supervised learning using regression baselines . Our results show experimental evidence that recent off-policy learning methods can improve upon state-of-the-art supervised learning techniques on a large-scale real-world data set .
In this note we prove a tight lower bound for the MNL-bandit assortment selection model that matches the upper bound given in ( Agrawal et al . , 0000a , b ) for all parameters , up to logarithmic factors .
Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process . We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors . Its key properties are : 0 ) it has simple , generic code applicable to many models , 0 ) it has no free parameters , 0 ) it works well for a variety of Gaussian process based models . These properties make our method ideal for use while model building , removing the need to spend time deriving and tuning updates for more complex algorithms .
In this paper , we investigate the sample size requirement for exact recovery of a high order tensor of low rank from a subset of its entries . We show that a gradient descent algorithm with initial value obtained from a spectral method can , in particular , reconstruct a ${d\times d\times d}$ tensor of multilinear ranks $ ( r , r , r ) $ with high probability from as few as $O ( r^{0/0}d^{0/0}\log^{0/0}d+r^0d\log^0d ) $ entries . In the case when the ranks $r=O ( 0 ) $ , our sample size requirement matches those for nuclear norm minimization ( Yuan and Zhang , 0000a ) , or alternating least squares assuming orthogonal decomposability ( Jain and Oh , 0000 ) . Unlike these earlier approaches , however , our method is efficient to compute , easy to implement , and does not impose extra structures on the tensor . Numerical results are presented to further demonstrate the merits of the proposed approach .
Entity resolution ( ER ) is the task of identifying records belonging to the same entity ( e . g . individual , group ) across one or multiple databases . Ironically , it has multiple names : deduplication and record linkage , among others . In this paper we survey metrics used to evaluate ER results in order to iteratively improve performance and guarantee sufficient quality prior to deployment . Some of these metrics are borrowed from multi-class classification and clustering domains , though some key differences exist differentiating entity resolution from general clustering . Menestrina et al . empirically showed rankings from these metrics often conflict with each other , thus our primary motivation for studying them . This paper provides practitioners the basic knowledge to begin evaluating their entity resolution results .
The main goal of this study is to extract a set of brain networks in multiple time-resolutions to analyze the connectivity patterns among the anatomic regions for a given cognitive task . We suggest a deep architecture which learns the natural groupings of the connectivity patterns of human brain in multiple time-resolutions . The suggested architecture is tested on task data set of Human Connectome Project ( HCP ) where we extract multi-resolution networks , each of which corresponds to a cognitive task . At the first level of this architecture , we decompose the fMRI signal into multiple sub-bands using wavelet decompositions . At the second level , for each sub-band , we estimate a brain network extracted from short time windows of the fMRI signal . At the third level , we feed the adjacency matrices of each mesh network at each time-resolution into an unsupervised deep learning algorithm , namely , a Stacked De- noising Auto-Encoder ( SDAE ) . The outputs of the SDAE provide a compact connectivity representation for each time window at each sub-band of the fMRI signal . We concatenate the learned representations of all sub-bands at each window and cluster them by a hierarchical algorithm to find the natural groupings among the windows . We observe that each cluster represents a cognitive task with a performance of 00% Rand Index and 00% Adjusted Rand Index . We visualize the mean values and the precisions of the networks at each component of the cluster mixture . The mean brain networks at cluster centers show the variations among cognitive tasks and the precision of each cluster shows the within cluster variability of networks , across the subjects .
One of the objectives of designing feature selection learning algorithms is to obtain classifiers that depend on a small number of attributes and have verifiable future performance guarantees . There are few , if any , approaches that successfully address the two goals simultaneously . Performance guarantees become crucial for tasks such as microarray data analysis due to very small sample sizes resulting in limited empirical evaluation . To the best of our knowledge , such algorithms that give theoretical bounds on the future performance have not been proposed so far in the context of the classification of gene expression data . In this work , we investigate the premise of learning a conjunction ( or disjunction ) of decision stumps in Occam ' s Razor , Sample Compression , and PAC-Bayes learning settings for identifying a small subset of attributes that can be used to perform reliable classification tasks . We apply the proposed approaches for gene identification from DNA microarray data and compare our results to those of well known successful approaches proposed for the task . We show that our algorithm not only finds hypotheses with much smaller number of genes while giving competitive classification accuracy but also have tight risk guarantees on future performance unlike other approaches . The proposed approaches are general and extensible in terms of both designing novel algorithms and application to other domains .
We study the problem of estimating a manifold from random samples . In particular , we consider piecewise constant and piecewise linear estimators induced by k-means and k-flats , and analyze their performance . We extend previous results for k-means in two separate directions . First , we provide new results for k-means reconstruction on manifolds and , secondly , we prove reconstruction bounds for higher-order approximation ( k-flats ) , for which no known results were previously available . While the results for k-means are novel , some of the technical tools are well-established in the literature . In the case of k-flats , both the results and the mathematical tools are new .
Prototypal analysis is introduced to overcome two shortcomings of archetypal analysis : its sensitivity to outliers and its non-locality , which reduces its applicability as a learning tool . Same as archetypal analysis , prototypal analysis finds prototypes through convex combination of the data points and approximates the data through convex combination of the archetypes , but it adds a penalty for using prototypes distant from the data points for their reconstruction . Prototypal analysis can be extended---via kernel embedding---to probability distributions , since the convexity of the prototypes makes them interpretable as mixtures . Finally , prototypal regression is developed , a robust supervised procedure which allows the use of distributions as either features or labels .
This letter presents a novel Block Bayesian Hypothesis Testing Algorithm ( Block-BHTA ) for reconstructing block sparse signals with unknown block structures . The Block-BHTA comprises the detection and recovery of the supports , and the estimation of the amplitudes of the block sparse signal . The support detection and recovery is performed using a Bayesian hypothesis testing . Then , based on the detected and reconstructed supports , the nonzero amplitudes are estimated by linear MMSE . The effectiveness of Block-BHTA is demonstrated by numerical experiments .
In this paper we explore the applicability of the unsupervised machine learning technique of Self Organizing Maps ( SOM ) to estimate galaxy photometric redshift probability density functions ( PDFs ) . This technique takes a spectroscopic training set , and maps the photometric attributes , but not the redshifts , to a two dimensional surface by using a process of competitive learning where neurons compete to more closely resemble the training data multidimensional space . The key feature of a SOM is that it retains the topology of the input set , revealing correlations between the attributes that are not easily identified . We test three different 0D topological mapping : rectangular , hexagonal , and spherical , by using data from the DEEP0 survey . We also explore different implementations and boundary conditions on the map and also introduce the idea of a random atlas where a large number of different maps are created and their individual predictions are aggregated to produce a more robust photometric redshift PDF . We also introduced a new metric , the $I$-score , which efficiently incorporates different metrics , making it easier to compare different results ( from different parameters or different photometric redshift codes ) . We find that by using a spherical topology mapping we obtain a better representation of the underlying multidimensional topology , which provides more accurate results that are comparable to other , state-of-the-art machine learning algorithms . Our results illustrate that unsupervised approaches have great potential for many astronomical problems , and in particular for the computation of photometric redshifts .
Appropriately designing the proposal kernel of particle filters is an issue of significant importance , since a bad choice may lead to deterioration of the particle sample and , consequently , waste of computational power . In this paper we introduce a novel algorithm adaptively approximating the so-called optimal proposal kernel by a mixture of integrated curved exponential distributions with logistic weights . This family of distributions , referred to as mixtures of experts , is broad enough to be used in the presence of multi-modality or strongly skewed distributions . The mixtures are fitted , via online-EM methods , to the optimal kernel through minimisation of the Kullback-Leibler divergence between the auxiliary target and instrumental distributions of the particle filter . At each iteration of the particle filter , the algorithm is required to solve only a single optimisation problem for the whole particle sample , yielding an algorithm with only linear complexity . In addition , we illustrate in a simulation study how the method can be successfully applied to optimal filtering in nonlinear state-space models .
Predicting the biological function of molecules , be it proteins or drug-like compounds , from their atomic structure is an important and long-standing problem . Function is dictated by structure , since it is by spatial interactions that molecules interact with each other , both in terms of steric complementarity , as well as intermolecular forces . Thus , the electron density field and electrostatic potential field of a molecule contain the " raw fingerprint " of how this molecule can fit to binding partners . In this paper , we show that deep learning can predict biological function of molecules directly from their raw 0D approximated electron density and electrostatic potential fields . Protein function based on EC numbers is predicted from the approximated electron density field . In another experiment , the activity of small molecules is predicted with quality comparable to state-of-the-art descriptor-based methods . We propose several alternative computational models for the GPU with different memory and runtime requirements for different sizes of molecules and of databases . We also propose application-specific multi-channel data representations . With future improvements of training datasets and neural network settings in combination with complementary information sources ( sequence , genomic context , expression level ) , deep learning can be expected to show its generalization power and revolutionize the field of molecular function prediction .
A novel extrapolation method is proposed for longitudinal forecasting . A hierarchical Gaussian process model is used to combine nonlinear population change and individual memory of the past to make prediction . The prediction error is minimized through the hierarchical design . The method is further extended to joint modeling of continuous measurements and survival events . The baseline hazard , covariate and joint effects are conveniently modeled in this hierarchical structure . The estimation and inference are implemented in fully Bayesian framework using the objective and shrinkage priors . In simulation studies , this model shows robustness in latent estimation , correlation detection and high accuracy in forecasting . The model is illustrated with medical monitoring data from cystic fibrosis ( CF ) patients . Estimation and forecasts are obtained in the measurement of lung function and records of acute respiratory events . Keyword : Extrapolation , Joint Model , Longitudinal Model , Hierarchical Gaussian Process , Cystic Fibrosis , Medical Monitoring
Deep neural networks ( DNNs ) form the backbone of almost every state-of-the-art technique in the fields such as computer vision , speech processing , and text analysis . The recent advances in computational technology have made the use of DNNs more practical . Despite the overwhelming performances by DNN and the advances in computational technology , it is seen that very few researchers try to train their models from the scratch . Training of DNNs still remains a difficult and tedious job . The main challenges that researchers face during training of DNNs are the vanishing/exploding gradient problem and the highly non-convex nature of the objective function which has up to million variables . The approaches suggested in He and Xavier solve the vanishing gradient problem by providing a sophisticated initialization technique . These approaches have been quite effective and have achieved good results on standard datasets , but these same approaches do not work very well on more practical datasets . We think the reason for this is not making use of data statistics for initializing the network weights . Optimizing such a high dimensional loss function requires careful initialization of network weights . In this work , we propose a data dependent initialization and analyze its performance against the standard initialization techniques such as He and Xavier . We performed our experiments on some practical datasets and the results show our algorithm ' s superior classification accuracy .
We consider analysis of relational data ( a matrix ) , in which the rows correspond to subjects ( e . g . , people ) and the columns correspond to attributes . The elements of the matrix may be a mix of real and categorical . Each subject and attribute is characterized by a latent binary feature vector , and an inferred matrix maps each row-column pair of binary feature vectors to an observed matrix element . The latent binary features of the rows are modeled via a multivariate Gaussian distribution with low-rank covariance matrix , and the Gaussian random variables are mapped to latent binary features via a probit link . The same type construction is applied jointly to the columns . The model infers latent , low-dimensional binary features associated with each row and each column , as well correlation structure between all rows and between all columns .
For large matrix factorisation problems , we develop a distributed Markov Chain Monte Carlo ( MCMC ) method based on stochastic gradient Langevin dynamics ( SGLD ) that we call Parallel SGLD ( PSGLD ) . PSGLD has very favourable scaling properties with increasing data size and is comparable in terms of computational requirements to optimisation methods based on stochastic gradient descent . PSGLD achieves high performance by exploiting the conditional independence structure of the MF models to sub-sample data in a systematic manner as to allow parallelisation and distributed computation . We provide a convergence proof of the algorithm and verify its superior performance on various architectures such as Graphics Processing Units , shared memory multi-core systems and multi-computer clusters .
We revisit the development of grid based recursive approximate filtering of general Markov processes in discrete time , partially observed in conditionally Gaussian noise . The grid based filters considered rely on two types of state quantization : The \textit{Markovian} type and the \textit{marginal} type . We propose a set of novel , relaxed sufficient conditions , ensuring strong and fully characterized pathwise convergence of these filters to the respective MMSE state estimator . In particular , for marginal state quantizations , we introduce the notion of \textit{conditional regularity of stochastic kernels} , which , to the best of our knowledge , constitutes the most relaxed condition proposed , under which asymptotic optimality of the respective grid based filters is guaranteed . Further , we extend our convergence results , including filtering of bounded and continuous functionals of the state , as well as recursive approximate state prediction . For both Markovian and marginal quantizations , the whole development of the respective grid based filters relies more on linear-algebraic techniques and less on measure theoretic arguments , making the presentation considerably shorter and technically simpler .
Recurrent major mood episodes and subsyndromal mood instability cause substantial disability in patients with bipolar disorder . Early identification of mood episodes enabling timely mood stabilisation is an important clinical goal . Recent technological advances allow the prospective reporting of mood in real time enabling more accurate , efficient data capture . The complex nature of these data streams in combination with challenge of deriving meaning from missing data mean pose a significant analytic challenge . The signature method is derived from stochastic analysis and has the ability to capture important properties of complex ordered time series data . To explore whether the onset of episodes of mania and depression can be identified using self-reported mood data .
We propose RoBiRank , a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification . The algorithm shows a very competitive performance on standard benchmark datasets against other representative algorithms in the literature . On the other hand , in large scale problems where explicit feature vectors and scores are not given , our algorithm can be efficiently parallelized across a large number of machines ; for a task that requires 000 , 000 x 00 , 000 , 000 pairwise interactions between items to be ranked , our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm , given the same amount of wall-clock time for computation .
Prediction of disease onset from patient survey and lifestyle data is quickly becoming an important tool for diagnosing a disease before it progresses . In this study , data from the National Health and Nutrition Examination Survey ( NHANES ) questionnaire is used to predict the onset of type II diabetes . An ensemble model using the output of five classification algorithms was developed to predict the onset on diabetes based on 00 features . The ensemble model had an AUC of 0 . 000 indicating high performance .
Recent work in learning ontologies ( hierarchical and partially-ordered structures ) has leveraged the intrinsic geometry of spaces of learned representations to make predictions that automatically obey complex structural constraints . We explore two extensions of one such model , the order-embedding model for hierarchical relation learning , with an aim towards improved performance on text data for commonsense knowledge representation . Our first model jointly learns ordering relations and non-hierarchical knowledge in the form of raw text . Our second extension exploits the partial order structure of the training data to find long-distance triplet constraints among embeddings which are poorly enforced by the pairwise training procedure . We find that both incorporating free text and augmented training constraints improve over the original order-embedding model and other strong baselines .
Popular sparse estimation methods based on $\ell_0$-relaxation , such as the Lasso and the Dantzig selector , require the knowledge of the variance of the noise in order to properly tune the regularization parameter . This constitutes a major obstacle in applying these methods in several frameworks---such as time series , random fields , inverse problems---for which the noise is rarely homoscedastic and its level is hard to know in advance . In this paper , we propose a new approach to the joint estimation of the conditional mean and the conditional variance in a high-dimensional ( auto- ) regression setting . An attractive feature of the proposed estimator is that it is efficiently computable even for very large scale problems by solving a second-order cone program ( SOCP ) . We present theoretical analysis and numerical results assessing the performance of the proposed procedure .
Boosting is a general method of generating many simple classification rules and combining them into a single , highly accurate rule . In this talk , I will review the AdaBoost boosting algorithm and some of its underlying theory , and then look at how this theory has helped us to face some of the challenges of applying AdaBoost in two domains : In the first of these , we used boosting for predicting and modeling the uncertainty of prices in complicated , interacting auctions . The second application was to the classification of caller utterances in a telephone spoken-dialogue system where we faced two challenges : the need to incorporate prior knowledge to compensate for initially insufficient data ; and a later need to filter the large stream of unlabeled examples being collected to select the ones whose labels are likely to be most informative .
Common statistical prediction models often require and assume stationarity in the data . However , in many practical applications , changes in the relationship of the response and predictor variables are regularly observed over time , resulting in the deterioration of the predictive performance of these models . This paper presents Linear Four Rates ( LFR ) , a framework for detecting these concept drifts and subsequently identifying the data points that belong to the new concept ( for relearning the model ) . Unlike conventional concept drift detection approaches , LFR can be applied to both batch and stream data ; is not limited by the distribution properties of the response variable ( e . g . , datasets with imbalanced labels ) ; is independent of the underlying statistical-model ; and uses user-specified parameters that are intuitively comprehensible . The performance of LFR is compared to benchmark approaches using both simulated and commonly used public datasets that span the gamut of concept drift types . The results show LFR significantly outperforms benchmark approaches in terms of recall , accuracy and delay in detection of concept drifts across datasets .
A universal unanswered question in neuroscience and machine learning is whether computers can decode the patterns of the human brain . Multi-Voxels Pattern Analysis ( MVPA ) is a critical tool for addressing this question . However , there are two challenges in the previous MVPA methods , which include decreasing sparsity and noises in the extracted features and increasing the performance of prediction . In overcoming mentioned challenges , this paper proposes Anatomical Pattern Analysis ( APA ) for decoding visual stimuli in the human brain . This framework develops a novel anatomical feature extraction method and a new imbalance AdaBoost algorithm for binary classification . Further , it utilizes an Error-Correcting Output Codes ( ECOC ) method for multi-class prediction . APA can automatically detect active regions for each category of the visual stimuli . Moreover , it enables us to combine homogeneous datasets for applying advanced classification . Experimental studies on 0 visual categories ( words , consonants , objects and scrambled photos ) demonstrate that the proposed approach achieves superior performance to state-of-the-art methods .
In this letter , we present a novel exponentially embedded families ( EEF ) based classification method , in which the probability density function ( PDF ) on raw data is estimated from the PDF on features . With the PDF construction , we show that class-specific features can be used in the proposed classification method , instead of a common feature subset for all classes as used in conventional approaches . We apply the proposed EEF classifier for text categorization as a case study and derive an optimal Bayesian classification rule with class-specific feature selection based on the Information Gain ( IG ) score . The promising performance on real-life data sets demonstrates the effectiveness of the proposed approach and indicates its wide potential applications .
We present a new statistical learning paradigm for Boltzmann machines based on a new inference principle we have proposed : the latent maximum entropy principle ( LME ) . LME is different both from Jaynes maximum entropy principle and from standard maximum likelihood estimation . We demonstrate the LME principle BY deriving new algorithms for Boltzmann machine parameter estimation , and show how robust and fast new variant of the EM algorithm can be developed . Our experiments show that estimation based on LME generally yields better results than maximum likelihood estimation , particularly when inferring hidden units from small amounts of data .
This paper introduces the first deep neural network-based estimation metric for the jigsaw puzzle problem . Given two puzzle piece edges , the neural network predicts whether or not they should be adjacent in the correct assembly of the puzzle , using nothing but the pixels of each piece . The proposed metric exhibits an extremely high precision even though no manual feature extraction is performed . When incorporated into an existing puzzle solver , the solution ' s accuracy increases significantly , achieving thereby a new state-of-the-art standard .
Topic models have emerged as fundamental tools in unsupervised machine learning . Most modern topic modeling algorithms take a probabilistic view and derive inference algorithms based on Latent Dirichlet Allocation ( LDA ) or its variants . In contrast , we study topic modeling as a combinatorial optimization problem , and propose a new objective function derived from LDA by passing to the small-variance limit . We minimize the derived objective by using ideas from combinatorial optimization , which results in a new , fast , and high-quality topic modeling algorithm . In particular , we show that our results are competitive with popular LDA-based topic modeling approaches , and also discuss the ( dis ) similarities between our approach and its probabilistic counterparts .
Population migration is valuable information which leads to proper decision in urban-planning strategy , massive investment , and many other fields . For instance , inter-city migration is a posterior evidence to see if the government ' s constrain of population works , and inter-community immigration might be a prior evidence of real estate price hike . With timely data , it is also impossible to compare which city is more favorable for the people , suppose the cities release different new regulations , we could also compare the customers of different real estate development groups , where they come from , where they probably will go . Unfortunately these data was not available . In this paper , leveraging the data generated by positioning team in Didi , we propose a novel approach that timely monitoring population migration from community scale to provincial scale . Migration can be detected as soon as in a week . It could be faster , the setting of a week is for statistical purpose . A monitoring system is developed , then applied nation wide in China , some observations derived from the system will be presented in this paper . This new method of migration perception is origin from the insight that nowadays people mostly moving with their personal Access Point ( AP ) , also known as WiFi hotspot . Assume that the ratio of AP moving to the migration of population is constant , analysis of comparative population migration would be feasible . More exact quantitative research would also be done with few sample research and model regression . The procedures of processing data includes many steps : eliminating the impact of pseudo-migration AP , for instance pocket WiFi , and second-hand traded router ; distinguishing moving of population with moving of companies ; identifying shifting of AP by the finger print clusters , etc . .
The counting grid is a grid of microtopics , sparse word/feature distributions . The generative model associated with the grid does not use these microtopics individually . Rather , it groups them in overlapping rectangular windows and uses these grouped microtopics as either mixture or admixture components . This paper builds upon the basic counting grid model and it shows that hierarchical reasoning helps avoid bad local minima , produces better classification accuracy and , most interestingly , allows for extraction of large numbers of coherent microtopics even from small datasets . We evaluate this in terms of consistency , diversity and clarity of the indexed content , as well as in a user study on word intrusion tasks . We demonstrate that these models work well as a technique for embedding raw images and discuss interesting parallels between hierarchical CG models and other deep architectures .
We study statistical risk minimization problems under a privacy model in which the data is kept confidential even from the learner . In this local privacy framework , we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures . As a consequence , we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility , as measured by convergence rate , of any statistical estimator or learning procedure .
We propose a robust classifier to predict buying intentions based on user behaviour within a large e-commerce website . In this work we compare traditional machine learning techniques with the most advanced deep learning approaches . We show that both Deep Belief Networks and Stacked Denoising auto-Encoders achieved a substantial improvement by extracting features from high dimensional data during the pre-train phase . They prove also to be more convenient to deal with severe class imbalance .
Being an unsupervised machine learning and data mining technique , biclustering and its multimodal extensions are becoming popular tools for analysing object-attribute data in different domains . Apart from conventional clustering techniques , biclustering is searching for homogeneous groups of objects while keeping their common description , e . g . , in binary setting , their shared attributes . In bioinformatics , biclustering is used to find genes , which are active in a subset of situations , thus being candidates for biomarkers . However , the authors of those biclustering techniques that are popular in gene expression analysis , may overlook the existing methods . For instance , BiMax algorithm is aimed at finding biclusters , which are well-known for decades as formal concepts . Moreover , even if bioinformatics classify the biclustering methods according to reasonable domain-driven criteria , their classification taxonomies may be different from survey to survey and not full as well . So , in this paper we propose to use concept lattices as a tool for taxonomy building ( in the biclustering domain ) and attribute exploration as means for cross-domain taxonomy completion .
State-space smoothing has found many applications in science and engineering . Under linear and Gaussian assumptions , smoothed estimates can be obtained using efficient recursions , for example Rauch-Tung-Striebel and Mayne-Fraser algorithms . Such schemes are equivalent to linear algebraic techniques that minimize a convex quadratic objective function with structure induced by the dynamic model . These classical formulations fall short in many important circumstances . For instance , smoothers obtained using quadratic penalties can fail when outliers are present in the data , and cannot track impulsive inputs and abrupt state changes . Motivated by these shortcomings , generalized Kalman smoothing formulations have been proposed in the last few years , replacing quadratic models with more suitable , often nonsmooth , convex functions . In contrast to classical models , these general estimators require use of iterated algorithms , and these have received increased attention from control , signal processing , machine learning , and optimization communities . In this survey we show that the optimization viewpoint provides the control and signal processing community great freedom in the development of novel modeling and inference frameworks for dynamical systems . We discuss general statistical models for dynamic systems , making full use of nonsmooth convex penalties and constraints , and providing links to important models in signal processing and machine learning . We also survey optimization techniques for these formulations , paying close attention to dynamic problem structure . Modeling concepts and algorithms are illustrated with numerical examples .
We consider the problem of identifying the most profitable product design from a finite set of candidates under unknown consumer preference . A standard approach to this problem follows a two-step strategy : First , estimate the preference of the consumer population , represented as a point in part-worth space , using an adaptive discrete-choice questionnaire . Second , integrate the estimated part-worth vector with engineering feasibility and cost models to determine the optimal design . In this work , we ( 0 ) demonstrate that accurate preference estimation is neither necessary nor sufficient for identifying the optimal design , ( 0 ) introduce a novel adaptive questionnaire that leverages knowledge about engineering feasibility and manufacturing costs to directly determine the optimal design , and ( 0 ) interpret product design in terms of a nonlinear segmentation of part-worth space , and use this interpretation to illuminate the intrinsic difficulty of optimal design in the presence of noisy questionnaire responses . We establish the superiority of the proposed approach using a well-documented optimal product design task . This study demonstrates how the identification of optimal product design can be accelerated by integrating marketing and manufacturing knowledge into the adaptive questionnaire .
We present a new recommendation setting for picking out two items from a given set to be highlighted to a user , based on contextual input . These two items are presented to a user who chooses one of them , possibly stochastically , with a bias that favours the item with the higher value . We propose a second-order algorithm framework that members of it use uses relative upper-confidence bounds to trade off exploration and exploitation , and some explore via sampling . We analyze one algorithm in this framework in an adversarial setting with only mild assumption on the data , and prove a regret bound of $O ( Q_T + \sqrt{TQ_T\log T} + \sqrt{T}\log T ) $ , where $T$ is the number of rounds and $Q_T$ is the cumulative approximation error of item values using a linear model . Experiments with product reviews from 00 domains show the advantage of our methods over algorithms designed for related settings , and that UCB based algorithms are inferior to greed or sampling based algorithms .
Despite being the standard loss function to train multi-class neural networks , the log-softmax has two potential limitations . First , it involves computations that scale linearly with the number of output classes , which can restrict the size of problems we are able to tackle with current hardware . Second , it remains unclear how close it matches the task loss such as the top-k error rate or other non-differentiable evaluation metrics which we aim to optimize ultimately . In this paper , we introduce an alternative classification loss function , the Z-loss , which is designed to address these two issues . Unlike the log-softmax , it has the desirable property of belonging to the spherical loss family ( Vincent et al . , 0000 ) , a class of loss functions for which training can be performed very efficiently with a complexity independent of the number of output classes . We show experimentally that it significantly outperforms the other spherical loss functions previously investigated . Furthermore , we show on a word language modeling task that it also outperforms the log-softmax with respect to certain ranking scores , such as top-k scores , suggesting that the Z-loss has the flexibility to better match the task loss . These qualities thus makes the Z-loss an appealing candidate to train very efficiently large output networks such as word-language models or other extreme classification problems . On the One Billion Word ( Chelba et al . , 0000 ) dataset , we are able to train a model with the Z-loss 00 times faster than the log-softmax and more than 0 times faster than the hierarchical softmax .
Classification is the task of predicting the class labels of objects based on the observation of their features . In contrast , quantification has been defined as the task of determining the prevalences of the different sorts of class labels in a target dataset . The simplest approach to quantification is Classify & Count where a classifier is optimised for classification on a training set and applied to the target dataset for the prediction of class labels . In the case of binary quantification , the number of predicted positive labels is then used as an estimate of the prevalence of the positive class in the target dataset . Since the performance of Classify & Count for quantification is known to be inferior its results typically are subject to adjustments . However , some researchers recently have suggested that Classify & Count might actually work without adjustments if it is based on a classifer that was specifically trained for quantification . We discuss the theoretical foundation for this claim and explore its potential and limitations with a numerical example based on the binormal model with equal variances . In order to identify an optimal quantifier in the binormal setting , we introduce the concept of local Bayes optimality . As a side remark , we present a complete proof of a theorem by Ye et al . ( 0000 ) .
A number of problems in statistical physics and computer science can be expressed as the computation of marginal probabilities over a Markov random field . Belief propagation , an iterative message-passing algorithm , computes exactly such marginals when the underlying graph is a tree . But it has gained its popularity as an efficient way to approximate them in the more general case , even if it can exhibits multiple fixed points and is not guaranteed to converge . In this paper , we express a new sufficient condition for local stability of a belief propagation fixed point in terms of the graph structure and the beliefs values at the fixed point . This gives credence to the usual understanding that Belief Propagation performs better on sparse graphs .
The Artificial Prediction Market is a recent machine learning technique for multi-class classification , inspired from the financial markets . It involves a number of trained market participants that bet on the possible outcomes and are rewarded if they predict correctly . This paper generalizes the scope of the Artificial Prediction Markets to regression , where there are uncountably many possible outcomes and the error is usually the MSE . For that , we introduce the reward kernel that rewards each participant based on its prediction error and we derive the price equations . Using two reward kernels we obtain two different learning rules , one of which is approximated using Hermite-Gauss quadrature . The market setting makes it easy to aggregate specialized regressors that only predict when an observation falls into their specialization domain . Experiments show that regression markets based on the two learning rules outperform Random Forest Regression on many UCI datasets and are rarely outperformed .
We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on rank-SVM . Data points are first ranked based on scores derived from nearest neighbor graphs on n-point nominal data . We then train a rank-SVM using this ranked data . A test-point is declared as an anomaly at alpha-false alarm level if the predicted score is in the alpha-percentile . The resulting anomaly detector is shown to be asymptotically optimal and adaptive in that for any false alarm rate alpha , its decision region converges to the alpha-percentile level set of the unknown underlying density . In addition we illustrate through a number of synthetic and real-data experiments both the statistical performance and computational efficiency of our anomaly detector .
In this paper , we consider the Graphical Lasso ( GL ) , a popular optimization problem for learning the sparse representations of high-dimensional datasets , which is well-known to be computationally expensive for large-scale problems . Recently , we have shown that the sparsity pattern of the optimal solution of GL is equivalent to the one obtained from simply thresholding the sample covariance matrix , for sparse graphs under different conditions . We have also derived a closed-form solution that is optimal when the thresholded sample covariance matrix has an acyclic structure . As a major generalization of the previous result , in this paper we derive a closed-form solution for the GL for graphs with chordal structures . We show that the GL and thresholding equivalence conditions can significantly be simplified and are expected to hold for high-dimensional problems if the thresholded sample covariance matrix has a chordal structure . We then show that the GL and thresholding equivalence is enough to reduce the GL to a maximum determinant matrix completion problem and drive a recursive closed-form solution for the GL when the thresholded sample covariance matrix has a chordal structure . For large-scale problems with up to 000 million variables , the proposed method can solve the GL problem in less than 0 minutes , while the state-of-the-art methods converge in more than 0 hours .
Persistence diagrams are two-dimensional plots that summarize the topological features of functions and are an important part of topological data analysis . A problem that has received much attention is how deal with sets of persistence diagrams . How do we summarize them , average them or cluster them ? One approach -- the persistence intensity function -- was introduced informally by Edelsbrunner , Ivanov , and Karasev ( 0000 ) . Here we provide a modification and formalization of this approach . Using the persistence intensity function , we can visualize multiple diagrams , perform clustering and conduct two-sample tests .
We introduce an application of the group lasso to design of exper- iments . We show that the problem of constructing an optimal design matrix can be transformed into a problem of the group lasso . We also give a numerical example that we can obtain several orthogonal arrays as the solutions of the group lasso problems .
