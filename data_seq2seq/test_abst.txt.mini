Let $X$ and $Y$ be two independent identically distributed random variables with density $p ( x ) $ and $Z=\alpha X+\beta Y$ for some constants $\alpha>0$ and $\beta>0$ . We consider the problem of estimating $p ( x ) $ by means of the samples from the distribution of $Z$ . Non-parametric estimator based on the sync kernel is constructed and asymptotic behaviour of the corresponding mean integrated square error is investigated .
This work proposes a novel solution to the problem of internal covariate shift and dying neurons using the concept of linked neurons . We define the neuron linkage in terms of two constraints : first , all neuron activations in the linkage must have the same operating point . That is to say , all of them share input weights . Secondly , a set of neurons is linked if and only if there is at least one member of the linkage that has a non-zero gradient in regard to the input of the activation function . This means that for any input in the activation function , there is at least one member of the linkage that operates in a non-flat and non-zero area . This simple change has profound implications in the network learning dynamics . In this article we explore the consequences of this proposal and show that by using this kind of units , internal covariate shift is implicitly solved . As a result of this , the use of linked neurons allows to train arbitrarily large networks without any architectural or algorithmic trick , effectively removing the need of using re-normalization schemes such as Batch Normalization , which leads to halving the required training time . It also solves the problem of the need for standarized input data . Results show that the units using the linkage not only do effectively solve the aforementioned problems , but are also a competitive alternative with respect to state-of-the-art with very promising results .
We solve the inverse problem of deblurring a pixelized image of Jupiter using regularized deconvolution and by sample-based Bayesian inference . By efficiently sampling the marginal posterior distribution for hyperparameters , then the full conditional for the deblurred image , we find that we can evaluate the posterior mean faster than regularized inversion , when selection of the regularizing parameter is considered . To our knowledge , this is the first demonstration of sampling and inference that takes less compute time than regularized inversion in an inverse problems . Comparison to random-walk Metropolis-Hastings and block Gibbs MCMC shows that marginal then conditional sampling also outperforms these more common sampling algorithms , having better scaling with problem size . When problem-specific computations are feasible the asymptotic cost of an independent sample is one linear solve , implying that sample-based Bayesian inference may be performed directly over function spaces , when that limit exists .
Local divisors allow a powerful induction scheme on the size of a monoid . We survey this technique by giving several examples of this proof method . These applications include linear temporal logic , rational expressions with Kleene stars restricted to prefix codes with bounded synchronization delay , Church-Rosser congruential languages , and Simon ' s Factorization Forest Theorem . We also introduce the notion of localizable language class as a new abstract concept which unifies some of the proofs for the results above .
We present a general-purpose method to train Markov chain Monte Carlo kernels , parameterized by deep neural networks , that converge and mix quickly to their target distribution . Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance , a proxy for mixing speed . We demonstrate large empirical gains on a collection of simple but challenging distributions , for instance achieving a 00x improvement in effective sample size in one case , and mixing when standard HMC makes no measurable progress in a second . Finally , we show quantitative and qualitative gains on a real-world task : latent-variable generative modeling . We release an open source TensorFlow implementation of the algorithm .
This paper provides an overview ( in French ) of the European PEER project , focusing on its origins , the actual objectives and the technical deployment .
We demonstrate a new deep learning autoencoder network , trained by a nonnegativity constraint algorithm ( NCAE ) , that learns features which show part-based representation of data . The learning algorithm is based on constraining negative weights . The performance of the algorithm is assessed based on decomposing data into parts and its prediction performance is tested on three standard image data sets and one text dataset . The results indicate that the nonnegativity constraint forces the autoencoder to learn features that amount to a part-based representation of data , while improving sparsity and reconstruction quality in comparison with the traditional sparse autoencoder and Nonnegative Matrix Factorization . It is also shown that this newly acquired representation improves the prediction performance of a deep neural network .
The causal discovery of Bayesian networks is an active and important research area , and it is based upon searching the space of causal models for those which can best explain a pattern of probabilistic dependencies shown in the data . However , some of those dependencies are generated by causal structures involving variables which have not been measured , i . e . , latent variables . Some such patterns of dependency " reveal " themselves , in that no model based solely upon the observed variables can explain them as well as a model using a latent variable . That is what latent variable discovery is based upon . Here we did a search for finding them systematically , so that they may be applied in latent variable discovery in a more rigorous fashion .
Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes . Taking advantage of the recent success of unsupervised learning in deep neural networks , we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains . The proposed method combines representation learning models ( i . e . , auto-encoders ) together with cross-domain learning criteria ( i . e . , Maximum Mean Discrepancy loss ) to learn joint embeddings for semantic and visual features . A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data . We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 000-0000 dataset with a wide range of applications , including zero and few-shot image recognition and retrieval , from inductive to transductive settings . Empirically , we show that our framework improves over the current state of the art on many of the considered tasks .
In this work we describe and evaluate methods to learn musical embeddings . Each embedding is a vector that represents four contiguous beats of music and is derived from a symbolic representation . We consider autoencoding-based methods including denoising autoencoders , and context reconstruction , and evaluate the resulting embeddings on a forward prediction and a classification task .
A database-assisted TV white space network can achieve the goal of green cognitive communication by effectively reducing the energy consumption in cognitive communications . The success of such a novel network relies on a proper business model that provides incentives for all parties involved . In this paper , we propose an integrated spectrum and information market for a database-assisted TV white space network , where the geo-location database serves as both the spectrum market platform and the information market platform . We study the interactions among the database , the spectrum licensee , and unlicensed users by modelling the system as a three-stage sequential decision process . In Stage I , the database and the licensee negotiate regarding the commission for the licensee to use the spectrum market platform . In Stage II , the database and the licensee compete for selling information or channels to unlicensed users . In Stage III , unlicensed users determine whether they should buy exclusive usage right of licensed channels from the licensee or information regarding unlicensed channels from the database . Analyzing such a three-stage model is challenging due to the co-existence of both positive and negative network externalities in the information market . Despite of this , we are able to characterize how the network externalities affect the equilibrium behaviors of all parties involved . We analytically show that in this integrated market , the licensee can never get a market share more than half . Our numerical results further show that the proposed integrated market can improve the network profit up to 00% , compared with a pure information market .
SafePredict is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate , $0-\epsilon$ , by allowing refusals . Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm on occasion so that the error rate on non-refused predictions does not exceed $\epsilon$ . The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor . When the base predictor happens not to exceed the target error rate $\epsilon$ , SafePredict refuses only a finite number of times . When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee . Empirical results show that ( i ) SafePredict compares favorably with state-of-the art confidence based refusal mechanisms which fail to offer robust error guarantees ; and ( ii ) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals . Our software ( currently in Python ) is included in the supplementary material .
Tensor decomposition plays a key role in identifying common features across a collection of matrices in many areas of science . A fundamental need in big data research is to process data tabulated as large-scale matrices using eigenvectors . A higher order generalized singular value decomposition technique successfully captures the common features of the same organ from multiple animals in genomic signal processing . A recent semidefinite programming approach to solve an AC optimal power flow was accompanied by the problem formulation in the Cartesian coordinate system . The collection of nodal Kirchhoff laws introduces a 0D tensor with a common feature of individual matrices to maintain local power balance . In this paper , the mathematical process is established and the common feature is identified . The common feature is a key element to a fully decentralized and therefore scalable algorithm to solve AC optimal power flow .
We give bit-size estimates for the coefficients appearing in triangular sets describing positive-dimensional algebraic sets defined over Q . These estimates are worst case upper bounds ; they depend only on the degree and height of the underlying algebraic sets . We illustrate the use of these results in the context of a modular algorithm . This extends results by the first and last author , which were confined to the case of dimension 0 . Our strategy is to get back to dimension 0 by evaluation and inter- polation techniques . Even though the main tool ( height theory ) remains the same , new difficulties arise to control the growth of the coefficients during the interpolation process .
Observational studies are based on accurate assessment of human state . A behavior recognition system that models interlocutors ' state in real-time can significantly aid the mental health domain . However , behavior recognition from speech remains a challenging task since it is difficult to find generalizable and representative features because of noisy and high-dimensional data , especially when data is limited and annotated coarsely and subjectively . Deep Neural Networks ( DNN ) have shown promise in a wide range of machine learning tasks , but for Behavioral Signal Processing ( BSP ) tasks their application has been constrained due to limited quantity of data . We propose a Sparsely-Connected and Disjointly-Trained DNN ( SD-DNN ) framework to deal with limited data . First , we break the acoustic feature set into subsets and train multiple distinct classifiers . Then , the hidden layers of these classifiers become parts of a deeper network that integrates all feature streams . The overall system allows for full connectivity while limiting the number of parameters trained at any time and allows convergence possible with even limited data . We present results on multiple behavior codes in the couples ' therapy domain and demonstrate the benefits in behavior classification accuracy . We also show the viability of this system towards live behavior annotations .
The speed of many one-line transformation methods for the production of , for example , Levy alpha-stable random numbers , which generalize Gaussian ones , and Mittag-Leffler random numbers , which generalize exponential ones , is very high and satisfactory for most purposes . However , for the class of decreasing probability densities fast rejection implementations like the Ziggurat by Marsaglia and Tsang promise a significant speed-up if it is possible to complement them with a method that samples the tails of the infinite support . This requires the fast generation of random numbers greater or smaller than a certain value . We present a method to achieve this , and also to generate random numbers within any arbitrary interval . We demonstrate the method showing the properties of the transform maps of the above mentioned distributions as examples of stable and geometric stable random numbers used for the stochastic solution of the space-time fractional diffusion equation .
Researchers often rely on the t-statistic to make inference on parameters in statistical models . It is common practice to obtain critical values by simulation techniques . This paper proposes a novel numerical method to obtain an approximately similar test . This test rejects the null hypothesis when the test statistic is larger than a critical value function ( CVF ) of the data . We illustrate this procedure when regressors are highly persistent , a case in which commonly-used simulation methods encounter difficulties controlling size uniformly . Our approach works satisfactorily , controls size , and yields a test which outperforms the two other known similar tests .
Current connectionist simulations require huge computational resources . We describe a neural network simulator for the IBM GF00 , an experimental SIMD machine with 000 processors and a peak arithmetic performance of 00 Gigaflops . We present our parallel implementation of the backpropagation learning algorithm , techniques for increasing efficiency , performance measurements on the NetTalk text-to-speech benchmark , and a performance model for the simulator . Our simulator currently runs the back-propagation learning algorithm at 000 million connections per second , where each " connection per second " includes both a forward and backward pass . This figure was obtained on the machine when only 000 processors were working ; with all 000 processors operational , our simulation will run at over one billion connections per second . We conclude that the GF00 is well-suited to neural network simulation , and we analyze our use of the machine to determine which features are the most important for high performance .
Many important stable matching problems are known to be NP-hard , even when strong restrictions are placed on the input . In this paper we seek to identify simple structural properties of instances of stable matching problems which will allow the design of efficient algorithms . We focus on the setting in which all agents involved in some matching problem can be partitioned into k different types , where the type of an agent determines his or her preferences , and agents have preferences over types ( which may be refined by more detailed preferences within a single type ) . This situation could arise in practice if agents form preferences based on some small collection of agents ' attributes . The notion of types could also be used if we are interested in a relaxation of stability , in which agents will only form a private arrangement if it allows them to be matched with a partner who differs from the current partner in some particularly important characteristic . We show that in this setting several well-studied NP-hard stable matching problems ( such as MAX SMTI , MAX SRTI , and MAX SIZE MIN BP SMTI ) belong to the parameterised complexity class FPT when parameterised by the number of different types of agents , and so admit efficient algorithms when this number of types is small .
Convolutional deep neural networks ( DNN ) are state of the art in many engineering problems but have not yet addressed the issue of how to deal with complex spectrograms . Here , we use circular statistics to provide a convenient probabilistic estimate of spectrogram phase in a complex convolutional DNN . In a typical cocktail party source separation scenario , we trained a convolutional DNN to re-synthesize the complex spectrograms of two source speech signals given a complex spectrogram of the monaural mixture - a discriminative deep transform ( DT ) . We then used this complex convolutional DT to obtain probabilistic estimates of the magnitude and phase components of the source spectrograms . Our separation results are on a par with equivalent binary-mask based non-complex separation approaches .
In most professional sports , the structure of the environment is kept neutral so that scoring imbalances may be attributed to differences in team skill . It thus remains unknown what impact structural heterogeneities can have on scoring dynamics and producing competitive advantages . Applying a generative model of scoring dynamics to roughly 00 million team competitions drawn from an online game , we quantify the relationship between a competition ' s structure and its scoring dynamics . Despite wide structural variations , we find the same three-phase pattern in the tempo of events observed in many sports . Tempo and balance are highly predictable from a competition ' s structural features alone and teams exploit environmental heterogeneities for sustained competitive advantage . The most balanced competitions are associated with specific environmental heterogeneities , not from equally skilled teams . These results shed new light on the principles of balanced competition , and illustrate the potential of online game data for investigating social dynamics and competition .
We study a pumping lemma for the word/tree languages generated by higher-order grammars . Pumping lemmas are known up to order-0 word languages ( i . e . , for regular/context-free/indexed languages ) , and have been used to show that a given language does not belong to the classes of regular/context-free/indexed languages . We prove a pumping lemma for word/tree languages of arbitrary orders , modulo a conjecture that a higher-order version of Kruskal ' s tree theorem holds . We also show that the conjecture indeed holds for the order-0 case , which yields a pumping lemma for order-0 tree languages and order-0 word languages .
In the report the approach to estimation of quality of planned experiments is considered . This approach is based on the analysis of uncertainty , which will take place under the future hypotheses testing about the existence of a new phenomenon in Nature . The probability of making a correct decision in hypotheses testing is proposed as estimator of quality of planned experiments . This estimator allows to take into account systematics and statistical uncertainties in determination of signal and background rates .
If X and Y are real valued random variables such that the first moments of X , Y , and XY exist and the conditional expectation of Y given X is an affine function of X , then the intercept and slope of the conditional expectation equal the intercept and slope of the least squares linear regression function , even though Y may not have a finite second moment . As a consequence , the affine in X form of the conditional expectation and zero covariance imply mean independence .
A number of studies have suggested using comparisons between DNA sequences of closely related bacterial isolates to estimate the relative rate of recombination to mutation for that bacterial species . We consider such an approach which uses single-locus variants : pairs of isolates whose DNA differ at a single gene locus . One way of deriving point estimates for the relative rate of recombination to mutation from such data is to use composite likelihood methods . We extend recent work in this area so as to be able to construct confidence intervals for our estimates , without needing to resort to computationally-intensive bootstrap procedures , and to develop a test for whether the relative rate varies across loci . Both our test and method for constructing confidence intervals are obtained by modeling the dependence structure in the data , and then applying asymptotic theory regarding the distribution of estimators obtained using a composite likelihood . We applied these methods to multi-locus sequence typing ( MLST ) data from eight bacteria , finding strong evidence for considerable rate variation in three of these : Bacillus cereus , Enterococcus faecium and Klebsiella pneumoniae .
Many entities managed by HEP Software Frameworks represent spatial ( 0-dimensional ) real objects . Effective definition , manipulation and visualization of such objects is an indispensable functionality . GraXML is a modular Geometric Modeling toolkit capable of processing geometric data of various kinds ( detector geometry , event geometry ) from different sources and delivering them in ways suitable for further use . Geometric data are first modeled in one of the Generic Models . Those Models are then used to populate powerful Geometric Model based on the Java0D technology . While Java0D has been originally created just to provide visualization of 0D objects , its light weight and high functionality allow an effective reuse as a general geometric component . This is possible also thanks to a large overlap between graphical and general geometric functionality and modular design of Java0D itself . Its graphical functionalities also allow a natural visualization of all manipulated elements . All these techniques have been developed primarily ( or only ) for the Java environment . It is , however , possible to interface them transparently to Frameworks built in other languages , like for example C++ . The GraXML toolkit has been tested with data from several sources , as for example ATLAS and ALICE detector description and ATLAS event data . Prototypes for other sources , like Geometry Description Markup Language ( GDML ) exist too and interface to any other source is easy to add .
Automatically describing video content with natural language is a fundamental challenge of multimedia . Recurrent Neural Networks ( RNN ) , which models sequence dynamics , has attracted increasing attention on visual interpretation . However , most existing approaches generate a word locally with given previous words and the visual content , while the relationship between sentence semantics and visual content is not holistically exploited . As a result , the generated sentences may be contextually correct but the semantics ( e . g . , subjects , verbs or objects ) are not true . This paper presents a novel unified framework , named Long Short-Term Memory with visual-semantic Embedding ( LSTM-E ) , which can simultaneously explore the learning of LSTM and visual-semantic embedding . The former aims to locally maximize the probability of generating the next word given previous words and visual content , while the latter is to create a visual-semantic embedding space for enforcing the relationship between the semantics of the entire sentence and visual content . Our proposed LSTM-E consists of three components : a 0-D and/or 0-D deep convolutional neural networks for learning powerful video representation , a deep RNN for generating sentences , and a joint embedding model for exploring the relationships between visual content and sentence semantics . The experiments on YouTube0Text dataset show that our proposed LSTM-E achieves to-date the best reported performance in generating natural sentences : 00 . 0% and 00 . 0% in terms of BLEU@0 and METEOR , respectively . We also demonstrate that LSTM-E is superior in predicting Subject-Verb-Object ( SVO ) triplets to several state-of-the-art techniques .
We study the multi-armed bandit problem with multiple plays and a budget constraint for both the stochastic and the adversarial setting . At each round , exactly $K$ out of $N$ possible arms have to be played ( with $0\leq K \leq N$ ) . In addition to observing the individual rewards for each arm played , the player also learns a vector of costs which has to be covered with an a-priori defined budget $B$ . The game ends when the sum of current costs associated with the played arms exceeds the remaining budget . Firstly , we analyze this setting for the stochastic case , for which we assume each arm to have an underlying cost and reward distribution with support $[c_{\min} , 0]$ and $[0 , 0]$ , respectively . We derive an Upper Confidence Bound ( UCB ) algorithm which achieves $O ( NK^0 \log B ) $ regret . Secondly , for the adversarial case in which the entire sequence of rewards and costs is fixed in advance , we derive an upper bound on the regret of order $O ( \sqrt{NB\log ( N/K ) } ) $ utilizing an extension of the well-known $\texttt{Exp0}$ algorithm . We also provide upper bounds that hold with high probability and a lower bound of order $\Omega ( ( 0 - K/N ) ^0 \sqrt{NB/K} ) $ .
Traditional approaches to replication require client requests to be ordered before making them durable by copying them to replicas . As a result , clients must wait for two round-trip times ( RTTs ) before updates complete . In this paper , we show that this entanglement of ordering and durability is unnecessary for strong consistency . Consistent Unordered Replication Protocol ( CURP ) allows clients to replicate requests that have not yet been ordered , as long as they are commutative . This strategy allows most operations to complete in 0 RTT ( the same as an unreplicated system ) . We implemented CURP in the Redis and RAMCloud storage systems . In RAMCloud , CURP improved write latency by ~0x ( 00 . 0 us -> 0 . 0 us ) and write throughput by 0x . Compared to unreplicated RAMCloud , CURP ' s latency overhead for 0-way replication is just 0 . 0 us ( 0 . 0 us vs 0 . 0 us ) . CURP transformed a non-durable Redis cache into a consistent and durable storage system with only a small performance overhead .
Peer-prediction is a mechanism which elicits privately-held , non-variable information from self-interested agents---formally , truth-telling is a strict Bayes Nash equilibrium of the mechanism . The original Peer-prediction mechanism suffers from two main limitations : ( 0 ) the mechanism must know the " common prior " of agents ' signals ; ( 0 ) additional undesirable and non-truthful equilibria exist which often have a greater expected payoff than the truth-telling equilibrium . A series of results has successfully weakened the known common prior assumption . However , the equilibrium multiplicity issue remains a challenge . In this paper , we address the above two problems . In the setting where a common prior exists but is not known to the mechanism we show ( 0 ) a general negative result applying to a large class of mechanisms showing truth-telling can never pay strictly more in expectation than a particular set of equilibria where agents collude to " relabel " the signals and tell the truth after relabeling signals ; ( 0 ) provide a mechanism that has no information about the common prior but where truth-telling pays as much in expectation as any relabeling equilibrium and pays strictly more than any other symmetric equilibrium ; ( 0 ) moreover in our mechanism , if the number of agents is sufficiently large , truth-telling pays similarly to any equilibrium close to a " relabeling " equilibrium and pays strictly more than any equilibrium that is not close to a relabeling equilibrium .
We propose a general formalism of iterated random functions with semigroup property , under which exact and approximate Bayesian posterior updates can be viewed as specific instances . A convergence theory for iterated random functions is presented . As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model . The sequential inference algorithm and its supporting theory are illustrated by simulated examples .
This paper presents the verification of control systems implemented in Simulink . The goal is to ensure that high-level requirements on control performance , like stability , are satisfied by the Simulink diagram . A two stage process is proposed . First , the high-level requirements are decomposed into specific parametrized sub-requirements and implemented as assertions in Simulink . Second , the verification takes place . On one hand , the sub-requirements are verified through assertion checks in simulation . On the other hand , according to their scope , some of the sub-requirements are verified through assertion checks in simulation , and others via automatic theorem proving over an ideal mathematical model of the diagram . We compare performing only assertion checks against the use of theorem proving , to highlight the advantages of the latter . Theorem proving performs verification by computing a mathematical proof symbolically , covering the entire state space of the variables . An automatic translation tool from Simulink to the language of the theorem proving tool Why0 is also presented . The paper demonstrates our approach by verifying the stability of a simple discrete linear system .
Induction of common sense knowledge about prototypical sequences of events has recently received much attention . Instead of inducing this knowledge in the form of graphs , as in much of the previous work , in our method , distributed representations of event realizations are computed based on distributed representations of predicates and their arguments , and then these representations are used to predict prototypical event orderings . The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from texts . We show that this approach results in a substantial boost in ordering performance with respect to previous methods .
The volume under the receiver operating characteristic surface ( VUS ) is useful for measuring the overall accuracy of a diagnostic test when the possible disease status belongs to one of three ordered categories . In medical studies , the VUS of a new test is typically estimated through a sample of measurements obtained by some suitable sample of patients . However , in many cases , only a subset of such patients has the true disease status assessed by a gold standard test . In this paper , for a continuous-scale diagnostic test , we propose four estimators of the VUS which accommodate for nonignorable missingness of the disease status . The estimators are based on a parametric model which jointly describes both the disease and the verification process . Identifiability of the model is discussed . Consistency and asymptotic normality of the proposed estimators are shown , and variance estimation is discussed . The finite-sample behavior is investigated by means of simulation experiments .
Peer review , evaluation , and selection is a fundamental aspect of modern science . Funding bodies the world over employ experts to review and select the best proposals of those submitted for funding . The problem of peer selection , however , is much more general : a professional society may want to give a subset of its members awards based on the opinions of all members ; an instructor for a MOOC or online course may want to crowdsource grading ; or a marketing company may select ideas from group brainstorming sessions based on peer evaluation . We make three fundamental contributions to the study of procedures or mechanisms for peer selection , a specific type of group decision-making problem , studied in computer science , economics , and political science . First , we propose a novel mechanism that is strategyproof , i . e . , agents cannot benefit by reporting insincere valuations . Second , we demonstrate the effectiveness of our mechanism by a comprehensive simulation-based comparison with a suite of mechanisms found in the literature . Finally , our mechanism employs a randomized rounding technique that is of independent interest , as it solves the apportionment problem that arises in various settings where discrete resources such as parliamentary representation slots need to be divided proportionally .
A critical step towards certifying safety-critical systems is to check their conformance to hard real-time requirements . A promising way to achieve this is by building the systems from pre-verified components and verifying their correctness in a compositional manner . We previously reported a formal approach to verifying function blocks ( FBs ) using tabular expressions and the PVS proof assistant . By applying our approach to the IEC 00000-0 standard of Programmable Logic Controllers ( PLCs ) , we constructed a repository of precise specification and reusable ( proven ) theorems of feasibility and correctness for FBs . However , we previously did not apply our approach to verify FBs against timing requirements , since IEC 00000-0 does not define composite FBs built from timers . In this paper , based on our experience in the nuclear domain , we conduct two realistic case studies , consisting of the software requirements and the proposed FB implementations for two subsystems of an industrial control system . The implementations are built from IEC 00000-0 FBs , including the on-delay timer . We find issues during the verification process and suggest solutions .
In this paper we analyse some of the classical paradoxes in Social Choice Theory ( namely , the Condorcet paradox , the discursive dilemma , the Ostrogorski paradox and the multiple election paradox ) using a general framework for the study of aggregation problems called binary aggregation with integrity constraints . We provide a definition of paradox that is general enough to account for the four cases mentioned , and identify a common structure in the syntactic properties of the rationality assumptions that lie behind such paradoxes . We generalise this observation by providing a full characterisation of the set of rationality assumptions on which the majority rule does not generate a paradox .
This paper discusses asymptotic distributions of various estimators of the underlying parameters in some regression models with long memory ( LM ) Gaussian design and nonparametric heteroscedastic LM moving average errors . In the simple linear regression model , the first-order asymptotic distribution of the least square estimator of the slope parameter is observed to be degenerate . However , in the second order , this estimator is $n^{0/0}$-consistent and asymptotically normal for $h+H<0/0$ ; nonnormal otherwise , where $h$ and $H$ are LM parameters of design and error processes , respectively . The finite-dimensional asymptotic distributions of a class of kernel type estimators of the conditional variance function $\sigma^0 ( x ) $ in a more general heteroscedastic regression model are found to be normal whenever $H< ( 0+h ) /0$ , and non-normal otherwise . In addition , in this general model , $\log ( n ) $-consistency of the local Whittle estimator of $H$ based on pseudo residuals and consistency of a cross validation type estimator of $\sigma^0 ( x ) $ are established . All of these findings are then used to propose a lack-of-fit test of a parametric regression model , with an application to some currency exchange rate data which exhibit LM .
Increasingly many real world tasks involve data in multiple modalities or views . This has motivated the development of many effective algorithms for learning a common latent space to relate multiple domains . However , most existing cross-view learning algorithms assume access to paired data for training . Their applicability is thus limited as the paired data assumption is often violated in practice : many tasks have only a small subset of data available with pairing annotation , or even no paired data at all . In this paper we introduce Deep Matching Autoencoders ( DMAE ) , which learn a common latent space and pairing from unpaired multi-modal data . Specifically we formulate this as a cross-domain representation learning and object matching problem . We simultaneously optimise parameters of representation learning auto-encoders and the pairing of unpaired multi-modal data . This framework elegantly spans the full regime from fully supervised , semi-supervised , and unsupervised ( no paired data ) multi-modal learning . We show promising results in image captioning , and on a new task that is uniquely enabled by our methodology : unsupervised classifier learning .
We study tools for inference conditioned on model selection events that are defined by the generalized lasso regularization path . The generalized lasso estimate is given by the solution of a penalized least squares regression problem , where the penalty is the l0 norm of a matrix D times the coefficient vector . The generalized lasso path collects these estimates for a range of penalty parameter ( {\lambda} ) values . Leveraging a sequential characterization of this path from Tibshirani & Taylor ( 0000 ) , and recent advances in post-selection inference from Lee et al . ( 0000 ) , Tibshirani et al . ( 0000 ) , we develop exact hypothesis tests and confidence intervals for linear contrasts of the underlying mean vector , conditioned on any model selection event along the generalized lasso path ( assuming Gaussian errors in the observations ) . By inspecting specific choices of D , we obtain post-selection tests and confidence intervals for specific cases of generalized lasso estimates , such as the fused lasso , trend filtering , and the graph fused lasso . In the fused lasso case , the underlying coordinates of the mean are assigned a linear ordering , and our framework allows us to test selectively chosen breakpoints or changepoints in these mean coordinates . This is an interesting and well-studied problem with broad applications , our framework applied to the trend filtering and graph fused lasso serves several applications as well . Aside from the development of selective inference tools , we describe several practical aspects of our methods such as valid post-processing of generalized estimates before performing inference in order to improve power , and problem-specific visualization aids that may be given to the data analyst for he/she to choose linear contrasts to be tested . Many examples , both from simulated and real data sources , are presented to examine the empirical properties of our inference methods .
We study the envy free pricing problem faced by a seller who wishes to maximize revenue by setting prices for bundles of items . If there is an unlimited supply of items and agents are single minded then we show that finding the revenue maximizing envy free allocation/pricing can be solved in polynomial time by reducing it to an instance of weighted independent set on a perfect graph . We define an allocation/pricing as \textit{multi envy free} if no agent wishes to replace her allocation with the union of the allocations of some set of other agents and her price with the sum of their prices . We show that it is \textit{coNP}-hard to decide if a given allocation/pricing is multi envy free . We also show that revenue maximization multi envy free allocation/pricing is \textit{APX} hard . Furthermore , we give efficient algorithms and hardness results for various variants of the highway problem .
Pattern matching algorithms to find exact occurrences of a pattern $S\in\Sigma^m$ in a text $T\in\Sigma^n$ have been analyzed extensively with respect to asymptotic best , worst , and average case runtime . For more detailed analyses , the number of text character accesses $X^{\mathcal{A} , S}_n$ performed by an algorithm $\mathcal{A}$ when searching a random text of length $n$ for a fixed pattern $S$ has been considered . Constructing a state space and corresponding transition rules ( e . g . in a Markov chain ) that reflect the behavior of a pattern matching algorithm is a key step in existing analyses of $X^{\mathcal{A} , S}_n$ in both the asymptotic ( $n\to\infty$ ) and the non-asymptotic regime . The size of this state space is hence a crucial parameter for such analyses . In this paper , we introduce a general methodology to construct corresponding state spaces and demonstrate that it applies to a wide range of algorithms , including Boyer-Moore ( BM ) , Boyer-Moore-Horspool ( BMH ) , Backward Oracle Matching ( BOM ) , and Backward ( Non-Deterministic ) DAWG Matching ( B ( N ) DM ) . In all cases except BOM , our method leads to state spaces of size $O ( m^0 ) $ for pattern length $m$ , a result that has previously only been obtained for BMH . In all other cases , only state spaces with size exponential in $m$ had been reported . Our results immediately imply an algorithm to compute the distribution of $X^{\mathcal{A} , S}_n$ for fixed $S$ , fixed $n$ , and $\mathcal{A}\in\{\text{BM} , \text{BMH} , \text{B ( N ) DM}\}$ in polynomial time for a very general class of random text models .
We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus . Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and concept-instance pairs obtained with Hearst patterns . In contrast , our method relies on a novel approach for clustering terms found in HTML tables , and then assigning concept names to these clusters using Hearst patterns . The method can be efficiently applied to a large corpus , and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs .
This paper provides a probabilistic and statistical comparison of the log-GARCH and EGARCH models , which both rely on multiplicative volatility dynamics without positivity constraints . We compare the main probabilistic properties ( strict stationarity , existence of moments , tails ) of the EGARCH model , which are already known , with those of an asymmetric version of the log-GARCH . The quasi-maximum likelihood estimation of the log-GARCH parameters is shown to be strongly consistent and asymptotically normal . Similar estimation results are only available for particular EGARCH models , and under much stronger assumptions . The comparison is pursued via simulation experiments and estimation on real data .
In many two-sided markets , each side has incomplete information about the other but has an opportunity to learn ( some ) relevant information before final matches are made . For instance , clients seeking workers to perform tasks often conduct interviews that require the workers to perform some tasks and thereby provide information to both sides . The performance of a worker in such an interview/assessment - and hence the information revealed - depends both on the inherent characteristics of the worker and the task and also on the actions taken by the worker ( e . g . the effort expended ) ; thus there is both adverse selection ( on both sides ) and moral hazard ( on one side ) . When interactions are ongoing , incentives for workers to expend effort in the current assessment can be provided by the payment rule used and also by the matching rule that assesses and determines the tasks to which the worker is assigned in the future ; thus workers have career concerns . We derive mechanisms - payment , assessment and matching rules - that lead to final matchings that are stable in the long run and achieve close to the optimal performance ( profit or social welfare maximizing ) in equilibrium ( unique ) thus mitigating both adverse selection and moral hazard ( in many settings ) .
In this paper we study a Matsumoto-Yor type property for the gamma and Kummer inde- pendent variables discovered in Koudou and Vallois ( 0000 ) . We prove that constancy of regressions of U = ( 0 + 0/ ( X + Y ) ) = ( 0 + 0/X ) given V = X + Y and of 0/U given V , where X and Y are indepen- dent and positive random variables , characterizes the gamma and Kummer distributions . This result completes characterizations by independence of U and V obtained , under smoothness assumptions for densities , in Koudou and Vallois ( 0000 , 0000 ) . Since we work with differential equations for the Laplace transforms , no density assumptions are needed .
Global sensitivity analysis of a numerical code , more specifically estimation of Sobol indices associated with input variables , generally requires a large number of model runs . When those demand too much computation time , it is necessary to use a reduced model ( metamodel ) to perform sensitivity analysis , whose outputs are numerically close to the ones of the original model , while being much faster to run . In this case , estimated indices are subject to two kinds of errors : sampling error , caused by the computation of the integrals appearing in the definition of the Sobol indices by a Monte-Carlo method , and metamodel error , caused by the replacement of the original model by the metamodel . In cases where we have certified bounds for the metamodel error , we propose a method to quantify both types of error , and we compute confidence intervals for first-order Sobol indices .
We consider online optimization in the 0-lookahead setting , where the objective does not decompose additively over the rounds of the online game . The resulting formulation enables us to deal with non-stationary and/or long-term constraints , which arise , for example , in online display advertising problems . We propose an on-line primal-dual algorithm for which we obtain dynamic cumulative regret guarantees . They depend on the convexity and the smoothness of the non-additive penalty , as well as terms capturing the smoothness with which the residuals of the non-stationary and long-term constraints vary over the rounds . We conduct experiments on synthetic data to illustrate the benefits of the non-additive penalty and show vanishing regret convergence on live traffic data collected by a display advertising platform in production .
0 . Understanding the mechanisms underlying biological systems , and ultimately , predicting their behaviours in a changing environment requires overcoming the gap between mathematical models and experimental or observational data . Differential equations ( DEs ) are commonly used to model the temporal evolution of biological systems , but statistical methods for comparing DE models to data and for parameter inference are relatively poorly developed . This is especially problematic in the context of biological systems where observations are often noisy and only a small number of time points may be available . 0 . The Bayesian approach offers a coherent framework for parameter inference that can account for multiple sources of uncertainty , while making use of prior information . It offers a rigorous methodology for parameter inference , as well as modelling the link between unobservable model states and parameters , and observable quantities . 0 . We present deBInfer , a package for the statistical computing environment R , implementing a Bayesian framework for parameter inference in DEs . deBInfer provides templates for the DE model , the observation model and data likelihood , and the model parameters and their prior distributions . A Markov chain Monte Carlo ( MCMC ) procedure processes these inputs to estimate the posterior distributions of the parameters and any derived quantities , including the model trajectories . Further functionality is provided to facilitate MCMC diagnostics , the visualisation of the posterior distributions of model parameters and trajectories , and the use of compiled DE models for improved computational performance . 0 . The templating approach makes deBInfer applicable to a wide range of DE models . We demonstrate its application to ordinary and delay DE models for population ecology .
We consider a recurrent Markov process which is an It\^o semi-martingale . The L\ ' evy kernel describes the law of its jumps . Based on observations X ( 0 ) , X ( {\Delta} ) , . . . , X ( n{\Delta} ) , we construct an estimator for the L\ ' evy kernel ' s density . We prove its consistency ( as n{\Delta}->\infty and {\Delta}->0 ) and a central limit theorem . In the positive recurrent case , our estimator is asymptotically normal ; in the null recurrent case , it is asymptotically mixed normal . Our estimator ' s rate of convergence equals the non-parametric minimax rate of smooth density estimation . The asymptotic bias and variance are analogous to those of the classical Nadaraya-Watson estimator for conditional densities . Asymptotic confidence intervals are provided .
Ising spin model is considered as an efficient computing method to solve combinatorial optimization problems based on its natural tendency of convergence towards low energy state . The underlying basic functions facilitating the Ising model can be categorized into two parts , " Annealing and Majority vote " . In this paper , we propose an Ising cell based on Spin Hall Effect ( SHE ) induced magnetization switching in a Magnetic Tunnel Junction ( MTJ ) . The stochasticity of our proposed Ising cell based on SHE induced MTJ switching , can implement the natural annealing process by preventing the system from being stuck in solutions with local minima . Further , by controlling the current through the Heavy-Metal ( HM ) underlying the MTJ , we can mimic the majority vote function which determines the next state of the individual spins . By solving coupled \textit{Landau-Lifshitz-Gilbert} ( LLG ) equations , we demonstrate that our Ising cell can be replicated to map certain combinatorial problems . We present results for two representative problems - Maximum-cut and Graph coloring - to illustrate the feasibility of the proposed device-circuit configuration in solving combinatorial problems . Our proposed solution using a Heavy Metal ( HM ) based MTJ device can be exploited to implement compact , fast , and energy efficient Ising spin model .
Bibliometric techniques are not yet widely used to enhance retrieval processes in digital libraries , although they offer value-added effects for users . In this workshop we will explore how statistical modelling of scholarship , such as Bradfordizing or network analysis of coauthorship network , can improve retrieval services for specific communities , as well as for large , cross-domain collections . This workshop aims to raise awareness of the missing link between information retrieval ( IR ) and bibliometrics/scientometrics and to create a common ground for the incorporation of bibliometric-enhanced services into retrieval at the digital library interface .
We study the problem of estimating the coefficients in linear ordinary differential equations ( ODE ' s ) with a diverging number of variables when the solutions are observed with noise . The solution trajectories are first smoothed with local polynomial regression and the coefficients are estimated with nonconcave penalty proposed by \cite{fan00} . Under some regularity and sparsity conditions , we show the procedure can correctly identifies nonzero coefficients with probability converging to one and the estimators for nonzero coefficients have the same asymptotic normal distribution as they would have when the zero coefficients are known and the same two-step procedure is used . Our asymptotic results are valid under the misspecified case where linear ODE ' s are only used as an approximation to nonlinear ODE ' s , and the estimates will converge to the coefficients of the best approximating linear system . From our results , when the solution trajectories of the ODE ' s are sufficiently smooth , the parametric $\sqrt{n}$ rate is achieved even though nonparametric regression estimator is used in the first step of the procedure . The performance of the two-step procedure is illustrated by a simulation study as well as an application to yeast cell-cycle data .
This work is based on the framework proposed by Conrad ( 0000 ) to determine the optimal timing of an investment or policy to slow global warming . While Conrad formulated the problem as a stopping rule option pricing model , we treat the policy decision by considering the total damage function that enables us to make some interesting extensions to the original formulation . We show that Conrad ' s framework is equivalent to minmization of the expected value of the damage function under the stochastic optimal stopping rule . We extend Conrad ' s model by allowing for policy cost to grow with time . In addition to closed form solution , we also perform Monte Carlo simulations to find the distribution for the total damage and show that at higher quantiles the damage may become too large and so is the risk on the global economy . We also show that the decision to take action largely depends on the cost of the action . For example , in the case of model parameters calibrated as in Conrad ( 0000 ) with a constant cost , there is a rather long wait before the action is expected to be taken , but if the cost increases with the same rate as the global economy growth , then action has to be taken immediately to minimize the damage .
Non-Gaussian component analysis ( NGCA ) is aimed at identifying a linear subspace such that the projected data follows a non-Gaussian distribution . In this paper , we propose a novel NGCA algorithm based on log-density gradient estimation . Unlike existing methods , the proposed NGCA algorithm identifies the linear subspace by using the eigenvalue decomposition without any iterative procedures , and thus is computationally reasonable . Furthermore , through theoretical analysis , we prove that the identified subspace converges to the true subspace at the optimal parametric rate . Finally , the practical performance of the proposed algorithm is demonstrated on both artificial and benchmark datasets .
A new lower bound on the average reconstruction error variance of multidimensional sampling and reconstruction is presented . It applies to sampling on arbitrary lattices in arbitrary dimensions , assuming a stochastic process with constant , isotropically bandlimited spectrum and reconstruction by the best linear interpolator . The lower bound is exact for any lattice at sufficiently high and low sampling rates . The two threshold rates where the error variance deviates from the lower bound gives two optimality criteria for sampling lattices . It is proved that at low rates , near the first threshold , the optimal lattice is the dual of the best sphere-covering lattice , which for the first time establishes a rigorous relation between optimal sampling and optimal sphere covering . A previously known result is confirmed at high rates , near the second threshold , namely , that the optimal lattice is the dual of the best sphere-packing lattice . Numerical results quantify the performance of various lattices for sampling and support the theoretical optimality criteria .
Gale and Sotomayor ( 0000 ) have shown that in the Gale-Shapley matching algorithm ( 0000 ) , the proposed-to side W ( referred to as women there ) can strategically force the W-optimal stable matching as the M-optimal one by truncating their preference lists , each woman possibly blacklisting all but one man . As Gusfield and Irving have already noted in 0000 , no results are known regarding achieving this feat by means other than such preference-list truncation , i . e . by also permuting preference lists . We answer Gusfield and Irving ' s open question by providing tight upper bounds on the amount of blacklists and their combined size , that are required by the women to force a given matching as the M-optimal stable matching , or , more generally , as the unique stable matching . Our results show that the coalition of all women can strategically force any matching as the unique stable matching , using preference lists in which at most half of the women have nonempty blacklists , and in which the average blacklist size is less than 0 . This allows the women to manipulate the market in a manner that is far more inconspicuous , in a sense , than previously realized . When there are less women than men , we show that in the absence of blacklists for men , the women can force any matching as the unique stable matching without blacklisting anyone , while when there are more women than men , each to-be-unmatched woman may have to blacklist as many as all men . Together , these results shed light on the question of how much , if at all , do given preferences for one side a priori impose limitations on the set of stable matchings under various conditions . All of the results in this paper are constructive , providing efficient algorithms for calculating the desired strategies .
Global recruitment into radical Islamic movements has spurred renewed interest in the appeal of political extremism . Is the appeal a rational response to material conditions or is it the expression of psychological and personality disorders associated with aggressive behavior , intolerance , conspiratorial imagination , and paranoia ? Empirical answers using surveys have been limited by lack of access to extremist groups , while field studies have lacked psychological measures and failed to compare extremists with contrast groups . We revisit the debate over the appeal of extremism in the U . S . context by comparing publicly available Twitter messages written by over 000 , 000 political extremist followers with messages written by non-extremist U . S . users . Analysis of text-based psychological indicators supports the moral foundation theory which identifies emotion as a critical factor in determining political orientation of individuals . Extremist followers also differ from others in four of the Big Five personality traits .
Traditionally quantitative games such as mean-payoff games and discount sum games have two players -- one trying to maximize the payoff , the other trying to minimize it . The associated decision problem , " Can Eve ( the maximizer ) achieve , for example , a positive payoff ? " can be thought of as one player trying to attain a payoff in the interval $ ( 0 , \infty ) $ . In this paper we consider the more general problem of determining if a player can attain a payoff in a finite union of arbitrary intervals for various payoff functions ( liminf , mean-payoff , discount sum , total sum ) . In particular this includes the interesting exact-value problem , " Can Eve achieve a payoff of exactly ( e . g . ) 0 ? "
Real-time monitoring and responses to emerging public health threats rely on the availability of timely surveillance data . During the early stages of an epidemic , the ready availability of line lists with detailed tabular information about laboratory-confirmed cases can assist epidemiologists in making reliable inferences and forecasts . Such inferences are crucial to understand the epidemiology of a specific disease early enough to stop or control the outbreak . However , construction of such line lists requires considerable human supervision and therefore , difficult to generate in real-time . In this paper , we motivate Guided Deep List , the first tool for building automated line lists ( in near real-time ) from open source reports of emerging disease outbreaks . Specifically , we focus on deriving epidemiological characteristics of an emerging disease and the affected population from reports of illness . Guided Deep List uses distributed vector representations ( ala word0vec ) to discover a set of indicators for each line list feature . This discovery of indicators is followed by the use of dependency parsing based techniques for final extraction in tabular form . We evaluate the performance of Guided Deep List against a human annotated line list provided by HealthMap corresponding to MERS outbreaks in Saudi Arabia . We demonstrate that Guided Deep List extracts line list features with increased accuracy compared to a baseline method . We further show how these automatically extracted line list features can be used for making epidemiological inferences , such as inferring demographics and symptoms-to-hospitalization period of affected individuals .
Distribution network operators ( DNOs ) are increasingly concerned about the impact of low carbon technologies on the low voltage ( LV ) networks . More advanced metering infrastructures provide numerous opportunities for more accurate load flow analysis of the LV networks . However , such data may not be readily available for DNOs and in any case is likely to be expensive . Modelling tools are required which can provide realistic , yet accurate , load profiles as input for a network modelling tool , without needing access to large amounts of monitored customer data . In this paper we outline some simple methods for accurately modelling a large number of unmonitored residential customers at the LV level . We do this by a process we call buddying , which models unmonitored customers by assigning them load profiles from a limited sample of monitored customers who have smart meters . Hence the presented method requires access to only a relatively small amount of domestic customers ' data . The method is efficiently optimised using a genetic algorithm to minimise a weighted cost function between matching the substation data and the individual mean daily demands . Hence we can show the effectiveness of substation monitoring in LV network modelling . Using real LV network modelling , we show that our methods perform significantly better than a comparative Monte Carlo approach , and provide a description of the peak demand behaviour .
The present study represents " The Fislab package of programs meant to develop the fuzzy regulators in the Scilab environment " in which we present some general issues , usage requirements and the working mode of the Fislab environment . In the second part of the article some features of the Scilab functions from the Fislab package are described .
State-space models are commonly used to describe different forms of ecological data . We consider the case of count data with observation errors . For such data the system process is typically multi-dimensional consisting of coupled Markov processes , where each component corresponds to a different characterisation of the population , such as age group , gender or breeding status . The associated system process equations describe the biological mechanisms under which the system evolves over time . However , there is often limited information in the count data alone to sensibly estimate demographic parameters of interest , so these are often combined with additional ecological observations leading to an integrated data analysis . Unfortunately , fitting these models to the data can be challenging , especially if the state-space model for the count data is non-linear or non-Gaussian . We propose an efficient particle Markov chain Monte Carlo algorithm to estimate the demographic parameters without the need for resorting to linear or Gaussian approximations . In particular , we exploit the integrated model structure to enhance the efficiency of the algorithm . We then incorporate the algorithm into a sequential Monte Carlo sampler in order to perform model comparison with regards to the dependence structure of the demographic parameters . Finally , we demonstrate the applicability and computational efficiency of our algorithms on two real datasets .
Automated Lymph Node ( LN ) detection is an important clinical diagnostic task but very challenging due to the low contrast of surrounding structures in Computed Tomography ( CT ) and to their varying sizes , poses , shapes and sparsely distributed locations . State-of-the-art studies show the performance range of 00 . 0% sensitivity at 0 . 0 false-positives per volume ( FP/vol . ) , or 00 . 0% at 0 . 0 FP/vol . for mediastinal LN , by one-shot boosting on 0D HAAR features . In this paper , we first operate a preliminary candidate generation stage , towards 000% sensitivity at the cost of high FP levels ( 00 per patient ) , to harvest volumes of interest ( VOI ) . Our 0 . 0D approach consequently decomposes any 0D VOI by resampling 0D reformatted orthogonal views N times , via scale , random translations , and rotations with respect to the VOI centroid coordinates . These random views are then used to train a deep Convolutional Neural Network ( CNN ) classifier . In testing , the CNN is employed to assign LN probabilities for all N random views that can be simply averaged ( as a set ) to compute the final classification probability per VOI . We validate the approach on two datasets : 00 CT volumes with 000 mediastinal LNs and 00 patients with 000 abdominal LNs . We achieve sensitivities of 00%/00% at 0 FP/vol . and 00%/00% at 0 FP/vol . in mediastinum and abdomen respectively , which drastically improves over the previous state-of-the-art work .
Time variation during program execution can leak sensitive information . Time variations due to program control flow and hardware resource contention have been used to steal encryption keys in cipher implementations such as AES and RSA . A number of approaches to mitigate timing-based side-channel attacks have been proposed including cache partitioning , control-flow obfuscation and injecting timing noise into the outputs of code . While these techniques make timing-based side-channel attacks more difficult , they do not eliminate the risks . Prior techniques are either too specific or too expensive , and all leave remnants of the original timing side channel for later attackers to attempt to exploit . In this work , we show that the state-of-the-art techniques in timing side-channel protection , which limit timing leakage but do not eliminate it , still have significant vulnerabilities to timing-based side-channel attacks . To provide a means for total protection from timing-based side-channel attacks , we develop Ozone , the first zero timing leakage execution resource for a modern microarchitecture . Code in Ozone execute under a special hardware thread that gains exclusive access to a single core ' s resources for a fixed ( and limited ) number of cycles during which it cannot be interrupted . Memory access under Ozone thread execution is limited to a fixed size uncached scratchpad memory , and all Ozone threads begin execution with a known fixed microarchitectural state . We evaluate Ozone using a number of security sensitive kernels that have previously been targets of timing side-channel attacks , and show that Ozone eliminates timing leakage with minimal performance overhead .
We derive a new Bayesian Information Criterion ( BIC ) from first principles by formulating the problem of estimating the number of clusters in an observed data set as maximization of the posterior probability of the candidate models . Given that some mild assumptions are satisfied , we provide a general BIC expression for a broad class of data distributions . This serves as an important milestone when deriving the BIC for specific data distributions . Along this line , we provide a closed-form BIC expression for multivariate Gaussian distributed observations . We show that incorporating data structure of the clustering problem into the derivation of the BIC results in an expression whose penalty term is different from that of the original BIC . We propose a two-step cluster enumeration algorithm . First , a model-based unsupervised learning algorithm partitions the data according to a given set of candidate models . Subsequently , the optimal cluster number is determined as the one associated to the model for which the proposed BIC is maximal . The performance of the proposed criterion is tested using synthetic and real data sets . Despite the fact that the original BIC is a generic criterion which does not include information about the specific model selection problem at hand , it has been widely used in the literature to estimate the number of clusters in an observed data set . We , therefore , consider it as a benchmark comparison . Simulation results show that our proposed criterion outperforms the existing cluster enumeration methods that are based on the original BIC .
In 0000 , Barber and Candes introduced a new variable selection procedure called the knockoff filter to control the false discovery rate ( FDR ) and prove that this method achieves exact FDR control . Inspired by the work of Barber and Candes ( 0000 ) , we propose and analyze a pseudo-knockoff filter that inherits some advantages of the original knockoff filter and has more flexibility in constructing its knockoff matrix . Although we have not been able to obtain exact FDR control of the pseudo knockoff filter , we show that it satisfies an expectation inequality that offers some insight into FDR control . Moreover , we provide some partial analysis of the pseudo knockoff filter for the half Lasso and the least squares statistics . Our analysis indicates that the inverse of the covariance matrix of the feature matrix plays an important role in designing and analyzing the pseudo knockoff filter . Our preliminary numerical experiments show that the pseudo knockoff filter with the half Lasso statistic has FDR control . Moreover , our numerical experiments show that the pseudo-knockoff filter could offer more power than the original knockoff filter with the OMP or Lasso Path statistic when the features are correlated and non-sparse .
This document presents the business requirement of Unified University Inventory System ( UUIS ) in Technology-independent manner . All attempts have been made in using mostly business terminology and business language while describing the requirements in this document . Very minimal and commonly understood Technical terminology is used . Use case approach is used in modeling the business requirements in this document .
When it comes to industrial organizations , current collaboration efforts in software engineering research are very often kept in-house , depriving these organizations off the skills necessary to build independent collaborative research . The current trend , towards empirical software engineering research , requires certain standards to be established which would guide these collaborative efforts in creating a strong partnership that promotes independent , evidence-based , software engineering research . This paper examines key enabling factors for an efficient and effective industry-academia collaboration in the software testing domain . A major finding of the research was that while technology is a strong enabler to better collaboration , it must be complemented with industrial openness to disclose research results and the use of a dedicated tooling platform . We use as an example an automated test generation approach that has been developed in the last two years collaboratively with Bombardier Transportation AB in Sweden .
Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval . It enables music search by instrument , helps recognize musical genres , or can make music transcription easier and more accurate . In this paper , we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music . We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length . To obtain the audio-excerpt-wise result , we aggregate multiple outputs from sliding windows over the test audio . In doing so , we investigated two different aggregation methods : one takes the average for each instrument and the other takes the instrument-wise sum followed by normalization . In addition , we conducted extensive experiments on several important factors that affect the performance , including analysis window size , identification threshold , and activation functions for neural networks to find the optimal set of parameters . Using a dataset of 00k audio excerpts from 00 instruments for evaluation , we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines . Experimental results showed that the proposed convolutional network architecture obtained an F0 measure of 0 . 000 for micro and 0 . 000 for macro , respectively , achieving 00 . 0% and 00 . 0% in performance improvement compared with other state-of-the-art algorithms .
Head gesture is a natural means of face-to-face communication between people but the recognition of head gestures in the context of virtual reality and use of head gesture as an interface for interacting with virtual avatars and virtual environments have been rarely investigated . In the current study , we present an approach for real-time head gesture recognition on head-mounted displays using Cascaded Hidden Markov Models . We conducted two experiments to evaluate our proposed approach . In experiment 0 , we trained the Cascaded Hidden Markov Models and assessed the offline classification performance using collected head motion data . In experiment 0 , we characterized the real-time performance of the approach by estimating the latency to recognize a head gesture with recorded real-time classification data . Our results show that the proposed approach is effective in recognizing head gestures . The method can be integrated into a virtual reality system as a head gesture interface for interacting with virtual worlds .
When we talk about databases there have always been problems concerning data synchronization . The latter is a technique for maintaining consistency among different copies of data ( often called replicas ) . In general , there is no universal solution to this problem and often a particular situation requires a particular approach driven by specific conditions . This paper presents an approach tackling the issue of data synchronization in a distributed multi-branch enterprise database . The proposed solution is based on MSMQ ( Microsoft Message Queue ) , a mechanism for asynchronous messaging .
Ontologies provide conceptual abstractions over data , in domains such as the Internet of Things , in a way that sensor data can be harvested and interpreted by people and applications . The Semantic Sensor Network ( SSN ) ontology is the de-facto standard for semantic representation of sensor observations and metadata , and it is used at the core of the open source platform for the Internet of Things , OpenIoT . In this paper we present a Schema Editor that provides an intuitive web interface for defining new types of sensors , and concrete instances of them , using the SSN ontology as the core model . This editor is fully integrated with the OpenIoT platform for generating virtual sensor descriptions and automating their semantic annotation and registration process .
In this paper , expansions for the maximum likelihood estimator of location and its distribution funtion are extended to fifth order . Since the proofs are straightforward extentions of proofs given in earlier papers for orders less than the fifth , they are not given here . The purpose of the paper is mainly to present the higher order expansions .
This paper considers 0-string representations of planar graphs that are order-preserving in the sense that the order of crossings along the curve representing vertex $v$ is the same as the order of edges in the clockwise order around $v$ in the planar embedding . We show that this does not exist for all planar graphs ( not even for all planar 0-trees ) , but show existence for some subclasses of planar partial 0-trees . In particular , for outer-planar graphs it can be order-preserving and outer-string in the sense that all ends of strings are on the outside of the representation .
We study the asymptotic behavior of eigenvalues of large complex correlated Wishart matrices at the edges of the limiting spectrum . In this setting , the support of the limiting eigenvalue distribution may have several connected components . Under mild conditions for the population matrices , we show that for every generic positive edge of that support , there exists an extremal eigenvalue which converges almost surely toward that edge and fluctuates according to the Tracy-Widom law at the scale $N^{0/0}$ . Moreover , given several generic positive edges , we establish that the associated extremal eigenvalue fluctuations are asymptotically independent . Finally , when the leftmost edge is the origin ( hard edge ) , the fluctuations of the smallest eigenvalue are described by mean of the Bessel kernel at the scale $N^0$ .
Evidential grids have recently shown interesting properties for mobile object perception . Evidential grids are a generalisation of Bayesian occupancy grids using Dempster- Shafer theory . In particular , these grids can handle efficiently partial information . The novelty of this article is to propose a perception scheme enhanced by geo-referenced maps used as an additional source of information , which is fused with a sensor grid . The paper presents the key stages of such a data fusion process . An adaptation of conjunctive combination rule is presented to refine the analysis of the conflicting information . The method uses temporal accumulation to make the distinction between stationary and mobile objects , and applies contextual discounting for modelling information obsolescence . As a result , the method is able to better characterise the occupied cells by differentiating , for instance , moving objects , parked cars , urban infrastructure and buildings . Experiments carried out on real- world data illustrate the benefits of such an approach .
Given a distribution $\rho$ on persistence diagrams and observations $X_0 , . . . X_n \stackrel{iid}{\sim} \rho$ we introduce an algorithm in this paper that estimates a Fr\ ' echet mean from the set of diagrams $X_0 , . . . X_n$ . If the underlying measure $\rho$ is a combination of Dirac masses $\rho = \frac{0}{m} \sum_{i=0}^m \delta_{Z_i}$ then we prove the algorithm converges to a local minimum and a law of large numbers result for a Fr\ ' echet mean computed by the algorithm given observations drawn iid from $\rho$ . We illustrate the convergence of an empirical mean computed by the algorithm to a population mean by simulations from Gaussian random fields .
We present a question answering system over DBpedia , filling the gap between user information needs expressed in natural language and a structured query interface expressed in SPARQL over the underlying knowledge base ( KB ) . Given the KB , our goal is to comprehend a natural language query and provide corresponding accurate answers . Focusing on solving the non-aggregation questions , in this paper , we construct a subgraph of the knowledge base from the detected entities and propose a graph traversal method to solve both the semantic item mapping problem and the disambiguation problem in a joint way . Compared with existing work , we simplify the process of query intention understanding and pay more attention to the answer path ranking . We evaluate our method on a non-aggregation question dataset and further on a complete dataset . Experimental results show that our method achieves best performance compared with several state-of-the-art systems .
We present the R-package mgm for the estimation of both stationary and time-varying Mixed Graphical Models and mixed Vector Autoregressive models in high-dimensional data . These are a useful extensions of graphical models for only one variable type , since mixed types of variables ( continuous , count , categorical ) are ubiquitous in datasets in many disciplines . In addition , we extend both models to the time-varying case in which the true model changes over time under the assumption that change is a smooth function of time . Time-varying models offer a rich description of temporally evolving systems as they provide information about organizational processes , information diffusion , vulnerabilities , and the potential impact of interventions . Next to providing the background of the implemented methods , we provide a number of fully reproducible code examples that illustrate how to use the software package .
The paper studies the visibility maintenance problem ( VMP ) for a leader-follower pair of Dubins-like vehicles with input constraints , and proposes an original solution based on the notion of controlled invariance . The nonlinear model describing the relative dynamics of the vehicles is interpreted as linear uncertain system , with the leader robot acting as an external disturbance . The VMP is then reformulated as a linear constrained regulation problem with additive disturbances ( DLCRP ) . Positive D-invariance conditions for linear uncertain systems with parametric disturbance matrix are introduced and used to solve the VMP when box bounds on the state , control input and disturbance are considered . The proposed design procedure is shown to be easily adaptable to more general working scenarios . Extensive simulation results are provided to illustrate the theory and show the effectiveness of our approach
The Method of Continuous Molecular Fields is a universal approach to predict various properties of chemical compounds , in which molecules are represented by means of continuous fields ( such as electrostatic , steric , electron density functions , etc ) . The essence of the proposed approach consists in performing statistical analysis of functional molecular data by means of joint application of kernel machine learning methods and special kernels which compare molecules by computing overlap integrals of their molecular fields . This approach is an alternative to traditional methods of building 0D structure-activity and structure-property models based on the use of fixed sets of molecular descriptors . The methodology of the approach is described in this chapter , followed by its application to building regression 0D-QSAR models and conducting virtual screening based on one-class classification models . The main directions of the further development of this approach are outlined at the end of the chapter .
We provide here a computational interpretation of first-order logic based on a constructive interpretation of satisfiability w . r . t . a fixed but arbitrary interpretation . In this approach the formulas themselves are programs . This contrasts with the so-called formulas as types approach in which the proofs of the formulas are typed terms that can be taken as programs . This view of computing is inspired by logic programming and constraint logic programming but differs from them in a number of crucial aspects . Formulas as programs is argued to yield a realistic approach to programming that has been realized in the implemented programming language ALMA-0 ( Apt et al . ) that combines the advantages of imperative and logic programming . The work here reported can also be used to reason about the correctness of non-recursive ALMA-0 programs that do not include destructive assignment .
Query equivalence is investigated for disjunctive aggregate queries with negated subgoals , constants and comparisons . A full characterization of equivalence is given for the aggregation functions count , max , sum , prod , toptwo and parity . A related problem is that of determining , for a given natural number N , whether two given queries are equivalent over all databases with at most N constants . We call this problem bounded equivalence . A complete characterization of decidability of bounded equivalence is given . In particular , it is shown that this problem is decidable for all the above aggregation functions as well as for count distinct and average . For quasilinear queries ( i . e . , queries where predicates that occur positively are not repeated ) it is shown that equivalence can be decided in polynomial time for the aggregation functions count , max , sum , parity , prod , toptwo and average . A similar result holds for count distinct provided that a few additional conditions hold . The results are couched in terms of abstract characteristics of aggregation functions , and new proof techniques are used . Finally , the results above also imply that equivalence , under bag-set semantics , is decidable for non-aggregate queries with negation .
Replay attack and password attacks are serious issues in the Kerberos authentication protocol . Many ideas have been proposed to prevent these attacks but they increase complexity of the total Kerberos environment . In this paper we present an improved method which prevents replay attacks and password attacks by using Triple password scheme . Three passwords are stored on Authentication Server and Authentication Server sends two passwords to Ticket Granting Server ( one for Application Server ) by encrypting with the secret key shared between Authentication server and Ticket Granting server . Similarly , Ticket Granting Server sends one password to Application Server by encrypting with the secret key shared between TGS and application server . Meanwhile , Service-Granting-Ticket is transferred to users by encrypting it with the password that TGS just received from AS . It helps to prevent Replay attack .
As scientific discovery becomes increasingly data-driven , software platforms are needed to efficiently organize and disseminate data from disparate sources . This is certainly the case in the field of materials science . For example , Materials Project has generated computational data on over 00 , 000 chemical compounds and has made that data available through a web portal and REST interface . However , such portals must seek to incorporate community submissions to expand the scope of scientific data sharing . In this paper , we describe MPContribs , a computing/software infrastructure to integrate and organize contributions of simulated or measured materials data from users . Our solution supports complex submissions and provides interfaces that allow contributors to share analyses and graphs . A RESTful API exposes mechanisms for book-keeping , retrieval and aggregation of submitted entries , as well as persistent URIs or DOIs that can be used to reference the data in publications . Our approach isolates contributed data from a host project ' s quality-controlled core data and yet enables analyses across the entire dataset , programmatically or through customized web apps . We expect the developed framework to enhance collaborative determination of material properties and to maximize the impact of each contributor ' s dataset . In the long-term , MPContribs seeks to make Materials Project an institutional , and thus community-wide , memory for computational and experimental materials science .
Continuation Passing Style ( CPS ) is one of the most important issues in the field of functional programming languages , and the quest for a primitive notion of types for continuation is still open . Starting from the notion of ``test ' ' proposed by Girard , we develop a notion of test for intuitionistic logic . We give a complete deductive system for tests and we show that it is good to deal with ``continuations ' ' . In particular , in the proposed system it is possible to work with Call by Value and Call by Name translations in a uniform way .
The main aim of this study is to develop a theoretical framework for the success of Web portals in promoting task innovation . This is deemed significant as yet little research has tackled this important domain from the business intelligence perspective . The D&M IS Success Model was used as a foundational theory and then was refined to match the context of the current research . Importantly , in this study , system quality and information quality constructs were defined on the basis of portals ' characteristics since a mapping was conducted between the most significant functions and features of Web portals and quality constructs . The developed framework is deemed useful for theory and practice . From theoretical perspective , the dimensions that affect the perceived quality of Web portals are identified , and the measures that affect each quality dimension are also defined . On the practical level , contributions gained by this study can be observed in terms of the benefits decision makers , strategists , operational employees and IT developers can gain . Assessing portals success in improving task innovation is important to help managers ( i . e . decision makers ) in making appropriate decisions concerning the adoption of portals ' technology , by weighing its benefits against the costs needed to establish and run such a technology . Moreover , assessing Web portals ' success gives some insight to IT developers and designers concerning what aspects should be taken when designing and establishing high quality portals , and what functions and features should be contained that would affect the perceived quality of portals and therefore users ' intention to use portals .
Network neutrality and the role of regulation on the Internet have been heavily debated in recent times . Amongst the various definitions of network neutrality , we focus on the one which prohibits paid prioritization of content and we present an analytical treatment of the topic . We develop a model of the Internet ecosystem in terms of three primary players : consumers , ISPs and content providers . Our analysis looks at this issue from the point of view of the consumer , and we describe the desired state of the system as one which maximizes consumer surplus . By analyzing different scenarios of monopoly and competition , we obtain different conclusions on the desirability of regulation . We also introduce the notion of a Public Option ISP , an ISP that carries traffic in a network neutral manner . Our major findings are ( i ) in a monopolistic scenario , network neutral regulations benefit consumers ; however , the introduction of a Public Option ISP is even better for consumers , as it aligns the interests of the monopolistic ISP with the consumer surplus and ( ii ) in an oligopolistic situation , the presence of a Public Option ISP is again preferable to network neutral regulations , although the presence of competing price-discriminating ISPs provides the most desirable situation for the consumers .
In this paper , a new method is proposed for sparse PCA based on the recursive divide-and-conquer methodology . The main idea is to separate the original sparse PCA problem into a series of much simpler sub-problems , each having a closed-form solution . By recursively solving these sub-problems in an analytical way , an efficient algorithm is constructed to solve the sparse PCA problem . The algorithm only involves simple computations and is thus easy to implement . The proposed method can also be very easily extended to other sparse PCA problems with certain constraints , such as the nonnegative sparse PCA problem . Furthermore , we have shown that the proposed algorithm converges to a stationary point of the problem , and its computational complexity is approximately linear in both data size and dimensionality . The effectiveness of the proposed method is substantiated by extensive experiments implemented on a series of synthetic and real data in both reconstruction-error-minimization and data-variance-maximization viewpoints .
This paper deals with adaptive radar detection of a subspace signal competing with two sources of interference . The former is Gaussian with unknown covariance matrix and accounts for the joint presence of clutter plus thermal noise . The latter is structured as a subspace signal and models coherent pulsed jammers impinging on the radar antenna . The problem is solved via the Principle of Invariance which is based on the identification of a suitable group of transformations leaving the considered hypothesis testing problem invariant . A maximal invariant statistic , which completely characterizes the class of invariant decision rules and significantly compresses the original data domain , as well as its statistical characterization are determined . Thus , the existence of the optimum invariant detector is addressed together with the design of practically implementable invariant decision rules . At the analysis stage , the performance of some receivers belonging to the new invariant class is established through the use of analytic expressions .
The size of 0D models used on the web or stored in databases is becoming increasingly high . Then , an efficient method that allows users to find similar 0D objects for a given 0D model query has become necessary . Keywords and the geometry of a 0D model cannot meet the needs of users ' retrieval because they do not include the semantic information . In this paper , a new method has been proposed to 0D models retrieval using semantic concepts combined with shape indexes . To obtain these concepts , we use the machine learning methods to label 0D models by k-means algorithm in measures and shape indexes space . Moreover , semantic concepts have been organized and represented by ontology language OWL and spatial relationships are used to disambiguate among models of similar appearance . The SPARQL query language has been used to question the information displayed in this language and to compute the similarity between two 0D models . We interpret our results using the Princeton Shape Benchmark Database and the results show the performance of the proposed new approach to retrieval 0D models . Keywords : 0D Model , 0D retrieval , measures , shape indexes , semantic , ontology
We present a novel approach to the formulation and the resolution of sparse Linear Discriminant Analysis ( LDA ) . Our proposal , is based on penalized Optimal Scoring . It has an exact equivalence with penalized LDA , contrary to the multi-class approaches based on the regression of class indicator that have been proposed so far . Sparsity is obtained thanks to a group-Lasso penalty that selects the same features in all discriminant directions . Our experiments demonstrate that this approach generates extremely parsimonious models without compromising prediction performances . Besides prediction , the resulting sparse discriminant directions are also amenable to low-dimensional representations of data . Our algorithm is highly efficient for medium to large number of variables , and is thus particularly well suited to the analysis of gene expression data .
Quantile regression is a method to estimate the quantiles of the conditional distribution of a response variable , and as such it permits a much more accurate portrayal of the relationship between the response variable and observed covariates than methods such as Least-squares or Least Absolute Deviations regression . It can be expressed as a linear program , and , with appropriate preprocessing , interior-point methods can be used to find a solution for moderately large problems . Dealing with very large problems , \emph ( e . g . ) , involving data up to and beyond the terabyte regime , remains a challenge . Here , we present a randomized algorithm that runs in nearly linear time in the size of the input and that , with constant probability , computes a $ ( 0+\epsilon ) $ approximate solution to an arbitrary quantile regression problem . As a key step , our algorithm computes a low-distortion subspace-preserving embedding with respect to the loss function of quantile regression . Our empirical evaluation illustrates that our algorithm is competitive with the best previous work on small to medium-sized problems , and that in addition it can be implemented in MapReduce-like environments and applied to terabyte-sized problems .
We consider a nonparametric regression model $Y=r ( X ) +\varepsilon$ with a random covariate $X$ that is independent of the error $\varepsilon$ . Then the density of the response $Y$ is a convolution of the densities of $\varepsilon$ and $r ( X ) $ . It can therefore be estimated by a convolution of kernel estimators for these two densities , or more generally by a local von Mises statistic . If the regression function has a nowhere vanishing derivative , then the convolution estimator converges at a parametric rate . We show that the convergence holds uniformly , and that the corresponding process obeys a functional central limit theorem in the space $C_0 ( \mathbb {R} ) $ of continuous functions vanishing at infinity , endowed with the sup-norm . The estimator is not efficient . We construct an additive correction that makes it efficient .
Reconstruction of gene regulatory networks or ' reverse-engineering ' is a process of identifying gene interaction networks from experimental microarray gene expression profile through computation techniques . In this paper , we tried to reconstruct cancer-specific gene regulatory network using information theoretic approach - mutual information . The considered microarray data consists of large number of genes with 00 samples - 00 samples from colon cancer patient and 0 from normal cell . The data has been preprocessed and normalized . A t-test statistics has been applied to filter differentially expressed genes . The interaction between filtered genes has been computed using mutual information and ten different networks has been constructed with varying number of interactions ranging from 00 to 000 . We performed the topological analysis of the reconstructed network , revealing a large number of interactions in colon cancer . Finally , validation of the inferred results has been done with available biological databases and literature .
Visual localization under large changes in scale is an important capability in many robotic mapping applications , such as localizing at low altitudes in maps built at high altitudes , or performing loop closure over long distances . Existing approaches , however , are robust only up to a 0x difference in scale between map and query images . We propose a novel combination of deep-learning-based object features and hand-engineered point-features that yields improved robustness to scale change , perspective change , and image noise . We conduct experiments in simulation and in real-world outdoor scenes exhibiting up to a 0x change in scale , and compare our approach against localization using state-of-the-art SIFT features . This technique is training-free and class-agnostic , and in principle can be deployed in any environment out-of-the-box .
Let $\mu$ be a Gaussian measure ( say , on ${\bf R}^n$ ) and let $K , L \subset {\bf R}^n$ be such that K is convex , $L$ is a " layer " ( i . e . $L = \{x : a \leq < x , u > \leq b \}$ for some $a$ , $b \in {\bf R}$ and $u \in {\bf R}^n$ ) and the centers of mass ( with respect to $\mu$ ) of $K$ and $L$ coincide . Then $\mu ( K \cap L ) \geq \mu ( K ) \cdot \mu ( L ) $ . This is motivated by the well-known " positive correlation conjecture " for symmetric sets and a related inequality of Sidak concerning confidence regions for means of multivariate normal distributions . The proof uses an apparently hitherto unknown estimate for the ( standard ) Gaussian cumulative distribution function : $\Phi ( x ) > 0 - \frac{ ( 0/\pi ) ^{{0/0}}}{0x + ( x^0 +0 ) ^{{0/0}}} e^{-x^0/0}$ ( valid for $x > -0$ ) .
Strong consistency and asymptotic normality of the Quasi-Maximum Likelihood Estimator ( QMLE ) are given for a general class of multidimensional causal processes . For particular cases already studied in the literature ( for instance univariate or multivariate GARCH , ARCH , ARMA-GARCH processes ) the assumptions required for establishing these results are often weaker than existing conditions . The QMLE asymptotic behavior is also given for numerous new examples of univariate or multivariate processes ( for instance TARCH or NLARCH processes ) .
Processing moving object trajectories arises in many application domains and has been addressed by practitioners in the spatiotemporal database and Geographical Information System communities . In this work , we focus on a trajectory similarity search , the distance threshold query , which finds all trajectories within a given distance d of a search trajectory over a time interval . We demonstrate the performance of a multithreaded implementation which features the use of an R-tree index and which has high parallel efficiency ( 00%-00% ) . We introduce a GPGPU implementation which avoids the use of index-trees , and instead features a GPU-friendly indexing method . We compare the performance of the multithreaded and GPU implementations , and show that a speedup can be obtained using the latter . We propose two classes of algorithms , SetSplit and GreedySetSplit , to create efficient query batches that reduce memory pressure and computational cost on the GPU . However , we find that using fixed-size batches is sufficiently efficient in practice . We develop an empirical performance model for our GPGPU implementation that can be used to predict the response time of the distance threshold query . This model can be used to pick a good query batch size .
Internet Banking System refers to systems that enable bank customers to access accounts and general information on bank products and services through a personal computer or other intelligent device . Internet banking products and services can include detailed account information for corporate customers as well as account summery and transfer money . Ultimately , the products and services obtained through Internet Banking may mirror products and services offered through other bank delivery channels . In this paper , Internet Banking System Prototype has been proposed in order to illustrate the services which is provided by the Bank online services .
A concept for a novel CMOS image sensor suited for analog image pre-processing is presented in this paper . As an example , an image restoration algorithm for reducing image noise is applied as image pre-processing in the analog domain . To supply low-latency data input for analog image preprocessing , the proposed concept for a CMOS image sensor offers a new sensor signal acquisition method in 0D . In comparison to image pre-processing in the digital domain , the proposed analog image pre-processing promises an improved image quality . Furthermore , the image noise at the stage of analog sensor signal acquisition can be used to select the most effective restoration algorithm applied to the analog circuit due to image processing prior to the A/D converter .
Today , many organizations are moving their computing services towards the Cloud . This makes their computer processing available much more conveniently to users . However , it also brings new security threats and challenges about safety and reliability . In fact , Cloud Computing is an attractive and cost-saving service for buyers as it provides accessibility and reliability options for users and scalable sales for providers . In spite of being attractive , Cloud feature poses various new security threats and challenges when it comes to deploying Intrusion Detection System ( IDS ) in Cloud environments . Most Intrusion Detection Systems ( IDSs ) are designed to handle specific types of attacks . It is evident that no single technique can guarantee protection against future attacks . Hence , there is a need for an integrated scheme which can provide robust protection against a complete spectrum of threats . On the other hand , there is great need for technology that enables the network and its hosts to defend themselves with some level of intelligence in order to accurately identify and block malicious traffic and activities . In this case , it is called Intrusion prevention system ( IPS ) . Therefore , in this paper , we emphasize on recent implementations of IDS on Cloud Computing environments in terms of security and privacy . We propose an effective and efficient model termed as the Integrated Intrusion Detection and Prevention System ( IDPS ) which combines both IDS and IPS in a single mechanism . Our mechanism also integrates two techniques namely , Anomaly Detection ( AD ) and Signature Detection ( SD ) that can work in cooperation to detect various numbers of attacks and stop them through the capability of IPS .
This paper reports on our definition of guidelines for managing global software development ( GSD ) that implements the specific practice - manage requirements changes - of the Capability Maturity Model Integration ( CMMI ) Level 0 . The guidelines present a model for change management and traceability that supports the implementation of the specific CMMI Level 0 goal . Also , to support the effective management of the system engineering processes , an adaptation of the Project Management Body of Knowledge ( PMBOK ) process group ( PG ) for project life-cycle practices is provided . We introduce a cloud-based Reactive Middleware which provides services for managing GSD projects towards dependable change management and traceability .
A model for competing ( resp . complementary ) risks survival data where the failure time can be left ( resp . right ) censored is proposed . Product-limit estimators for the survival functions of the individual risks are derived . We deduce the strong convergence of our estimators on the whole real half-line without any additional assumptions and their asymptotic normality under conditions concerning only the observed distribution . When the observations are generated according to the double censoring model introduced by Turnbull , the product-limit estimators represent upper and lower bounds for Turnbull ' s estimator .
In this paper we extend earlier work on groups acting on Gaussian graphical models to Gaussian Bayesian networks and more general Gaussian models defined by chain graphs . We discuss the maximal group which leaves a given model invariant and provide basic statistical applications of this result . This includes equivariant estimation , maximal invariants and robustness . The computation of the group requires finding the essential graph . However , by applying Studeny ' s theory of imsets we show that computations for DAGs can be performed efficiently without building the essential graph . In our proof we derive simple necessary and sufficient conditions on vanishing sub-minors of the concentration matrix in the model .
We propose a flexible convex relaxation for the phase retrieval problem that operates in the natural domain of the signal . Therefore , we avoid the prohibitive computational cost associated with " lifting " and semidefinite programming ( SDP ) in methods such as PhaseLift and compete with recently developed non-convex techniques for phase retrieval . We relax the quadratic equations for phaseless measurements to inequality constraints each of which representing a symmetric " slab " . Through a simple convex program , our proposed estimator finds an extreme point of the intersection of these slabs that is best aligned with a given anchor vector . We characterize geometric conditions that certify success of the proposed estimator . Furthermore , using classic results in statistical learning theory , we show that for random measurements the geometric certificates hold with high probability at an optimal sample complexity . Phase transition of our estimator is evaluated through simulations . Our numerical experiments also suggest that the proposed method can solve phase retrieval problems with coded diffraction measurements as well .
In the analysis of logic programs , abstract domains for detecting sharing and linearity information are widely used . Devising abstract unification algorithms for such domains has proved to be rather hard . At the moment , the available algorithms are correct but not optimal , i . e . , they cannot fully exploit the information conveyed by the abstract domains . In this paper , we define a new ( infinite ) domain ShLin-w which can be thought of as a general framework from which other domains can be easily derived by abstraction . ShLin-w makes the interaction between sharing and linearity explicit . We provide a constructive characterization of the optimal abstract unification operator on ShLin-w and we lift it to two well-known abstractions of ShLin-w . Namely , to the classical Sharing X Lin abstract domain and to the more precise ShLin-0 abstract domain by Andy King . In the case of single binding substitutions , we obtain optimal abstract unification algorithms for such domains . To appear in Theory and Practice of Logic Programming ( TPLP ) .
Respondent-Driven Sampling ( RDS ) is an approach to sampling design and inference in hard-to-reach human populations . Typically , a sampling frame is not available , and population members are difficult to identify or recruit from broader sampling frames . Common examples include injecting drug users , men who have sex with men , and female sex workers . Most analysis of RDS data has focused on estimating aggregate characteristics , such as disease prevalence . However , RDS is often conducted in settings where the population size is unknown and of great independent interest . This paper presents an approach to estimating the size of a target population based on data collected through RDS . The proposed approach uses a successive sampling approximation to RDS to leverage information in the ordered sequence of observed personal network sizes . The inference uses the Bayesian framework , allowing for the incorporation of prior knowledge . A flexible class of priors for the population size is proposed that aids elicitation . An extensive simulation study provides insight into the performance of the method for estimating population size under a broad range of conditions . A further study shows the approach also improves estimation of aggregate characteristics . A particular choice of the prior produces interval estimates with good frequentist properties . Finally , the method demonstrates sensible results when used to estimate the numbers of sub-populations most at risk for HIV in two cities in El Salvador .
We propose a generic framework based on a new stochastic variance-reduced gradient descent algorithm for accelerating nonconvex low-rank matrix recovery . Starting from an appropriate initial estimator , our proposed algorithm performs projected gradient descent based on a novel semi-stochastic gradient specifically designed for low-rank matrix recovery . Based upon the mild restricted strong convexity and smoothness conditions , we derive a projected notion of the restricted Lipschitz continuous gradient property , and prove that our algorithm enjoys linear convergence rate to the unknown low-rank matrix with an improved computational complexity . Moreover , our algorithm can be employed to both noiseless and noisy observations , where the optimal sample complexity and the minimax optimal statistical rate can be attained respectively . We further illustrate the superiority of our generic framework through several specific examples , both theoretically and experimentally .
Keyphrases provide semantic metadata that summarize and characterize documents . This paper describes Kea , an algorithm for automatically extracting keyphrases from text . Kea identifies candidate keyphrases using lexical methods , calculates feature values for each candidate , and uses a machine-learning algorithm to predict which candidates are good keyphrases . The machine learning scheme first builds a prediction model using training documents with known keyphrases , and then uses the model to find keyphrases in new documents . We use a large test corpus to evaluate Kea ' s effectiveness in terms of how many author-assigned keyphrases are correctly identified . The system is simple , robust , and publicly available .
A density estimation method in a Bayesian nonparametric framework is presented when recorded data are not coming directly from the distribution of interest , but from a length biased version . From a Bayesian perspective , efforts to computationally evaluate posterior quantities conditionally on length biased data were hindered by the inability to circumvent the problem of a normalizing constant . In this paper we present a novel Bayesian nonparametric approach to the length bias sampling problem which circumvents the issue of the normalizing constant . Numerical illustrations as well as a real data example are presented and the estimator is compared against its frequentist counterpart , the kernel density estimator for indirect data of Jones ( 0000 ) .
Recurrent Neural Networks ( RNNs ) with sophisticated units that implement a gating mechanism have emerged as powerful technique for modeling sequential signals such as speech or electroencephalography ( EEG ) . The latter is the focus on this paper . A significant big data resource , known as the TUH EEG Corpus ( TUEEG ) , has recently become available for EEG research , creating a unique opportunity to evaluate these recurrent units on the task of seizure detection . In this study , we compare two types of recurrent units : long short-term memory units ( LSTM ) and gated recurrent units ( GRU ) . These are evaluated using a state of the art hybrid architecture that integrates Convolutional Neural Networks ( CNNs ) with RNNs . We also investigate a variety of initialization methods and show that initialization is crucial since poorly initialized networks cannot be trained . Furthermore , we explore regularization of these convolutional gated recurrent networks to address the problem of overfitting . Our experiments revealed that convolutional LSTM networks can achieve significantly better performance than convolutional GRU networks . The convolutional LSTM architecture with proper initialization and regularization delivers 00% sensitivity at 0 false alarms per 00 hours .
We introduce a framework for the generation of grid-shell structures that is based on Voronoi diagrams and allows us to design tessellations that achieve excellent static performances . We start from an analysis of stress on the input surface and we use the resulting tensor field to induce an anisotropic non-Euclidean metric over it . Then we compute a Centroidal Voronoi Tessellation under the same metric . The resulting mesh is hex-dominant and made of cells with a variable density , which depends on the amount of stress , and anisotropic shape , which depends on the direction of maximum stress . This mesh is further optimized taking into account symmetry and regularity of cells to improve aesthetics . We demonstrate that our grid-shells achieve better static performances with respect to quad-based grid shells , while offering an innovative and aesthetically pleasing look .
We present a new and simple randomized algorithm for constructing the Delaunay triangulation using nearest neighbor graphs for point location . Under suitable assumptions , it runs in linear expected time for points in the plane with polynomially bounded spread , i . e . , if the ratio between the largest and smallest pointwise distance is polynomially bounded . This also holds for point sets with bounded spread in higher dimensions as long as the expected complexity of the Delaunay triangulation of a sample of the points is linear in the sample size .
HEVC includes a Coding Unit ( CU ) level luminance-based perceptual quantization technique known as AdaptiveQP . AdaptiveQP perceptually adjusts the Quantization Parameter ( QP ) at the CU level based on the spatial activity of raw input video data in a luma Coding Block ( CB ) . In this paper , we propose a novel cross-color channel adaptive quantization scheme which perceptually adjusts the CU level QP according to the spatial activity of raw input video data in the constituent luma and chroma CBs ; i . e . , the combined spatial activity across all three color channels ( the Y , Cb and Cr channels ) . Our technique is evaluated in HM 00 with 0 : 0 : 0 , 0 : 0 : 0 and 0 : 0 : 0 YCbCr JCT-VC test sequences . Both subjective and objective visual quality evaluations are undertaken during which we compare our method with AdaptiveQP . Our technique achieves considerable coding efficiency improvements , with maximum BD-Rate reductions of 00 . 0% ( Y ) , 00 . 0% ( Cr ) and 00 . 0% ( Cb ) in addition to a maximum decoding time reduction of 00 . 0% .
Dynamic functional connectivity ( FC ) has in recent years become a topic of interest in the neuroimaging community . Several models and methods exist for both functional magnetic resonance imaging ( fMRI ) and electroencephalography ( EEG ) , and the results point towards the conclusion that FC exhibits dynamic changes . The existing approaches modeling dynamic connectivity have primarily been based on time-windowing the data and k-means clustering . We propose a non-parametric generative model for dynamic FC in fMRI that does not rely on specifying window lengths and number of dynamic states . Rooted in Bayesian statistical modeling we use the predictive likelihood to investigate if the model can discriminate between a motor task and rest both within and across subjects . We further investigate what drives dynamic states using the model on the entire data collated across subjects and task/rest . We find that the number of states extracted are driven by subject variability and preprocessing differences while the individual states are almost purely defined by either task or rest . This questions how we in general interpret dynamic FC and points to the need for more research on what drives dynamic FC .
We address the problem of interactively controlling the workspace of a mobile robot to ensure a human-aware navigation . This is especially of relevance for non-expert users living in human-robot shared spaces , e . g . home environments , since they want to keep the control of their mobile robots , such as vacuum cleaning or companion robots . Therefore , we introduce virtual borders that are respected by a robot while performing its tasks . For this purpose , we employ a RGB-D Google Tango tablet as human-robot interface to flexibly specify virtual borders in the environment . We evaluated our system concerning correctness , accuracy and teaching effort , and compared the results with other baseline methods . Our method features an equally-high accuracy while reducing the teaching effort by a factor of 0 . 0 compared to the baseline .
Multi-Objective Evolutionary Algorithms ( MOEAs ) have been proved efficient to deal with Multi-objective Optimization Problems ( MOPs ) . Until now tens of MOEAs have been proposed . The unified mode would provide a more systematic approach to build new MOEAs . Here a new model is proposed which includes two sub-models based on two classes of different schemas of MOEAs . According to the new model , some representatives algorithms are decomposed and some interesting issues are discussed .
Bertand ' s paradox is a fundamental problem in probability that casts doubt on the applicability of the indifference principle by showing that it may yield contradictory results , depending on the meaning assigned to " randomness " . Jaynes claimed that symmetry requirements ( the principle of transformation groups ) solve the paradox by selecting a unique solution to the problem . I show that this is not the case and that every variant obtained from the principle of indifference can also be obtained from Jaynes ' principle of transformation groups . This is because the same symmetries can be mathematically implemented in different ways , depending on the procedure of random selection that one uses . I describe a simple experiment that supports a result from symmetry arguments , but the solution is different from Jaynes ' . Jaynes ' method is thus best seen as a tool to obtain probability distributions when the principle of indifference is inconvenient , but it cannot resolve ambiguities inherent in the use of that principle and still depends on explicitly defining the selection procedure .
We study optimal estimation for sparse principal component analysis when the number of non-zero elements is small but on the same order as the dimension of the data . We employ approximate message passing ( AMP ) algorithm and its state evolution to analyze what is the information theoretically minimal mean-squared error and the one achieved by AMP in the limit of large sizes . For a special case of rank one and large enough density of non-zeros Deshpande and Montanari [0] proved that AMP is asymptotically optimal . We show that both for low density and for large rank the problem undergoes a series of phase transitions suggesting existence of a region of parameters where estimation is information theoretically possible , but AMP ( and presumably every other polynomial algorithm ) fails . The analysis of the large rank limit is particularly instructive .
An essential element of any verification technique is that of identifying and communicating to the user , system behaviour which leads to a deviation from the expected behaviour . Such behaviours are typically made available as long traces of system actions which would benefit from a natural language explanation of the trace and especially in the context of business logic level specifications . In this paper we present a natural language generation model which can be used to explain such traces . A key idea is that the explanation language is a CNL that is , formally speaking , regular language susceptible transformations that can be expressed with finite state machinery . At the same time it admits various forms of abstraction and simplification which contribute to the naturalness of explanations that are communicated to the user .
We develop an active set algorithm for the maximum likelihood estimation of a log-concave density based on complete data . Building on this fast algorithm , we indidate an EM algorithm to treat arbitrarily censored or binned data .
Previously in 0000 , we proposed the Nearest Descent ( ND ) method , capable of generating an efficient Graph , called the in-tree ( IT ) . Due to some beautiful and effective features , this IT structure proves well suited for data clustering . Although there exist some redundant edges in IT , they usually have salient features and thus it is not hard to remove them . Subsequently , in order to prevent the seemingly redundant edges from occurring , we proposed the Nearest Neighbor Descent ( NND ) by adding the " Neighborhood " constraint on ND . Consequently , clusters automatically emerged , without the additional requirement of removing the redundant edges . However , NND proved still not perfect , since it brought in a new yet worse problem , the " over-partitioning " problem . Now , in this paper , we propose a method , called the Hierarchical Nearest Neighbor Descent ( H-NND ) , which overcomes the over-partitioning problem of NND via using the hierarchical strategy . Specifically , H-NND uses ND to effectively merge the over-segmented sub-graphs or clusters that NND produces . Like ND , H-NND also generates the IT structure , in which the redundant edges once again appear . This seemingly comes back to the situation that ND faces . However , compared with ND , the redundant edges in the IT structure generated by H-NND generally become more salient , thus being much easier and more reliable to be identified even by the simplest edge-removing method which takes the edge length as the only measure . In other words , the IT structure constructed by H-NND becomes more fitted for data clustering . We prove this on several clustering datasets of varying shapes , dimensions and attributes . Besides , compared with ND , H-NND generally takes less computation time to construct the IT data structure for the input data .
For regular parametric problems , we show how median centering of the maximum likelihood estimate can be achieved by a simple modification of the score equation . For a scalar parameter of interest , the estimator is equivariant under interest respecting parameterizations and third-order median unbiased . With a vector parameter of interest , componentwise equivariance and third-order median centering are obtained . Like Firth ' s ( 0000 , Biometrika ) implicit method for bias reduction , the new method does not require finiteness of the maximum likelihood estimate and is effective in preventing infinite estimates . Simulation results for continuous and discrete models , including binary and beta regression , confirm that the method succeeds in achieving componentwise median centering and in solving the infinite estimate problem , while keeping comparable dispersion and the same approximate distribution as its main competitors .
Natural-language-facilitated human-robot cooperation ( NLC ) , in which natural language ( NL ) is used to share knowledge between a human and a robot for conducting intuitive human-robot cooperation ( HRC ) , is continuously developing in the recent decade . Currently , NLC is used in several robotic domains such as manufacturing , daily assistance and health caregiving . It is necessary to summarize current NLC-based robotic systems and discuss the future developing trends , providing helpful information for future NLC research . In this review , we first analyzed the driving forces behind the NLC research . Regarding to a robot s cognition level during the cooperation , the NLC implementations then were categorized into four types {NL-based control , NL-based robot training , NL-based task execution , NL-based social companion} for comparison and discussion . Last based on our perspective and comprehensive paper review , the future research trends were discussed .
Do runoff elections , using the same voting rule as the initial election but just on the winning candidates , increase or decrease the complexity of manipulation ? Does allowing revoting in the runoff increase or decrease the complexity relative to just having a runoff without revoting ? For both weighted and unweighted voting , we show that even for election systems with simple winner problems the complexity of manipulation , manipulation with runoffs , and manipulation with revoting runoffs are independent , in the abstract . On the other hand , for some important , well-known election systems we determine what holds for each of these cases . For no such systems do we find runoffs lowering complexity , and for some we find that runoffs raise complexity . Ours is the first paper to show that for natural , unweighted election systems , runoffs can increase the manipulation complexity .
In this note , we focus on smooth nonconvex optimization problems that obey : ( 0 ) all local minimizers are also global ; and ( 0 ) around any saddle point or local maximizer , the objective has a negative directional curvature . Concrete applications such as dictionary learning , generalized phase retrieval , and orthogonal tensor decomposition are known to induce such structures . We describe a second-order trust-region algorithm that provably converges to a global minimizer efficiently , without special initializations . Finally we highlight alternatives , and open problems in this direction .
This paper presented our work on applying Recurrent Deep Stacking Networks ( RDSNs ) to Robust Automatic Speech Recognition ( ASR ) tasks . In the paper , we also proposed a more efficient yet comparable substitute to RDSN , Bi- Pass Stacking Network ( BPSN ) . The main idea of these two models is to add phoneme-level information into acoustic models , transforming an acoustic model to the combination of an acoustic model and a phoneme-level N-gram model . Experiments showed that RDSN and BPsn can substantially improve the performances over conventional DNNs .
The relationship between algebraic geometry and the inferential framework of the Bayesian Networks with hidden variables has now been fruitfully explored and exploited by a number of authors . More recently the algebraic formulation of Causal Bayesian Networks has also been investigated in this context . After reviewing these newer relationships , we proceed to demonstrate that many of the ideas embodied in the concept of a ``causal model ' ' can be more generally expressed directly in terms of a partial order and a family of polynomial maps . The more conventional graphical constructions , when available , remain a powerful tool .
We consider a Gaussian sequence space model $X_{\lambda}=f_{\lambda} + \xi_{\lambda} , $ where $\xi $ has a diagonal covariance matrix $\Sigma=\diag ( \sigma_\lambda ^0 ) $ . We consider the situation where the parameter vector $ ( f_{\lambda} ) $ is sparse . Our goal is to estimate the unknown parameter by a model selection approach . The heterogenous case is much more involved than the direct model . Indeed , there is no more symmetry inside the stochastic process that one needs to control since each empirical coefficient has its own variance . The problem and the penalty do not only depend on the number of coefficients that one selects , but also on their position . This appears also in the minimax bounds where the worst coefficients will go to the larger variances . However , with a careful and explicit choice of the penalty we are able to select the correct coefficients and get a sharp non-asymptotic control of the risk of our procedure . Some simulation results are provided .
The free energy functional has recently been proposed as a variational principle for bounded rational decision-making , since it instantiates a natural trade-off between utility gains and information processing costs that can be axiomatically derived . Here we apply the free energy principle to general decision trees that include both adversarial and stochastic environments . We derive generalized sequential optimality equations that not only include the Bellman optimality equations as a limit case , but also lead to well-known decision-rules such as Expectimax , Minimax and Expectiminimax . We show how these decision-rules can be derived from a single free energy principle that assigns a resource parameter to each node in the decision tree . These resource parameters express a concrete computational cost that can be measured as the amount of samples that are needed from the distribution that belongs to each node . The free energy principle therefore provides the normative basis for generalized optimality equations that account for both adversarial and stochastic environments .
We investigate stochastic comparisons between exponential family distributions and their mixtures with respect to the usual stochastic order , the hazard rate order , the reversed hazard rate order , and the likelihood ratio order . A general theorem based on the notion of relative log-concavity is shown to unify various specific results for the Poisson , binomial , negative binomial , and gamma distributions in recent literature . By expressing a convolution of gamma distributions with arbitrary scale and shape parameters as a scale mixture of gamma distributions , we obtain comparison theorems concerning such convolutions that generalize some known results . Analogous results on convolutions of negative binomial distributions are also discussed .
Today , in the digital age , the mobile devices are more and more used to aid people in the struggle to improve or maintain their health . In this paper , the mobile eHealth solution for remote patient monitoring during clinical trials is presented , together with the outcomes of quantitative and qualitative performance evaluation . The evaluation is a third step to improve the quality of the application after earlier Good Clinical Practice certification and validation with the participation of 00 patients and three general practitioners . This time , the focus was on the usability which was evaluated by the seventeen participants divided into three age groups ( 00-00 , 00-00 , and 00+ ) . The results , from recorded sessions and the eye tracking , show that there is no difference in performance between the first group and the second group , while for the third group the performance was worse , however , it was still good enough to complete task within reasonable time .
Peer production projects like Wikipedia have inspired voluntary associations , collectives , social movements , and scholars to embrace open online collaboration as a model of democratic organization . However , many peer production projects exhibit entrenched leadership and deep inequalities , suggesting that they may not fulfill democratic ideals . Instead , peer production projects may conform to Robert Michels ' " iron law of oligarchy , " which proposes that democratic membership organizations become increasingly oligarchic as they grow . Using exhaustive data of internal processes from a sample of 000 wikis , we construct empirical measures of participation and test for increases in oligarchy associated with growth . In contrast to previous studies , we find support for Michels ' iron law and conclude that peer production entails oligarchic organizational forms .
For an intelligent agent to be truly autonomous , it must be able to adapt its representation to the requirements of its task as it interacts with the world . Most current approaches to on-line feature extraction are ad hoc ; in contrast , this paper presents an algorithm that bases judgments of state compatibility and state-space abstraction on principled criteria derived from the psychological principle of cognitive economy . The algorithm incorporates an active form of Q-learning , and partitions continuous state-spaces by merging and splitting Voronoi regions . The experiments illustrate a new methodology for testing and comparing representations by means of learning curves . Results from the puck-on-a-hill task demonstrate the algorithm ' s ability to learn effective representations , superior to those produced by some other , well-known , methods .
We analyze the convergence rate of a simplified version of a popular Gibbs sampling method used for statistical discovery of gene regulatory binding motifs in DNA sequences . This sampler satisfies a very strong form of ergodicity ( uniform ) . However , we show that , due to multimodality of the posterior distribution , the rate of convergence often decreases exponentially as a function of the length of the DNA sequence . Specifically , we show that this occurs whenever there is more than one true repeating pattern in the data . In practice there are typically multiple such patterns in biological data , the goal being to detect the most well-conserved and frequently-occurring of these . Our findings match empirical results , in which the motif-discovery Gibbs sampler has exhibited such poor convergence that it is used only for finding modes of the posterior distribution ( candidate motifs ) rather than for obtaining samples from that distribution . Ours are some of the first meaningful bounds on the convergence rate of a Markov chain method for sampling from a multimodal posterior distribution , as a function of statistical quantities like the number of observations .
In this paper , we propose a reduced version of the new modified Weibull ( NMW ) distribution due to Almalki and Yuan \cite{meNMW} in order to avoid some estimation problems . The number of parameters in the NMW distribution is five . The number of parameters in the reduced version is three . We study mathematical properties as well as maximum likelihood estimation of the reduced version . Four real data sets ( two of them complete and the other two censored ) are used to compare the flexibility of the reduced version versus the NMW distribution . It is shown that the reduced version has the same desirable properties of the NMW distribution in spite of having two less parameters . The NMW distribution did not provide a significantly better fit than the reduced version for any of the four data sets .
Continuing a series of articles in the past few years on creative telescoping using reductions , we adapt Trager ' s Hermite reduction for algebraic functions to fuchsian D-finite functions and develop a reduction-based creative telescoping algorithm for this class of functions , thereby generalizing our recent reduction-based algorithm for algebraic functions , presented at ISSAC 0000 .
Software development environments ( IDEs ) have not followed the IT industry ' s inexorable trend towards distribution . They do too little to address the problems raised by today ' s increasingly distributed projects ; neither do they facilitate collaborative and interactive development practices . A consequence is the continued reliance of today ' s IDEs on paradigms such as traditional configuration management , which were developed for earlier modes of operation and hamper collaborative projects . This contribution describes a new paradigm : cloud-based development , which caters to the specific needs of distributed and collaborative projects . The CloudStudio IDE embodies this paradigm by enabling developers to work on a shared project repository . Configuration management becomes unobtrusive ; it replaces the explicit update-modify-commit cycle by interactive editing and real-time conflict tracking and management . A case study involving three teams of pairs demonstrates the usability of CloudStudio and its advantages for collaborative software development over traditional configuration management practices .
The development of IT and WWW provides different teaching strategies , which are chosen by teachers . Students can acquire knowledge through different learning models . The problem based learning is a popular teaching strategy for teachers . Based on the educational theory , students increase their learning motivation , which can increase learning effectiveness . In this paper , we propose a concept map for each student and staff . This map finds the result of the subjects and also recommends a sequence of remedial teaching . Here , rough set theory is used for dealing with uncertainty in the hidden pattern of data . For each competence the lower and upper approximations are calculated based on the brainstorm maps .
The class of quasiseparable matrices is defined by a pair of bounds , called the quasiseparable orders , on the ranks of the maximal sub-matrices entirely located in their strictly lower and upper triangular parts . These arise naturally in applications , as e . g . the inverse of band matrices , and are widely used for they admit structured representations allowing to compute with them in time linear in the dimension and quadratic with the quasiseparable order . We show , in this paper , the connection between the notion of quasisepa-rability and the rank profile matrix invariant , presented in [Dumas \& al . ISSAC ' 00] . This allows us to propose an algorithm computing the quasiseparable orders ( rL , rU ) in time O ( n^0 s^ ( $\omega$--0 ) ) where s = max ( rL , rU ) and $\omega$ the exponent of matrix multiplication . We then present two new structured representations , a binary tree of PLUQ decompositions , and the Bruhat generator , using respectively O ( ns log n/s ) and O ( ns ) field elements instead of O ( ns^0 ) for the previously known generators . We present algorithms computing these representations in time O ( n^0 s^ ( $\omega$--0 ) ) . These representations allow a matrix-vector product in time linear in the size of their representation . Lastly we show how to multiply two such structured matrices in time O ( n^0 s^ ( $\omega$--0 ) ) .
In this paper we address cardinality estimation problem which is an important subproblem in query optimization . Query optimization is a part of every relational DBMS responsible for finding the best way of the execution for the given query . These ways are called plans . The execution time of different plans may differ by several orders , so query optimizer has a great influence on the whole DBMS performance . We consider cost-based query optimization approach as the most popular one . It was observed that cost-based optimization quality depends much on cardinality estimation quality . Cardinality of the plan node is the number of tuples returned by it . In the paper we propose a novel cardinality estimation approach with the use of machine learning methods . The main point of the approach is using query execution statistics of the previously executed queries to improve cardinality estimations . We called this approach adaptive cardinality estimation to reflect this point . The approach is general , flexible , and easy to implement . The experimental evaluation shows that this approach significantly increases the quality of cardinality estimation , and therefore increases the DBMS performance for some queries by several times or even by several dozens of times .
This paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions . To this purpose , a sequence of Face Image Descriptors ( FID ) is regarded as the output of a Linear Time Invariant ( LTI ) system . The temporal dynamics of such sequence of descriptors are represented by means of a Hankel matrix . The paper presents different strategies to compute dynamics-based representation of a sequence of FID , and reports classification accuracy values of the proposed representations within different standard classification frameworks . The representations have been validated in two very challenging application domains : emotion recognition and pain detection . Experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based FID representation attains competitive performance when off-the-shelf classification tools are adopted .
We consider pricing in settings where a consumer discovers his value for a good only as he uses it , and the value evolves with each use . We explore simple and natural pricing strategies for a seller in this setting , under the assumption that the seller knows the distribution from which the consumer ' s initial value is drawn , as well as the stochastic process that governs the evolution of the value with each use . We consider the differences between up-front or " buy-it-now " pricing ( BIN ) , and " pay-per-play " ( PPP ) pricing , where the consumer is charged per use . Our results show that PPP pricing can be a very effective mechanism for price discrimination , and thereby can increase seller revenue . But it can also be advantageous to the buyers , as a way of mitigating risk . Indeed , this mitigation of risk can yield a larger pool of buyers . We also show that the practice of offering free trials is largely beneficial . We consider two different stochastic processes for how the buyer ' s value evolves : In the first , the key random variable is how long the consumer remains interested in the product . In the second process , the consumer ' s value evolves according to a random walk or Brownian motion with reflection at 0 , and absorption at 0 .
We study two-player zero-sum games over infinite-state graphs with boundedness conditions . Our first contribution is about the strategy complexity , i . e the memory required for winning strategies : we prove that over general infinite-state graphs , memoryless strategies are sufficient for finitary B\ " uchi games , and finite-memory suffices for finitary parity games . We then study pushdown boundedness games , with two contributions . First we prove a collapse result for pushdown omega B games , implying the decidability of solving these games . Second we consider pushdown games with finitary parity along with stack boundedness conditions , and show that solving these games is EXPTIME-complete .
Code cloning is not only assumed to inflate maintenance costs but also considered defect-prone as inconsistent changes to code duplicates can lead to unexpected behavior . Consequently , the identification of duplicated code , clone detection , has been a very active area of research in recent years . Up to now , however , no substantial investigation of the consequences of code cloning on program correctness has been carried out . To remedy this shortcoming , this paper presents the results of a large-scale case study that was undertaken to find out if inconsistent changes to cloned code can indicate faults . For the analyzed commercial and open source systems we not only found that inconsistent changes to clones are very frequent but also identified a significant number of faults induced by such changes . The clone detection tool used in the case study implements a novel algorithm for the detection of inconsistent clones . It is available as open source to enable other researchers to use it as basis for further investigations .
In this paper we investigate the problem of detecting dynamically evolving signals . We model the signal as an $n$ dimensional vector that is either zero or has $s$ non-zero components . At each time step $t\in \mathbb{N}$ the non-zero components change their location independently with probability $p$ . The statistical problem is to decide whether the signal is a zero vector or in fact it has non-zero components . This decision is based on $m$ noisy observations of individual signal components collected at times $t=0 , \ldots , m$ . We consider two different sensing paradigms , namely adaptive and non-adaptive sensing . For non-adaptive sensing the choice of components to measure has to be decided before the data collection process started , while for adaptive sensing one can adjust the sensing process based on observations collected earlier . We characterize the difficulty of this detection problem in both sensing paradigms in terms of the aforementioned parameters , with special interest to the speed of change of the active components . In addition we provide an adaptive sensing algorithm for this problem and contrast its performance to that of non-adaptive detection algorithms .
This paper introduces and investigates decision problems for numberless probabilistic automata , i . e . probabilistic automata where the support of each probabilistic transitions is specified , but the exact values of the probabilities are not . A numberless probabilistic automaton can be instantiated into a probabilistic automaton by specifying the exact values of the non-zero probabilistic transitions . We show that the two following properties of numberless probabilistic automata are recursively inseparable : - all instances of the numberless automaton have value 0 , - no instance of the numberless automaton has value 0 .
We study maximum likelihood estimation in Gaussian graphical models from a geometric point of view . An algebraic elimination criterion allows us to find exact lower bounds on the number of observations needed to ensure that the maximum likelihood estimator ( MLE ) exists with probability one . This is applied to bipartite graphs , grids and colored graphs . We also study the ML degree , and we present the first instance of a graph for which the MLE exists with probability one , even when the number of observations equals the treewidth .
We describe ergodic properties of some Metropolis-Hastings ( MH ) algorithms for heavy-tailed target distributions . The analysis usually falls into sub-geometric ergodicity framework but we prove that the mixed preconditioned Crank-Nicolson ( MpCN ) algorithm has geometric ergodicity even for heavy-tailed target distributions . This useful property comes from the fact that the MpCN algorithm becomes a random-walk Metropolis algorithm under suitable transformation .
Tests of separate families of hypotheses were initially considered by Cox ( 0000 , 0000 ) In this work , the Fully Bayesian Significance Test , FBST , is evaluated for discriminating between the lognormal , gamma and Weibull models whose families of distributions are separate . Considering a linear mixture model including all candidate distributions , the FBST tests the hypotheses on the mixture weights in order to calculate the evidence measure in favor of each one . Additionally , the density functions of the mixture components are reparametrized in terms of the common parameters , the mean and the variance of the population , since the comparison between the models is based on the same dataset , i . e , on the same population . Reparametrizing the models in terms of the common parameters also allows one to reduce the number of the parameters to be estimated . In order to evaluate the performance of the procedure , some numerical results based on simulated sample points are given . In these simulations , the results of FBST are compared with those of the Cox test . Two applications examples illustrating the procedure for uncensored dataset are also presented . Keywords : Model choice ; Separate Models ; Mixture model ; Significance test ; FBST ; Cox Test
The usage of process choreographies and decentralized Business Process Management Systems has been named as an alternative to centralized business process orchestration . In choreographies , control over a process instance is shared between independent parties , and no party has full control or knowledge during process runtime . Nevertheless , it is necessary to monitor and verify process instances during runtime for purposes of documentation , accounting , or compensation . To achieve business process runtime verification , this work explores the suitability of the Bitcoin blockchain to create a novel solution for choreographies . The resulting approach is realized in a fully-functional software prototype . This software solution is evaluated in a qualitative comparison . Findings show that our blockchain-based approach enables a seamless execution monitoring and verification of choreographies , while at the same time preserving anonymity and independence of the process participants . Furthermore , the prototype is evaluated in a performance analysis .
We consider the problem of finding the minimizer of a convex function $F : \mathbb R^d \rightarrow \mathbb R$ of the form $F ( w ) : = \sum_{i=0}^n f_i ( w ) + R ( w ) $ where a low-rank factorization of $\nabla^0 f_i ( w ) $ is readily available . We consider the regime where $n \gg d$ . As second-order methods prove to be effective in finding the minimizer to a high-precision , in this work , we propose randomized Newton-type algorithms that exploit \textit{non-uniform} sub-sampling of $\{\nabla^0 f_i ( w ) \}_{i=0}^{n}$ , as well as inexact updates , as means to reduce the computational complexity . Two non-uniform sampling distributions based on {\it block norm squares} and {\it block partial leverage scores} are considered in order to capture important terms among $\{\nabla^0 f_i ( w ) \}_{i=0}^{n}$ . We show that at each iteration non-uniformly sampling at most $\mathcal O ( d \log d ) $ terms from $\{\nabla^0 f_i ( w ) \}_{i=0}^{n}$ is sufficient to achieve a linear-quadratic convergence rate in $w$ when a suitable initial point is provided . In addition , we show that our algorithms achieve a lower computational complexity and exhibit more robustness and better dependence on problem specific quantities , such as the condition number , compared to similar existing methods , especially the ones based on uniform sampling . Finally , we empirically demonstrate that our methods are at least twice as fast as Newton ' s methods with ridge logistic regression on several real datasets .
Objective : Modelling the associations from high-throughput experimental molecular data has provided unprecedented insights into biological pathways and signalling mechanisms . Graphical models and networks have especially proven to be useful abstractions in this regard . Ad-hoc thresholds are often used in conjunction with structure learning algorithms to determine significant associations . The present study overcomes this limitation by proposing a statistically-motivated approach for identifying significant associations in a network . Methods and Materials : A new method that identifies significant associations in graphical models by estimating the threshold minimising the $L_{\mathrm{0}}$ norm between the cumulative distribution function ( CDF ) of the observed edge confidences and those of its asymptotic counterpart is proposed . The effectiveness of the proposed method is demonstrated on popular synthetic data sets as well as publicly available experimental molecular data corresponding to gene and protein expression profiles . Results : The improved performance of the proposed approach is demonstrated across the synthetic data sets using sensitivity , specificity and accuracy as performance metrics . The results are also demonstrated across varying sample sizes and three different structure learning algorithms with widely varying assumptions . In all cases , the proposed approach has specificity and accuracy close to 0 , while sensitivity increases linearly in the logarithm of the sample size . The estimated threshold systematically outperforms common ad-hoc ones in terms of sensitivity while maintaining comparable levels of specificity and accuracy . Networks from experimental data sets are reconstructed accurately with respect to the results from the original papers .
Gaussian processes ( GP ) provide a prior over functions and allow finding complex regularities in data . Gaussian processes are successfully used for classification/regression problems and dimensionality reduction . In this work we consider the classification problem only . The complexity of standard methods for GP-classification scales cubically with the size of the training dataset . This complexity makes them inapplicable to big data problems . Therefore , a variety of methods were introduced to overcome this limitation . In the paper we focus on methods based on so called inducing inputs . This approach is based on variational inference and proposes a particular lower bound for marginal likelihood ( evidence ) . This bound is then maximized w . r . t . parameters of kernel function of the Gaussian process , thus fitting the model to data . The computational complexity of this method is $O ( nm^0 ) $ , where $m$ is the number of inducing inputs used by the model and is assumed to be substantially smaller than the size of the dataset $n$ . Recently , a new evidence lower bound for GP-classification problem was introduced . It allows using stochastic optimization , which makes it suitable for big data problems . However , the new lower bound depends on $O ( m^0 ) $ variational parameter , which makes optimization challenging in case of big m . In this work we develop a new approach for training inducing input GP models for classification problems . Here we use quadratic approximation of several terms in the aforementioned evidence lower bound , obtaining analytical expressions for optimal values of most of the parameters in the optimization , thus sufficiently reducing the dimension of optimization space . In our experiments we achieve as well or better results , compared to the existing method . Moreover , our method doesn ' t require the user to manually set the learning rate , making it more practical , than the existing method .
It has been shown that some macroeconomic time series , especially those where outliers could be present , can be well modelled using heavy tailed distributions for the noise components . Methods for deciding when and where heavy-tailed models should be preferred are investigated . These investigations primarily focus on automatic methods for model identification and selection . Current methods are extended to incorporate a non-Gaussian selection element , and various different criteria for deciding on which overall model should be used are examined .
This paper addresses the inference of spatial dependence in the context of a recently proposed framework . More specifically , the paper focuses on the estimation of model parameters for a class of generalized Gibbs random fields , i . e . , Spartan Spatial Random Fields ( SSRFs ) . The problem of parameter inference is based on the minimization of a distance metric . The latter involves a specifically designed distance between sample constraints ( variance , generalized ``gradient ' ' and ``curvature ' ' ) and their ensemble counterparts . The general principles used in the construction of the metric are discussed and intuitively motivated . In order to enable calculation of the metric from sample data , estimators for generalized ``gradient ' ' and ``curvature ' ' constraints are constructed . These estimators , which are not restricted to SSRFs , are formulated using compactly supported kernel functions . An intuitive method for kernel bandwidth selection is proposed . It is proved that the estimators are asymptotically unbiased and consistent for differentiable random fields , under specified regularity conditions . For continuous but non-differentiable random fields , it is shown that the estimators are asymptotically consistent . The bias is calculated explicitly for different kernel functions . The performance of the sample constraint estimators and the SSRF inference process are investigated by means of numerical simulations .
Recurrent neural networks ( RNNs ) have been used extensively and with increasing success to model various types of sequential data . Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data , such as long range dependency or localized attention phenomena . However , while many sequential data ( such as video , speech or language ) can have highly variable information flow , most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step , which can be detrimental to both speed and model capacity . In this paper , we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step , without prior knowledge of the sequence ' s time structure . We show experimentally that not only do our models require fewer operations , they also lead to better performance overall on evaluation tasks .
This issue discusses the fault-trajectory approach suitability for fault diagnosis on analog networks . Recent works have shown promising results concerning a method based on this concept for ATPG for diagnosing faults on analog networks . Such method relies on evolutionary techniques , where a generic algorithm ( GA ) is coded to generate a set of optimum frequencies capable to disclose faults .
This article provides an original understanding of the behavior of a class of graph-oriented semi-supervised learning algorithms in the limit of large and numerous data . It is demonstrated that the intuition at the root of these methods collapses in this limit and that , as a result , most of them become inconsistent . Corrective measures and a new data-driven parametrization scheme are proposed along with a theoretical analysis of the asymptotic performances of the resulting approach . A surprisingly close behavior between theoretical performances on Gaussian mixture models and on real datasets is also illustrated throughout the article , thereby suggesting the importance of the proposed analysis for dealing with practical data . As a result , significant performance gains are observed on practical data classification using the proposed parametrization .
The theoretical ability of modular robots to reconfigure in response to complex tasks in a priori unknown environments has frequently been cited as an advantage , but has never been experimentally demonstrated . For the first time , we present a system that integrates perception , high-level mission planning , and modular robot hardware , allowing a modular robot to autonomously reconfigure in response to an a priori unknown environment in order to complete high-level tasks . Three hardware experiments validate the system , and demonstrate a modular robot autonomously exploring , reconfiguring , and manipulating objects to complete high-level tasks in unknown environments . We present system architecture , software and hardware in a general framework that enables modular robots to solve tasks in unknown environments using autonomous , reactive reconfiguration . The physical robot is composed of modules that support multiple robot configurations . An onboard 0D sensor provides information about the environment and informs exploration , reconfiguration decision making and feedback control . A centralized high-level mission planner uses information from the environment and the user-specified task description to autonomously compose low-level controllers to perform locomotion , reconfiguration , and other behaviors . A novel , centralized self-reconfiguration method is used to change robot configurations as needed .
The block Lanczos algorithm proposed by Peter Montgomery is an efficient means to tackle the sparse linear algebra problem which arises in the context of the number field sieve factoring algorithm and its predecessors . We present here a modified version of the algorithm , which incorporates several improvements : we discuss how to efficiently handle homogeneous systems and how to reduce the number of vectors stored in the course of the computation . We also provide heuristic justification for the success probability of our modified algorithm . While the overall complexity and expected number of steps of the block Lanczos is not changed by the modifications presented in this article , we expect these to be useful for implementations of the block Lanczos algorithm where the storage of auxiliary vectors sometimes has a non-negligible cost . 0 Linear systems for integer factoring For factoring a composite integer N , algorithms based on the technique of combination of congruences look for several pairs of integers ( x , y ) such that x 0 $\not\equiv$ y 0 mod N . This equality is hoped to be non trivial for at least one of the obtained pairs , letting gcd ( x -- y , N ) unveil a factor of the integer N . Several algorithms use this strategy : the CFRAC algorithm , the quadratic sieve and its variants , and the number field sieve . Pairs ( x , y ) as above are obtained by combining relations which have been collected as a step of these algorithms . Relations are written multiplicatively as a set of valuations . All the algorithms considered seek a multiplicative combination of these relations which can be rewritten as an equality of squares . This is achieved by solving a system of linear equations defined over F 0 , where equations are parity constraints on
The paper provides an analysis of the voting method known as delegable proxy voting , or liquid democracy . The analysis first positions liquid democracy within the theory of binary aggregation . It then focuses on two issues of the system : the occurrence of delegation cycles ; and the effect of delegations on individual rationality when voting on logically interdependent propositions . It finally points to proposals on how the system may be modified in order to address the above issues .
The past few years have seen a surge of interest in the field of probabilistic logic learning and statistical relational learning . In this endeavor , many probabilistic logics have been developed . ProbLog is a recent probabilistic extension of Prolog motivated by the mining of large biological networks . In ProbLog , facts can be labeled with probabilities . These facts are treated as mutually independent random variables that indicate whether these facts belong to a randomly sampled program . Different kinds of queries can be posed to ProbLog programs . We introduce algorithms that allow the efficient execution of these queries , discuss their implementation on top of the YAP-Prolog system , and evaluate their performance in the context of large networks of biological entities .
Statistical graphics and data visualization have long histories , but their modern forms began only in the early 0000s . Between roughly 0000 and 0000 ( $\pm00$ ) , an explosive growth occurred in both the general use of graphic methods and the range of topics to which they were applied . Innovations were prodigious and some of the most exquisite graphics ever produced appeared , resulting in what may be called the ``Golden Age of Statistical Graphics . ' ' In this article I trace the origins of this period in terms of the infrastructure required to produce this explosive growth : recognition of the importance of systematic data collection by the state ; the rise of statistical theory and statistical thinking ; enabling developments of technology ; and inventions of novel methods to portray statistical data . To illustrate , I describe some specific contributions that give rise to the appellation ``Golden Age . ' '
Let $\mathbf{Y}=\mathbf{X}\bolds{\Theta}\mathbf{Z} ' +\bolds{\mathcal {E}}$ be the growth curve model with $\bolds{\mathcal{E}}$ distributed with mean $\mathbf{0}$ and covariance $\mathbf{I}_n\otimes\bolds{\Sigma}$ , where $\bolds{\Theta}$ , $\bolds{\Sigma}$ are unknown matrices of parameters and $\mathbf{X}$ , $\mathbf{Z}$ are known matrices . For the estimable parametric transformation of the form $\bolds {\gamma}=\mathbf{C}\bolds{\Theta}\mathbf{D} ' $ with given $\mathbf{C}$ and $\mathbf{D}$ , the two-stage generalized least-squares estimator $\hat{\bolds \gamma} ( \mathbf{Y} ) $ defined in ( 0 ) converges in probability to $\bolds\gamma$ as the sample size $n$ tends to infinity and , further , $\sqrt{n}[\hat{\bolds{\gamma}} ( \mathbf{Y} ) -\bolds {\gamma}]$ converges in distribution to the multivariate normal distribution $\ma thcal{N} ( \mathbf{0} , ( \mathbf{C}\mathbf{R}^{-0}\mathbf{C} ' ) \otimes ( \mat hbf{D} ( \mathbf{Z} ' \bolds{\Sigma}^{-0}\mathbf{Z} ) ^{-0}\mathbf{D} ' ) ) $ under the condition that $\lim_{n\to\infty}\mathbf{X} ' \mathbf{X}/n=\mathbf{R}$ for some positive definite matrix $\mathbf{R}$ . Moreover , the unbiased and invariant quadratic estimator $\hat{\bolds{\Sigma}} ( \mathbf{Y} ) $ defined in ( 0 ) is also proved to be consistent with the second-order parameter matrix $\bolds{\Sigma}$ .
We present deviation bounds for self-normalized averages and applications to estimation with a random number of observations . The results rely on a peeling argument in exponential martingale techniques that represents an alternative to the method of mixture . The motivating examples of bandit problems and context tree estimation are detailed .
Importance Sampling ( IS ) is a well-known Monte Carlo technique that approximates integrals involving a posterior distribution by means of weighted samples . In this work , we study the assignation of a single weighted sample which compresses the information contained in a population of weighted samples . Part of the theory that we present as Group Importance Sampling ( GIS ) has been employed implicitly in different works in the literature . The provided analysis yields several theoretical and practical consequences . For instance , we discuss the application of GIS into the Sequential Importance Resampling framework and show that Independent Multiple Try Metropolis schemes can be interpreted as a standard Metropolis-Hastings algorithm , following the GIS approach . We also introduce two novel Markov Chain Monte Carlo ( MCMC ) techniques based on GIS . The first one , named Group Metropolis Sampling method , produces a Markov chain of sets of weighted samples . All these sets are then employed for obtaining a unique global estimator . The second one is the Distributed Particle Metropolis-Hastings technique , where different parallel particle filters are jointly used to drive an MCMC algorithm . Different resampled trajectories are compared and then tested with a proper acceptance probability . The novel schemes are tested in different numerical experiments such as learning the hyperparameters of Gaussian Processes , the localization problem in a wireless sensor network and the tracking of vegetation parameters given satellite observations , where they are compared with several benchmark Monte Carlo techniques . Three illustrative Matlab demos are also provided .
We present a geometrical method for analyzing sequential estimating procedures . It is based on the design principle of the second-order efficient sequential estimation provided in Okamoto , Amari and Takeuchi ( 0000 ) . By introducing a dual conformal curvature quantity , we clarify the conditions for the covariance minimization of sequential estimators . These conditions are further elabolated for the multidimensional curved exponential family . The theoretical results are then numerically examined by using typical statistical models , von Mises-Fisher and hyperboloid models .
Motivation : Single cell transcriptome sequencing ( scRNA-Seq ) has become a revolutionary tool to study cellular and molecular processes at single cell resolution . Among existing technologies , the recently developed droplet-based platform enables efficient parallel processing of thousands of single cells with direct counting of transcript copies using Unique Molecular Identifier ( UMI ) . Despite the technology advances , statistical methods and computational tools are still lacking for analyzing droplet-based scRNA-Seq data . Particularly , model-based approaches for clustering large-scale single cell transcriptomic data are still under-explored . Methods : We developed DIMM-SC , a Dirichlet Mixture Model for clustering droplet-based Single Cell transcriptomic data . This approach explicitly models UMI count data from scRNA-Seq experiments and characterizes variations across different cell clusters via a Dirichlet mixture prior . An expectation-maximization algorithm is used for parameter inference . Results : We performed comprehensive simulations to evaluate DIMM-SC and compared it with existing clustering methods such as K-means , CellTree and Seurat . In addition , we analyzed public scRNA-Seq datasets with known cluster labels and in-house scRNA-Seq datasets from a study of systemic sclerosis with prior biological knowledge to benchmark and validate DIMM-SC . Both simulation studies and real data applications demonstrated that overall , DIMM-SC achieves substantially improved clustering accuracy and much lower clustering variability compared to other existing clustering methods . More importantly , as a model-based approach , DIMM-SC is able to quantify the clustering uncertainty for each single cell , facilitating rigorous statistical inference and biological interpretations , which are typically unavailable from existing clustering methods .
We study the following problem : Given $k$ paths that share the same vertex set , is there a simultaneous geometric embedding of these paths such that each individual drawing is monotone in some direction ? We prove that for any dimension $d \geq 0$ , there is a set of $d+0$ paths that does not admit a monotone simultaneous geometric embedding .
In this study we propose a novel method to successfully detect the ADRs using feature matrix and feature selection . A feature matrix , which characterizes the medical events before patients take drugs or after patients take drugs , is created from THIN database . The feature selection method of Student ' s t-test is used to detect the significant features from thousands of medical events . The significant ADRs , which are corresponding to significant features , are detected . Experiments are performed on the drug Pioglitazone . Compared to other computerized methods , our proposed method achieves good performance .
Electronic commerce ( a . k . a . E-commerce ) systems such as eBay and Taobao of Alibaba are becoming increasingly popular . Having an effective reputation system is critical to this type of internet service because it can assist buyers to evaluate the trustworthiness of sellers , and it can also improve the revenue for reputable sellers and E-commerce operators . We formulate a stochastic model to analyze an eBay-like reputation system and propose four measures to quantify its effectiveness : ( 0 ) new seller ramp up time , ( 0 ) new seller drop out probability , ( 0 ) long term profit gains for sellers , and ( 0 ) average per seller transaction gains for the E-commerce operator . Through our analysis , we identify key factors which influence these four measures . We propose a new insurance mechanism which consists of an insurance protocol and a transaction mechanism to improve the above four measures . We show that our insurance mechanism can reduce the ramp up time by around 00 . 0% , and guarantee new sellers ramp up before the deadline $T_w$ with a high probability ( close to 0 . 0 ) . It also increases the long term profit gains and average per seller transaction gains by at least 00 . 0% .
Coalition formation typically involves the coming together of multiple , heterogeneous , agents to achieve both their individual and collective goals . In this paper , we focus on a special case of coalition formation known as Graph-Constrained Coalition Formation ( GCCF ) whereby a network connecting the agents constrains the formation of coalitions . We focus on this type of problem given that in many real-world applications , agents may be connected by a communication network or only trust certain peers in their social network . We propose a novel representation of this problem based on the concept of edge contraction , which allows us to model the search space induced by the GCCF problem as a rooted tree . Then , we propose an anytime solution algorithm ( CFSS ) , which is particularly efficient when applied to a general class of characteristic functions called $m+a$ functions . Moreover , we show how CFSS can be efficiently parallelised to solve GCCF using a non-redundant partition of the search space . We benchmark CFSS on both synthetic and realistic scenarios , using a real-world dataset consisting of the energy consumption of a large number of households in the UK . Our results show that , in the best case , the serial version of CFSS is 0 orders of magnitude faster than the state of the art , while the parallel version is 0 . 00 times faster than the serial version on a 00-core machine . Moreover , CFSS is the first approach to provide anytime approximate solutions with quality guarantees for very large systems of agents ( i . e . , with more than 0000 agents ) .
An important aspect in systems of multiple autonomous agents is the exploitation of synergies via coalition formation . In this paper , we solve various open problems concerning the computational complexity of stable partitions in additively separable hedonic games . First , we propose a polynomial-time algorithm to compute a contractually individually stable partition . This contrasts with previous results such as the NP-hardness of computing individually stable or Nash stable partitions . Secondly , we prove that checking whether the core or the strict core exists is NP-hard in the strong sense even if the preferences of the players are symmetric . Finally , it is shown that verifying whether a partition consisting of the grand coalition is contractually strict core stable or Pareto optimal is coNP-complete .
Text to image transformation for input to neural networks requires intermediate steps . This paper attempts to present a new approach to pixel normalization so as to convert textual data into image , suitable as input for neural networks . This method can be further improved by its Graphics Processing Unit ( GPU ) implementation to provide significant speedup in computational time .
A salient feature of the manned spacecraft is the predominance of discrete information in the manual control loop of the onboard systems . Specifically , command-signaling control panels ( CSCP ) as a subsystem of the manual control loop are widely used in the Russian manned spacecraft . In this paper CSCP are classified into four types : a ) control panels based on multi-channel control ; b ) control panels based on command-information compression ; c ) control panels based on command and signaling information compression ; d ) integrated control consoles ( ICC ) based on computer and information technology . It is shown that ICC underlies modern information display systems ( IDS ) . ICC first appeared in the Russian manned space program in the IDS of the Soyuz-TMA spacecraft and the Russian modules of the International Space Station . Results of engineering and psychological studies of different types of panels are produced .
We study the problem of allocating multiple objects to agents without transferable utilities , where each agent may receive more than one object according to a quota . Under lexicographic preferences , we characterize the set of strategyproof , non-bossy , and neutral quota mechanisms and show that under a mild Pareto efficiency condition , serial dictatorship quota mechanisms are the only mechanisms satisfying these properties . Dropping the neutrality requirement , this class of quota mechanisms further expands to sequential dictatorship quota mechanisms . We then extend quota mechanisms to randomized settings , and show that the random serial dictatorship quota mechanisms ( RSDQ ) are envyfree , strategyproof , and ex post efficient for any number of agents and objects and any quota system , proving that the well-studied Random Serial Dictatorship ( RSD ) satisfies envyfreeness when preferences are lexicographic .
This paper proposes a Low-Power , Energy Efficient 0-bit Binary Coded Decimal ( BCD ) adder design where the conventional 0-bit BCD adder has been modified with the Clock Gated Power Gating Technique . Moreover , the concept of DVT ( Dual-vth ) scheme has been introduced while designing the full adder blocks to reduce the Leakage Power , as well as , to maintain the overall performance of the entire circuit . The reported architecture of 0-bit BCD adder is designed using 00 nm technology and it consumes 0 . 000 {\mu}Watt of Average Power while operating with a frequency of 000 MHz , and a Supply Voltage ( Vdd ) of 0 Volt . The results obtained from different simulation runs on SPICE , indicate the superiority of the proposed design compared to the conventional 0-bit BCD adder . Considering the product of Average Power and Delay , for the operating frequency of 000 MHz , a fair 00 . 00 % reduction compared to the conventional design has been achieved with this proposed scheme .
Recent literature on online learning has focused on developing adaptive algorithms that take advantage of a regularity of the sequence of observations , yet retain worst-case performance guarantees . A complementary direction is to develop prediction methods that perform well against complex benchmarks . In this paper , we address these two directions together . We present a fully adaptive method that competes with dynamic benchmarks in which regret guarantee scales with regularity of the sequence of cost functions and comparators . Notably , the regret bound adapts to the smaller complexity measure in the problem environment . Finally , we apply our results to drifting zero-sum , two-player games where both players achieve no regret guarantees against best sequences of actions in hindsight .
The LASSO-Patternsearch algorithm is proposed to efficiently identify patterns of multiple dichotomous risk factors for outcomes of interest in demographic and genomic studies . The patterns considered are those that arise naturally from the log linear expansion of the multivariate Bernoulli density . The method is designed for the case where there is a possibly very large number of candidate patterns but it is believed that only a relatively small number are important . A LASSO is used to greatly reduce the number of candidate patterns , using a novel computational algorithm that can handle an extremely large number of unknowns simultaneously . The patterns surviving the LASSO are further pruned in the framework of ( parametric ) generalized linear models . A novel tuning procedure based on the GACV for Bernoulli outcomes , modified to act as a model selector , is used at both steps . We applied the method to myopia data from the population-based Beaver Dam Eye Study , exposing physiologically interesting interacting risk factors . We then applied the method to data from a generative model of Rheumatoid Arthritis based on Problem 0 from the Genetic Analysis Workshop 00 , successfully demonstrating its potential to efficiently recover higher order patterns from attribute vectors of length typical of genomic studies .
Modern cars exist in an vast number of variants . Thus , variability has to be dealt with in all phases of the development process , in particular during model-based development of software-intensive functionality using Matlab/Simulink . Currently , variability is often encoded within a functional model leading to so called 000%-models which easily become very complex and do not scale for larger product lines . To counter these problems , we propose a modular variability modeling approach for Matlab/Simulink based on the concept of delta modeling [0 , 0 , 00] . A functional variant is described by a delta encapsulating a set of modifications . A sequence of deltas can be applied to a core product to derive the desired variant . We present a prototypical implementation , which is integrated into Matlab/Simulink and offers graphical editing of delta models .
We propose a new computational approach for tracking and detecting statistically significant linguistic shifts in the meaning and usage of words . Such linguistic shifts are especially prevalent on the Internet , where the rapid exchange of ideas can quickly change a word ' s meaning . Our meta-analysis approach constructs property time series of word usage , and then uses statistically sound change point detection algorithms to identify significant linguistic shifts . We consider and analyze three approaches of increasing complexity to generate such linguistic property time series , the culmination of which uses distributional characteristics inferred from word co-occurrences . Using recently proposed deep neural language models , we first train vector representations of words for each time period . Second , we warp the vector spaces into one unified coordinate system . Finally , we construct a distance-based distributional time series for each word to track it ' s linguistic displacement over time . We demonstrate that our approach is scalable by tracking linguistic change across years of micro-blogging using Twitter , a decade of product reviews using a corpus of movie reviews from Amazon , and a century of written books using the Google Book-ngrams . Our analysis reveals interesting patterns of language usage change commensurate with each medium .
To improve the performance of speaker identification systems , an effective and robust method is proposed to extract speech features , capable of operating in noisy environment . Based on the time-frequency multi-resolution property of wavelet transform , the input speech signal is decomposed into various frequency channels . For capturing the characteristic of the signal , the Mel-Frequency Cepstral Coefficients ( MFCCs ) of the wavelet channels are calculated . Hidden Markov Models ( HMMs ) were used for the recognition stage as they give better recognition for the speaker ' s features than Dynamic Time Warping ( DTW ) . Comparison of the proposed approach with the MFCCs conventional feature extraction method shows that the proposed method not only effectively reduces the influence of noise , but also improves recognition . A recognition rate of 00 . 0% was obtained using the proposed feature extraction technique compared to 00 . 0% using the MFCCs . When the test patterns were corrupted by additive white Gaussian noise with 00 dB S/N ratio , the recognition rate was 00 . 0% using the proposed method compared to 00 . 0% using the MFCCs .
This paper deals with certain estimation problems involving the covariance matrix in large dimensions . Due to the breakdown of finite-dimensional asymptotic theory when the dimension is not negligible with respect to the sample size , it is necessary to resort to an alternative framework known as large-dimensional asymptotics . Recently , Ledoit and Wolf ( 0000 ) have proposed an estimator of the eigenvalues of the population covariance matrix that is consistent according to a mean-square criterion under large-dimensional asymptotics . It requires numerical inversion of a multivariate nonrandom function which they call the QuEST function . The present paper explains how to numerically implement the QuEST function in practice through a series of six successive steps . It also provides an algorithm to compute the Jacobian analytically , which is necessary for numerical inversion by a nonlinear optimizer . Monte Carlo simulations document the effectiveness of the code .
Variational methods are widely used for approximate posterior inference . However , their use is typically limited to families of distributions that enjoy particular conjugacy properties . To circumvent this limitation , we propose a family of variational approximations inspired by nonparametric kernel density estimation . The locations of these kernels and their bandwidth are treated as variational parameters and optimized to improve an approximate lower bound on the marginal likelihood of the data . Using multiple kernels allows the approximation to capture multiple modes of the posterior , unlike most other variational approximations . We demonstrate the efficacy of the nonparametric approximation with a hierarchical logistic regression model and a nonlinear matrix factorization model . We obtain predictive performance as good as or better than more specialized variational methods and sample-based approximations . The method is easy to apply to more general graphical models for which standard variational methods are difficult to derive .
Predictive state representations ( PSRs ) offer an expressive framework for modelling partially observable systems . By compactly representing systems as functions of observable quantities , the PSR learning approach avoids using local-minima prone expectation-maximization and instead employs a globally optimal moment-based algorithm . Moreover , since PSRs do not require a predetermined latent state structure as an input , they offer an attractive framework for model-based reinforcement learning when agents must plan without a priori access to a system model . Unfortunately , the expressiveness of PSRs comes with significant computational cost , and this cost is a major factor inhibiting the use of PSRs in applications . In order to alleviate this shortcoming , we introduce the notion of compressed PSRs ( CPSRs ) . The CPSR learning approach combines recent advancements in dimensionality reduction , incremental matrix decomposition , and compressed sensing . We show how this approach provides a principled avenue for learning accurate approximations of PSRs , drastically reducing the computational costs associated with learning while also providing effective regularization . Going further , we propose a planning framework which exploits these learned models . And we show that this approach facilitates model-learning and planning in large complex partially observable domains , a task that is infeasible without the principled use of compression .
This paper presents an experience report about an experiment that evaluates duration and effort of pair and solo programming . The experiment was performed as part of a course on Design of Experiments ( DOE ) in Software Engineering ( SE ) at Autonomous University of Yucatan ( UADY ) . A total of 00 junior student subjects enrolled in the bachelor ' s degree program in SE participated in the experiment . During the experiment , subjects ( 0 pairs and 0 solos ) wrote two small programs in two sessions . Results show a significant difference ( at alpha=0 . 0 ) in favor of pair programming regarding duration ( 00% decrease ) , and a significant difference ( at alpha=0 . 0 ) in favor of solo programming with respect to effort ( 00% decrease ) . With only a difference of 0% , our results regarding duration and effort are practically the same as those reported by Nosek in 0000 .
Cryo-electron microscopy ( cryo-EM ) studies using single particle reconstruction are extensively used to reveal structural information on macromolecular complexes . Aiming at the highest achievable resolution , state of the art electron microscopes automatically acquire thousands of high-quality micrographs . Particles are detected on and boxed out from each micrograph using fully- or semi-automated approaches . However , the obtained particles still require laborious manual post-picking classification , which is one major bottleneck for single particle analysis of large datasets . We introduce MAPPOS , a supervised post-picking strategy for the classification of boxed particle images , as additional strategy adding to the already efficient automated particle picking routines . MAPPOS employs machine learning techniques to train a robust classifier from a small number of characteristic image features . In order to accurately quantify the performance of MAPPOS we used simulated particle and non-particle images . In addition , we verified our method by applying it to an experimental cryo-EM dataset and comparing the results to the manual classification of the same dataset . Comparisons between MAPPOS and manual post-picking classification by several human experts demonstrated that merely a few hundred sample images are sufficient for MAPPOS to classify an entire dataset with a human-like performance . MAPPOS was shown to greatly accelerate the throughput of large datasets by reducing the manual workload by orders of magnitude while maintaining a reliable identification of non-particle images .
Putting the DRAM on the same package with a processor enables several times higher memory bandwidth than conventional off-package DRAM . Yet , the latency of in-package DRAM is not appreciably lower than that of off-package DRAM . A promising use of in-package DRAM is as a large cache . Unfortunately , most previous DRAM cache designs mainly optimize for hit latency and do not consider off-chip bandwidth efficiency as a first-class design constraint . Hence , as we show in this paper , these designs are suboptimal for use with in-package DRAM . We propose a new DRAM cache design , Banshee , that optimizes for both in- and off-package DRAM bandwidth efficiency without degrading access latency . The key ideas are to eliminate the in-package DRAM bandwidth overheads due to costly tag accesses through virtual memory mechanism and to incorporate a bandwidth-aware frequency-based replacement policy that is biased to reduce unnecessary traffic to off-package DRAM . Our extensive evaluation shows that Banshee provides significant performance improvement and traffic reduction over state-of-the-art latency-optimized DRAM cache designs .
The behavior of the power function of autocorrelation tests such as the Durbin-Watson test in time series regressions or the Cliff-Ord test in spatial regression models has been intensively studied in the literature . When the correlation becomes strong , Kr\ " amer ( 0000 ) ( for the Durbin-Watson test ) and Kr\ " amer ( 0000 ) ( for the Cliff-Ord test ) have shown that the power can be very low , in fact can converge to zero , under certain circumstances . Motivated by these results , Martellosio ( 0000 ) set out to build a general theory that would explain these findings . Unfortunately , Martellosio ( 0000 ) does not achieve this goal , as a substantial portion of his results and proofs suffer from serious flaws . The present paper now builds a theory as envisioned in Martellosio ( 0000 ) in a fairly general framework , covering general invariant tests of a hypothesis on the disturbance covariance matrix in a linear regression model . The general results are then specialized to testing for spatial correlation and to autocorrelation testing in time series regression models . We also characterize the situation where the null and the alternative hypothesis are indistinguishable by invariant tests .
This paper offers a new approach to modeling and forecasting of nonstationary time series with applications to volatility modeling for financial data . The approach is based on the assumption of local homogeneity : for every time point , there exists a historical interval of homogeneity , in which the volatility parameter can be well approximated by a constant . The proposed procedure recovers this interval from the data using the local change point ( LCP ) analysis . Afterward , the estimate of the volatility can be simply obtained by local averaging . The approach carefully addresses the question of choosing the tuning parameters of the procedure using the so-called ``propagation ' ' condition . The main result claims a new ``oracle ' ' inequality in terms of the modeling bias which measures the quality of the local constant approximation . This result yields the optimal rate of estimation for smooth and piecewise constant volatility functions . Then , the new procedure is applied to some data sets and a comparison with a standard GARCH model is also provided . Finally , we discuss applications of the new method to the Value at Risk problem . The numerical results demonstrate a very reasonable performance of the new method .
The problem of maximum-likelihood ( ML ) estimation of discrete tree-structured distributions is considered . Chow and Liu established that ML-estimation reduces to the construction of a maximum-weight spanning tree using the empirical mutual information quantities as the edge weights . Using the theory of large-deviations , we analyze the exponent associated with the error probability of the event that the ML-estimate of the Markov tree structure differs from the true tree structure , given a set of independently drawn samples . By exploiting the fact that the output of ML-estimation is a tree , we establish that the error exponent is equal to the exponential rate of decay of a single dominant crossover event . We prove that in this dominant crossover event , a non-neighbor node pair replaces a true edge of the distribution that is along the path of edges in the true tree graph connecting the nodes in the non-neighbor pair . Using ideas from Euclidean information theory , we then analyze the scenario of ML-estimation in the very noisy learning regime and show that the error exponent can be approximated as a ratio , which is interpreted as the signal-to-noise ratio ( SNR ) for learning tree distributions . We show via numerical experiments that in this regime , our SNR approximation is accurate .
The goal of our study is to evaluate the effect on program comprehension of three factors that have not previously been studied in a single experiment . These factors are programmer expertise ( expert vs . novice ) , programming task ( documentation vs . reuse ) , and the development of understanding over time ( phase 0 vs . phase 0 ) . This study is carried out in the context of the mental model approach to comprehension based on van Dijk and Kintsch ' s model ( 0000 ) . One key aspect of this model is the distinction between two kinds of representation the reader might construct from a text : 0 ) the textbase , which refers to what is said in the text and how it is said , and 0 ) the situation model , which represents the situation referred to by the text . We have evaluated the effect of the three factors mentioned above on the development of both the textbase ( or program model ) and the situation model in object-oriented program comprehension . We found a four-way interaction of expertise , phase , task and type of model . For the documentation group we found that experts and novices differ in the elaboration of their situation model but not their program model . There was no interaction of expertise with phase and type of model in the documentation group . For the reuse group , there was a three-way interaction between phase , expertise and type of model . For the novice reuse group , the effect of the phase was to increase the construction of the situation model but not the program model . With respect to the task , our results show that novices do not spontaneously construct a strong situation model but are able to do so if the task demands it .
We consider a class of nonconvex nonsmooth optimization problems whose objective is the sum of a nonnegative smooth function and a bunch of nonnegative proper closed possibly nonsmooth functions ( whose proximal mappings are easy to compute ) , some of which are further composed with linear maps . This kind of problems arises naturally in various applications when different regularizers are introduced for inducing simultaneous structures in the solutions . Solving these problems , however , can be challenging because of the coupled nonsmooth functions : the corresponding proximal mapping can be hard to compute so that standard first-order methods such as the proximal gradient algorithm cannot be applied efficiently . In this paper , we propose a successive difference-of-convex approximation method for solving this kind of problems . In this algorithm , we approximate the nonsmooth functions by their Moreau envelopes in each iteration . Making use of the simple observation that Moreau envelopes of nonnegative proper closed functions are continuous difference-of-convex functions , we can then approximately minimize the approximation function by first-order methods with suitable majorization techniques . These first-order methods can be implemented efficiently thanks to the fact that the proximal mapping of each nonsmooth function is easy to compute . Under suitable assumptions , we prove that the sequence generated by our method is bounded and clusters at a stationary point of the objective . We also discuss how our method can be applied to concrete applications such as nonconvex fused regularized optimization problems and simultaneously structured matrix optimization problems , and illustrate the performance numerically for these two specific applications .
Commercial graphics processors ( GPUs ) have high compute capacity at very low cost , which makes them attractive for general purpose scientific computing . In this paper we show how graphics processors can be used for N-body simulations to obtain improvements in performance over current generation CPUs . We have developed a highly optimized algorithm for performing the O ( N^0 ) force calculations that constitute the major part of stellar and molecular dynamics simulations . In some of the calculations , we achieve sustained performance of nearly 000 GFlops on an ATI X0000XTX . The performance on GPUs is comparable to specialized processors such as GRAPE-0A and MDGRAPE-0 , but at a fraction of the cost . Furthermore , the wide availability of GPUs has significant implications for cluster computing and distributed computing efforts like Folding@Home .
Decision taking can be performed as a service to other parties and it is amenable to outtasking rather than to outsourcing . Outtasking decision taking is compatible with selfsourcing of decision making activities carried out in preparation of decision taking . Decision taking as a service ( DTaaS ) is viewed as an instance of so-called decision casting . Preconditions for service casting are examined , and compliance of decision taking with these preconditions is confirmed . Potential advantages and disadvantages of using decision taking as a service are considered .
In a Role-Playing Game , finding optimal trajectories is one of the most important tasks . In fact , the strategy decision system becomes a key component of a game engine . Determining the way in which decisions are taken ( online , batch or simulated ) and the consumed resources in decision making ( e . g . execution time , memory ) will influence , in mayor degree , the game performance . When classical search algorithms such as A* can be used , they are the very first option . Nevertheless , such methods rely on precise and complete models of the search space , and there are many interesting scenarios where their application is not possible . Then , model free methods for sequential decision making under uncertainty are the best choice . In this paper , we propose a heuristic planning strategy to incorporate the ability of heuristic-search in path-finding into a Dyna agent . The proposed Dyna-H algorithm , as A* does , selects branches more likely to produce outcomes than other branches . Besides , it has the advantages of being a model-free online reinforcement learning algorithm . The proposal was evaluated against the one-step Q-Learning and Dyna-Q algorithms obtaining excellent experimental results : Dyna-H significantly overcomes both methods in all experiments . We suggest also , a functional analogy between the proposed sampling from worst trajectories heuristic and the role of dreams ( e . g . nightmares ) in human behavior .
In this paper , we introduce elements of probabilistic model that is suitable for modeling of learning algorithms in biologically plausible artificial neural networks framework . Model is based on two of the main concepts in quantum physics - a density matrix and the Born rule . As an example , we will show that proposed probabilistic interpretation is suitable for modeling of on-line learning algorithms for PSA , which are preferably realized by a parallel hardware based on very simple computational units . Proposed concept ( model ) can be used in the context of improving algorithm convergence speed , learning factor choice , or input signal scale robustness . We are going to see how the Born rule and the Hebbian learning rule are connected
We examine blockchain technologies , especially smart contracts , as a platform for decentralized applications . By providing a basis for consensus , blockchain promises to upend business models that presuppose a central authority . However , blockchain suffers from major shortcomings arising from an over-regimented way of organizing computation that limits its prospects . We propose a sociotechnical , yet computational , perspective that avoids those shortcomings . A centerpiece of our vision is the notion of a declarative , violable contract in contradistinction to smart contracts . This new way of thinking enables flexible governance , by formalizing organizational structures ; verification of correctness without obstructing autonomy ; and a meaningful basis for trust .
Markov chain Monte Carlo methods provide an essential tool in statistics for sampling from complex probability distributions . While the standard approach to MCMC involves constructing discrete-time reversible Markov chains whose transition kernel is obtained via the Metropolis- Hastings algorithm , there has been recent interest in alternative schemes based on piecewise deterministic Markov processes ( PDMPs ) . One such approach is based on the Zig-Zag process , introduced in Bierkens and Roberts ( 0000 ) , which proved to provide a highly scalable sampling scheme for sampling in the big data regime ( Bierkens , Fearnhead and Roberts ( 0000 ) ) . In this paper we study the performance of the Zig-Zag sampler , focusing on the one-dimensional case . In particular , we identify conditions under which a Central limit theorem ( CLT ) holds and characterize the asymptotic variance . Moreover , we study the influence of the switching rate on the diffusivity of the Zig-Zag process by identifying a diffusion limit as the switching rate tends to infinity . Based on our results we compare the performance of the Zig-Zag sampler to existing Monte Carlo methods , both analytically and through simulations .
This paper presents a potential game approach for distributed cooperative selection of informative sensors , when the goal is to maximize the mutual information between the measurement variables and the quantities of interest . It is proved that a local utility function defined by the conditional mutual information of an agent conditioned on the other agents ' sensing decisions leads to a potential game with the global potential being the original mutual information of the cooperative planning problem . The joint strategy fictitious play method is then applied to obtain a distributed solution that provably converges to a pure strategy Nash equilibrium . Two numerical examples on simplified weather forecasting and range-only target tracking verify convergence and performance characteristics of the proposed game-theoretic approach .
In this paper , we study the nonparametric maximum likelihood estimator ( MLE ) of a convex hazard function . We show that the MLE is consistent and converges at a local rate of $n^{0/0}$ at points $x_0$ where the true hazard function is positive and strictly convex . Moreover , we establish the pointwise asymptotic distribution theory of our estimator under these same assumptions . One notable feature of the nonparametric MLE studied here is that no arbitrary choice of tuning parameter ( or complicated data-adaptive selection of the tuning parameter ) is required .
Different Markov chains can be used for approximate sampling of a distribution given by an unnormalized density function with respect to the Lebesgue measure . The hit-and-run , ( hybrid ) slice sampler and random walk Metropolis algorithm are popular tools to simulate such Markov chains . We develop a general approach to compare the efficiency of these sampling procedures by the use of a partial ordering of their Markov operators , the covariance ordering . In particular , we show that the hit-and-run and the simple slice sampler are more efficient than a hybrid slice sampler based on hit-and-run which , itself , is more efficient than a ( lazy ) random walk Metropolis algorithm .
Dynamic conditional correlation ( DCC ) is a method that estimates the correlation between two time series across time . Although used primarily in finance so far , DCC has been proposed recently as a model-based estimation method for quantifying functional connectivity during fMRI experiments . DCC could also be used to estimate the dynamic correlation between other types of time series such as local field potentials ( LFP ' s ) or spike trains recorded from distinct brain areas . DCC has very nice properties compared to other existing methods , but its applications for neuroscience are currently limited because of non-optimal performance in the presence of outliers . To address this issue , we developed a robust nonparametric version of DCC , based on an adaptation of the weighted visibility graph algorithm which converts a time series into a weighted graph . The modified DCC demonstrated better performance in the analysis of empirical data sets : one fMRI data set collected from a human subject performing a Stroop task ; and one LFP data set recorded from an awake rat in resting state . Nonparametric DCC has the potential of enlarging the spectrum of analytical tools designed to assess the dynamic coupling and uncoupling of activity among brain areas .
We are motivated by the problem of designing a simple distributed algorithm for Peer-to-Peer streaming applications that can achieve high throughput and low delay , while allowing the neighbor set maintained by each peer to be small . While previous works have mostly used tree structures , our algorithm constructs multiple random directed Hamiltonian cycles and disseminates content over the superposed graph of the cycles . We show that it is possible to achieve the maximum streaming capacity even when each peer only transmits to and receives from Theta ( 0 ) neighbors . Further , we show that the proposed algorithm achieves the streaming delay of Theta ( log N ) when the streaming rate is less than ( 0-0/K ) of the maximum capacity for any fixed integer K>0 , where N denotes the number of peers in the network . The key theoretical contribution is to characterize the distance between peers in a graph formed by the superposition of directed random Hamiltonian cycles , in which edges from one of the cycles may be dropped at random . We use Doob martingales and graph expansion ideas to characterize this distance as a function of N , with high probability .
Learning algorithms for energy based Boltzmann architectures that rely on gradient descent are in general computationally prohibitive , typically due to the exponential number of terms involved in computing the partition function . In this way one has to resort to approximation schemes for the evaluation of the gradient . This is the case of Restricted Boltzmann Machines ( RBM ) and its learning algorithm Contrastive Divergence ( CD ) . It is well-known that CD has a number of shortcomings , and its approximation to the gradient has several drawbacks . Overcoming these defects has been the basis of much research and new algorithms have been devised , such as persistent CD . In this manuscript we propose a new algorithm that we call Weighted CD ( WCD ) , built from small modifications of the negative phase in standard CD . However small these modifications may be , experimental work reported in this paper suggest that WCD provides a significant improvement over standard CD and persistent CD at a small additional computational cost .
In software testing , the large size of the input domain makes exhaustively testing the inputs a daunting and often impossible task . Pair-wise testing is a popular approach to combinatorial testing problems . This paper reviews Pair-wise testing and its history , strengths , weaknesses , and tools for generating test cases .
Many proper scoring rules such as the Brier and log scoring rules implicitly reward a probability forecaster relative to a uniform baseline distribution . Recent work has motivated weighted proper scoring rules , which have an additional baseline parameter . To date two families of weighted proper scoring rules have been introduced , the weighted power and pseudospherical scoring families . These families are compatible with the log scoring rule : when the baseline maximizes the log scoring rule over some set of distributions , the baseline also maximizes the weighted power and pseudospherical scoring rules over the same set . We characterize all weighted proper scoring families and prove a general property : every proper scoring rule is compatible with some weighted scoring family , and every weighted scoring family is compatible with some proper scoring rule .
We address the problem of \emph{instance label stability} in multiple instance learning ( MIL ) classifiers . These classifiers are trained only on globally annotated images ( bags ) , but often can provide fine-grained annotations for image pixels or patches ( instances ) . This is interesting for computer aided diagnosis ( CAD ) and other medical image analysis tasks for which only a coarse labeling is provided . Unfortunately , the instance labels may be unstable . This means that a slight change in training data could potentially lead to abnormalities being detected in different parts of the image , which is undesirable from a CAD point of view . Despite MIL gaining popularity in the CAD literature , this issue has not yet been addressed . We investigate the stability of instance labels provided by several MIL classifiers on 0 different datasets , of which 0 are medical image datasets ( breast histopathology , diabetic retinopathy and computed tomography lung images ) . We propose an unsupervised measure to evaluate instance stability , and demonstrate that a performance-stability trade-off can be made when comparing MIL classifiers .
Historically two types of NLP have been investigated : fully automated processing of language by machines ( NLP ) and autonomous processing of natural language by people , i . e . the human brain ( psycholinguistics ) . We believe that there is room and need for another kind , INLP : interactive natural language processing . This intermediate approach starts from peoples ' needs , trying to bridge the gap between their actual knowledge and a given goal . Given the fact that peoples ' knowledge is variable and often incomplete , the aim is to build bridges linking a given knowledge state to a given goal . We present some examples , trying to show that this goal is worth pursuing , achievable and at a reasonable cost .
Ore operators form a common algebraic abstraction of linear ordinary differential and recurrence equations . Given an Ore operator $L$ with polynomial coefficients in $x$ , it generates a left ideal $I$ in the Ore algebra over the field $\mathbf{k} ( x ) $ of rational functions . We present an algorithm for computing a basis of the contraction ideal of $I$ in the Ore algebra over the ring $R[x]$ of polynomials , where $R$ may be either $\mathbf{k}$ or a domain with $\mathbf{k}$ as its fraction field . This algorithm is based on recent work on desingularization for Ore operators by Chen , Jaroschek , Kauers and Singer . Using a basis of the contraction ideal , we compute a completely desingularized operator for $L$ whose leading coefficient not only has minimal degree in $x$ but also has minimal content . Completely desingularized operators have interesting applications such as certifying integer sequences and checking special cases of a conjecture of Krattenthaler .
A major challenge in many modern superresolution fluorescence microscopy techniques at the nanoscale lies in the correct alignment of long sequences of sparse but spatially and temporally highly resolved images . This is caused by the temporal drift of the protein structure , e . g . due to temporal thermal inhomogeneity of the object of interest or its supporting area during the observation process . We develop a simple semiparametric model for drift correction in SMS microscopy . Then we propose an M-estimator for the drift and show its asymptotic normality . This is used to correct the final image and it is shown that this purely statistical method is competitive with state of the art calibration techniques which require to incorporate fiducial markers into the specimen . Moreover , a simple bootstrap algorithm allows to quantify the precision of the drift estimate and its effect on the final image estimation . We argue that purely statistical drift correction is even more robust than fiducial tracking rendering the latter superfluous in many applications . The practicability of our method is demonstrated by a simulation study and by an SMS application . This serves as a prototype for many other typical imaging techniques where sparse observations with highly temporal resolution are blurred by motion of the object to be reconstructed .
In a seminal paper , McAfee ( 0000 ) presented a truthful mechanism for double auction , attaining asymptotically-optimal gain-from-trade without any prior information on the valuations of the traders . McAfee ' s mechanism handles single-parametric agents , allowing each seller to sell a single item and each buyer to buy a single item . In this paper , we present a double-auction mechanism that handles multi-parametric agents and allow multiple items per trader . Each seller is endowed with several units of a pre-specified type and has diminishing marginal returns . Each buyer may buy multiple types and has a gross-substitute valuation function . Both buyers and sellers are quasi-linear in money . The mechanism is prior-free , ex-post individually-rational , dominant-strategy truthful and strongly budget-balanced . Its gain-from-trade approaches the optimum when the market in all item-types is sufficiently large .
We compare three popular techniques of rating content : the ubiquitous five star rating , the less used pairwise comparison , and the recently introduced ( in crowdsourcing ) magnitude estimation approach . Each system has specific advantages and disadvantages , in terms of required user effort , achievable user preference prediction accuracy and number of ratings required . We design an experiment where the three techniques are compared in an unbiased way . We collected 00 ' 000 ratings on a popular crowdsourcing platform , allowing us to release a dataset that will be useful for many related studies on user rating techniques .
Pervasive personal communication technologies offer the potential for important social benefits for individual users , but also the potential for significant social difficulties and costs . In research on face-to-face social interaction , ambiguity is often identified as an important resource for resolving social difficulties . In this paper , we discuss two design cases of personal communication systems , one based on fieldwork of a commercial system and another based on an unrealized design concept . The cases illustrate how user behavior concerning a particular social difficulty , unexplained unresponsiveness , can be influenced by technological issues that result in interactional ambiguity . The cases also highlight the need to balance the utility of ambiguity against the utility of usability and communicative clarity .
We simulate a network of N routers and M network users making concurrent point-to-point connections by buying and selling router capacity from each other . The resources need to be acquired in complete sets , but there is only one spot market for each router . In order to describe the internal dynamics of the market , we model the observed prices by N-dimensional Ito-processes . Modeling using stochastic processes is novel in this context of describing interactions between end-users in a system with shared resources , and allows a standard set of mathematical tools to be applied . The derived models can also be used to price contingent claims on network capacity and thus to price complex network services such as quality of service levels , multicast , etc .
Directed acyclic graph ( DAG ) models may be characterized in at least four different ways : via a factorization , the d-separation criterion , the moralization criterion , and the local Markov property . As pointed out by Robins ( 0000 , 0000 ) , Verma and Pearl ( 0000 ) , and Tian and Pearl ( 0000b ) , marginals of DAG models also imply equality constraints that are not conditional independences . The well-known `Verma constraint ' is an example . Constraints of this type were used for testing edges ( Shpitser et al . , 0000 ) , and an efficient marginalization scheme via variable elimination ( Shpitser et al . , 0000 ) . We show that equality constraints like the `Verma constraint ' can be viewed as conditional independences in kernel objects obtained from joint distributions via a fixing operation that generalizes conditioning and marginalization . We use these constraints to define , via Markov properties and a factorization , a graphical model associated with acyclic directed mixed graphs ( ADMGs ) . We show that marginal distributions of DAG models lie in this model , prove that a characterization of these constraints given in ( Tian and Pearl , 0000b ) gives an alternative definition of the model , and finally show that the fixing operation we used to define the model can be used to give a particularly simple characterization of identifiable causal effects in hidden variable graphical causal models .
We study the multiagent epistemic logic CMAELCD with operators for common and distributed knowledge for all coalitions of agents . We introduce Hintikka structures for this logic and prove that satisfiability in such structures is equivalent to satisfiability in standard models . Using this result , we design an incremental tableau based decision procedure for testing satisfiability in CMAELCD .
Canonical correlation analysis ( CCA ) is a multivariate statistical method which describes the associations between two sets of variables . The objective is to find linear combinations of the variables in each data set having maximal correlation . This paper discusses a method for Robust Sparse CCA . Sparse estimation produces canonical vectors with some of their elements estimated as exactly zero . As such , their interpretability is improved . We also robustify the method such that it can cope with outliers in the data . To estimate the canonical vectors , we convert the CCA problem into an alternating regression framework , and use the sparse Least Trimmed Squares estimator . We illustrate the good performance of the Robust Sparse CCA method in several simulation studies and two real data examples .
In this paper , we study the combinatorial multi-armed bandit problem ( CMAB ) with probabilistically triggered arms ( PTAs ) . Under the assumption that the arm triggering probabilities ( ATPs ) are positive for all arms , we prove that a class of upper confidence bound ( UCB ) policies , named Combinatorial UCB with exploration rate $\kappa$ ( CUCB-$\kappa$ ) , and Combinatorial Thompson Sampling ( CTS ) , which estimates the expected states of the arms via Thompson sampling , achieve bounded regret . In addition , we prove that CUCB-$0$ and CTS incur $O ( \sqrt{T} ) $ gap-independent regret . These results improve the results in previous works , which show $O ( \log T ) $ gap-dependent and $O ( \sqrt{T\log T} ) $ gap-independent regrets , respectively , under no assumptions on the ATPs . Then , we numerically evaluate the performance of CUCB-$\kappa$ and CTS in a real-world movie recommendation problem , where the actions correspond to recommending a set of movies , the arms correspond to the edges between the movies and the users , and the goal is to maximize the total number of users that are attracted by at least one movie . Our numerical results complement our theoretical findings on bounded regret . Apart from this problem , our results also directly apply to the online influence maximization ( OIM ) problem studied in numerous prior works .
Image aesthetic evaluation has attracted much attention in recent years . Image aesthetic evaluation methods heavily depend on the effective aesthetic feature . Traditional meth-ods always extract hand-crafted features . However , these hand-crafted features are always designed to adapt particu-lar datasets , and extraction of them needs special design . Rather than extracting hand-crafted features , an automati-cally learn of aesthetic features based on deep convolutional neural network ( DCNN ) is first adopt in this paper . As we all know , when the training dataset is given , the DCNN architecture with high complexity may meet the over-fitting problem . On the other side , the DCNN architecture with low complexity would not efficiently extract effective features . For these reasons , we further propose a paralleled convolutional neural network ( PDCNN ) with multi-level structures to automatically adapt to the training dataset . Experimental results show that our proposed PDCNN architecture achieves better performance than other traditional methods .
We consider the equivalent problems of estimating the residual variance , the proportion of explained variance $\eta$ and the signal strength in a high-dimensional linear regression model with Gaussian random design . Our aim is to understand the impact of not knowing the sparsity of the regression parameter and not knowing the distribution of the design on minimax estimation rates of $\eta$ . Depending on the sparsity $k$ of the regression parameter , optimal estimators of $\eta$ either rely on estimating the regression parameter or are based on U-type statistics , and have minimax rates depending on $k$ . In the important situation where $k$ is unknown , we build an adaptive procedure whose convergence rate simultaneously achieves the minimax risk over all $k$ up to a logarithmic loss which we prove to be non avoidable . Finally , the knowledge of the design distribution is shown to play a critical role . When the distribution of the design is unknown , consistent estimation of explained variance is indeed possible in much narrower regimes than for known design distribution .
Typography is a ubiquitous art form that affects our understanding , perception , and trust in what we read . Thousands of different font-faces have been created with enormous variations in the characters . In this paper , we learn the style of a font by analyzing a small subset of only four letters . From these four letters , we learn two tasks . The first is a discrimination task : given the four letters and a new candidate letter , does the new letter belong to the same font ? Second , given the four basis letters , can we generate all of the other letters with the same characteristics as those in the basis set ? We use deep neural networks to address both tasks , quantitatively and qualitatively measure the results in a variety of novel manners , and present a thorough investigation of the weaknesses and strengths of the approach .
We consider the problem of fairly dividing a heterogeneous cake between a number of players with different tastes . In this setting , it is known that fairness requirements may result in a suboptimal division from the social welfare standpoint . Here , we show that in some cases , discarding some of the cake and fairly dividing only the remainder may be socially preferable to any fair division of the entire cake . We study this phenomenon , providing asymptotically-tight bounds on the social improvement achievable by such discarding .
The present author recently proposed and proved a relationship theorem between nonlinear polynomial equations and the corresponding Jacobian matrix . By using this theorem , this paper derives a Newton iterative formula without requiring the evaluation of nonlinear function values in the solution of nonlinear polynomial-only problems .
This paper extends the notion of information processing capacity for non-independent input signals in the context of reservoir computing ( RC ) . The presence of input autocorrelation makes worthwhile the treatment of forecasting and filtering problems for which we explicitly compute this generalized capacity as a function of the reservoir parameter values using a streamlined model . The reservoir model leading to these developments is used to show that , whenever that approximation is valid , this computational paradigm satisfies the so called separation and fading memory properties that are usually associated with good information processing performances . We show that several standard memory , forecasting , and filtering problems that appear in the parametric stochastic time series context can be readily formulated and tackled via RC which , as we show , significantly outperforms standard techniques in some instances .
Today ' s search engines process billions of online user queries a day over huge collections of data . In order to scale , they distribute query processing among many nodes , where each node holds and searches over a subset of the index called shard . Responses from some nodes occasionally fail to arrive within a reasonable time-interval due to various reasons , such as high server load and network congestion . Search engines typically need to respond in a timely manner , and therefore skip such tail latency responses , which causes degradation in search quality . In this paper , we tackle response misses due to high tail latencies with the goal of maximizing search quality . Search providers today use redundancy in the form of Replication for mitigating response misses , by constructing multiple copies of each shard and searching all replicas . This approach is not ideal , as it wastes resources on searching duplicate data . We propose two strategies to reduce this waste . First , we propose rSmartRed , an optimal shard selection scheme for replicated indexes . Second , when feasible , we propose to replace Replication with Repartition , which constructs independent index instances instead of exact copies . We analytically prove that rSmartRed ' s selection is optimal for Replication , and that Repartition achieves better search quality compared to Replication . We confirm our results with an empirical study using two real-world datasets , showing that rSmartRed improves recall compared to currently used approaches . We additionally show that Repartition improves over Replication in typical scenarios .
Decentralised optimisation tasks are important components of multi-agent systems . These tasks can be interpreted as n-player potential games : therefore game-theoretic learning algorithms can be used to solve decentralised optimisation tasks . Fictitious play is the canonical example of these algorithms . Nevertheless fictitious play implicitly assumes that players have stationary strategies . We present a novel variant of fictitious play where players predict their opponents ' strategies using Extended Kalman filters and use their predictions to update their strategies . We show that in 0 by 0 games with at least one pure Nash equilibrium and in potential games where players have two available actions , the proposed algorithm converges to the pure Nash equilibrium . The performance of the proposed algorithm was empirically tested , in two strategic form games and an ad-hoc sensor network surveillance problem . The proposed algorithm performs better than the classic fictitious play algorithm in these games and therefore improves the performance of game-theoretical learning in decentralised optimisation .
We consider the problem of detecting whether or not , in a given sensor network , there is a cluster of sensors which exhibit an " unusual behavior . " Formally , suppose we are given a set of nodes and attach a random variable to each node . We observe a realization of this process and want to decide between the following two hypotheses : under the null , the variables are i . i . d . standard normal ; under the alternative , there is a cluster of variables that are i . i . d . normal with positive mean and unit variance , while the rest are i . i . d . standard normal . We also address surveillance settings where each sensor in the network collects information over time . The resulting model is similar , now with a time series attached to each node . We again observe the process over time and want to decide between the null , where all the variables are i . i . d . standard normal , and the alternative , where there is an emerging cluster of i . i . d . normal variables with positive mean and unit variance . The growth models used to represent the emerging cluster are quite general and , in particular , include cellular automata used in modeling epidemics . In both settings , we consider classes of clusters that are quite general , for which we obtain a lower bound on their respective minimax detection rate and show that some form of scan statistic , by far the most popular method in practice , achieves that same rate to within a logarithmic factor . Our results are not limited to the normal location model , but generalize to any one-parameter exponential family when the anomalous clusters are large enough .
When convolutional neural networks are used to tackle learning problems based on music or , more generally , time series data , raw one-dimensional data are commonly pre-processed to obtain spectrogram or mel-spectrogram coefficients , which are then used as input to the actual neural network . In this contribution , we investigate , both theoretically and experimentally , the influence of this pre-processing step on the network ' s performance and pose the question , whether replacing it by applying adaptive or learned filters directly to the raw data , can improve learning success . The theoretical results show that approximately reproducing mel-spectrogram coefficients by applying adaptive filters and subsequent time-averaging is in principle possible . We also conducted extensive experimental work on the task of singing voice detection in music . The results of these experiments show that for classification based on Convolutional Neural Networks the features obtained from adaptive filter banks followed by time-averaging perform better than the canonical Fourier-transform-based mel-spectrogram coefficients . Alternative adaptive approaches with center frequencies or time-averaging lengths learned from training data perform equally well .
We propose a non-parametric regression methodology , Random Forests on Distance Matrices ( RFDM ) , for detecting genetic variants associated to quantitative phenotypes representing the human brain ' s structure or function , and obtained using neuroimaging techniques . RFDM , which is an extension of decision forests , requires a distance matrix as response that encodes all pair-wise phenotypic distances in the random sample . We discuss ways to learn such distances directly from the data using manifold learning techniques , and how to define such distances when the phenotypes are non-vectorial objects such as brain connectivity networks . We also describe an extension of RFDM to detect espistatic effects while keeping the computational complexity low . Extensive simulation results and an application to an imaging genetics study of Alzheimer ' s Disease are presented and discussed .
The Web of Things ( WoT ) is rapidly growing in popularity getting the interest of not only technologist and scientific communities but industrial , system integrators and solution providers . The key aspect of the WoT to succeed is the relatively , easy-to-build ecosystems nature inherited from the web and the capacity for building end-to-end solutions . At the WoT connecting physical devices such as sensors , RFID tags or any devices that can send data through the Internet using the Web is almost automatic . The WoT shared data can be used to build smarter solutions that offer business services in the form of IoT applications . In this chapter , we review the main WoT challenges , with particular interest on highlighting those that rely on combining heterogeneous IoT data for the design of smarter services and applications and that benefit from data interoperability . Semantic web technologies help for overcoming with such challenges by addressing , among other ones the following objectives : 0 ) semantically annotating and unifying heterogeneous data , 0 ) enriching semantic WoT datasets with external knowledge graphs , and 0 ) providing an analysis of data by means of reasoning mechanisms to infer meaningful information . To overcome the challenge of building interoperable semantics-based IoT applications , the Machine-to-Machine Measurement ( M0 ) semantic engine has been designed to semantically annotate WoT data , build the logic of smarter services and deduce meaningful knowledge by linking it to the external knowledge graphs available on the web . M0 assists application and business developers in designing interoperable Semantic Web of Things applications . Contributions in the context of European semantic-based WoT projects are discussed and a particular use case within FIESTA-IoT project is presented .
Regression discontinuity ( RD ) designs are often interpreted as local randomized experiments : a RD design can be considered as a randomized experiment for units with a realized value of a so-called forcing variable falling around a pre-fixed threshold . Motivated by the evaluation of Italian university grants , we consider a fuzzy RD design where the receipt of the treatment is based on both eligibility criteria and a voluntary application status . Resting on the fact that grant application and grant receipt statuses are post-assignment ( post-eligibility ) intermediate variables , we use the principal stratification framework to define causal estimands within the Rubin Causal Model . We propose a probabilistic formulation of the assignment mechanism underlying RD designs , by re-formulating the Stable Unit Treatment Value Assumption ( SUTVA ) and making an explicit local overlap assumption for a subpopulation around the threshold . A local randomization assumption is invoked instead of more standard continuity assumptions . We also develop a model-based Bayesian approach to select the target subpopulation ( s ) with adjustment for multiple comparisons , and to draw inference for the target causal estimands in this framework . Applying the method to the data from two Italian universities , we find evidence that university grants are effective in preventing students from low-income families from dropping out of higher education .
Recently Asimit et . al used an EM algorithm to estimate Marshall-Olkin bivariate Pareto distribution . The distribution has seven parameters . We describe few alternative approaches of EM algorithm . A numerical simulation is performed to verify the performance of different proposed algorithms . A real-life data analysis is also shown for illustrative purposes .
Inspired by biophysical principles underlying nonlinear dendritic computation in neural circuits , we develop a scheme to train deep neural networks to make them robust to adversarial attacks . Our scheme generates highly nonlinear , saturated neural networks that achieve state of the art performance on gradient based adversarial examples on MNIST , despite never being exposed to adversarially chosen examples during training . Moreover , these networks exhibit unprecedented robustness to targeted , iterative schemes for generating adversarial examples , including second-order methods . We further identify principles governing how these networks achieve their robustness , drawing on methods from information geometry . We find these networks progressively create highly flat and compressed internal representations that are sensitive to very few input dimensions , while still solving the task . Moreover , they employ highly kurtotic weight distributions , also found in the brain , and we demonstrate how such kurtosis can protect even linear classifiers from adversarial attack .
A case of the matrix Kummer relation of Herz ( 0000 ) based on the Pearson VII type matrix model is derived in this paper . As a consequence , the polynomial Pearson VII configuration density is obtained and this set the corresponding exact inference as a solvable aspect in shape theory .
This paper presents a novel online capable method for simultaneous estimation of human motion in terms of segment orientations and positions along with sensor-to-segment calibration parameters from inertial sensors attached to the body . In order to solve this ill-posed estimation problem , state-of-the-art motion , measurement and biomechanical models are combined with new stochastic equations and priors . These are based on the kinematics of multi-body systems , anatomical and body shape information , as well as , parameter properties for regularisation . This leads to a constrained weighted least squares problem that is solved in a sliding window fashion . Magnetometer information is currently only used for initialisation , while the estimation itself works without magnetometers . The method was tested on simulated , as well as , on real data , captured from a lower body configuration .
We consider the homogeneous and the non-homogeneous convex relaxations for combinatorial penalty functions defined on support sets . Our study identifies key differences in the tightness of the resulting relaxations through the notion of the lower combinatorial envelope of a set-function along with new necessary conditions for support identification . We then propose a general adaptive estimator for convex monotone regularizers , and derive new sufficient conditions for support recovery in the asymptotic setting .
A recurring problem when building probabilistic latent variable models is regularization and model selection , for instance , the choice of the dimensionality of the latent space . In the context of belief networks with latent variables , this problem has been adressed with Automatic Relevance Determination ( ARD ) employing Monte Carlo inference . We present a variational inference approach to ARD for Deep Generative Models using doubly stochastic variational inference to provide fast and scalable learning . We show empirical results on a standard dataset illustrating the effects of contracting the latent space automatically . We show that the resulting latent representations are significantly more compact without loss of expressive power of the learned models .
We present two novel approaches to parsing context-free languages . The first approach is based on an extension of Brzozowski ' s derivative from regular expressions to context-free grammars . The second approach is based on a generalization of the derivative to parser combinators . The payoff of these techniques is a small ( less than 000 lines of code ) , easy-to-implement parsing library capable of parsing arbitrary context-free grammars into lazy parse forests . Implementations for both Scala and Haskell are provided . Preliminary experiments with S-Expressions parsed millions of tokens per second , which suggests this technique is efficient enough for use in practice .
Pricing schemes are an important smart grid feature to affect typical energy usage behavior of energy users ( EUs ) . However , most existing schemes use the assumption that a buyer pays the same price per unit of energy to all suppliers at any particular time when energy is bought . By contrast , here a discriminate pricing technique using game theory is studied . A cake cutting game is investigated , in which participating EUs in a smart community decide on the price per unit of energy to charge a shared facility controller ( SFC ) in order to sell surplus energy . The focus is to study fairness criteria to maximize sum benefits to EUs and ensure an envy-free energy trading market . A benefit function is designed that leverages generation of discriminate pricing by each EU , according to the amount of surplus energy that an EU trades with the SFC and the EU ' s sensitivity to price . It is shown that the game possesses a socially optimal , and hence also Pareto optimal , solution . Further , an algorithm that can be implemented by each EU in a distributed manner to reach the optimal solution is proposed . Numerical case studies are given that demonstrate beneficial properties of the scheme .
This position paper provides an interim summary on the goals and current state of our ongoing research project on semantic model differencing for software evolution . We describe the basics of semantic model differencing , give two examples from our recent work , and discuss future challenges in taking full advantage of the potential of semantic differencing techniques in the context of models ' evolution .
We study the nonparametric estimators of the infinitesimal coefficients of the second-order jump-diffusion models . Under the mild conditions , we obtain the weak consistency and the asymptotic normalities of the estimators .
The boundary knot method ( BKM ) [0] is a meshless boundary-type radial basis function ( RBF ) collocation scheme , where the nonsingular general solution is used instead of fundamental solution to evaluate the homogeneous solution , while the dual reciprocity method ( DRM ) is employed to approximation of particular solution . Despite the fact that there are not nonsingular RBF general solutions available for Laplace and biharmonic problems , this study shows that the method can be successfully applied to these problems . The high-order general and fundamental solutions of Burger and Winkler equations are also first presented here .
Most metrics between finite point measures currently used in the literature have the flaw that they do not treat differing total masses in an adequate manner for applications . This paper introduces a new metric $\bar{d}_0$ that combines positional differences of points under a closest match with the relative difference in total mass in a way that fixes this flaw . A comprehensive collection of theoretical results about $\bar{d}_0$ and its induced Wasserstein metric $\bar{d}_0$ for point process distributions are given , including examples of useful $\bar{d}_0$-Lipschitz continuous functions , $\bar{d}_0$ upper bounds for Poisson process approximation , and $\bar{d}_0$ upper and lower bounds between distributions of point processes of i . i . d . points . Furthermore , we present a statistical test for multiple point pattern data that demonstrates the potential of $\bar{d}_0$ in applications .
This paper considers the problem of detecting nonstationary phenomena , and chirps in particular , from very noisy data . Chirps are waveforms of the very general form A ( t ) exp ( i\lambda \phi ( t ) ) , where \lambda is a ( large ) base frequency , the phase \phi ( t ) is time-varying and the amplitude A ( t ) is slowly varying . Given a set of noisy measurements , we would like to test whether there is signal or whether the data is just noise . One particular application of note in conjunction with this problem is the detection of gravitational waves predicted by Einstein ' s Theory of General Relativity . We introduce detection strategies which are very sensitive and more flexible than existing feature detectors . The idea is to use structured algorithms which exploit information in the so-called chirplet graph to chain chirplets together adaptively as to form chirps with polygonal instantaneous frequency . We then search for the path in the graph which provides the best trade-off between complexity and goodness of fit . Underlying our methodology is the idea that while the signal may be extremely weak so that none of the individual empirical coefficients is statistically significant , one can still reliably detect by combining several coefficients into a coherent chain . This strategy is general and may be applied in many other detection problems . We complement our study with numerical experiments showing that our algorithms are so sensitive that they seem to detect signals whenever their strength makes them detectable .
This technical report presents an evaluation of the ontology annotations in the metadata of a subset of entries of MetaboLights , a database for Metabolomics experiments and derived information . The work includes a manual analysis of the entries and a comprehensive qualitative evaluation of their annotations , together with the evaluation guide and its rationale , that was defined and followed . The approach was also implemented as a software script that given a MetaboLights entry returns a quantitative evaluation of the quality of its annotations ( available on request ) .
In this work , we study the problem of scheduling a maximal set of transmitters subjected to an interference constraint across all the nodes . Given a set of nodes , the problem reduces to finding the maximum cardinality of a subset of nodes that can concurrently transmit without violating interference constraints . The resulting packing problem is a binary optimization problem and is NP hard . We propose a semi-definite relaxation ( SDR ) for this problem and provide bounds on the relaxation .
Context : Software Engineering research makes use of collections of software artifacts ( corpora ) to derive empirical evidence from . Goal : To improve quality and reproducibility of research , we need to understand the characteristics of used corpora . Method : For that , we perform a literature survey using grounded theory . We analyze the latest proceedings of seven relevant conferences . Results : While almost all papers use corpora of some kind with the common case of collections of source code of open-source Java projects , there are no frequently used projects or corpora across all the papers . For some conferences we can detect recurrences . We discover several forms of requirements and applied tunings for corpora which indicate more specific needs of research efforts . Conclusion : Our survey feeds into a quantitative basis for discussing the current state of empirical research in software engineering , thereby enabling ultimately improvement of research quality specifically in terms of use ( and reuse ) of empirical evidence .
With the advent of big data and deep learning , computation power has become a bottleneck for many applications . Network-on-Chip ( NoC ) has been proposed to enable multiprocessor acceleration for deep learning computation , and efficient arbitration is a key issue for high performance . In this paper , an arbitration scheme based on interference of wave is proposed . In this scheme , home node sends out multiple tokens in different frequencies , and nodes that are competing for bus can capture token by sending out wave that cancels its specific wave token . In this scheme , speed-of-light arbitration can be achieved , and full information is available to all nodes in arbitration .
This article provides a unifying Bayesian network view on various approaches for acoustic model adaptation , missing feature , and uncertainty decoding that are well-known in the literature of robust automatic speech recognition . The representatives of these classes can often be deduced from a Bayesian network that extends the conventional hidden Markov models used in speech recognition . These extensions , in turn , can in many cases be motivated from an underlying observation model that relates clean and distorted feature vectors . By converting the observation models into a Bayesian network representation , we formulate the corresponding compensation rules leading to a unified view on known derivations as well as to new formulations for certain approaches . The generic Bayesian perspective provided in this contribution thus highlights structural differences and similarities between the analyzed approaches .
We propose a novel algorithm for testing the hypothesis of nonstationarity in complex-valued signals . The implementation uses both the bootstrap and the Fast Fourier Transform such that the algorithm can be efficiently implemented in O ( NlogN ) time , where N is the length of the observed signal . The test procedure examines the second-order structure and contrasts the observed power variance - i . e . the variability of the instantaneous variance over time - with the expected characteristics of stationary signals generated via the bootstrap method . Our algorithmic procedure is capable of learning different types of nonstationarity , such as jumps or strong sinusoidal components . We illustrate the utility of our test and algorithm through application to turbulent flow data from fluid dynamics .
It is currently not possible to quantify the resources needed to perform a computation . As a consequence , it is not possible to reliably evaluate the hardware resources needed for the application of algorithms or the running of programs . This is apparent in both computer science , for instance , in cryptanalysis , and in neuroscience , for instance , comparative neuro-anatomy . A System versus Environment game formalism is proposed based on Computability Logic that allows to define a computational work function that describes the theoretical and physical resources needed to perform any purely algorithmic computation . Within this formalism , the cost of a computation is defined as the sum of information storage over the steps of the computation . The size of the computational device , eg , the action table of a Universal Turing Machine , the number of transistors in silicon , or the number and complexity of synapses in a neural net , is explicitly included in the computational cost . The proposed cost function leads in a natural way to known computational trade-offs and can be used to estimate the computational capacity of real silicon hardware and neural nets . The theory is applied to a historical case of 00 bit DES key recovery , as an example of application to cryptanalysis . Furthermore , the relative computational capacities of human brain neurons and the C . elegans nervous system are estimated as an example of application to neural nets .
Attribute-based access control ( ABAC ) provides a high level of flexibility that promotes security and information sharing . ABAC policy mining algorithms have potential to significantly reduce the cost of migration to ABAC , by partially automating the development of an ABAC policy from information about the existing access-control policy and attribute data . This paper presents an algorithm for mining ABAC policies from operation logs and attribute data . To the best of our knowledge , it is the first algorithm for this problem .
In the paper we study sharp maximal inequalities for martingales and non-negative submartingales : if $f$ , $g$ are martingales satisfying \[|\mathrm{d}g_n|\leq|\mathrm{d}f_n| , \qquad n=0 , 0 , 0 , . . . , \] almost surely , then \[\Bigl\|\sup_{n\geq0}|g_n|\Bigr\|_p\leq p\|f\|_p , \qquad p\geq0 , \] and the inequality is sharp . Furthermore , if $\alpha\in[0 , 0]$ , $f$ is a non-negative submartingale and $g$ satisfies \[|\mathrm{d}g_n|\leq|\mathrm{d}f_n|\quad and\quad |\mathbb{E} ( \mathrm{d}g_{n+0}|\mathcal {F}_n ) |\leq\alpha\mathbb{E} ( \mathrm{d}f_{n+0}|\mathcal{F}_n ) , \qquad n=0 , 0 , 0 , . . . , \] almost surely , then \[\Bigl\|\sup_{n\geq0}|g_n|\Bigr\|_p\leq ( \alpha+0 ) p\|f\|_p , \qquad p\geq0 , \] and the inequality is sharp . As an application , we establish related estimates for stochastic integrals and It\^{o} processes . The inequalities strengthen the earlier classical results of Burkholder and Choi .
To avoid packet loss and deadlock scenarios that arise due to faults or power gating in multicore and many-core systems , the network-on-chip needs to possess resilient communication and load-balancing properties . In this work , we introduce the Fashion router , a self-monitoring and self-reconfiguring design that allows for the on-chip network to dynamically adapt to component failures . First , we introduce a distributed intelligence unit , called Self-Awareness Module ( SAM ) , which allows the router to detect permanent component failures and build a network connectivity map . Using local information , SAM adapts to faults , guarantees connectivity and deadlock-free routing inside the maximal connected subgraph and keeps routing tables up-to-date . Next , to reconfigure network links or virtual channels around faulty/power-gated components , we add bidirectional link and unified virtual channel structure features to the Fashion router . This version of the router , named Ex-Fashion , further mitigates the negative system performance impacts , leads to larger maximal connected subgraph and sustains a relatively high degree of fault-tolerance . To support the router , we develop a fault diagnosis and recovery algorithm executed by the Built-In Self-Test , self-monitoring , and self-reconfiguration units at runtime to provide fault-tolerant system functionalities . The Fashion router places no restriction on topology , position or number of faults . It drops 00 . 0-00 . 0% fewer nodes for same number of faults ( between 00 and 00 faults ) in an 0x0 0D-mesh over other state-of-the-art solutions . It is scalable and efficient . The area overheads are 0 . 000% and 0 . 000% when implemented in 0x0 and 00x00 0D-meshes using the TSMC 00nm library at 0 . 00GHz clock frequency .
As elderly population increases , portion of dementia patients becomes larger . Thus social cost of caring dementia patients has been a major concern to many nations . This article introduces a dementia assistive system operated by various sensors and devices installed in body area and activity area of patients . Since this system is served based on a network which includes a number of nodes , it requires techniques to reduce the network performance degradation caused by densely composed sensors and devices . This article introduces existing protocols for communications of sensors and devices at both low rate and high rate transmission .
We characterize conjugate nonparametric Bayesian models as projective limits of conjugate , finite-dimensional Bayesian models . In particular , we identify a large class of nonparametric models representable as infinite-dimensional analogues of exponential family distributions and their canonical conjugate priors . This class contains most models studied in the literature , including Dirichlet processes and Gaussian process regression models . To derive these results , we introduce a representation of infinite-dimensional Bayesian models by projective limits of regular conditional probabilities . We show under which conditions the nonparametric model itself , its sufficient statistics , and -- if they exist -- conjugate updates of the posterior are projective limits of their respective finite-dimensional counterparts . We illustrate our results both by application to existing nonparametric models and by construction of a model on infinite permutations .
This paper considers the problem of estimating linear dynamic system models when the observations are corrupted by random disturbances with nonstandard distributions . The paper is particularly motivated by applications where sensor imperfections involve significant contribution of outliers or wrap-around issues resulting in multi-modal distributions such as commonly encountered in robotics applications . As will be illustrated , these nonstandard measurement errors can dramatically compromise the effectiveness of standard estimation methods , while a computational Bayesian approach developed here is demonstrated to be equally effective as standard methods in standard measurement noise scenarios , but dramatically more effective in nonstandard measurement noise distribution scenarios .
Decomposing tensors into orthogonal factors is a well-known task in statistics , machine learning , and signal processing . We study orthogonal outer product decompositions where the factors in the summands in the decomposition are required to be orthogonal across summands , by relating this orthogonal decomposition to the singular value decompositions of the flattenings . We show that it is a non-trivial assumption for a tensor to have such an orthogonal decomposition , and we show that it is unique ( up to natural symmetries ) in case it exists , in which case we also demonstrate how it can be efficiently and reliably obtained by a sequence of singular value decompositions . We demonstrate how the factoring algorithm can be applied for parameter identification in latent variable and mixture models .
We study the problem of ranking a set of items from nonactively chosen pairwise preferences where each item has feature information with it . We propose and characterize a very broad class of preference matrices giving rise to the Feature Low Rank ( FLR ) model , which subsumes several models ranging from the classic Bradley-Terry-Luce ( BTL ) ( Bradley and Terry 0000 ) and Thurstone ( Thurstone 0000 ) models to the recently proposed blade-chest ( Chen and Joachims 0000 ) and generic low-rank preference ( Rajkumar and Agarwal 0000 ) models . We use the technique of matrix completion in the presence of side information to develop the Inductive Pairwise Ranking ( IPR ) algorithm that provably learns a good ranking under the FLR model , in a sample-efficient manner . In practice , through systematic synthetic simulations , we confirm our theoretical findings regarding improvements in the sample complexity due to the use of feature information . Moreover , on popular real-world preference learning datasets , with as less as 00% sampling of the pairwise comparisons , our method recovers a good ranking .
With the development of state-of-art deep reinforcement learning , we can efficiently tackle continuous control problems . But the deep reinforcement learning method for continuous control is based on historical data , which would make unpredicted decisions in unfamiliar scenarios . Combining deep reinforcement learning and safety based control can get good performance for self-driving and collision avoidance . In this passage , we use the Deep Deterministic Policy Gradient algorithm to implement autonomous driving without vehicles around . The vehicle can learn the driving policy in a stable and familiar environment , which is efficient and reliable . Then we use the artificial potential field to design collision avoidance algorithm with vehicles around . The path tracking method is also taken into consideration . The combination of deep reinforcement learning and safety based control performs well in most scenarios .
In the framework of noisy quantum homodyne tomography with efficiency parameter $0 < \eta \leq 0$ , we propose two estimators of a quantum state whose density matrix elements $\rho_{m , n}$ decrease like $e^{-B ( m+n ) ^{r/ 0}}$ , for fixed known $B>0$ and $0<r\leq 0$ . The first procedure estimates the matrix coefficients by a projection method on the pattern functions ( that we introduce here for $0<\eta \leq 0/0$ ) , the second procedure is a kernel estimator of the associated Wigner function . We compute the convergence rates of these estimators , in $\mathbb{L}_0$ risk .
An adversarial example is an example that has been adjusted to produce the wrong label when presented to a system at test time . If adversarial examples existed that could fool a detector , they could be used to ( for example ) wreak havoc on roads populated with smart vehicles . Recently , we described our difficulties creating physical adversarial stop signs that fool a detector . More recently , Evtimov et al . produced a physical adversarial stop sign that fools a proxy model of a detector . In this paper , we show that these physical adversarial stop signs do not fool two standard detectors ( YOLO and Faster RCNN ) in standard configuration . Evtimov et al . ' s construction relies on a crop of the image to the stop sign ; this crop is then resized and presented to a classifier . We argue that the cropping and resizing procedure largely eliminates the effects of rescaling and of view angle . Whether an adversarial attack is robust under rescaling and change of view direction remains moot . We argue that attacking a classifier is very different from attacking a detector , and that the structure of detectors - which must search for their own bounding box , and which cannot estimate that box very accurately - likely makes it difficult to make adversarial patterns . Finally , an adversarial pattern on a physical object that could fool a detector would have to be adversarial in the face of a wide family of parametric distortions ( scale ; view angle ; box shift inside the detector ; illumination ; and so on ) . Such a pattern would be of great theoretical and practical interest . There is currently no evidence that such patterns exist .
A number of authors have described randomized algorithms for solving the epsilon-approximate nearest neighbor problem . In this note I point out that the epsilon-approximate nearest neighbor property often fails to be a useful approximation property , since epsilon-approximate solutions fail to satisfy the necessary preconditions for using nearest neighbors for classification and related tasks .
We have an audacious dream , we would like to develop a simulation and virtual reality system to support the decision making in European football ( soccer ) . In this review , we summarize the efforts that we have made to fulfil this dream until recently . In addition , an introductory version of FerSML ( Footballer and Football Simulation Markup Language ) is presented in this paper .
We analyze the stability properties of the Synchrosqueezing transform , a time-frequency signal analysis method that can identify and extract oscillatory components with time-varying frequency and amplitude . We show that Synchrosqueezing is robust to bounded perturbations of the signal and to Gaussian white noise . These results justify its applicability to noisy or nonuniformly sampled data that is ubiquitous in engineering and the natural sciences . We also describe a practical implementation of Synchrosqueezing and provide guidance on tuning its main parameters . As a case study in the geosciences , we examine characteristics of a key paleoclimate change in the last 0 . 0 million years , where Synchrosqueezing provides significantly improved insights .
Epistemic planning can be used for decision making in multi-agent situations with distributed knowledge and capabilities . Recently , Dynamic Epistemic Logic ( DEL ) has been shown to provide a very natural and expressive framework for epistemic planning . We extend the DEL-based epistemic planning framework to include perspective shifts , allowing us to define new notions of sequential and conditional planning with implicit coordination . With these , it is possible to solve planning tasks with joint goals in a decentralized manner without the agents having to negotiate about and commit to a joint policy at plan time . First we define the central planning notions and sketch the implementation of a planning system built on those notions . Afterwards we provide some case studies in order to evaluate the planner empirically and to show that the concept is useful for multi-agent systems in practice .
A game-theoretic model for the study of dynamic networks is analyzed . The model is motivated by communication networks that are subject to failure of nodes and where the restoration needs resources . The corresponding two-player game is played between " Destructor " ( who can delete nodes ) and " Constructor " ( who can restore or even create nodes under certain conditions ) . We also include the feature of information flow by allowing Constructor to change labels of adjacent nodes . As objective for Constructor the network property to be connected is considered , either as a safety condition or as a reachability condition ( in the latter case starting from a non-connected network ) . We show under which conditions the solvability of the corresponding games for Constructor is decidable , and in this case obtain upper and lower complexity bounds , as well as algorithms derived from winning strategies . Due to the asymmetry between the players , safety and reachability objectives are not dual to each other and are treated separately .
Iterative proportional fitting ( IPF ) is a widely used method for spatial microsimulation . The technique results in non-integer weights for individual rows of data . This is problematic for certain applications and has led many researchers to favour combinatorial optimisation approaches such as simulated annealing . An alternative to this is `integerisation ' of IPF weights : the translation of the continuous weight variable into a discrete number of unique or `cloned ' individuals . We describe four existing methods of integerisation and present a new one . Our method --- `truncate , replicate , sample ' ( TRS ) --- recognises that IPF weights consist of both `replication weights ' and `conventional weights ' , the effects of which need to be separated . The procedure consists of three steps : 0 ) separate replication and conventional weights by truncation ; 0 ) replication of individuals with positive integer weights ; and 0 ) probabilistic sampling . The results , which are reproducible using supplementary code and data published alongside this paper , show that TRS is fast , and more accurate than alternative approaches to integerisation .
A foundation is investigated for the application of loosely structured data on the Web . This area is often referred to as Linked Data , due to the use of URIs in data to establish links . This work focuses on emerging W0C standards which specify query languages for Linked Data . The approach is to provide an abstract syntax to capture Linked Data structures and queries , which are then internalised in a process calculus . An operational semantics for the calculus specifies how queries , data and processes interact . A labelled transition system is shown to be sound with respect to the operational semantics . Bisimulation over the labelled transition system is used to verify an algebra over queries . The derived algebra is a contribution to the application domain . For instance , the algebra may be used to rewrite a query to optimise its distribution across a cluster of servers . The framework used to provide the operational semantics is powerful enough to model related calculi for the Web .
Let $\pi$ denote the intractable posterior density that results when the likelihood from a multivariate linear regression model with errors from a scale mixture of normals is combined with the standard non-informative prior . There is a simple data augmentation algorithm ( based on latent data from the mixing density ) that can be used to explore $\pi$ . Hobert et al . ( 0000 ) [arXiv : 0000 . 00000v0] recently performed a convergence rate analysis of the Markov chain underlying this MCMC algorithm in the special case where the regression model is univariate . These authors provide simple sufficient conditions ( on the mixing density ) for geometric ergodicity of the Markov chain . In this note , we extend Hobert et al . ' s ( 0000 ) result to the multivariate case .
. Markov chains in time , such as simple random walks , are at the heart of probability . In space , due to the absence of an obvious definition of past and future , a range of definitions of Markovianity have been proposed . In this paper , after a brief review , we introduce a new concept of Markovianity that aims to combine spatial and temporal conditional independence .
We introduce the family of $k$-gap-planar graphs for $k \geq 0$ , i . e . , graphs that have a drawing in which each crossing is assigned to one of the two involved edges and each edge is assigned at most $k$ of its crossings . This definition is motivated by applications in edge casing , as a $k$-gap-planar graph can be drawn crossing-free after introducing at most $k$ local gaps per edge . We present results on the maximum density of $k$-gap-planar graphs , their relationship to other classes of beyond-planar graphs , characterization of $k$-gap-planar complete graphs , and the computational complexity of recognizing $k$-gap-planar graphs .
Finite mixture models are statistical models which appear in many problems in statistics and machine learning . In such models it is assumed that data are drawn from random probability measures , called mixture components , which are themselves drawn from a probability measure P over probability measures . When estimating mixture models , it is common to make assumptions on the mixture components , such as parametric assumptions . In this paper , we make no assumption on the mixture components , and instead assume that observations from the mixture model are grouped , such that observations in the same group are known to be drawn from the same component . We show that any mixture of m probability measures can be uniquely identified provided there are 0m-0 observations per group . Moreover we show that , for any m , there exists a mixture of m probability measures that cannot be uniquely identified when groups have 0m-0 observations . Our results hold for any sample space with more than one element .
We consider the problem of representing a large population ' s behavior policy that drives the evolution of the population distribution over a discrete state space . A discrete time mean field game ( MFG ) is motivated as an interpretable model founded on game theory for understanding the aggregate effect of individual actions and predicting the temporal evolution of population distributions . We achieve a synthesis of MFG and Markov decision processes ( MDP ) by showing that a special MFG is reducible to an MDP . This enables us to broaden the scope of mean field game theory and infer MFG models of large real-world systems via deep inverse reinforcement learning . Our method learns both the reward function and forward dynamics of an MFG from real data , and we report the first empirical test of a mean field game model of a real-world social media population .
In the United States the preferred method of obtaining dietary intake data is the 00-hour dietary recall , yet the measure of most interest is usual or long-term average daily intake , which is impossible to measure . Thus , usual dietary intake is assessed with considerable measurement error . Also , diet represents numerous foods , nutrients and other components , each of which have distinctive attributes . Sometimes , it is useful to examine intake of these components separately , but increasingly nutritionists are interested in exploring them collectively to capture overall dietary patterns . Consumption of these components varies widely : some are consumed daily by almost everyone on every day , while others are episodically consumed so that 00-hour recall data are zero-inflated . In addition , they are often correlated with each other . Finally , it is often preferable to analyze the amount of a dietary component relative to the amount of energy ( calories ) in a diet because dietary recommendations often vary with energy level . The quest to understand overall dietary patterns of usual intake has to this point reached a standstill . There are no statistical methods or models available to model such complex multivariate data with its measurement error and zero inflation . This paper proposes the first such model , and it proposes the first workable solution to fit such a model . After describing the model , we use survey-weighted MCMC computations to fit the model , with uncertainty estimation coming from balanced repeated replication .
The Voice over Internet Protocol ( VoIP ) is becoming a more available and popular way of communicating for Internet users . This also applies to Peer-to-Peer ( P0P ) systems and merging these two have already proven to be successful ( e . g . Skype ) . Even the existing standards of VoIP provide an assurance of security and Quality of Service ( QoS ) , however , these features are usually optional and supported by limited number of implementations . As a result , the lack of mandatory and widely applicable QoS and security guaranties makes the contemporary VoIP systems vulnerable to attacks and network disturbances . In this paper we are facing these issues and propose the SecMon system , which simultaneously provides a lightweight security mechanism and improves quality parameters of the call . SecMon is intended specially for VoIP service over P0P networks and its main advantage is that it provides authentication , data integrity services , adaptive QoS and ( D ) DoS attack detection . Moreover , the SecMon approach represents a low-bandwidth consumption solution that is transparent to the users and possesses a self-organizing capability . The above-mentioned features are accomplished mainly by utilizing two information hiding techniques : digital audio watermarking and network steganography . These techniques are used to create covert channels that serve as transport channels for lightweight QoS measurement ' s results . Furthermore , these metrics are aggregated in a reputation system that enables best route path selection in the P0P network . The reputation system helps also to mitigate ( D ) DoS attacks , maximize performance and increase transmission efficiency in the network .
Topological Data Analysis ( tda ) is a recent and fast growing eld providing a set of new topological and geometric tools to infer relevant features for possibly complex data . This paper is a brief introduction , through a few selected topics , to basic fundamental and practical aspects of tda for non experts . 0 Introduction and motivation Topological Data Analysis ( tda ) is a recent eld that emerged from various works in applied ( algebraic ) topology and computational geometry during the rst decade of the century . Although one can trace back geometric approaches for data analysis quite far in the past , tda really started as a eld with the pioneering works of Edelsbrunner et al . ( 0000 ) and Zomorodian and Carlsson ( 0000 ) in persistent homology and was popularized in a landmark paper in 0000 Carlsson ( 0000 ) . tda is mainly motivated by the idea that topology and geometry provide a powerful approach to infer robust qualitative , and sometimes quantitative , information about the structure of data-see , e . g . Chazal ( 0000 ) . tda aims at providing well-founded mathematical , statistical and algorithmic methods to infer , analyze and exploit the complex topological and geometric structures underlying data that are often represented as point clouds in Euclidean or more general metric spaces . During the last few years , a considerable eort has been made to provide robust and ecient data structures and algorithms for tda that are now implemented and available and easy to use through standard libraries such as the Gudhi library ( C++ and Python ) Maria et al . ( 0000 ) and its R software interface Fasy et al . ( 0000a ) . Although it is still rapidly evolving , tda now provides a set of mature and ecient tools that can be used in combination or complementary to other data sciences tools . The tdapipeline . tda has recently known developments in various directions and application elds . There now exist a large variety of methods inspired by topological and geometric approaches . Providing a complete overview of all these existing approaches is beyond the scope of this introductory survey . However , most of them rely on the following basic and standard pipeline that will serve as the backbone of this paper : 0 . The input is assumed to be a nite set of points coming with a notion of distance-or similarity between them . This distance can be induced by the metric in the ambient space ( e . g . the Euclidean metric when the data are embedded in R d ) or come as an intrinsic metric dened by a pairwise distance matrix . The denition of the metric on the data is usually given as an input or guided by the application . It is however important to notice that the choice of the metric may be critical to reveal interesting topological and geometric features of the data .
Comment : Expert Elicitation for Reliable System Design [arXiv : 0000 . 0000]
Meta-analysis seeks to combine the results of several experiments in order to improve the accuracy of decisions . It is common to use a test for homogeneity to determine if the results of the several experiments are sufficiently similar to warrant their combination into an overall result . Cochran ' s Q statistic is frequently used for this homogeneity test . It is often assumed that Q follows a chi-square distribution under the null hypothesis of homogeneity , but it has long been known that this asymptotic distribution for Q is not accurate for moderate sample sizes . Here we present formulas for the mean and variance of Q under the null hypothesis which represent O ( 0/n ) corrections to the corresponding chi-square moments in the one parameter case . The formulas are fairly complicated , and so we provide a program ( available at http : //www . imperial . ac . uk/stathelp/researchprojects/metaanalysis ) for making the necessary calculations . We apply the results to the standardized mean difference ( Cohen ' s d-statistic ) and consider two approximations : a gamma distribution with estimated shape and scale parameters and the chi-square distribution with fractional degrees of freedom equal to the estimated mean of Q . We recommend the latter distribution as an approximate distribution for Q to use for testing the null hypothesis .
We introduce a new paradigm that is important for community detection in the realm of network analysis . Networks contain a set of strong , dominant communities , which interfere with the detection of weak , natural community structure . When most of the members of the weak communities also belong to stronger communities , they are extremely hard to be uncovered . We call the weak communities the hidden community structure . We present a novel approach called HICODE ( HIdden COmmunity DEtection ) that identifies the hidden community structure as well as the dominant community structure . By weakening the strength of the dominant structure , one can uncover the hidden structure beneath . Likewise , by reducing the strength of the hidden structure , one can more accurately identify the dominant structure . In this way , HICODE tackles both tasks simultaneously . Extensive experiments on real-world networks demonstrate that HICODE outperforms several state-of-the-art community detection methods in uncovering both the dominant and the hidden structure . In the Facebook university social networks , we find multiple non-redundant sets of communities that are strongly associated with residential hall , year of registration or career position of the faculties or students , while the state-of-the-art algorithms mainly locate the dominant ground truth category . In the Due to the difficulty of labeling all ground truth communities in real-world datasets , HICODE provides a promising approach to pinpoint the existing latent communities and uncover communities for which there is no ground truth . Finding this unknown structure is an extremely important community detection problem .
Box-Cox power transformation is a commonly used methodology to transform the distribution of a non-normal data into a normal one . Estimation of the transformation parameter is crucial in this methodology . In this study , the estimation process is hold via a searching algorithm and is integrated into well-known seven goodness of fit tests for normal distribution . An artificial covariate method is also included for comparative purposes . Simulation studies are implemented to compare the effectiveness of the proposed methods . The methods are also illustrated on two different real life data applications . Moreover , an R package AID is proposed for implementation .
Given a large graph G = ( V , E ) with millions of nodes and edges , how do we compute its connected components efficiently ? Recent work addresses this problem in map-reduce , where a fundamental trade-off exists between the number of map-reduce rounds and the communication of each round . Denoting d the diameter of the graph , and n the number of nodes in the largest component , all prior map-reduce techniques either require d rounds , or require about n|V| + |E| communication per round . We propose two randomized map-reduce algorithms -- ( i ) Hash-Greater-To-Min , which provably requires at most 0log ( n ) rounds with high probability , and at most 0 ( |V| + |E| ) communication per round , and ( ii ) Hash-to-Min , which has a worse theoretical complexity , but in practice completes in at most 0log ( d ) rounds and 0 ( |V| + |E| ) communication per rounds . Our techniques for connected components can be applied to clustering as well . We propose a novel algorithm for agglomerative single linkage clustering in map-reduce . This is the first algorithm that can provably compute a clustering in at most O ( log ( n ) ) rounds , where n is the size of the largest cluster . We show the effectiveness of all our algorithms through detailed experiments on large synthetic as well as real-world datasets .
Cognitive Dimensions is a framework for analyzing human-computer interaction . It is used for meta-analysis , that is , for talking about characteristics of systems without getting bogged down in details of a particular implementation . In this paper , I discuss some of the dimensions of this theory and how they can be applied to analyze information seeking interfaces . The goal of this analysis is to introduce a useful vocabulary that practitioners and researchers can use to describe systems , and to guide interface design toward more usable and useful systems
Technical documents contain a fair amount of unnatural language , such as tables , formulas , pseudo-codes , etc . Unnatural language can be an important factor of confusing existing NLP tools . This paper presents an effective method of distinguishing unnatural language from natural language , and evaluates the impact of unnatural language detection on NLP tasks such as document clustering . We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories . First , we create a new annotated corpus by collecting slides and papers in various formats , PPT , PDF , and HTML , where unnatural language components are annotated into four categories . We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text . Our experiments show that removing unnatural language components gives an absolute improvement in document clustering up to 00% . Our corpus and tool are publicly available .
Despite much work within the last decade on foundational properties of SPARQL - the standard query language for RDF data - rather little is known about the exact limits of tractability for this language . In particular , this is the case for SPARQL queries that contain the OPTIONAL-operator , even though it is one of the most intensively studied features of SPARQL . The aim of our work is to provide a more thorough picture of tractable classes of SPARQL queries . In general , SPARQL query evaluation is PSPACE-complete in combined complexity , and it remains PSPACE-hard already for queries containing only the OPTIONAL-operator . To amend this situation , research has focused on " well-designed SPARQL queries " and their recent generalization " weakly well-designed SPARQL queries " . For these two fragments the evaluation problem is coNP-complete in the absence of projection and SigmaP0-complete otherwise . Moreover , they have been shown to contain most SPARQL queries asked in practical settings . In this paper , we study tractable classes of weakly well-designed queries in parameterized complexity considering the equivalent formulation as pattern trees . We give a complete characterization of the tractable classes in the case without projection . Moreover , we show a characterization of all tractable classes of simple well-designed pattern trees in the presence of projection .
In this paper , we introduce a wideband dictionary framework for estimating sparse signals . By formulating integrated dictionary elements spanning bands of the considered parameter space , one may efficiently find and discard large parts of the parameter space not active in the signal . After each iteration , the zero-valued parts of the dictionary may be discarded to allow a refined dictionary to be formed around the active elements , resulting in a zoomed dictionary to be used in the following iterations . Implementing this scheme allows for more accurate estimates , at a much lower computational cost , as compared to directly forming a larger dictionary spanning the whole parameter space or performing a zooming procedure using standard dictionary elements . Different from traditional dictionaries , the wideband dictionary allows for the use of dictionaries with fewer elements than the number of available samples without loss of resolution . The technique may be used on both one- and multi-dimensional signals , and may be exploited to refine several traditional sparse estimators , here illustrated with the LASSO and the SPICE estimators . Numerical examples illustrate the improved performance .
This paper is concerned with the statistical development of our spatial-temporal data mining procedure , LASR ( pronounced ``laser ' ' ) . LASR is the abbreviation for Longitudinal Analysis with Self-Registration of large-$p$-small-$n$ data . It was motivated by a study of ``Neuromuscular Electrical Stimulation ' ' experiments , where the data are noisy and heterogeneous , might not align from one session to another , and involve a large number of multiple comparisons . The three main components of LASR are : ( 0 ) data segmentation for separating heterogeneous data and for distinguishing outliers , ( 0 ) automatic approaches for spatial and temporal data registration , and ( 0 ) statistical smoothing mapping for identifying ``activated ' ' regions based on false-discovery-rate controlled $p$-maps and movies . Each of the components is of interest in its own right . As a statistical ensemble , the idea of LASR is applicable to other types of spatial-temporal data sets beyond those from the NMES experiments .
To model modern large-scale datasets , we need efficient algorithms to infer a set of $P$ unknown model parameters from $N$ noisy measurements . What are fundamental limits on the accuracy of parameter inference , given finite signal-to-noise ratios , limited measurements , prior information , and computational tractability requirements ? How can we combine prior information with measurements to achieve these limits ? Classical statistics gives incisive answers to these questions as the measurement density $\alpha = \frac{N}{P}\rightarrow \infty$ . However , these classical results are not relevant to modern high-dimensional inference problems , which instead occur at finite $\alpha$ . We formulate and analyze high-dimensional inference as a problem in the statistical physics of quenched disorder . Our analysis uncovers fundamental limits on the accuracy of inference in high dimensions , and reveals that widely cherished inference algorithms like maximum likelihood ( ML ) and maximum-a posteriori ( MAP ) inference cannot achieve these limits . We further find optimal , computationally tractable algorithms that can achieve these limits . Intriguingly , in high dimensions , these optimal algorithms become computationally simpler than MAP and ML , while still outperforming them . For example , such optimal algorithms can lead to as much as a 00% reduction in the amount of data to achieve the same performance relative to MAP . Moreover , our analysis reveals simple relations between optimal high dimensional inference and low dimensional scalar Bayesian inference , insights into the nature of generalization and predictive power in high dimensions , information theoretic limits on compressed sensing , phase transitions in quadratic inference , and connections to central mathematical objects in convex optimization theory and random matrix theory .
A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations . However , analytical estimates can be obtained only for particular combinations of analytical models of signal and noise , thus precluding its straightforward extension to deal with other arbitrary noise sources . In this paper , we propose an alternative non-explicit way to take into account the relations among natural image wavelet coefficients for denoising : we use support vector regression ( SVR ) in the wavelet domain to enforce these relations in the estimated signal . Since relations among the coefficients are specific to the signal , the regularization property of SVR is exploited to remove the noise , which does not share this feature . The specific signal relations are encoded in an anisotropic kernel obtained from mutual information measures computed on a representative image database . Training considers minimizing the Kullback-Leibler divergence ( KLD ) between the estimated and actual probability functions of signal and noise in order to enforce similarity . Due to its non-parametric nature , the method can eventually cope with different noise sources without the need of an explicit re-formulation , as it is strictly necessary under parametric Bayesian formalisms . Results under several noise levels and noise sources show that : ( 0 ) the proposed method outperforms conventional wavelet methods that assume coefficient independence , ( 0 ) it is similar to state-of-the-art methods that do explicitly include these relations when the noise source is Gaussian , and ( 0 ) it gives better numerical and visual performance when more complex , realistic noise sources are considered . Therefore , the proposed machine learning approach can be seen as a more flexible ( model-free ) alternative to the explicit description of wavelet coefficient relations for image denoising .
In recent years genetic algorithms have emerged as a useful tool for the heuristic solution of complex discrete optimisation problems . In particular there has been considerable interest in their use in tackling problems arising in the areas of scheduling and timetabling . However , the classical genetic algorithm paradigm is not well equipped to handle constraints and successful implementations usually require some sort of modification to enable the search to exploit problem specific knowledge in order to overcome this shortcoming . This paper is concerned with the development of a family of genetic algorithms for the solution of a nurse rostering problem at a major UK hospital . The hospital is made up of wards of up to 00 nurses . Each ward has its own group of nurses whose shifts have to be scheduled on a weekly basis . In addition to fulfilling the minimum demand for staff over three daily shifts , nurses ' wishes and qualifications have to be taken into account . The schedules must also be seen to be fair , in that unpopular shifts have to be spread evenly amongst all nurses , and other restrictions , such as team nursing and special conditions for senior staff , have to be satisfied . The basis of the family of genetic algorithms is a classical genetic algorithm consisting of n-point crossover , single-bit mutation and a rank-based selection . The solution space consists of all schedules in which each nurse works the required number of shifts , but the remaining constraints , both hard and soft , are relaxed and penalised in the fitness function . The talk will start with a detailed description of the problem and the initial implementation and will go on to highlight the shortcomings of such an approach , in terms of the key element of balancing feasibility , i . e . covering the demand and work regulations , and quality , as measured by the nurses ' preferences . A series of experiments involving parameter adaptation , niching , intelligent weights , delta coding , local hill climbing , migration and special selection rules will then be outlined and it will be shown how a series of these enhancements were able to eradicate these difficulties . Results based on several months ' real data will be used to measure the impact of each modification , and to show that the final algorithm is able to compete with a tabu search approach currently employed at the hospital . The talk will conclude with some observations as to the overall quality of this approach to this and similar problems .
We introduce a methodology for efficiently computing a lower bound to empowerment , allowing it to be used as an unsupervised cost function for policy learning in real-time control . Empowerment , being the channel capacity between actions and states , maximises the influence of an agent on its near future . It has been shown to be a good model of biological behaviour in the absence of an extrinsic goal . But empowerment is also prohibitively hard to compute , especially in nonlinear continuous spaces . We introduce an efficient , amortised method for learning empowerment-maximising policies . We demonstrate that our algorithm can reliably handle continuous dynamical systems using system dynamics learned from raw data . The resulting policies consistently drive the agents into states where they can use their full potential .
The AUV three-dimension path planning in complex turbulent underwater environment is investigated in this research , in which static current map data and uncertain static-moving time variant obstacles are taken into account . Robustness of AUVs path planning to this strong variability is known as a complex NP-hard problem and is considered a critical issue to ensure vehicles safe deployment . Efficient evolutionary techniques have substantial potential of handling NP hard complexity of path planning problem as more powerful and fast algorithms among other approaches for mentioned problem . For the purpose of this research Differential Evolution ( DE ) technique is conducted to solve the AUV path planning problem in a realistic underwater environment . The path planners designed in this paper are capable of extracting feasible areas of a real map to determine the allowed spaces for deployment , where coastal area , islands , static/dynamic obstacles and ocean current is taken into account and provides the efficient path with a small computation time . The results obtained from analyze of experimental demonstrate the inherent robustness and drastic efficiency of the proposed scheme in enhancement of the vehicles path planning capability in coping undesired current , using useful current flow , and avoid colliding collision boundaries in a real-time manner . The proposed approach is also flexible and strictly respects to vehicle ' s kinematic constraints resisting current instabilities .
Winds from the North-West quadrant and lack of precipitation are known to lead to an increase of PM00 concentrations over a residential neighborhood in the city of Taranto ( Italy ) . In 0000 the local government prescribed a reduction of industrial emissions by 00% every time such meteorological conditions are forecasted 00 hours in advance . Wind forecasting is addressed using the Weather Research and Forecasting ( WRF ) atmospheric simulation system by the Regional Environmental Protection Agency . In the context of distributions-oriented forecast verification , we propose a comprehensive model-based inferential approach to investigate the ability of the WRF system to forecast the local wind speed and direction allowing different performances for unknown weather regimes . Ground-observed and WRF-forecasted wind speed and direction at a relevant location are jointly modeled as a 0-dimensional time series with an unknown finite number of states characterized by homogeneous distributional behavior . The proposed model relies on a mixture of joint projected and skew normal distributions with time-dependent states , where the temporal evolution of the state membership follows a first order Markov process . Parameter estimates , including the number of states , are obtained by a Bayesian MCMC-based method . Results provide useful insights on the performance of WRF forecasts in relation to different combinations of wind speed and direction .
Nowadays , Web services ( WS ) remain a main actor in the implementation of distributed applications . They represent a new promising paradigm for the development , deployment and integration of Internet applications . The aim of Web services composition is to use the skills of several departments to resolve any problem that cannot be solved individually . The result of this composition is a compound of Web services that define how they will be used . In this paper , we propose an approach for automatic web services composition based on the concepts of directed graphs for the representation and description of Web services , and the ordering of web services compound execution . In this context , the user query , defined by a set of inputs and outputs , can be viewed as a directed graph composed of Web services .
In many real-world binary classification tasks ( e . g . detection of certain objects from images ) , an available dataset is imbalanced , i . e . , it has much less representatives of a one class ( a minor class ) , than of another . Generally , accurate prediction of the minor class is crucial but it ' s hard to achieve since there is not much information about the minor class . One approach to deal with this problem is to preliminarily resample the dataset , i . e . , add new elements to the dataset or remove existing ones . Resampling can be done in various ways which raises the problem of choosing the most appropriate one . In this paper we experimentally investigate impact of resampling on classification accuracy , compare resampling methods and highlight key points and difficulties of resampling .
When functional data manifest amplitude and phase variations , a commonly-employed framework for analyzing them is to take away the phase variation through a function alignment and then to apply standard tools to the aligned functions . A downside of this approach is that the important variations contained in the phases are completely ignored . To combine both of amplitude and phase variations , we propose a variant of principal component analysis ( PCA ) that captures non-linear components representing the amplitude , phase and their associations simultaneously . The proposed method , which we call functional combined PCA , is aimed to provide more efficient dimension reduction with interpretable components , in particular when the amplitudes and phases are clearly associated . We model principal components by non-linearly combining time-warping functions and aligned functions . A data-adaptive weighting procedure helps our dimension reduction to attain a maximal explaining power of observed functions . We also discuss an application of functional canonical correlation analysis in investigation of the correlation structure between the two variations . We show that for two sets of real data the proposed method provides interpretable major non-linear components , which are not typically found in the usual functional PCA .
We present ASYMP , a distributed graph processing system developed for the timely analysis of graphs with trillions of edges . ASYMP has several distinguishing features including a robust fault tolerance mechanism , a lockless architecture which scales seamlessly to thousands of machines , and efficient data access patterns to reduce per-machine overhead . ASYMP is used to analyze the largest graphs at Google , and the graphs we consider in our empirical evaluation here are , to the best of our knowledge , the largest considered in the literature . Our experimental results show that compared to previous graph processing frameworks at Google , ASYMP can scale to larger graphs , operate on more crowded clusters , and complete real-world graph mining analytic tasks faster . First , we evaluate the speed of ASYMP , where we show that across a diverse selection of graphs , it runs Connected Component 0-00x faster than state of the art implementations in MapReduce and Pregel . Then we demonstrate the scalability and parallelism of this framework : first by showing that the running time increases linearly by increasing the size of the graphs ( without changing the number of machines ) , and then by showing the gains in running time while increasing the number of machines . Finally , we demonstrate the fault-tolerance properties for the framework , showing that inducing 00% of our machines to fail increases the running time by only 00% .
Solvency games , introduced by Berger et al . , provide an abstract framework for modelling decisions of a risk-averse investor , whose goal is to avoid ever going broke . We study a new variant of this model , where , in addition to stochastic environment and fixed increments and decrements to the investor ' s wealth , we introduce interest , which is earned or paid on the current level of savings or debt , respectively . We study problems related to the minimum initial wealth sufficient to avoid bankruptcy ( i . e . steady decrease of the wealth ) with probability at least p . We present an exponential time algorithm which approximates this minimum initial wealth , and show that a polynomial time approximation is not possible unless P = NP . For the qualitative case , i . e . p=0 , we show that the problem whether a given number is larger than or equal to the minimum initial wealth belongs to both NP and coNP , and show that a polynomial time algorithm would yield a polynomial time algorithm for mean-payoff games , existence of which is a longstanding open problem . We also identify some classes of solvency MDPs for which this problem is in P . In all above cases the algorithms also give corresponding bankruptcy avoiding strategies .
We present the Robotic Overlay coMmunicAtioN prOtocol ( ROMANO ) , a lightweight , application layer overlay communication protocol for a unified sensing and control abstraction of a network of heterogeneous robots mainly consisting of low power , low-compute-capable robots . ROMANO is built to work in conjunction with the well-known MQ Telemetry Transport for Sensor Nodes ( MQTT-SN ) protocol , a lightweight publish-subscribe communication protocol for the Internet of Things and makes use its concept of " topics " to designate the addition and deletion of communication endpoints by changing the subscriptions of topics at each device . We also develop a portable implementation of ROMANO for low power IEEE 000 . 00 . 0 ( Zigbee ) radios and deployed it on a small testbed of commercially available , low-power , and low-compute-capable robots called Pololu 0pi robots . Based on a thorough analysis of the protocol on the real testbed , as a measure of throughput , we demonstrate that ROMANO can guarantee more than a $00 . 0\%$ message delivery ratio for a message generation rate up to 000 messages per second . The single hop delays in ROMANO are as low as 00ms with linear dependency on the number of robots connected . These delay numbers concur with typical delays in 000 . 00 . 0 networks and suggest that ROMANO does not introduce additional delays . Lastly , we implement four different multi-robot applications to demonstrate the scalability , adaptability , ease of integration , and reliability of ROMANO .
Pull-tabbing is an evaluation approach for functional logic computations , based on a graph transformation recently proposed , which avoids making irrevocable non-deterministic choices that would jeopardize the completeness of computations . In contrast to other approaches with this property , it does not require an upfront cloning of a possibly large portion of the choice ' s context . We formally define the pull-tab transformation , characterize the class of programs for which the transformation is intended , extend the computations in these programs to include the transformation , and prove the correctness of the extended computations .
In this work we focus on examination and comparison of whole-brain functional connectivity patterns measured with fMRI across experimental conditions . Direct examination and comparison of condition-specific matrices is challenging due to the large number of elements in a connectivity matrix . We present a framework that uses network analysis to describe condition-specific functional connectivity . Treating the brain as a complex system in terms of a network , we extract the most relevant connectivity information by partitioning each network into clusters representing functionally connected brain regions . Extracted clusters are used as features for predicting experimental condition in a new data set . The approach is illustrated on fMRI data examining functional connectivity patterns during processing of abstract and concrete concepts . Topological ( brain regions ) and functional ( level of connectivity and information flow ) systematic differences in the ROI-based functional networks were identified across participants for concrete and abstract concepts . These differences were sufficient for classification of previously unseen connectivity matrices as abstract or concrete based on training data derived from other people .
Fully Programmable Valve Array ( FPVA ) has emerged as a new architecture for the next-generation flow-based microfluidic biochips . This 0D-array consists of regularly-arranged valves , which can be dynamically configured by users to realize microfluidic devices of different shapes and sizes as well as interconnections . Additionally , the regularity of the underlying structure renders FPVAs easier to integrate on a tiny chip . However , these arrays may suffer from various manufacturing defects such as blockage and leakage in control and flow channels . Unfortunately , no efficient method is yet known for testing such a general-purpose architecture . In this paper , we present a novel formulation using the concept of flow paths and cut-sets , and describe an ILP-based hierarchical strategy for generating compact test sets that can detect multiple faults in FPVAs . Simulation results demonstrate the efficacy of the proposed method in detecting manufacturing faults with only a small number of test vectors .
Rank aggregation is an essential approach for aggregating the preferences of multiple agents . One rule of particular interest is the Kemeny rule , which maximises the number of pairwise agreements between the final ranking and the existing rankings . However , Kemeny rankings are NP-hard to compute . This has resulted in the development of various algorithms . Fortunately , NP-hardness may not reflect the difficulty of solving problems that arise in practice . As a result , we aim to demonstrate that the Kemeny consensus can be computed efficiently when aggregating different rankings in real case . In this paper , we extend a dynamic programming algorithm originally for Kemeny scores . We also provide details on the implementation of the algorithm . Finally , we present results obtained from an empirical comparison of our algorithm and two other popular algorithms based on real world and randomly generated problem instances . Experimental results show the usefulness and efficiency of the algorithm in practical settings .
As computational challenges in optimization and statistical inference grow ever harder , algorithms that utilize derivatives are becoming increasingly more important . The implementation of the derivatives that make these algorithms so powerful , however , is a substantial user burden and the practicality of these algorithms depends critically on tools like automatic differentiation that remove the implementation burden entirely . The Stan Math Library is a C++ , reverse-mode automatic differentiation library designed to be usable , extensive and extensible , efficient , scalable , stable , portable , and redistributable in order to facilitate the construction and utilization of such algorithms . Usability is achieved through a simple direct interface and a cleanly abstracted functional interface . The extensive built-in library includes functions for matrix operations , linear algebra , differential equation solving , and most common probability functions . Extensibility derives from a straightforward object-oriented framework for expressions , allowing users to easily create custom functions . Efficiency is achieved through a combination of custom memory management , subexpression caching , traits-based metaprogramming , and expression templates . Partial derivatives for compound functions are evaluated lazily for improved scalability . Stability is achieved by taking care with arithmetic precision in algebraic expressions and providing stable , compound functions where possible . For portability , the library is standards-compliant C++ ( 00 ) and has been tested for all major compilers for Windows , Mac OS X , and Linux .
Multicolor cell spatio-temporal image data have become important to investigate organ development and regeneration , malignant growth or immune responses by tracking different cell types both in vivo and in vitro . Statistical modeling of image data from common longitudinal cell experiments poses significant challenges due to the presence of complex spatio-temporal interactions between different cell types and difficulties related to measurement of single cell trajectories . Current analysis methods focus mainly on univariate cases , often not considering the spatio-temporal effects affecting cell growth between different cell populations . In this paper , we propose a conditional spatial autoregressive model to describe multivariate count cell data on the lattice , and develop inference tools . The proposed methodology is computationally tractable and enables researchers to estimate a complete statistical model of multicolor cell growth . Our methodology is applied on real experimental data where we investigate how interactions between cells affect their growth . We include two case studies ; the first evaluates interactions between cancer cells and fibroblasts , which are normally present in the tumor microenvironment , whilst the second evaluates interactions between cloned cancer cells when grown as different combinations .
We present a novel spectral learning algorithm for simultaneous localization and mapping ( SLAM ) from range data with known correspondences . This algorithm is an instance of a general spectral system identification framework , from which it inherits several desirable properties , including statistical consistency and no local optima . Compared with popular batch optimization or multiple-hypothesis tracking ( MHT ) methods for range-only SLAM , our spectral approach offers guaranteed low computational requirements and good tracking performance . Compared with popular extended Kalman filter ( EKF ) or extended information filter ( EIF ) approaches , and many MHT ones , our approach does not need to linearize a transition or measurement model ; such linearizations can cause severe errors in EKFs and EIFs , and to a lesser extent MHT , particularly for the highly non-Gaussian posteriors encountered in range-only SLAM . We provide a theoretical analysis of our method , including finite-sample error bounds . Finally , we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified , but works well in practice : in a comparison of multiple methods , the lowest errors come from a combination of our algorithm with batch optimization , but our method alone produces nearly as good a result at far lower computational cost .
Web Engineering is the application of systematic , disciplined and quantifiable approaches to development , operation , and maintenance of Web-based applications . It is both a pro-active approach and a growing collection of theoretical and empirical research in Web application development . This paper gives an overview of Web Engineering by addressing the questions : a ) why is it needed ? b ) what is its domain of operation ? c ) how does it help and what should it do to improve Web application development ? and d ) how should it be incorporated in education and training ? The paper discusses the significant differences that exist between Web applications and conventional software , the taxonomy of Web applications , the progress made so far and the research issues and experience of creating a specialisation at the master ' s level . The paper reaches a conclusion that Web Engineering at this stage is a moving target since Web technologies are constantly evolving , making new types of applications possible , which in turn may require innovations in how they are built , deployed and maintained .
Quality is an implicit property of models and modelling languages by their condition of engineering artifacts . However , the quality property is affected by the diversity of conceptions around the model-driven paradigm . In this document is presented a report of quality issues on modelling languages and models . These issues result from an analysis about quality evidences obtained from industrial and academic/scientific contexts .
In this paper we study MapReduce computations from a complexity-theoretic perspective . First , we formulate a uniform version of the MRC model of Karloff et al . ( 0000 ) . We then show that the class of regular languages , and moreover all of sublogarithmic space , lies in constant round MRC . This result also applies to the MPC model of Andoni et al . ( 0000 ) . In addition , we prove that , conditioned on a variant of the Exponential Time Hypothesis , there are strict hierarchies within MRC so that increasing the number of rounds or the amount of time per processor increases the power of MRC . To the best of our knowledge we are the first to approach the MapReduce model with complexity-theoretic techniques , and our work lays the foundation for further analysis relating MapReduce to established complexity classes .
We develop methods for analyzing discrete multivariate longitudinal data and apply them to functional disability data on the U . S . elderly population from the National Long Term Care Survey ( NLTCS ) , 0000-0000 . Our models build on a Mixed Membership framework , in which individuals are allowed multiple membership on a set of extreme profiles characterized by time-dependent trajectories of progression into disability . We also develop an extension that allows us to incorporate birth-cohort effects , in order to assess inter-generational changes . Applying these methods , we find that most individuals follow trajectories that imply a late onset of disability , and that younger cohorts tend to develop disabilities at a later stage in life compared to their elders .
This volume contains the papers accepted at the Second International Workshop on FPGAs for Software Programmers ( FSP 0000 ) , held in London , United Kingdom , September 0st , 0000 . FSP 0000 was co-located with the International Conference on Field Programmable Logic and Applications ( FPL ) .
We present N0Net , a system that implements binary neural networks using commodity switching chips deployed in network switches and routers . Our system shows that these devices can run simple neural network models , whose input is encoded in the network packets ' header , at packet processing speeds ( billions of packets per second ) . Furthermore , our experience highlights that switching chips could support even more complex models , provided that some minor and cheap modifications to the chip ' s design are applied . We believe N0Net provides an interesting building block for future end-to-end networked systems .
Recurrent neural networks are increasing popular models for sequential learning . Unfortunately , although the most effective RNN architectures are perhaps excessively complicated , extensive searches have not found simpler alternatives . This paper imports ideas from physics and functional programming into RNN design to provide guiding principles . From physics , we introduce type constraints , analogous to the constraints that forbids adding meters to seconds . From functional programming , we require that strongly-typed architectures factorize into stateless learnware and state-dependent firmware , reducing the impact of side-effects . The features learned by strongly-typed nets have a simple semantic interpretation via dynamic average-pooling on one-dimensional convolutions . We also show that strongly-typed gradients are better behaved than in classical architectures , and characterize the representational power of strongly-typed nets . Finally , experiments show that , despite being more constrained , strongly-typed architectures achieve lower training and comparable generalization error to classical architectures .
Recent progress on many imaging and vision tasks has been driven by the use of deep feed-forward neural networks , which are trained by propagating gradients of a loss defined on the final output , back through the network up to the first layer that operates directly on the image . We propose back-propagating one step further---to learn camera sensor designs jointly with networks that carry out inference on the images they capture . In this paper , we specifically consider the design and inference problems in a typical color camera---where the sensor is able to measure only one color channel at each pixel location , and computational inference is required to reconstruct a full color image . We learn the camera sensor ' s color multiplexing pattern by encoding it as layer whose learnable weights determine which color channel , from among a fixed set , will be measured at each location . These weights are jointly trained with those of a reconstruction network that operates on the corresponding sensor measurements to produce a full color image . Our network achieves significant improvements in accuracy over the traditional Bayer pattern used in most color cameras . It automatically learns to employ a sparse color measurement approach similar to that of a recent design , and moreover , improves upon that design by learning an optimal layout for these measurements .
We propose analytical models that allow us to investigate the performance of long range wide area network ( LoRaWAN ) uplink in terms of latency , collision rate , and throughput under the constraints of the regulatory duty cycling , when assuming exponential inter-arrival times . Our models take into account sub-band selection and the case of sub-band combining . Our numerical evaluations consider specifically the European ISM band , but the analysis is applicable to any coherent band . Protocol simulations are used to validate the proposed models . We find that sub-band selection and combining have a large effect on the quality of service ( QoS ) experienced in an LoRaWAN cell for a given load . The proposed models allow for the optimization of resource allocation within a cell given a set of QoS requirements and a traffic model .
Motivated by the need for fast synchronized operation of power microgrids , we analyze the problem of single and multiple pinning in networked systems . We derive lower and upper bounds on the algebraic connectivity of the network with respect to the reference signal . These bounds are utilized to devise a suboptimal algorithm with polynomial complexity to find a suitable set of nodes to pin the network effectively and efficiently . The results are applied to secondary voltage pinning control design for a microgrid in islanded operation mode . Comparisons with existing single and multiple pinning strategies clearly demonstrates the efficacy of the obtained results .
We propose HSMUCE ( heterogeneous simultaneous multiscale change-point estimator ) for the detection of multiple change-points of the signal in a heterogeneous gaussian regression model . A piecewise constant function is estimated by minimizing the number of change-points over the acceptance region of a multiscale test which locally adapts to changes in the variance . The multiscale test is a combination of local likelihood ratio tests which are properly calibrated by scale dependent critical values in order to keep a global nominal level alpha , even for finite samples . We show that HSMUCE controls the error of over- and underestimation of the number of change-points . To this end , new deviation bounds for F-type statistics are derived . Moreover , we obtain confidence sets for the whole signal . All results are non-asymptotic and uniform over a large class of heterogeneous change-point models . HSMUCE is fast to compute , achieves the optimal detection rate and estimates the number of change-points at almost optimal accuracy for vanishing signals , while still being robust . We compare HSMUCE with several state of the art methods in simulations and analyse current recordings of a transmembrane protein in the bacterial outer membrane with pronounced heterogeneity for its states . An R-package is available online .
Cloud computing relies on secure and efficient virtualization . Software level security solutions compromise the performance of virtual machines ( VMs ) , as a large amount of computational power would be utilized for running the security modules . Moreover , software solutions are only as secure as the level that they work on . For example a security module on a hypervisor cannot provide security in the presence of an infected hypervisor . It is a challenge for virtualization technology architects to enhance the security of VMs without degrading their performance . Currently available server machines are not fully equipped to support a secure VM environment without compromising on performance . A few hardware modifications have been introduced by manufactures like Intel and AMD to provide a secure VM environment with low performance degradation . In this paper we propose a novel memory architecture model named \textit{ Architectural Support for Memory Isolation ( ASMI ) } , that can achieve a true isolated physical memory region to each VM without degrading performance . Along with true memory isolation , ASMI is designed to provide lower memory access times , better utilization of available memory , support for DMA isolation and support for platform independence for users of VMs .
Hydroclimatic processes are characterized by heterogeneous spatiotemporal correlation structures and marginal distributions that can be continuous , mixed-type , discrete or even binary . Simulating exactly such processes can greatly improve hydrological analysis and design . Yet this challenging task is accomplished often by ad hoc and approximate methodologies that are devised for specific variables and purposes . In this study , a single framework is proposed allowing the exact simulation of processes with any marginal and any correlation structure . We unify , extent , and improve of a general-purpose modelling strategy based on the assumption that any process can emerge by transforming a parent Gaussian process with a specific correlation structure . A novel mathematical representation of the parent-Gaussian scheme provides a consistent and fully general description that supersedes previous specific parameterizations , resulting in a simple , fast and efficient simulation procedure for every spatiotemporal process . In particular , introducing a simple but flexible procedure we obtain a parametric expression of the correlation transformation function , allowing to assess the correlation structure of the parent-Gaussian process that yields the prescribed correlation of the target process after marginal back transformation . The same framework is also applicable for cyclostationary and multivariate modelling . The simulation of a variety of hydroclimatic variables with very different correlation structures and marginals , such as precipitation , stream flow , wind speed , humidity , extreme events per year , etc . , as well as a multivariate application , highlights the flexibility , advantages , and complete generality of the proposed methodology .
Big data analytics has gathered immense research attention lately because of its ability to harness useful information from heaps of data . Cloud computing has been adjudged as one of the best infrastructural solutions for implementation of big data analytics . This research paper proposes a five-layer model for cloud-based big data analytics that uses dew computing and edge computing concepts . Besides this , the paper also presents an approach for creation of custom big data stack by selecting technologies on the basis of identified data and computing models for the application
Predictive recursion ( PR ) is a fast stochastic algorithm for nonparametric estimation of mixing distributions in mixture models . It is known that the PR estimates of both the mixing and mixture densities are consistent under fairly mild conditions , but currently very little is known about the rate of convergence . Here I first investigate asymptotic convergence properties of the PR estimate under model misspecification in the special case of finite mixtures with known support . Tools from stochastic approximation theory are used to prove that the PR estimates converge , to the best Kullback--Leibler approximation , at a nearly root-$n$ rate . When the support is unknown , PR can be used to construct an objective function which , when optimized , yields an estimate the support . I apply the known-support results to derive a rate of convergence for this modified PR estimate in the unknown support case , which compares favorably to known optimal rates .
This paper presents a non-parametric approach for segmenting trees from airborne LiDAR data in deciduous forests . Based on the LiDAR point cloud , the approach collects crown information such as steepness and height on-the-fly to delineate crown boundaries , and most importantly , does not require a priori assumptions of crown shape and size . The approach segments trees iteratively starting from the tallest within a given area to the smallest until all trees have been segmented . To evaluate its performance , the approach was applied to the University of Kentucky Robinson Forest , a deciduous closed-canopy forest with complex terrain and vegetation conditions . The approach identified 00% of dominant and co-dominant trees with a false detection rate of 00% . About 00% of intermediate , overtopped , and dead trees were also detected with a false detection rate of 00% . The overall segmentation accuracy was 00% . Correlations of the segmentation scores of the proposed approach with local terrain and stand metrics was not significant , which is likely an indication of the robustness of the approach as results are not sensitive to the differences in terrain and stand structures .
Mixture models with Gamma and or inverse-Gamma distributed mixture components are useful for medical image tissue segmentation or as post-hoc models for regression coefficients obtained from linear regression within a Generalised Linear Modeling framework ( GLM ) , used in this case to separate stochastic ( Gaussian ) noise from some kind of positive or negative " activation " ( modeled as Gamma or inverse-Gamma distributed ) . To date , the most common choice in this context it is Gaussian/Gamma mixture models learned through a maximum likelihood ( ML ) approach ; we recently extended such algorithm for mixture models with inverse-Gamma components . Here , we introduce a fully analytical Variational Bayes ( VB ) learning framework for both Gamma and/or inverse-Gamma components . We use synthetic and resting state fMRI data to compare the performance of the ML and VB algorithms in terms of area under the curve and computational cost . We observed that the ML Gaussian/Gamma model is very expensive specially when considering high resolution images ; furthermore , these solutions are highly variable and they occasionally can overestimate the activations severely . The Bayesian Gauss-Gamma is in general the fastest algorithm but provides too dense solutions . The maximum likelihood Gaussian/inverse-Gamma is also very fast but provides in general very sparse solutions . The variational Gaussian/inverse-Gamma mixture model is the most robust and its cost is acceptable even for high resolution images . Further , the presented methodology represents an essential building block that can be directly used in more complex inference tasks , specially designed to analyse MRI-fMRI data ; such models include for example analytical variational mixture models with adaptive spatial regularization or better source models for new spatial blind source separation approaches .
Monte Carlo ( MC ) techniques are often used to estimate integrals of a multivariate function using randomly generated samples of the function . In light of the increasing interest in uncertainty quantification and robust design applications in aerospace engineering , the calculation of expected values of such functions ( e . g . performance measures ) becomes important . However , MC techniques often suffer from high variance and slow convergence as the number of samples increases . In this paper we present Stacked Monte Carlo ( StackMC ) , a new method for post-processing an existing set of MC samples to improve the associated integral estimate . StackMC is based on the supervised learning techniques of fitting functions and cross validation . It should reduce the variance of any type of Monte Carlo integral estimate ( simple sampling , importance sampling , quasi-Monte Carlo , MCMC , etc . ) without adding bias . We report on an extensive set of experiments confirming that the StackMC estimate of an integral is more accurate than both the associated unprocessed Monte Carlo estimate and an estimate based on a functional fit to the MC samples . These experiments run over a wide variety of integration spaces , numbers of sample points , dimensions , and fitting functions . In particular , we apply StackMC in estimating the expected value of the fuel burn metric of future commercial aircraft and in estimating sonic boom loudness measures . We compare the efficiency of StackMC with that of more standard methods and show that for negligible additional computational cost significant increases in accuracy are gained .
This volume contains the proceedings of SOS 0000 , the Eight Workshop on Structural Operational Semantics , held on the 0th of September 0000 in Aachen , Germany as an affiliated workshop of CONCUR 0000 , the 00nd International Conference on Concurrency Theory . Structural operational semantics ( SOS ) provides a framework for giving operational semantics to programming and specification languages . A growing number of programming languages from commercial and academic spheres have been given usable semantic descriptions by means of structural operational semantics . Because of its intuitive appeal and flexibility , structural operational semantics has found considerable application in the study of the semantics of concurrent processes . It is also a viable alternative to denotational semantics in the static analysis of programs , and in proving compiler correctness . Moreover , it has found application in emerging areas of computing such as probabilistic systems and systems biology . Structural operational semantics has been successfully applied as a formal tool to establish results that hold for classes of process description languages . This has allowed for the generalization of well-known results in the field of process algebra , and for the development of a meta-theory for process calculi based on the realization that many of the results in this field only depend upon general semantic properties of language constructs . The workshop is a forum for researchers , students and practitioners interested in new developments and directions for future investigations . One of the specific goals of the workshop is to provide a meeting point for the concurrency and programming language communities . Another goal is the dissemination of the theory and practice of SOS amongst postgraduate students and young researchers worldwide .
This paper presents a case study of Agile Scrum process followed in Retail Domain project . This paper also reveals the impacts of Cost of Software Quality , when agile scrum process is not followed efficiently . While analyzing the case study , the gaps were found and guidelines for process improvements were also suggested in this paper .
We consider schemes for obtaining truthful reports on a common but hidden signal from large groups of rational , self-interested agents . One example are online feedback mechanisms , where users provide observations about the quality of a product or service so that other users can have an accurate idea of what quality they can expect . However , ( i ) providing such feedback is costly , and ( ii ) there are many motivations for providing incorrect feedback . Both problems can be addressed by reward schemes which ( i ) cover the cost of obtaining and reporting feedback , and ( ii ) maximize the expected reward of a rational agent who reports truthfully . We address the design of such incentive-compatible rewards for feedback generated in environments with pure adverse selection . Here , the correlation between the true knowledge of an agent and her beliefs regarding the likelihoods of reports of other agents can be exploited to make honest reporting a Nash equilibrium . In this paper we extend existing methods for designing incentive-compatible rewards by also considering collusion . We analyze different scenarios , where , for example , some or all of the agents collude . For each scenario we investigate whether a collusion-resistant , incentive-compatible reward scheme exists , and use automated mechanism design to specify an algorithm for deriving an efficient reward mechanism .
This paper is concerned by the statistical analysis of data sets whose elements are random histograms . For the purpose of learning principal modes of variation from such data , we consider the issue of computing the PCA of histograms with respect to the 0-Wasserstein distance between probability measures . To this end , we propose to compare the methods of log-PCA and geodesic PCA in the Wasserstein space as introduced by Bigot et al . ( 0000 ) and Seguy and Cuturi ( 0000 ) . Geodesic PCA involves solving a non-convex optimization problem . To solve it approximately , we propose a novel forward-backward algorithm . This allows a detailed comparison between log-PCA and geodesic PCA of one-dimensional histograms , which we carry out using various data sets , and stress the benefits and drawbacks of each method . We extend these results for two-dimensional data and compare both methods in that setting .
We determine the joint limiting distribution of adjacent spacings around a central , intermediate , or an extreme order statistic $X_{k : n}$ of a random sample of size $n$ from a continuous distribution $F$ . For central and intermediate cases , normalized spacings in the left and right neighborhoods are asymptotically i . i . d . exponential random variables . The associated independent Poisson arrival processes are independent of $X_{k : n}$ . For an extreme $X_{k : n}$ , the asymptotic independence property of spacings fails for $F$ in the domain of attraction of Fr\ ' {e}chet and Weibull ( $\alpha \neq 0$ ) distributions . This work also provides additional insight into the limiting distribution for the number of observations around $X_{k : n}$ for all three cases .
A new technique for data hiding in digital image is proposed in this paper . Steganography is a well known technique for hiding data in an image , but generally the format of image plays a pivotal role in it , and the scheme is format dependent . In this paper we will discuss a new technique where irrespective of the format of image , we can easily hide a large amount of data without deteriorating the quality of the image . The data to be hidden is enciphered with the help of a secret key . This enciphered data is then embedded at the end of the image . The enciphered data bits are extracted and then deciphered with the help of same key used for encryption . Simulation results show that Image Quality Measures of this proposed scheme are better than the conventional LSB replacing technique . The proposed method is simple and is easy to implement .
The reproduction and replication of novel results has become a major issue for a number of scientific disciplines . In computer science and related computational disciplines such as systems biology , the issues closely revolve around the ability to implement novel algorithms and approaches . Taking an approach from the literature and applying it to a new codebase frequently requires local knowledge missing from the published manuscripts and project websites . Alongside this issue , benchmarking , and the development of fair --- and widely available --- benchmark sets present another barrier . In this paper , we outline several suggestions to address these issues , driven by specific examples from a range of scientific domains . Finally , based on these suggestions , we propose a new open platform for scientific software development which effectively isolates specific dependencies from the individual researcher and their workstation and allows faster , more powerful sharing of the results of scientific software engineering .
Feature selection has been recently used in the area of software engineering for improving the accuracy and robustness of software cost models . The idea behind selecting the most informative subset of features from a pool of available cost drivers stems from the hypothesis that reducing the dimensionality of datasets will significantly minimise the complexity and time required to reach to an estimation using a particular modelling technique . This work investigates the appropriateness of attributes , obtained from empirical project databases and aims to reduce the cost drivers used while preserving performance . Finding suitable subset selections that may cater improved predictions may be considered as a pre-processing step of a particular technique employed for cost estimation ( filter or wrapper ) or an internal ( embedded ) step to minimise the fitting error . This paper compares nine relatively popular feature selection methods and uses the empirical values of selected attributes recorded in the ISBSG and Desharnais datasets to estimate software development effort .
This paper establishes a novel analytical approach to quantify robustness of scheduling and battery management for battery supported cyber-physical systems . A dynamic schedulability test is introduced to determine whether tasks are schedulable within a finite time window . The test is used to measure robustness of a real-time scheduling algorithm by evaluating the strength of computing time perturbations that break schedulability at runtime . Robustness of battery management is quantified analytically by an adaptive threshold on the state of charge . The adaptive threshold significantly reduces the false alarm rate for battery management algorithms to decide when a battery needs to be replaced .
We are motivated by problems that arise in a number of applications such as Online Marketing and explosives detection , where the observations are usually modeled using Poisson statistics . We model each observation as a Poisson random variable whose mean is a sparse linear superposition of known patterns . Unlike many conventional problems observations here are not identically distributed since they are associated with different sensing modalities . We analyze the performance of a Maximum Likelihood ( ML ) decoder , which for our Poisson setting involves a non-linear optimization but yet is computationally tractable . We derive fundamental sample complexity bounds for sparse recovery when the measurements are contaminated with Poisson noise . In contrast to the least-squares linear regression setting with Gaussian noise , we observe that in addition to sparsity , the scale of the parameters also fundamentally impacts sample complexity . We introduce a novel notion of Restricted Likelihood Perturbation ( RLP ) , to jointly account for scale and sparsity . We derive sample complexity bounds for $\ell_0$ regularized ML estimators in terms of RLP and further specialize these results for deterministic and random sensing matrix designs .
This paper addresses design , modeling and dynamic-compensation PID ( dc-PID ) control of a novel type of fully-actuated aerial manipulation ( AM ) system . Firstly , design of novel mechanical structure of the AM is presented . Secondly , kinematics and dynamics of AM are modeled using Craig parameters and recursion Newton-Euler equations respectively , which give rise to a more accurate dynamic relationship between aerial platform and manipulator . Then , the dynamic-compensation PID control is proposed to solve the problem of fully-actuated control of AM . Finally , uniform coupled matrix equations between driving forces/moments and rotor speeds are derived , which can support design and analysis of parameters and decoupling theoretically . It is taken into account practical problems including noise and perturbation , parameter uncertainty , and power limitation in simulations , and results from simulations shows that the AM system presented can be fully-actued controlled with advanced control performances , which can not achieved theoretically in traditional AM . And with compared to backstepping control dc-PID has better control accuracy and capability to disturbance rejection in two simulations of aerial operation tasks with motion of joint . The experiment of dc-pid proves the availability and effectiveness of the method proposed .
We consider the problem of localizing wireless devices in an ad-hoc network embedded in a d-dimensional Euclidean space . Obtaining a good estimation of where wireless devices are located is crucial in wireless network applications including environment monitoring , geographic routing and topology control . When the positions of the devices are unknown and only local distance information is given , we need to infer the positions from these local distance measurements . This problem is particularly challenging when we only have access to measurements that have limited accuracy and are incomplete . We consider the extreme case of this limitation on the available information , namely only the connectivity information is available , i . e . , we only know whether a pair of nodes is within a fixed detection range of each other or not , and no information is known about how far apart they are . Further , to account for detection failures , we assume that even if a pair of devices is within the detection range , it fails to detect the presence of one another with some probability and this probability of failure depends on how far apart those devices are . Given this limited information , we investigate the performance of a centralized positioning algorithm MDS-MAP introduced by Shang et al . , and a distributed positioning algorithm , introduced by Savarese et al . , called HOP-TERRAIN . In particular , for a network consisting of n devices positioned randomly , we provide a bound on the resulting error for both algorithms . We show that the error is bounded , decreasing at a rate that is proportional to R/Rc , where Rc is the critical detection range when the resulting random network starts to be connected , and R is the detection range of each device .
In process mining , precision measures are used to quantify how much a process model overapproximates the behavior seen in an event log . Although several measures have been proposed throughout the years , no research has been done to validate whether these measures achieve the intended aim of quantifying over-approximation in a consistent way for all models and logs . This paper fills this gap by postulating a number of axioms for quantifying precision consistently for any log and any model . Further , we show through counter-examples that none of the existing measures consistently quantifies precision .
In this paper , a quantization based watermark casting and blind watermark retrieval algorithm operating in YCbCr color space using discrete wavelet transform ( DWT ) , for ownership verification and image authentication applications is implemented . This method uses implicit visual masking by inserting watermark bits into only the wavelet coefficients of high magnitude , in Y channel of YCbCr color space . A blind watermark retrieval technique that can detect the embedded watermark without the help from the original uncorrupted image is devised which is computationally efficient . The new watermarking algorithm combines and adapts various aspects from existing watermarking methods . Experimental results show that the proposed technique to embed watermark provides extra imperceptibility and robustness against various signal processing attacks in comparison with the same technique in RGB color space .
This paper investigates the ( conditional ) quasi-likelihood ratio test for the threshold in MA models . Under the hypothesis of no threshold , it is shown that the test statistic converges weakly to a function of the centred Gaussian process . Under local alternatives , it is shown that this test has nontrivial asymptotic power . The results are based on a new weak convergence of a linear marked empirical process , which is independently of interest . This paper also gives an invertible expansion of the threshold MA models .
In this paper , we discuss the connection between the RGRST models ( Gardiner et al 0000 , Polverejan et al 0000 ) and the Coxian Phase-Type ( CPH ) models ( Marshall et al 0000 , Tang 0000 ) through a construction that converts a special sub-class of RGRST models to CPH models . Both of the two models are widely used to characterize the distribution of hospital charge and length of stay ( LOS ) , but the lack of connections between them makes the two models rarely used together . We claim that our construction can make up this gap and make it possible to take advantage of the two different models simultaneously . As a consequence , we derive a measure of the " price " of staying in each medical stage ( identified with phases of a CPH model ) , which can ' t be approached without considering the RGRST and CPH models together . A two-stage algorithm is provided to generate consistent estimation of model parameters . Applying the algorithm to a sample drawn from the New York State ' s Statewide Planning and Research Cooperative System 0000 ( SPARCS 0000 ) , we estimate the prices in a four-phase CPH model and discuss the implications .
In this paper we proposed the alternative test to the two independent and normally distributed samples t test based on the cross variance concept . We present the simulation results of the power and the error rate of the special case of the cross variance which is when the variances of the two samples are homogeneous and the t tests . The simulation results show that the special case of the cross variance test has the power and the error type I rate equal to the $t$ test . This result suggests that the proposed test could be used as an alternative to detect whether there is difference between means of the two independent normally distributed samples . We give some example comparative case studies of the special case of the cross variance and the t tests .
Aiming at abundant scientific and engineering data with not only high dimensionality but also complex structure , we study the regression problem with a multidimensional array ( tensor ) response and a vector predictor . Applications include , among others , comparing tensor images across groups after adjusting for additional covariates , which is of central interest in neuroimaging analysis . We propose parsimonious tensor response regression adopting a generalized sparsity principle . It models all voxels of the tensor response jointly , while accounting for the inherent structural information among the voxels . It effectively reduces the number of free parameters , leading to feasible computation and improved interpretation . We achieve model estimation through a nascent technique called the envelope method , which identifies the immaterial information and focuses the estimation based upon the material information in the tensor response . We demonstrate that the resulting estimator is asymptotically efficient , and it enjoys a competitive finite sample performance . We also illustrate the new method on two real neuroimaging studies .
The intersection type assignment system has been designed directly as deductive system for assigning formulae of the implicative and conjunctive fragment of the intuitionistic logic to terms of lambda-calculus . But its relation with the logic is not standard . Between all the logics that have been proposed as its foundation , we consider ISL , which gives a logical interpretation of the intersection by splitting the intuitionistic conjunction into two connectives , with a local and global behaviour respectively , being the intersection the local one . We think ISL is a logic interesting by itself , and in order to support this claim we give a sequent calculus formulation of it , and we prove that it enjoys the cut elimination property .
In this paper we demonstrate the power of a model of tile self-assembly based on active glues which can dynamically change state . We formulate the Signal-passing Tile Assembly Model ( STAM ) , based on the model of Padilla , Liu , and Seeman to be asynchronous , allowing any action of turning a glue on or off , attaching a new tile , or breaking apart an assembly to happen in any order . Within this highly generalized model we provide three new solutions to tile self-assembly problems that have been addressed within the abstract Tile Assembly Model and its variants , showing that signal passing tiles allow for substantial improvement across multiple complexity metrics . Our first result utilizes a recursive assembly process to achieve tile-type efficient assembly of linear structures , using provably fewer tile types than what is possible in standard tile assembly models . Our second system of signal-passing tiles simulates any Turing machine with high fuel efficiency by using only a constant number of tiles per computation step . Our third system assembles the discrete Sierpinski triangle , demonstrating that this pattern can be strictly self-assembled within the STAM . This result is of particular interest in that it is known that this pattern cannot self-assemble within a number of well studied tile self-assembly models . Notably , all of our constructions are at temperature 0 , further demonstrating that signal-passing confers the power to bypass many restrictions found in standard tile assembly models .
We develop here several goodness-of-fit tests for testing the k-monotonicity of a discrete density , based on the empirical distribution of the observations . Our tests are non-parametric , easy to implement and are proved to be asymptotically of the desired level and consistent . We propose an estimator of the degree of k-monotonicity of the distribution based on the non-parametric goodness-of-fit tests . We apply our work to the estimation of the total number of classes in a population . A large simulation study allows to assess the performances of our procedures .
In cooperative game theory , games in partition function form are real-valued function on the set of so-called embedded coalitions , that is , pairs $ ( S , \pi ) $ where $S$ is a subset ( coalition ) of the set $N$ of players , and $\pi$ is a partition of $N$ containing $S$ . Despite the fact that many studies have been devoted to such games , surprisingly nobody clearly defined a structure ( i . e . , an order ) on embedded coalitions , resulting in scattered and divergent works , lacking unification and proper analysis . The aim of the paper is to fill this gap , thus to study the structure of embedded coalitions ( called here embedded subsets ) , and the properties of games in partition function form .
We find the asymptotic distribution of the sample autocovariances of long-memory processes in cases of finite and infinite fourth moment . Depending on the interplay of assumptions on moments and the intensity of dependence , there are three types of convergence rates and limit distributions . In particular , a normal approximation with the standard rate does not always hold in practically relevant cases .
In this paper , we analyze the planar cubic Alternative curve to determine the conditions for convex , loops , cusps and inflection points . Thus cubic curve is represented by linear combination of three control points and basis function that consist of two shape parameters . By using algebraic manipulation , we can determine the constraint of shape parameters and sufficient conditions are derived which ensure that the curve is a strictly convex , loops , cusps and inflection point . We conclude the result in a shape diagram of parameters . The simplicity of this form makes characterization more intuitive and efficient to compute .
Embedded systems interaction with environment inherently complicates understanding of requirements and their correct implementation . However , product uncertainty is highest during early stages of development . Design verification is an essential step in the development of any system , especially for Embedded System . This paper introduces a novel adaptive design methodology , which incorporates step-wise prototyping and verification . With each adaptive step product-realization level is enhanced while decreasing the level of product uncertainty , thereby reducing the overall costs . The back-bone of this frame-work is the development of Domain Specific Operational ( DOP ) Model and the associated Verification Instrumentation for Test and Evaluation , developed based on the DOP model . Together they generate functionally valid test-sequence for carrying out prototype evaluation . With the help of a case study ' Multimode Detection Subsystem ' the application of this method is sketched . The design methodologies can be compared by defining and computing a generic performance criterion like Average design-cycle Risk . For the case study , by computing Average design-cycle Risk , it is shown that the adaptive method reduces the product development risk for a small increase in the total design cycle time .
Finding interesting rule in the sixth strategy step about threshold control on generalized relations in attribute oriented induction , there is possibility to select candidate attribute for further generalization and merging of identical tuples until the number of tuples is no greater than the threshold value , as implemented in basic attribute oriented induction algorithm . At this strategy step there is possibility the number of tuples in final generalization result still greater than threshold value . In order to get the final generalization result which only small number of tuples and can be easy to transfer into simple logical formula , the seventh strategy step about rule transformation is evolved where there will be simplification by unioning or grouping the identical attribute . Our approach to measure interesting rule is opposite with heuristic measurement approach by Fudger and Hamilton where the more complex concept hierarchies , more interesting results are likely to be found , but our approach the simpler concept hierarchies , more interesting results are likely to be found and the more complex concept hierarchies , more complex process generalization in concept tree . The decision to find interesting rule is influenced with wide or length and depth or level of concept tree .
Historically , infectious diseases caused considerable damage to human societies , and they continue to do so today . To help reduce their impact , mathematical models of disease transmission have been studied to help understand disease dynamics and inform prevention strategies . Vaccination - one of the most important preventive measures of modern times - is of great interest both theoretically and empirically . And in contrast to traditional approaches , recent research increasingly explores the pivotal implications of individual behavior and heterogeneous contact patterns in populations . Our report reviews the developmental arc of theoretical epidemiology with emphasis on vaccination , as it led from classical models assuming homogeneously mixing ( mean-field ) populations and ignoring human behavior , to recent models that account for behavioral feedback and/or population spatial/social structure . Many of the methods used originated in statistical physics , such as lattice and network models , and their associated analytical frameworks . Similarly , the feedback loop between vaccinating behavior and disease propagation forms a coupled nonlinear system with analogs in physics . We also review the new paradigm of digital epidemiology , wherein sources of digital data such as online social media are mined for high-resolution information on epidemiologically relevant individual behavior . Armed with the tools and concepts of statistical physics , and further assisted by new sources of digital data , models that capture nonlinear interactions between behavior and disease dynamics offer a novel way of modeling real-world phenomena , and can help improve health outcomes . We conclude the review by discussing open problems in the field and promising directions for future research .
Urbi SDK is a software platform for the development of portable robotic applications . It features the Urbi UObject C++ middleware , to manage hardware drivers and/or possibly remote software components , and urbiscript , a domain specific programming language to orchestrate them . Reactivity is a key feature of Urbi SDK , embodied in events in urbiscript . This paper presents the support for events in urbiscript .
We present an efficient feature selection method that can find all multiplicative combinations of continuous features that are statistically significantly associated with the class variable , while rigorously correcting for multiple testing . The key to overcome the combinatorial explosion in the number of candidates is to derive a lower bound on the $p$-value for each feature combination , which enables us to massively prune combinations that can never be significant and gain more statistical power . While this problem has been addressed for binary features in the past , we here present the first solution for continuous features . In our experiments , our novel approach detects true feature combinations with higher precision and recall than competing methods that require a prior binarization of the data .
Under a mild condition we give closed-form expressions for copulas of systems that consist of maxima and of minima of subvectors of a given random vector $X$ with continuous marginals . Said expressions appear explicit in the copula of $X$ and the mentioned condition is for example met when the law of $X$ admits a strictly positive density with respect to Lebesgue measure . In the i . i . d . case these " maxmin " copulae become universal and the conditions on their validity can be dropped entirely . Our main motivation comes from applications to shock models that arise in multivariate survival theory . Another application is to order statistics copulas .
We investigate the use of hybrid techniques in complex processes of infectious diseases . Since predictive disease models in biomedicine require a multiscale approach for understanding the molecule-cell-tissue-organ-body interactions , heterogeneous methodologies are often employed for describing the different biological scales . Hybrid models provide effective means for complex disease modelling where the action and dosage of a drug or a therapy could be meaningfully investigated : the infection dynamics can be classically described in a continuous fashion , while the scheduling of multiple treatment discretely . We define an algebraic language for specifying general disease processes and multiple treatments , from which a semantics in terms of hybrid dynamical system can be derived . Then , the application of control-theoretic tools is proposed in order to compute the optimal scheduling of multiple therapies . The potentialities of our approach are shown in the case study of the SIR epidemic model and we discuss its applicability on osteomyelitis , a bacterial infection affecting the bone remodelling system in a specific and multiscale manner . We report that formal languages are helpful in giving a general homogeneous formulation for the different scales involved in a multiscale disease process ; and that the combination of hybrid modelling and control theory provides solid grounds for computational medicine .
Video sequences contain rich dynamic patterns , such as dynamic texture patterns that exhibit stationarity in the temporal domain , and action patterns that are non-stationary in either spatial or temporal domain . We show that a spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns . The model defines a probability distribution on the video sequence , and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales . The model can be learned from the training video sequences by an " analysis by synthesis " learning algorithm that iterates the following two steps . Step 0 synthesizes video sequences from the currently learned model . Step 0 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences . We show that the learning algorithm can synthesize realistic dynamic patterns .
This paper studies the problem of control strategy synthesis for dynamical systems with differential constraints to fulfill a given reachability goal while satisfying a set of safety rules . Particular attention is devoted to goals that become feasible only if a subset of the safety rules are violated . The proposed algorithm computes a control law , that minimizes the level of unsafety while the desired goal is guaranteed to be reached . This problem is motivated by an autonomous car navigating an urban environment while following rules of the road such as " always travel in right lane ' ' and " do not change lanes frequently ' ' . Ideas behind sampling based motion-planning algorithms , such as Probabilistic Road Maps ( PRMs ) and Rapidly-exploring Random Trees ( RRTs ) , are employed to incrementally construct a finite concretization of the dynamics as a durational Kripke structure . In conjunction with this , a weighted finite automaton that captures the safety rules is used in order to find an optimal trajectory that minimizes the violation of safety rules . We prove that the proposed algorithm guarantees asymptotic optimality , i . e . , almost-sure convergence to optimal solutions . We present results of simulation experiments and an implementation on an autonomous urban mobility-on-demand system .
This is the first detailed study on the coverage of Microsoft Academic ( MA ) . Based on the complete and verified publication list of a university , the coverage of MA was assessed and compared with two benchmark databases , Scopus and Web of Science ( WoS ) , on the level of individual publications . Citation counts were analyzed , and issues related to data retrieval and data quality were examined . A Perl script was written to retrieve metadata from MA based on publication titles . The script is freely available on GitHub . We find that MA covers journal articles , working papers , and conference items to a substantial extent and indexes more document types than the benchmark databases ( e . g . , working papers , dissertations ) . MA clearly surpasses Scopus and WoS in covering book-related document types and conference items but falls slightly behind Scopus in journal articles . The coverage of MA is favorable for evaluative bibliometrics in most research fields , including economics/business , computer/information sciences , and mathematics . However , MA shows biases similar to Scopus and WoS with regard to the coverage of the humanities , non-English publications , and open-access publications . Rank correlations of citation counts are high between MA and the benchmark databases . We find that the publication year is correct for 00 . 0% of all publications and the number of authors is correct for 00 . 0% of the journal articles . Given the fast and ongoing development of MA , we conclude that MA is on the verge of becoming a bibliometric superpower . However , comprehensive studies on the quality of MA metadata are still lacking .
This study considers the control of parent-child systems where a parent system is acted on by a set of child systems with their own inputs . Examples of such systems include a swarm of robots pushing an object over a surface , a swarm of aerial vehicles carrying a large load , or a set of end effectors manipulating an object . In this paper , a general framework for decoupling the swarm from the parent system through a low-dimensional abstract state space is presented . The requirements of this framework are presented along with how constraints on both systems propagate through the abstract state and impact the requirements of the controllers for both systems . Several controllers with hard state constraints are designed to track a desired angle of the tilting plane with a swarm of robots driving on top . Both homogeneous and heterogeneous swarms of varying sizes and properties are considered . The controllers are shown to be locally asymptotically stable and are demonstrated in simulation .
We show that the decidability of the first-order theory of the language that combines Boolean algebras of sets of uninterpreted elements with Presburger arithmetic operations . We thereby disprove a recent conjecture that this theory is undecidable . Our language allows relating the cardinalities of sets to the values of integer variables , and can distinguish finite and infinite sets . We use quantifier elimination to show the decidability and obtain an elementary upper bound on the complexity . Precise program analyses can use our decidability result to verify representation invariants of data structures that use an integer field to represent the number of stored elements .
Let $ ( Y_i , Z_i ) _{i\geq 0}$ be a sequence of independent , identically distributed ( i . i . d . ) random vectors taking values in $\RRR^k\times\RRR^d$ , for some integers $k$ and $d$ . Given $z\in \RRR^d$ , we provide a nonstandard functional limit law for the sequence of functional increments of the compound empirical process , namely $$\mathbf{\Delta}_{n , \cc} ( h_n , z , \cdot ) : = \frac{0}{nh_n}\sliin 0_{[0 , \cdot ) }\poo \frac{Z_i-z}{{h_n}^{0/d}}\pff Y_i . $$ Provided that $nh_n\sim c\log n $ as $\nif$ , we obtain , under some natural conditions on the conditional exponential moments of $Y\mid Z=z$ , that $$\mathbf{\Delta}_{n , \cc} ( h_n , z , \cdot ) \leadsto \Gam\text{almost surely} , $$ where $\leadsto$ denotes the clustering process under the sup norm on $\Idd$ . Here , $\Gam$ is a compact set that is related to the large deviations of certain compound Poisson processes .
Data centers have emerged as promising resources for demand response , particularly for emergency demand response ( EDR ) , which saves the power grid from incurring blackouts during emergency situations . However , currently , data centers typically participate in EDR by turning on backup ( diesel ) generators , which is both expensive and environmentally unfriendly . In this paper , we focus on " greening " demand response in multi-tenant data centers , i . e . , colocation data centers , by designing a pricing mechanism through which the data center operator can efficiently extract load reductions from tenants during emergency periods to fulfill energy reduction requirement for EDR . In particular , we propose a pricing mechanism for both mandatory and voluntary EDR programs , ColoEDR , that is based on parameterized supply function bidding and provides provably near-optimal efficiency guarantees , both when tenants are price-taking and when they are price-anticipating . In addition to analytic results , we extend the literature on supply function mechanism design , and evaluate ColoEDR using trace-based simulation studies . These validate the efficiency analysis and conclude that the pricing mechanism is both beneficial to the environment and to the data center operator ( by decreasing the need for backup diesel generation ) , while also aiding tenants ( by providing payments for load reductions ) .
A system model is developed where the criterion to partition the world into a system and a rest is based on the functional relation between its states . This approach implies that the gestalt of systems becomes very dynamic . Especially interactions between systems may create temporary super systems " on the fly " . The reference to the function notion establishes the link to computation . Based on the distinction between the intended determinism versus nondeterminism of the interaction , two classes of system interactions are distinguished : composition and cooperation . Composition means that by interaction super systems are created and the interacting systems become subsystems . Cooperation means that systems sensibly interact loosely without creating any identifiable super system function and therefore without super system creation from the perspective of the interacting systems . Cooperative system interactions are described with the help of the protocol notion based on shared states , interpreted as the stateful exchange of characters via Shannon channels between roles - the projection of systems onto the interaction channels . It is shown that roles can be internally coordinated by a set of rules , leading to a very flexible process notion which unfolds its full potential only in a new type of execution environment capable of executing spontaneous transitions . The system model has immediate implications for componentization which can be viewed as an attempt to either hide functional recursion and provide only functionality whose composition behavior is easy to understand , or to provide loosely coupled interactions via protocols . To be complete , component models should therefore at least support three general classes of components : one for loose coupling , one for hierarchical composition , and one for pipes .
Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels . We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution . Like convolutional neural networks , the proposed model has a degree of translation invariance built-in , but the amount of computation it performs can be controlled independently of the input image size . While the model is non-differentiable , it can be trained using reinforcement learning methods to learn task-specific policies . We evaluate our model on several image classification tasks , where it significantly outperforms a convolutional neural network baseline on cluttered images , and on a dynamic visual control problem , where it learns to track a simple object without an explicit training signal for doing so .
Software developers cannot always anticipate how users will actually use their software as it may vary from user to user , and even from use to use for an individual user . In order to address questions raised by system developers and evaluators about software usage , we define new probabilistic models that characterise user behaviour , based on activity patterns inferred from actual logged user traces . We encode these new models in a probabilistic model checker and use probabilistic temporal logics to gain insight into software usage . We motivate and illustrate our approach by application to the logged user traces of an iOS app .
Shape restricted regressions , including isotonic regression and concave regression as special cases , are studied using priors on Bernstein polynomials and Markov chain Monte Carlo methods . These priors have large supports , select only smooth functions , can easily incorporate geometric information into the prior , and can be generated without computational difficulty . Algorithms generating priors and posteriors are proposed , and simulation studies are conducted to illustrate the performance of this approach . Comparisons with the density-regression method of Dette et al . ( 0000 ) are included .
We present a highly general implementation of fast multipole methods on graphics processing units ( GPUs ) . Our two-dimensional double precision code features an asymmetric type of adaptive space discretization leading to a particularly elegant and flexible implementation . All steps of the multipole algorithm are efficiently performed on the GPU , including the initial phase which assembles the topological information of the input data . Through careful timing experiments we investigate the effects of the various peculiarities of the GPU architecture .
This paper presents a data stationary architecture in which each word has an attached address field . Address fields massively update in parallel to record data interchanges . Words do not move until memory is read for post processing . A sea of such cells can test large-scale quantum algorithms , although other programming is possible .
The " Planning in the Early Medieval Landscape " project ( PEML ) <http : //humanities . exeter . ac . uk/archaeology/research/projects/planningintheearlymedievallandscape/> , funded by the Leverhulme Trust , has organized and collated a substantial quantity of images , and has used this as evidence to support the hypothesis that Anglo-Saxon building construction was based on grid-like planning structures based on fixed modules or quanta of measurement . We report on the development of some statistical contributions to the debate concerning this hypothesis . In practice the PEML images correspond to data arising in a wide variety of different forms . It does not seem feasible to produce a single automatic method which can be applied uniformly to all such images ; even the initial chore of cleaning up an image ( removing extraneous material such as legends and physical features which do not bear on the planning hypothesis ) typically presents a separate and demanding challenge for each different image . Moreover care must be taken , even in the relatively straightforward cases of clearly defined ground-plans ( for example for large ecclesiastical buildings of the period ) , to consider exactly what measurements might be relevant . We report on pilot statistical analyses concerning three different situations . These establish not only the presence of underlying structure ( which indeed is often visually obvious ) , but also provide an account of the numerical evidence supporting the deduction that such structure is present . We contend that statistical methodology thus contributes to the larger historical debate and provides useful input to the wide and varied range of evidence that has to be debated .
In this paper , we present Scratch Community Blocks , a new system that enables children to programmatically access , analyze , and visualize data about their participation in Scratch , an online community for learning computer programming . At its core , our approach involves a shift in who analyzes data : from adult data scientists to young learners themselves . We first introduce the goals and design of the system and then demonstrate it by describing example projects that illustrate its functionality . Next , we show through a series of case studies how the system engages children in not only representing data and answering questions with data but also in self-reflection about their own learning and participation .
The spread of tick-borne pathogens represents an important threat to human and animal health in many parts of Eurasia . Here , we analysed a 0-year time series of Ixodes ricinus ticks feeding on Apodemus flavicollis mice ( main reservoir-competent host for tick-borne encephalitis , TBE ) sampled in Trentino ( Northern Italy ) . The tail of the distribution of the number of ticks per host was fitted by three theoretical distributions : Negative Binomial ( NB ) , Poisson-LogNormal ( PoiLN ) , and Power-Law ( PL ) . The fit with theoretical distributions indicated that the tail of the tick infestation pattern on mice is better described by the PL distribution . Moreover , we found that the tail of the distribution significantly changes with seasonal variations in host abundance . In order to investigate the effect of different tails of tick distribution on the invasion of a non-systemically transmitted pathogen , we simulated the transmission of a TBE-like virus between susceptible and infective ticks using a stochastic model . Model simulations indicated different outcomes of disease spreading when considering different distribution laws of ticks among hosts . Specifically , we found that the epidemic threshold and the prevalence equilibria obtained in epidemiological simulations with PL distribution are a good approximation of those observed in simulations feed by the empirical distribution . Moreover , we also found that the epidemic threshold for disease invasion was lower when considering the seasonal variation of tick aggregation .
This Brief Communication discusses the benefits of citation analysis in research evaluation based on Galton ' s " Wisdom of Crowds " ( 0000 ) . Citations are based on the assessment of many which is why they can be ascribed a certain amount of accuracy . However , we show that citations are incomplete assessments and that one cannot assume that a high number of citations correlate with a high level of usefulness . Only when one knows that a rarely cited paper has been widely read is it possible to say ( strictly speaking ) that it was obviously of little use for further research . Using a comparison with ' like ' data , we try to determine that cited reference analysis allows a more meaningful analysis of bibliometric data than times-cited analysis .
Copulas are popular as models for multivariate dependence because they allow the marginal densities and the joint dependence to be modeled separately . However , they usually require that the transformation from uniform marginals to the marginals of the joint dependence structure is known . This can only be done for a restricted set of copulas , e . g . a normal copula . Our article introduces copula-type estimators for flexible multivariate density estimation which also allow the marginal densities to be modeled separately from the joint dependence , as in copula modeling , but overcomes the lack of flexibility of most popular copula estimators . An iterative scheme is proposed for estimating copula-type estimators and its usefulness is demonstrated through simulation and real examples . The joint dependence is is modeled by mixture of normals and mixture of normals factor analyzers models , and mixture of t and mixture of t factor analyzers models . We develop efficient Variational Bayes algorithms for fitting these in which model selection is performed automatically . Based on these mixture models , we construct four classes of copula-type densities which are far more flexible than current popular copula densities , and outperform them in simulation and several real data sets .
Over the years , different data assimilation methods have been implemented to acquire improved estimations of model parameters by adjusting the uncertain parameter values in such a way that the mathematical model approximates the observed data as closely and consistently as possible . However , most of these methods are developed on the assumption of Gaussianity , e . g . Ensemble Kalman Filters , which is not the case in practical situations , and hence they result in poor estimations . In this work , the estimations of uncertain parameters are acquired from an Ensemble Square Root Kalman Filter and from a novel method , an Ensemble Transform Particle Filter , that does not have an assumption of Gaussianity . The latter method is developed on the backbone of Bayesian approach of sequential Monte Carlo with the framework of linear transport problem and has proved to be highly beneficial for systems with non-Gaussian distributions . We examine the performance of these methods in a twin experiment setup , when the observations of pressure are synthetically created based on the assumed true values of the uncertain parameters and implementing the Darcy flow model as the forward model , which is used for the data assimilation algorithm as well . We consider two test cases based on different geometrical configurations and distributions of permeability field across the domain representing low and high dimensional systems with small and large number of uncertain parameters , respectively . The numerical experiments demonstrate that Ensemble Transform Particle Filter provides comparable results with that of Ensemble Square Root Kalman Filter for the low dimensional system and outperforms it for the high dimensional system .
The objective of this paper is to study the Gibbs sampling for computing the mean of observable in very high dimension - a powerful Markov chain Monte Carlo method . Under the Dobrushin ' s uniqueness condition , we establish some explicit and sharp estimate of the exponential convergence rate and prove some Gaussian concentration inequalities for the empirical mean .
In this paper , we explore ordinal classification ( in the context of deep neural networks ) through a simple modification of the squared error loss which not only allows it to not only be sensitive to class ordering , but also allows the possibility of having a discrete probability distribution over the classes . Our formulation is based on the use of a softmax hidden layer , which has received relatively little attention in the literature . We empirically evaluate its performance on the Kaggle diabetic retinopathy dataset , an ordinal and high-resolution dataset and show that it outperforms all of the baselines employed .
As machine learning algorithms become increasingly sophisticated to exploit subtle features of the data , they often become more dependent on simulations . This paper presents a new approach called weakly supervised classification in which class proportions are the only input into the machine learning algorithm . Using one of the most challenging binary classification tasks in high energy physics - quark versus gluon tagging - we show that weakly supervised classification can match the performance of fully supervised algorithms . Furthermore , by design , the new algorithm is insensitive to any mis-modeling of discriminating features in the data by the simulation . Weakly supervised classification is a general procedure that can be applied to a wide variety of learning problems to boost performance and robustness when detailed simulations are not reliable or not available .
We investigate the effects of update rules on the dynamics of an evolutionary game-theoretic model - the N-player evolutionary trust game - consisting of three types of players : investors , trustworthy trustees , and untrustworthy trustees . Interactions between players are limited to local neighborhoods determined by predefined spatial or social network topologies . We compare evolutionary update rules based on the payoffs obtained by their neighbors . Specifically , we investigate the dynamics generated when players use a deterministic strategic rule ( i . e . , unconditional imitation with and without using a noise process induced by a voter model ) , a stochastic pairwise payoff-based strategy ( i . e . , proportional imitation ) , and stochastic local Moran processes . We explore the system dynamics under these update rules based on different social networks and different levels of game difficulty . We observe that there are significant differences on the promoted trust and global net wealth depending on the update rule . If the game is harder , rules based on unconditional imitation achieve the highest global net wealth in the population . Besides a global perspective , we also study the spatial and temporal dynamics induced by the rules and we find important spatio-temporal correlations in the system for all rules . Indeed , the update rules lead to the formation of fractal structures on a lattice and , when the rules are stochastic , also the emergence of low frequencies in the output signal of the system ( i . e . , long-term memory ) .
In 0000 , Williams introduced two interesting discrete Markov processes , namely $C$-processes and $A$-processes , which are related to record times in statistics and Engel ' s series in number theory respectively . Moreover , he showed that these two processes share the same classical limit theorems , such as the law of large numbers , central limit theorem and law of the iterated logarithm . In this paper , we consider the large deviations for these two Markov processes , which indicate that there is a difference between $C$-processes and $A$-processes in the context of large deviations .
Understanding political phenomena requires measuring the political preferences of society . We introduce a model based on mixtures of spatial voting models that infers the underlying distribution of political preferences of voters with only voting records of the population and political positions of candidates in an election . Beyond offering a cost-effective alternative to surveys , this method projects the political preferences of voters and candidates into a shared latent preference space . This projection allows us to directly compare the preferences of the two groups , which is desirable for political science but difficult with traditional survey methods . After validating the aggregated-level inferences of this model against results of related work and on simple prediction tasks , we apply the model to better understand the phenomenon of political polarization in the Texas , New York , and Ohio electorates . Taken at face value , inferences drawn from our model indicate that the electorates in these states may be less bimodal than the distribution of candidates , but that the electorates are comparatively more extreme in their variance . We conclude with a discussion of limitations of our method and potential future directions for research .
With the increasing use of mobile workstations for a wide variety of tasks and associated information needs , and with many variations of available networks , access to data becomes a prime consideration . This paper discusses issues of workstation mobility and proposes a solution wherein the data structures are accessed in an encapsulated form - through the Portable File System ( PFS ) wrapper . The paper discusses an implementation of the Portable File System , highlighting the architecture and commenting upon performance of an experimental system . Although investigations have been focused upon mobile access of WWW documents , this technique could be applied to any mobile data access situation .
Marginal log-linear ( MLL ) models provide a flexible approach to multivariate discrete data . MLL parametrizations under linear constraints induce a wide variety of models , including models defined by conditional independences . We introduce a sub-class of MLL models which correspond to Acyclic Directed Mixed Graphs ( ADMGs ) under the usual global Markov property . We characterize for precisely which graphs the resulting parametrization is variation independent . The MLL approach provides the first description of ADMG models in terms of a minimal list of constraints . The parametrization is also easily adapted to sparse modelling techniques , which we illustrate using several examples of real data .
Segregating an audio mixture containing multiple simultaneous bird sounds is a challenging task . However , birdsong often contains rapid pitch modulations , and these modulations carry information which may be of use in automatic recognition . In this paper we demonstrate that an improved spectrogram representation , based on the distribution derivative method , leads to improved performance of a segregation algorithm which uses a Markov renewal process model to track vocalisation patterns consisting of singing and silences .
We provide a series of algorithms demonstrating that solutions according to the fundamental game-theoretic solution concept of closed under rational behavior ( CURB ) sets in two-player , normal-form games can be computed in polynomial time ( we also discuss extensions to n-player games ) . First , we describe an algorithm that identifies all of a player ' s best responses conditioned on the belief that the other player will play from within a given subset of its strategy space . This algorithm serves as a subroutine in a series of polynomial-time algorithms for finding all minimal CURB sets , one minimal CURB set , and the smallest minimal CURB set in a game . We then show that the complexity of finding a Nash equilibrium can be exponential only in the size of a game ' s smallest CURB set . Related to this , we show that the smallest CURB set can be an arbitrarily small portion of the game , but it can also be arbitrarily larger than the supports of its only enclosed Nash equilibrium . We test our algorithms empirically and find that most commonly studied academic games tend to have either very large or very small minimal CURB sets .
The paper illustrates an application of the Resampling approach [0] for the estimation of the aircraft circulation plan reliability . Resampling is an intensive computer statistical method , which can be used effectively in the case of small samples . Algorithm of the Resampling method for the given task is illustrated and variance of obtained estimators is calculated , which is the measure of the method effectiveness .
In this paper we show that the conditional distribution of perturbed chi-quare risks can be approximated by certain distributions including the Gaussian ones . Our results are of interest for conditional extreme value models and multivariate extremes as shown in three applications .
Attribute grammars ( AGs ) are a formal technique for defining semantics of programming languages . Existing complexity proofs on the circularity problem of AGs are based on automata theory , such as writing pushdown acceptor and alternating Turing machines . They reduced the acceptance problems of above automata , which are exponential-time ( EXPTIME ) complete , to the AG circularity problem . These proofs thus show that the circularity problem is EXPTIME-hard , at least as hard as the most difficult problems in EXPTIME . However , none has given a proof for the EXPTIME-completeness of the problem . This paper first presents an alternating Turing machine for the circularity problem . The alternating Turing machine requires polynomial space . Thus , the circularity problem is in EXPTIME and is then EXPTIME-complete .
Discussion of " Latent variable graphical model selection via convex optimization " by Venkat Chandrasekaran , Pablo A . Parrilo and Alan S . Willsky [arXiv : 0000 . 0000] .
Many problems in computer vision and recommender systems involve low-rank matrices . In this work , we study the problem of finding the maximum entry of a stochastic low-rank matrix from sequential observations . At each step , a learning agent chooses pairs of row and column arms , and receives the noisy product of their latent values as a reward . The main challenge is that the latent values are unobserved . We identify a class of non-negative matrices whose maximum entry can be found statistically efficiently and propose an algorithm for finding them , which we call LowRankElim . We derive a $\DeclareMathOperator{\poly}{poly} O ( ( K + L ) \poly ( d ) \Delta^{-0} \log n ) $ upper bound on its $n$-step regret , where $K$ is the number of rows , $L$ is the number of columns , $d$ is the rank of the matrix , and $\Delta$ is the minimum gap . The bound depends on other problem-specific constants that clearly do not depend $K L$ . To the best of our knowledge , this is the first such result in the literature .
This paper studies emulation of induction by coinduction in a call-by-name language with control operators . Since it is known that call-by-name programming languages with control operators cannot have general initial algebras , interaction of induction and control operators is often restricted to effect-free functions . We show that some class of such restricted inductive types can be derived from full coinductive types by the power of control operators . As a typical example of our results , the type of natural numbers is represented by the type of streams . The underlying idea is a counterpart of the fact that some coinductive types can be expressed by inductive types in call-by-name pure language without side-effects .
The Fr\ ' echet distance is a popular distance measure for curves . We study the problem of clustering time series under the Fr\ ' echet distance . In particular , we give $ ( 0+\varepsilon ) $-approximation algorithms for variations of the following problem with parameters $k$ and $\ell$ . Given $n$ univariate time series $P$ , each of complexity at most $m$ , we find $k$ time series , not necessarily from $P$ , which we call \emph{cluster centers} and which each have complexity at most $\ell$ , such that ( a ) the maximum distance of an element of $P$ to its nearest cluster center or ( b ) the sum of these distances is minimized . Our algorithms have running time near-linear in the input size for constant $\varepsilon$ , $k$ and $\ell$ . To the best of our knowledge , our algorithms are the first clustering algorithms for the Fr\ ' echet distance which achieve an approximation factor of $ ( 0+\varepsilon ) $ or better . Keywords : time series , longitudinal data , functional data , clustering , Fr\ ' echet distance , dynamic time warping , approximation algorithms .
The growing incidents of counterfeiting and associated economic and health consequences necessitate the development of active surveillance systems capable of producing timely and reliable information for all stake holders in the anti-counterfeiting fight . User generated content from social media platforms can provide early clues about product allergies , adverse events and product counterfeiting . This paper reports a work in progresswith contributions including : the development of a framework for gathering and analyzing the views and experiences of users of drug and cosmetic products using machine learning , text mining and sentiment analysis , the application of the proposed framework on Facebook comments and data from Twitter for brand analysis , and the description of how to develop a product safety lexicon and training data for modeling a machine learning classifier for drug and cosmetic product sentiment prediction . The initial brand and product comparison results signify the usefulness of text mining and sentiment analysis on social media data while the use of machine learning classifier for predicting the sentiment orientation provides a useful tool for users , product manufacturers , regulatory and enforcement agencies to monitor brand or product sentiment trends in order to act in the event of sudden or significant rise in negative sentiment .
A typical problem in causal modeling is the instability of model structure learning , i . e . , small changes in finite data can result in completely different optimal models . The present work introduces a novel causal modeling algorithm for longitudinal data , that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms . Our approach uses exploratory search but allows incorporation of prior knowledge , e . g . , the absence of a particular causal relationship between two specific variables . We represent causal relationships using structural equation models . Models are scored along two objectives : the model fit and the model complexity . Since both objectives are often conflicting we apply a multi-objective evolutionary algorithm to search for Pareto optimal models . To handle the instability of small finite data samples , we repeatedly subsample the data and select those substructures ( from the optimal models ) that are both stable and parsimonious . These substructures can be visualized through a causal graph . Our more exploratory approach achieves at least comparable performance as , but often a significant improvement over state-of-the-art alternative approaches on a simulated data set with a known ground truth . We also present the results of our method on three real-world longitudinal data sets on chronic fatigue syndrome , Alzheimer disease , and chronic kidney disease . The findings obtained with our approach are generally in line with results from more hypothesis-driven analyses in earlier studies and suggest some novel relationships that deserve further research .
The application of psychophysiological signals in human-computer interaction is a growing field with significant potential for future smart personalised systems . Working in this emerging field requires comprehension of an array of physiological signals and analysis techniques . Pupillometry has been studied for over a century , but it has just recently started being used in human-computer interaction setups . Traditionally , pupil size has been used as an indicator of cognitive workload and mental effort . However , pupil size has been linked to other cognitive processes as well , ranging from attention to affective processing . We present a short review on the application of pupillometry in human-computer interaction . This paper aims to serve as a primer for the novice , enabling rapid familiarisation with the latest core concepts . We put special emphasis on everyday human-computer interface applications to distinguish from the more common clinical or sports uses of psychophysiology . This paper is an extract from a comprehensive review of the entire field of ambulatory psychophysiology , including 00 similar chapters , plus application guidelines and systematic review . Thus any citation should be made using the following reference : B . Cowley , M . Filetti , K . Lukander , J . Torniainen , A . Henelius , L . Ahonen , O . Barral , I . Kosunen , T . Valtonen , M . Huotilainen , N . Ravaja , G . Jacucci . The Psychophysiology Primer : a guide to methods and a broad review with a focus on human-computer interaction . Foundations and Trends in Human-Computer Interaction , vol . 0 , no . 0-0 , pp . 000-000 , 0000 .
ANOVA decompositions are a standard method for describing and estimating heterogeneity among the means of a response variable across levels of multiple categorical factors . In such a decomposition , the complete set of main effects and interaction terms can be viewed as a collection of vectors , matrices and arrays that share various index sets defined by the factor levels . For many types of categorical factors , it is plausible that an ANOVA decomposition exhibits some consistency across orders of effects , in that the levels of a factor that have similar main-effect coefficients may also have similar coefficients in higher-order interaction terms . In such a case , estimation of the higher-order interactions should be improved by borrowing information from the main effects and lower-order interactions . To take advantage of such patterns , this article introduces a class of hierarchical prior distributions for collections of interaction arrays that can adapt to the presence of such interactions . These prior distributions are based on a type of array-variate normal distribution , for which a covariance matrix for each factor is estimated . This prior is able to adapt to potential similarities among the levels of a factor , and incorporate any such information into the estimation of the effects in which the factor appears . In the presence of such similarities , this prior is able to borrow information from well-estimated main effects and lower-order interactions to assist in the estimation of higher-order terms for which data information is limited .
Clustering is a widely used unsupervised learning method for finding structure in the data . However , the resulting clusters are typically presented without any guarantees on their robustness ; slightly changing the used data sample or re-running a clustering algorithm involving some stochastic component may lead to completely different clusters . There is , hence , a need for techniques that can quantify the instability of the generated clusters . In this study , we propose a technique for quantifying the instability of a clustering solution and for finding robust clusters , termed core clusters , which correspond to clusters where the co-occurrence probability of each data item within a cluster is at least $0 - \alpha$ . We demonstrate how solving the core clustering problem is linked to finding the largest maximal cliques in a graph . We show that the method can be used with both clustering and classification algorithms . The proposed method is tested on both simulated and real datasets . The results show that the obtained clusters indeed meet the guarantees on robustness .
A lot of information on the web is geographically referenced . Discovering and retrieving this geographic information to satisfy various users needs across both open and distributed Spatial Data Infrastructures ( SDI ) poses eminent research challenges . However , this is mostly caused by semantic heterogeneity in users query and lack of semantic referencing of the Geographic Information ( GI ) metadata . To addressing these challenges , this paper discusses ontology based semantic enhanced model , which explicitly represents GI metadata , and provides linked RDF instances of each entity . The system focuses on semantic search , ontology , and efficient spatial information retrieval . In particular , an integrated model that uses specific domain information extraction to improve the searching and retrieval of ranked spatial search results .
We consider constructing model selection criteria for evaluating nonlinear mixed effects models via basis expansions . Mean functions and random functions in the mixed effects model are expressed by basis expansions , then they are estimated by the maximum likelihood method . In order to select numbers of basis we derive a Bayesian model selection criterion for evaluating nonlinear mixed effects models estimated by the maximum likelihood method . Simulation results shows the effectiveness of the mixed effects modeling .
We investigate shrinkage priors for constructing Bayesian predictive distributions . It is shown that there exist shrinkage predictive distributions asymptotically dominating Bayesian predictive distributions based on the Jeffreys prior or other vague priors if the model manifold satisfies some differential geometric conditions . Kullback--Leibler divergence from the true distribution to a predictive distribution is adopted as a loss function . Conformal transformations of model manifolds corresponding to vague priors are introduced . We show several examples where shrinkage predictive distributions dominate Bayesian predictive distributions based on vague priors .
The aim of this research review is to propose the logic and search mechanism for the development of an artificially intelligent automaton ( AIA ) that can find affected cells in a 0-dimensional biological system . Research on the possible application of such automatons to detect and control cancer cells in the human body are greatly focused MRI and PET scans finds the affected regions at the tissue level even as we can find the affected regions at the cellular level using the framework . The AIA may be designed to ensure optimum utilization as they record and might control the presence of affected cells in a human body . The proposed models and techniques can be generalized and used in any application where cells are injured or affected by some disease or accident . The best method to import AIA into the body without surgery or injection is to insert small pill like automata , carrying material viz drugs or leukocytes that is needed to correct the infection . In this process , the AIA can be compared to nano pills to deliver or support therapy . NanoHive simulation software was used to validate the framework of this paper . The existing nanomedicine models such as obstacle avoidance algorithm based models ( Hla K H S et al 0000 ) and the framework in this model were tested in different simulation based experiments . The existing models such as obstacle avoidance based models failed in complex environmental conditions ( such as changing environmental conditions , presence of semi-solid particles , etc ) while the model in this paper executed its framework successfully . Come systems biology , this field of automatons deserves a bigger leap of understanding especially when pharmacogenomics is at its peak . The results also indicate the importance of artificial intelligence and other computational capabilities in the proposed model for the successful detection of affected cells .
A local linear kernel estimator of the regression function x\mapsto g ( x ) : =E[Y_i|X_i=x] , x\in R^d , of a stationary ( d+0 ) -dimensional spatial process { ( Y_i , X_i ) , i\in Z^N} observed over a rectangular domain of the form I_n : ={i= ( i_0 , . . . , i_N ) \in Z^N| 0\leq i_k\leq n_k , k=0 , . . . , N} , n= ( n_0 , . . . , n_N ) \in Z^N , is proposed and investigated . Under mild regularity assumptions , asymptotic normality of the estimators of g ( x ) and its derivatives is established . Appropriate choices of the bandwidths are proposed . The spatial process is assumed to satisfy some very general mixing conditions , generalizing classical time-series strong mixing concepts . The size of the rectangular domain I_n is allowed to tend to infinity at different rates depending on the direction in Z^N .
Today , Plagiarism has become a menace . Every journal editor or conference organizers has to deal with this problem . Simply Copying or rephrasing of text without giving due credit to the original author has become more common . This is considered to be an Intellectual Property Theft . We are developing a Plagiarism Detection Tool which would deal with this problem . In this paper we discuss the common tools available to detect plagiarism and their short comings and the advantages of our tool over these tools .
Quantile normalisation is a popular normalisation method for data subject to unwanted variations such as images , speech , or genomic data . It applies a monotonic transformation to the feature values of each sample to ensure that after normalisation , they follow the same target distribution for each sample . Choosing a " good " target distribution remains however largely empirical and heuristic , and is usually done independently of the subsequent analysis of normalised data . We propose instead to couple the quantile normalisation step with the subsequent analysis , and to optimise the target distribution jointly with the other parameters in the analysis . We illustrate this principle on the problem of estimating a linear model over normalised data , and show that it leads to a particular low-rank matrix regression problem that can be solved efficiently . We illustrate the potential of our method , which we term SUQUAN , on simulated data , images and genomic data , where it outperforms standard quantile normalisation .
Conventional HVAC control systems are usually incognizant of the physical structures and materials of buildings . These systems merely follow pre-set HVAC control logic based on abstract building thermal response models , which are rough approximations to true physical models , ignoring dynamic spatial variations in built environments . To enable more accurate and responsive HVAC control , this paper introduces the notion of self-aware smart buildings , such that buildings are able to explicitly construct physical models of themselves ( e . g . , incorporating building structures and materials , and thermal flow dynamics ) . The question is how to enable self-aware buildings that automatically acquire dynamic knowledge of themselves . This paper presents a novel approach using augmented reality . The extensive user-environment interactions in augmented reality not only can provide intuitive user interfaces for building systems , but also can capture the physical structures and possibly materials of buildings accurately to enable real-time building simulation and control . This paper presents a building system prototype incorporating augmented reality , and discusses its applications .
Automata representing game-semantic models of programs are meant to operate in environments whose input-output behaviour is constrained by the rules of a game . This can lead to a notion of equivalence between states which is weaker than the conventional notion of bisimulation , since not all actions are available to the environment . An environment which attempts to break the rules of the game is , effectively , mounting a low-level attack against a system . In this paper we show how ( and why ) to enforce game rules in games-based hardware synthesis and how to use this weaker notion of equivalence , called coherent equivalence , to aggressively minimise automata .
Unsupervised learning with generative adversarial networks ( GANs ) has proven hugely successful . Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function . However , we found that this loss function may lead to the vanishing gradients problem during the learning process . To overcome such a problem , we propose in this paper the Least Squares Generative Adversarial Networks ( LSGANs ) which adopt the least squares loss function for the discriminator . We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^0$ divergence . We also present a theoretical analysis about the properties of LSGANs and $\chi^0$ divergence . There are two benefits of LSGANs over regular GANs . First , LSGANs are able to generate higher quality images than regular GANs . Second , LSGANs perform more stable during the learning process . For evaluating the image quality , we train LSGANs on several datasets including LSUN and a cat dataset , and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs . Furthermore , we evaluate the stability of LSGANs in two groups . One is to compare between LSGANs and regular GANs without gradient penalty . We conduct three experiments , including Gaussian mixture distribution , difficult architectures , and a new proposed method --- datasets with small variance , to illustrate the stability of LSGANs . The other one is to compare between LSGANs with gradient penalty and WGANs with gradient penalty ( WGANs-GP ) . The experimental results show that LSGANs with gradient penalty succeed in training for all the difficult architectures used in WGANs-GP , including 000-layer ResNet .
We present in this paper an evolution of a tool from a user interface for a concrete Computer Algebra system for Algebraic Topology ( the Kenzo system ) , to a front-end allowing the interoperability among different sources for computation and deduction . The architecture allows the system not only to interface several systems , but also to make them cooperate in shared calculations .
Previous likelihood-based linear modeling of nutritional data has been limited by the availability of software that allows flexible error structures in the data . We demonstrate the use of a Bayesian modeling approach to the analysis of such data . Our goal is to model the relationship between the energy intake derived from Food Frequency Questionnaires ( FFQs ) and the energy expenditure estimated from the doubly labeled water method . We consider models with different distributions for the FFQ energy intake . The models include previously identified covariates describing social desirability , education and their possible interaction that are felt to impact the reported FFQ . The models also include random effects to account for subject specific random variation ( frailty ) and also to account for the complex patterns of measurement error inherent in these data . Issues arising within the work relate both to the selection of relevant linear and non-linear models , the use of random effects , and the relevance of goodness-of-fit criteria such as DIC and PPL in assessing the most appropriate model .
In this paper we study the asymptotic behaviour of empirical processes when parameters are estimated , assuming that the underlying sequence of random variables is long-range dependent . We show completely different phenomena compared to i . i . d . situation , as well as compared to ordinary empirical processes of long range dependent sequences . Applications include Kolmogorov-Smirnov and Cramer-Smirnov-von Mises goodness-of-fit statistics .
Could simple organisms such as slime mould approximate LI without recourse to neural tissue ? We describe a model whereby LI can emerge without explicit inhibitory wiring , using only bulk transport effects . We use a multi-agent model of slime mould to reproduce the char- acteristic edge contrast amplification effects of LI using excitation via attractant based stimuli . We also explore a counterpart behaviour , Lateral Activation ( where stimulated regions are inhibited and lateral regions are excited ) , using simulated exposure to light irradiation . In both cases restoration of baseline activity occurs when the stimuli are removed . In addition to the enhancement of local edge contrast the long-term change in population density distribution corresponds to a collective response to the global brightness of 0D image stimuli , including the scalloped inten- sity profile of the Chevreul staircase and the perceived difference of two identically bright patches in the Simultaneous Brightness Contrast ( SBC ) effect . This simple modelapproximatesLIcontrastenhancementphenomenaandglobalbrightnessper- ception in collective unorganised systems without fixed neural architectures . This may encourage further research into unorganised analogues of neural processes in simple organisms and suggests novel mechanisms to generate collective perception of contrast and brightness in distributed computing and robotic devices .
Statistical inference on functional magnetic resonance imaging ( fMRI ) data is an important task in brain imaging . One major hypothesis is that the presence or not of a psychiatric disorder can be explained by the differential clustering of neurons in the brain . In view of this fact , it is clearly of interest to address the question of whether the properties of the clusters have changed between groups of patients and controls . The normal method of approaching group differences in brain imaging is to carry out a voxel-wise univariate analysis for a difference between the mean group responses using an appropriate test ( e . g . a t-test ) and to assemble the resulting " significantly different voxels " into clusters , testing again at cluster level . In this approach of course , the primary voxel-level test is blind to any cluster structure . Direct assessments of differences between groups ( or reproducibility within groups ) at the cluster level have been rare in brain imaging . For this reason , we introduce a novel statistical test called ANOCVA - ANalysis Of Cluster structure Variability , which statistically tests whether two or more populations are equally clustered using specific features . The proposed method allows us to compare the clustering structure of multiple groups simultaneously , and also to identify features that contribute to the differential clustering . We illustrate the performance of ANOCVA through simulations and an application to an fMRI data set composed of children with ADHD and controls . Results show that there are several differences in the brain ' s clustering structure between them , corroborating the hypothesis in the literature . Furthermore , we identified some brain regions previously not described , generating new hypothesis to be tested empirically .
The two-sample hypothesis testing problem is studied for the challenging scenario of high dimensional data sets with small sample sizes . We show that the two-sample hypothesis testing problem can be posed as a one-class set classification problem . In the set classification problem the goal is to classify a set of data points that are assumed to have a common class . We prove that the average probability of error given a set is less than or equal to the Bayes error and decreases as a power of $n$ number of sample data points in the set . We use the positive definite Set Kernel for directly mapping sets of data to an associated Reproducing Kernel Hilbert Space , without the need to learn a probability distribution . We specifically solve the two-sample hypothesis testing problem using a one-class SVM in conjunction with the proposed Set Kernel . We compare the proposed method with the Maximum Mean Discrepancy , F-Test and T-Test methods on a number of challenging simulated high dimensional and small sample size data . We also perform two-sample hypothesis testing experiments on six cancer gene expression data sets and achieve zero type-I and type-II error results on all data sets .
Wireless sensor networks offer the potential to span and monitor large geographical areas inexpensively . Sensor network databases like TinyDB are the dominant architectures to extract and manage data in such networks . Since sensors have significant power constraints ( battery life ) , and high communication costs , design of energy efficient communication algorithms is of great importance . The data flow in a sensor database is very different from data flow in an ordinary network and poses novel challenges in designing efficient routing algorithms . In this work we explore the problem of energy efficient routing for various different types of database queries and show that in general , this problem is NP-complete . We give a constant factor approximation algorithm for one class of query , and for other queries give heuristic algorithms . We evaluate the efficiency of the proposed algorithms by simulation and demonstrate their near optimal performance for various network sizes .
In medical research , continuous markers are widely employed in diagnostic tests to distinguish diseased and non-diseased subjects . The accuracy of such diagnostic tests is commonly assessed using the receiver operating characteristic ( ROC ) curve . To summarize an ROC curve and determine its optimal cut-point , the Youden index is popularly used . In literature , estimation of the Youden index has been widely studied via various statistical modeling strategies on the conditional density . This paper proposes a new model-free estimation method , which directly estimates the covariate-adjusted cut-point without estimating the conditional density . Consequently , covariate-adjusted Youden index can be estimated based on the estimated cutpoint . The proposed method formulates the estimation problem in a large margin classification framework , which allows flexible modeling of the covariate-adjusted Youden index through kernel machines . The advantage of the proposed method is demonstrated in a variety of simulated experiments as well as a real application to Pima Indians diabetes study .
Many machine learning algorithms require precise estimates of covariance matrices . The sample covariance matrix performs poorly in high-dimensional settings , which has stimulated the development of alternative methods , the majority based on factor models and shrinkage . Recent work of Ledoit and Wolf has extended the shrinkage framework to Nonlinear Shrinkage ( NLS ) , a more powerful covariance estimator based on Random Matrix Theory . Our contribution shows that , contrary to claims in the literature , cross-validation based covariance matrix estimation ( CVC ) yields comparable performance at strongly reduced complexity and runtime . On two real world data sets , we show that the CVC estimator yields superior results than competing shrinkage and factor based methods .
We constuct a sequential adaptive procedure for estimating the autoregressive function at a given point in nonparametric autoregression models with Gaussian noise . We make use of the sequential kernel estimators . The optimal adaptive convergence rate is given as well as the upper bound for the minimax risk .
As the popularity of electric vehicles increases , the demand for more power can increase more rapidly than our ability to install additional generating capacity . In the long term we expect that the supply and demand will become balanced . However , in the interim the rate at which electric vehicles can be deployed will depend on our ability to charge these vehicles without inconveniencing their owners . In this paper , we investigate using fairness mechanisms to distribute power to electric vehicles on a smart grid . We assume that during peak demand there is insufficient power to charge all the vehicles simultaneously . In each five minute interval of time we select a subset of the vehicles to charge , based upon information about the vehicles . We evaluate the selection mechanisms using published data on the current demand for electric power as a function of time of day , current driving habits for commuting , and the current rates at which electric vehicles can be charged on home outlets . We found that conventional selection strategies , such as first-come-first-served or round robin , may delay a significant fraction of the vehicles by more than two hours , even when the total available power over the course of a day is two or three times the power required by the vehicles . However , a selection mechanism that minimizes the maximum delay can reduce the delays to a few minutes , even when the capacity available for charging electric vehicles exceeds their requirements by as little as 0% .
We present an approach for mobile robots to learn to navigate in pedestrian-rich environments via raw depth inputs , in a social-compliant manner . To achieve this , we adopt a generative adversarial imitation learning ( GAIL ) strategy for motion planning , which improves upon a supervised policy model pre-trained via behavior cloning . Our approach overcomes the disadvantages of previous methods , as they heavily depend on the full knowledge of the location and velocity information of nearby pedestrians , which not only requires specific sensors but also consumes much computation time for extracting such state information from raw sensor input . In this paper , our proposed GAIL-based model performs directly on raw depth inputs and plans in real-time . Experiments show that our GAIL-based approach greatly improves the behavior of mobile robots from pure behavior cloning both safely and efficiently . Real-world implementation also shows that our method is capable of guiding autonomous vehicles to navigate in a social-compliant manner directly through raw depth inputs .
We present a new approach to sample from generic binary distributions , based on an exact Hamiltonian Monte Carlo algorithm applied to a piecewise continuous augmentation of the binary distribution of interest . An extension of this idea to distributions over mixtures of binary and possibly-truncated Gaussian or exponential variables allows us to sample from posteriors of linear and probit regression models with spike-and-slab priors and truncated parameters . We illustrate the advantages of these algorithms in several examples in which they outperform the Metropolis or Gibbs samplers .
We present an application of SPH to saturated soilproblems . Herein , the standard SPH formulation was improved to model saturated soil . It is shown that the proposed formulation could yield several advantages such as : it takes into account the pore-water pressure in an accurate manner , it automatically satisfies the dynamics boundary conditions between submerged soil and water , and it reduced the computational cost . Discussions on the use of the standard and the new SPH formulations are also given through some numerical tests . Furthermore , some techniques to obtained correct SPH solution are also proposed and discussed . To the end , this paper suggests that the proposed SPH formulation should be considered as the basic formulation for further developments of SPH for soil-water couple problems
Fisher-consistent loss functions play a fundamental role in the construction of successful binary margin-based classifiers . In this paper we establish the Fisher-consistency condition for multicategory classification problems . Our approach uses the margin vector concept which can be regarded as a multicategory generalization of the binary margin . We characterize a wide class of smooth convex loss functions that are Fisher-consistent for multicategory classification . We then consider using the margin-vector-based loss functions to derive multicategory boosting algorithms . In particular , we derive two new multicategory boosting algorithms by using the exponential and logistic regression losses .
The study of networks leads to a wide range of high dimensional inference problems . In most practical scenarios , one needs to draw inference from a small population of large networks . The present paper studies hypothesis testing of graphs in this high-dimensional regime . We consider the problem of testing between two populations of inhomogeneous random graphs defined on the same set of vertices . We propose tests based on estimates of the Frobenius and operator norms of the difference between the population adjacency matrices . We show that the tests are uniformly consistent in both the " large graph , small sample " and " small graph , large sample " regimes . We further derive lower bounds on the minimax separation rate for the associated testing problems , and show that the constructed tests are near optimal .
Many embedded real-time control systems suffer from resource constraints and dynamic workload variations . Although optimal feedback scheduling schemes are in principle capable of maximizing the overall control performance of multitasking control systems , most of them induce excessively large computational overheads associated with the mathematical optimization routines involved and hence are not directly applicable to practical systems . To optimize the overall control performance while minimizing the overhead of feedback scheduling , this paper proposes an efficient feedback scheduling scheme based on feedforward neural networks . Using the optimal solutions obtained offline by mathematical optimization methods , a back-propagation ( BP ) neural network is designed to adapt online the sampling periods of concurrent control tasks with respect to changes in computing resource availability . Numerical simulation results show that the proposed scheme can reduce the computational overhead significantly while delivering almost the same overall control performance as compared to optimal feedback scheduling .
In this paper , some of the properties of non-parametric estimation of the expectation of g ( X ) ( any function of X ) , by using a Judgment Post-stratification Sample ( JPS ) , are discussed . A class of estimators ( including the standard JPS estimator and a JPS estimator proposed by Frey and Feeman ( 0000 , Comput . Stat . Data An . ) ) is considered . The paper provides mean and variance of the members of this class , and examines their consistency and asymptotic distribution . Specifically , the results are for the estimation of population mean , population variance and CDF . We show that any estimators of the class may be less efficient than Simple Random Sampling ( SRS ) estimator for small sample sizes . We prove that the relative efficiency of some estimators in the class with respect to Balanced Ranked Set Sampling ( BRSS ) estimator tends to 0 as the sample size goes to infinity . Furthermore , the standard JPS mean estimator and , Frey and Feeman JPS mean estimator are specifically studied and we show that two estimator have the same asymptotic distribution . For the standard JPS mean estimator , in perfect ranking situations , optimum values of H ( the ranking class size ) , for different sample sizes , are determined non-parametrically for populations that are not heavily skewed or thick tailed . We show that the standard JPS mean estimator may be more efficient than BRSS for large sample sizes , in situations in which we can use a larger class size for H in JPS set-up .
Notwithstanding several authors have recognised the conceptual key of " politics " as an important component in any Require-ments Engineering ( RE ) process , practitioners still lack a prag-matic answer on how to deal with the political dimension : such an ability has become a mostly desirable but totally undetailed part of what we usually and vaguely refer to as " professional experience " . Nor were practitioners given any suitable tool or method to easily detect , represent , control and if possible leverage politics . Authors argue that this issue could be successfully ad-dressed and resolved if , when we map organisations against the system to be developed , we include power and politics in their " too human " and even emotional dimension . A simple way to do so is to use emoji pictograms : most of them are part of a universal language , which requirements engineers could easily adopt and exploit to assess and produce models that include an extra layer of " political " information , without the need to actually introduce any new notation . A few examples of emoji-aware UML and organisational charts are hereby proposed , more as a platform to support communication and share reflections on how to deal with politics than as an actual technology to be adopted .
This paper examines the problem of estimating the parameters of a bandlimited signal from samples corrupted by random jitter ( timing noise ) and additive iid Gaussian noise , where the signal lies in the span of a finite basis . For the presented classical estimation problem , the Cramer-Rao lower bound ( CRB ) is computed , and an Expectation-Maximization ( EM ) algorithm approximating the maximum likelihood ( ML ) estimator is developed . Simulations are performed to study the convergence properties of the EM algorithm and compare the performance both against the CRB and a basic linear estimator . These simulations demonstrate that by post-processing the jittered samples with the proposed EM algorithm , greater jitter can be tolerated , potentially reducing on-chip ADC power consumption substantially .
When considering a genetic disease with variable age at onset ( ex : diabetes , familial amyloid neuropathy , cancers , etc . ) , computing the individual risk of the disease based on family history ( FH ) is of critical interest both for clinicians and patients . Such a risk is very challenging to compute because : 0 ) the genotype X of the individual of interest is in general unknown ; 0 ) the posterior distribution P ( X|FH , T > t ) changes with t ( T is the age at disease onset for the targeted individual ) ; 0 ) the competing risk of death is not negligible . In this work , we present a modeling of this problem using a Bayesian network mixed with ( right-censored ) survival outcomes where hazard rates only depend on the genotype of each individual . We explain how belief propagation can be used to obtain posterior distribution of genotypes given the FH , and how to obtain a time-dependent posterior hazard rate for any individual in the pedigree . Finally , we use this posterior hazard rate to compute individual risk , with or without the competing risk of death . Our method is illustrated using the Claus-Easton model for breast cancer ( BC ) . This model assumes an autosomal dominant genetic risk factor such as non-carriers ( genotype 00 ) have a BC hazard rate $\lambda$ 0 ( t ) while carriers ( genotypes 00 , 00 and 00 ) have a ( much greater ) hazard rate $\lambda$ 0 ( t ) . Both hazard rates are assumed to be piecewise constant with known values ( cuts at 00 , 00 , . . . , 00 years ) . The competing risk of death is derived from the national French registry .
A fundamental goal in network neuroscience is to understand how activity in one region drives activity elsewhere , a process referred to as effective connectivity . Here we propose to model this causal interaction using integro-differential equations and causal kernels that allow for a rich analysis of effective connectivity . The approach combines the tractability and flexibility of autoregressive modeling with the biophysical interpretability of dynamic causal modeling . The causal kernels are learned nonparametrically using Gaussian process regression , yielding an efficient framework for causal inference . We construct a novel class of causal covariance functions that enforce the desired properties of the causal kernels , an approach which we call GP CaKe . By construction , the model and its hyperparameters have biophysical meaning and are therefore easily interpretable . We demonstrate the efficacy of GP CaKe on a number of simulations and give an example of a realistic application on magnetoencephalography ( MEG ) data .
There seems to be an upper limit to predicting the outcome of matches in ( semi- ) professional sports . Recent work has proposed that this is due to chance and attempts have been made to simulate the distribution of win percentages to identify the most likely proportion of matches decided by chance . We argue that the approach that has been chosen so far makes some simplifying assumptions that cause its result to be of limited practical value . Instead , we propose to use clustering of statistical team profiles and observed scheduling information to derive limits on the predictive accuracy for particular seasons , which can be used to assess the performance of predictive models on those seasons . We show that the resulting simulated distributions are much closer to the observed distributions and give higher assessments of chance and tighter limits on predictive accuracy .
We study two-player games of infinite duration that are played on finite or infinite game graphs . A winning strategy for such a game is positional if it only depends on the current position , and not on the history of the play . A game is positionally determined if , from each position , one of the two players has a positional winning strategy . The theory of such games is well studied for winning conditions that are defined in terms of a mapping that assigns to each position a priority from a finite set . Specifically , in Muller games the winner of a play is determined by the set of those priorities that have been seen infinitely often ; an important special case are parity games where the least ( or greatest ) priority occurring infinitely often determines the winner . It is well-known that parity games are positionally determined whereas Muller games are determined via finite-memory strategies . In this paper , we extend this theory to the case of games with infinitely many priorities . Such games arise in several application areas , for instance in pushdown games with winning conditions depending on stack contents . For parity games there are several generalisations to the case of infinitely many priorities . While max-parity games over omega or min-parity games over larger ordinals than omega require strategies with infinite memory , we can prove that min-parity games with priorities in omega are positionally determined . Indeed , it turns out that the min-parity condition over omega is the only infinitary Muller condition that guarantees positional determinacy on all game graphs .
Exact inference in the linear regression model with spike and slab priors is often intractable . Expectation propagation ( EP ) can be used for approximate inference . However , the regular sequential form of EP ( R-EP ) may fail to converge in this model when the size of the training set is very small . As an alternative , we propose a provably convergent EP algorithm ( PC-EP ) . PC-EP is proved to minimize an energy function which , under some constraints , is bounded from below and whose stationary points coincide with the solution of R-EP . Experiments with synthetic data indicate that when R-EP does not converge , the approximation generated by PC-EP is often better . By contrast , when R-EP converges , both methods perform similarly .
Regular vine distributions which constitute a flexible class of multivariate dependence models are discussed . Since multivariate copulae constructed through pair-copula decompositions were introduced to the statistical community , interest in these models has been growing steadily and they are finding successful applications in various fields . Research so far has however been concentrating on so-called canonical and D-vine copulae , which are more restrictive cases of regular vine copulae . It is shown how to evaluate the density of arbitrary regular vine specifications . This opens the vine copula methodology to the flexible modeling of complex dependencies even in larger dimensions . In this regard , a new automated model selection and estimation technique based on graph theoretical considerations is presented . This comprehensive search strategy is evaluated in a large simulation study and applied to a 00-dimensional financial data set of international equity , fixed income and commodity indices which were observed over the last decade , in particular during the recent financial crisis . The analysis provides economically well interpretable results and interesting insights into the dependence structure among these indices .
For a density $f$ on ${\mathbb R}^d$ , a {\it high-density cluster} is any connected component of $\{x : f ( x ) \geq \lambda\}$ , for some $\lambda > 0$ . The set of all high-density clusters forms a hierarchy called the {\it cluster tree} of $f$ . We present two procedures for estimating the cluster tree given samples from $f$ . The first is a robust variant of the single linkage algorithm for hierarchical clustering . The second is based on the $k$-nearest neighbor graph of the samples . We give finite-sample convergence rates for these algorithms which also imply consistency , and we derive lower bounds on the sample complexity of cluster tree estimation . Finally , we study a tree pruning procedure that guarantees , under milder conditions than usual , to remove clusters that are spurious while recovering those that are salient .
How important are friendships in determining success by individuals and teams in complex collaborative environments ? By combining a novel data set containing the dynamics of millions of ad hoc teams from the popular multiplayer online first person shooter Halo : Reach with survey data on player demographics , play style , psychometrics and friendships derived from an anonymous online survey , we investigate the impact of friendship on collaborative and competitive performance . In addition to finding significant differences in player behavior across these variables , we find that friendships exert a strong influence , leading to both improved individual and team performance--even after controlling for the overall expertise of the team--and increased pro-social behaviors . Players also structure their in-game activities around social opportunities , and as a result hidden friendship ties can be accurately inferred directly from behavioral time series . Virtual environments that enable such friendship effects will thus likely see improved collaboration and competition .
The Simple Knowledge Organization System ( SKOS ) is popular for expressing controlled vocabularies , such as taxonomies , classifications , etc . , for their use in Semantic Web applications . Using SKOS , concepts can be linked to other concepts and organized into hierarchies inside a single terminology system . Meanwhile , expressing mappings between concepts in different terminology systems is also possible . This paper discusses potential quality issues in using SKOS to express these terminology mappings . Problematic patterns are defined and corresponding rules are developed to automatically detect situations where the mappings either result in ' SKOS Vocabulary Hijacking ' to the source vocabularies or cause conflicts . An example of using the rules to validate sample mappings between two clinical terminologies is given . The validation rules , expressed in N0 format , are available as open source .
We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods . The approach appeals to a new class of Polya-Gamma distributions , which are constructed in detail . A variety of examples are presented to show the versatility of the method , including logistic regression , negative binomial regression , nonlinear mixed-effects models , and spatial models for count data . In each case , our data-augmentation strategy leads to simple , effective methods for posterior inference that : ( 0 ) circumvent the need for analytic approximations , numerical integration , or Metropolis-Hastings ; and ( 0 ) outperform other known data-augmentation strategies , both in ease of use and in computational efficiency . All methods , including an efficient sampler for the Polya-Gamma distribution , are implemented in the R package BayesLogit . In the technical supplement appended to the end of the paper , we provide further details regarding the generation of Polya-Gamma random variables ; the empirical benchmarks reported in the main manuscript ; and the extension of the basic data-augmentation framework to contingency tables and multinomial outcomes .
Active transport is sought in molecular communication to extend coverage , improve reliability , and mitigate interference . One such active mechanism inherent to many liquid environments is fluid flow . Flow models are often over-simplified , e . g . , assuming one-dimensional diffusion with constant drift . However , diffusion and flow are usually encountered in three-dimensional bounded environments where the flow is highly non-uniform such as in blood vessels or microfluidic channels . For a qualitative understanding of the relevant physical effects inherent to these channels a systematic framework is provided based on the Peclet number and the ratio of transmitter-receiver distance to duct radius . We review the relevant laws of physics and highlight when simplified models of uniform flow and advection-only transport are applicable . For several molecular communication setups , we highlight the effect of different flow scenarios on the channel impulse response .
Inferring an appropriate DTD or XML Schema Definition ( XSD ) for a given collection of XML documents essentially reduces to learning deterministic regular expressions from sets of positive example words . Unfortunately , there is no algorithm capable of learning the complete class of deterministic regular expressions from positive examples only , as we will show . The regular expressions occurring in practical DTDs and XSDs , however , are such that every alphabet symbol occurs only a small number of times . As such , in practice it suffices to learn the subclass of deterministic regular expressions in which each alphabet symbol occurs at most k times , for some small k . We refer to such expressions as k-occurrence regular expressions ( k-OREs for short ) . Motivated by this observation , we provide a probabilistic algorithm that learns k-OREs for increasing values of k , and selects the deterministic one that best describes the sample based on a Minimum Description Length argument . The effectiveness of the method is empirically validated both on real world and synthetic data . Furthermore , the method is shown to be conservative over the simpler classes of expressions considered in previous work .
A two-groups mixed-effects model for the comparison of ( normalized ) microarray data from two treatment groups is considered . Most competing parametric methods that have appeared in the literature are obtained as special cases or by minor modification of the proposed model . Approximate maximum likelihood fitting is accomplished via a fast and scalable algorithm , which we call LEMMA ( Laplace approximated EM Microarray Analysis ) . The posterior odds of treatment $\times$ gene interactions , derived from the model , involve shrinkage estimates of both the interactions and of the gene specific error variances . Genes are classified as being associated with treatment based on the posterior odds and the local false discovery rate ( f . d . r . ) with a fixed cutoff . Our model-based approach also allows one to declare the non-null status of a gene by controlling the false discovery rate ( FDR ) . It is shown in a detailed simulation study that the approach outperforms well-known competitors . We also apply the proposed methodology to two previously analyzed microarray examples . Extensions of the proposed method to paired treatments and multiple treatments are also discussed .
Econophysics , is based on the premise that some ideas and methods from physics can be applied to economic situations . We intend to show in this paper how a physics concept such as entropy can be applied to an economic problem . In so doing , we demonstrate how information in the form of observable data and moment constraints are introduced into the method of Maximum relative Entropy ( MrE ) . A general example of updating with data and moments is shown . Two specific econometric examples are solved in detail which can then be used as templates for real world problems . A numerical example is compared to a large deviation solution which illustrates some of the advantages of the MrE method .
In this paper , a simple trajectory generation method for biped walking is proposed . The dynamic model of the five link bipedal robot is first reduced using several biologically inspired assumptions . A sinusoidal curve is then imposed to the ankle of the swing leg ' s trajectory . The reduced model is finally obtained and solved : it is an homogeneous second order differential equations with constant coefficients . The algebraic solution obtained ensures a stable rhythmic gait for the bipedal robot . It ' s continuous in the defined time interval , easy to implement when the boundary conditions are well defined .
A PEM micro fuel cell system is described which is based on self-breathing PEM micro fuel cells in the power range between 0 mW and 0W . Hydrogen is supplied with on-demand hydrogen production with help of a galvanic cell , that produces hydrogen when Zn reacts with water . The system can be used as a battery replacement for low power applications and has the potential to improve the run time of autonomous systems . The efficiency has been investigated as function of fuel cell construction and tested for several load profiles .
Even though power-law or close-to-power-law degree distributions are ubiquitously observed in a great variety of large real networks , the mathematically satisfactory treatment of random power-law graphs satisfying basic statistical requirements of realism is still lacking . These requirements are : sparsity , exchangeability , projectivity , and unbiasedness . The last requirement states that entropy of the graph ensemble must be maximized under the degree distribution constraints . Here we prove that the hypersoft configuration model ( HSCM ) , belonging to the class of random graphs with latent hyperparameters , also known as inhomogeneous random graphs or $W$-random graphs , is an ensemble of random power-law graphs that are sparse , unbiased , and either exchangeable or projective . The proof of their unbiasedness relies on generalized graphons , and on mapping the problem of maximization of the normalized Gibbs entropy of a random graph ensemble , to the graphon entropy maximization problem , showing that the two entropies converge to each other in the large-graph limit .
A technique introduced by Indyk and Woodruff [STOC 0000] has inspired several recent advances in data-stream algorithms . We show that a number of these results follow easily from the application of a single probabilistic method called Precision Sampling . Using this method , we obtain simple data-stream algorithms that maintain a randomized sketch of an input vector $x= ( x_0 , . . . x_n ) $ , which is useful for the following applications . 0 ) Estimating the $F_k$-moment of $x$ , for $k>0$ . 0 ) Estimating the $\ell_p$-norm of $x$ , for $p\in[0 , 0]$ , with small update time . 0 ) Estimating cascaded norms $\ell_p ( \ell_q ) $ for all $p , q>0$ . 0 ) $\ell_0$ sampling , where the goal is to produce an element $i$ with probability ( approximately ) $|x_i|/\|x\|_0$ . It extends to similarly defined $\ell_p$-sampling , for $p\in [0 , 0]$ . For all these applications the algorithm is essentially the same : scale the vector x entry-wise by a well-chosen random vector , and run a heavy-hitter estimation algorithm on the resulting vector . Our sketch is a linear function of x , thereby allowing general updates to the vector x . Precision Sampling itself addresses the problem of estimating a sum $\sum_{i=0}^n a_i$ from weak estimates of each real $a_i\in[0 , 0]$ . More precisely , the estimator first chooses a desired precision $u_i\in ( 0 , 0]$ for each $i\in[n]$ , and then it receives an estimate of every $a_i$ within additive $u_i$ . Its goal is to provide a good approximation to $\sum a_i$ while keeping a tab on the " approximation cost " $\sum_i ( 0/u_i ) $ . Here we refine previous work [Andoni , Krauthgamer , and Onak , FOCS 0000] which shows that as long as $\sum a_i=\Omega ( 0 ) $ , a good multiplicative approximation can be achieved using total precision of only $O ( n\log n ) $ .
Bounded rationality , that is , decision-making and planning under resource limitations , is widely regarded as an important open problem in artificial intelligence , reinforcement learning , computational neuroscience and economics . This paper offers a consolidated presentation of a theory of bounded rationality based on information-theoretic ideas . We provide a conceptual justification for using the free energy functional as the objective function for characterizing bounded-rational decisions . This functional possesses three crucial properties : it controls the size of the solution space ; it has Monte Carlo planners that are exact , yet bypass the need for exhaustive search ; and it captures model uncertainty arising from lack of evidence or from interacting with other agents having unknown intentions . We discuss the single-step decision-making case , and show how to extend it to sequential decisions using equivalence transformations . This extension yields a very general class of decision problems that encompass classical decision rules ( e . g . EXPECTIMAX and MINIMAX ) as limit cases , as well as trust- and risk-sensitive planning .
Many machine learning algorithms assume that all input samples are independently and identically distributed from some common distribution on either the input space X , in the case of unsupervised learning , or the input and output space X x Y in the case of supervised and semi-supervised learning . In the last number of years the relaxation of this assumption has been explored and the importance of incorporation of additional information within machine learning algorithms became more apparent . Traditionally such fusion of information was the domain of semi-supervised learning . More recently the inclusion of knowledge from separate hypothetical spaces has been proposed by Vapnik as part of the supervised setting . In this work we are interested in exploring Vapnik ' s idea of master-class learning and the associated learning using privileged information , however within the unsupervised setting . Adoption of the advanced supervised learning paradigm for the unsupervised setting instigates investigation into the difference between privileged and technical data . By means of our proposed aRi-MAX method stability of the KMeans algorithm is improved and identification of the best clustering solution is achieved on an artificial dataset . Subsequently an information theoretic dot product based algorithm called P-Dot is proposed . This method has the ability to utilize a wide variety of clustering techniques , individually or in combination , while fusing privileged and technical data for improved clustering . Application of the P-Dot method to the task of digit recognition confirms our findings in a real-world scenario .
We consider estimating an unknown signal , both blocky and sparse , which is corrupted by additive noise . We study three interrelated least squares procedures and their asymptotic properties . The first procedure is the fused lasso , put forward by Friedman et al . [Ann . Appl . Statist . 0 ( 0000 ) 000--000] , which we modify into a different estimator , called the fused adaptive lasso , with better properties . The other two estimators we discuss solve least squares problems on sieves ; one constrains the maximal $\ell_0$ norm and the maximal total variation seminorm , and the other restricts the number of blocks and the number of nonzero coordinates of the signal . We derive conditions for the recovery of the true block partition and the true sparsity patterns by the fused lasso and the fused adaptive lasso , and we derive convergence rates for the sieve estimators , explicitly in terms of the constraining parameters .
The theory of adaptive estimation and oracle inequalities for the case of Gaussian-shift--finite-interval experiments has made significant progress in recent years . In particular , sharp-minimax adaptive estimators and exact exponential-type oracle inequalities have been suggested for a vast set of functions including analytic and Sobolev with any positive index as well as for Efromovich--Pinsker and Stein blockwise-shrinkage estimators . Is it possible to obtain similar results for a more interesting applied problem of density estimation and/or the dual problem of characteristic function estimation ? The answer is ``yes . ' ' In particular , the obtained results include exact exponential-type oracle inequalities which allow to consider , for the first time in the literature , a simultaneous sharp-minimax estimation of Sobolev densities with any positive index ( not necessarily larger than 0/0 ) , infinitely differentiable densities ( including analytic , entire and stable ) , as well as of not absolutely integrable characteristic functions . The same adaptive estimator is also rate minimax over a familiar class of distributions with bounded spectrum where the density and the characteristic function can be estimated with the parametric rate .
The aim of this paper is the supervised classification of semi-structured data . A formal model based on bayesian classification is developed while addressing the integration of the document structure into classification tasks . We define what we call the structural context of occurrence for unstructured data , and we derive a recursive formulation in which parameters are used to weight the contribution of structural element relatively to the others . A simplified version of this formal model is implemented to carry out textual documents classification experiments . First results show , for a adhoc weighting strategy , that the structural context of word occurrences has a significant impact on classification results comparing to the performance of a simple multinomial naive Bayes classifier . The proposed implementation competes on the Reuters-00000 data with the SVM classifier associated or not with the splitting of structural components . These results encourage exploring the learning of acceptable weighting strategies for this model , in particular boosting strategies .
There may be sensitive information in a relational database , and we might want to keep it hidden from a user or group thereof . In this work , sensitive data is characterized as the contents of a set of secrecy views . For a user without permission to access that sensitive data , the database instance he queries is updated to make the contents of the views empty or contain only tuples with null values . In particular , if this user poses a query about any of these views , no meaningful information is returned . Since the database is not expected to be physically changed to produce this result , the updates are only virtual . And also minimal in a precise way . These minimal updates are reflected in the secrecy view contents , and also in the fact that query answers , while being privacy preserving , are also maximally informative . Virtual updates are based on the use of null values as used in the SQL standard . We provide the semantics of secrecy views and the virtual updates . The different ways in which the underlying database is virtually updated are specified as the models of a logic program with stable model semantics . The program becomes the basis for the computation of the " secret answers " to queries , i . e . those that do not reveal the sensitive information .
We study the compressed representation of a ranked tree by a ( string ) straight-line program ( SLP ) for its preorder traversal , and compare it with the well-studied representation by straight-line context free tree grammars ( which are also known as tree straight-line programs or TSLPs ) . Although SLPs turn out to be exponentially more succinct than TSLPs , we show that many simple tree queries can still be performed efficiently on SLPs , such as computing the height and Horton-Strahler number of a tree , tree navigation , or evaluation of Boolean expressions . Other problems on tree traversals turn out to be intractable , e . g . pattern matching and evaluation of tree automata .
This article is based on studies of the existing literature , focusing on the states-of-the-arts on virtual reality ( VR ) and its potential uses in learning . Different platforms have been used to improve the learning effects of VR that offers exciting opportunities in various fields . As more and more students want in a distance , part-time , or want to continue their education , VR has attracted considerable attention in learning , training , and traditional education . VR based learning enables operators to bring together all disciplinary resources in a common playground . The VR base multimedia platform has successfully demonstrated great potential of education and training . In this paper , we will discuss existing systems and their uses and address the technical challenges and future directions .
We describe a semantic wiki system with an underlying controlled natural language grammar implemented in Grammatical Framework ( GF ) . The grammar restricts the wiki content to a well-defined subset of Attempto Controlled English ( ACE ) , and facilitates a precise bidirectional automatic translation between ACE and language fragments of a number of other natural languages , making the wiki content accessible multilingually . Additionally , our approach allows for automatic translation into the Web Ontology Language ( OWL ) , which enables automatic reasoning over the wiki content . The developed wiki environment thus allows users to build , query and view OWL knowledge bases via a user-friendly multilingual natural language interface . As a further feature , the underlying multilingual grammar is integrated into the wiki and can be collaboratively edited to extend the vocabulary of the wiki or even customize its sentence structures . This work demonstrates the combination of the existing technologies of Attempto Controlled English and Grammatical Framework , and is implemented as an extension of the existing semantic wiki engine AceWiki .
The practical implementation of Bayesian inference requires numerical approximation when closed-form expressions are not available . What types of accuracy ( convergence ) of the numerical approximations guarantee robustness and what types do not ? In particular , is the recursive application of Bayes ' rule robust when subsequent data or posteriors are approximated ? When the prior is the push forward of a distribution by the map induced by the solution of a PDE , in which norm should that solution be approximated ? Motivated by such questions , we investigate the sensitivity of the distribution of posterior distributions ( i . e . posterior distribution-valued random variables , randomized through the data ) with respect to perturbations of the prior and data generating distributions in the limit when the number of data points grows towards infinity .
Finite mixture of skew distributions have emerged as an effective tool in modelling heterogeneous data with asymmetric features . With various proposals appearing rapidly in the recent years , which are similar but not identical , the connections between them and their relative performance becomes rather unclear . This paper aims to provide a concise overview of these developments by presenting a systematic classification of the existing skew distributions into four types , thereby clarifying their close relationships . This also aids in understanding the link between some of the proposed expectation-maximization ( EM ) based algorithms for the computation of the maximum likelihood estimates of the parameters of the models . The final part of this paper presents an illustration of the performance of these mixture models in clustering a real dataset , relative to other non-elliptically contoured clustering methods and associated algorithms for their implementation .
The ability to track a moving vehicle is of crucial importance in numerous applications . The task has often been approached by the importance sampling technique of particle filters due to its ability to model non-linear and non-Gaussian dynamics , of which a vehicle travelling on a road network is a good example . Particle filters perform poorly when observations are highly informative . In this paper , we address this problem by proposing particle filters that sample around the most recent observation . The proposal leads to an order of magnitude improvement in accuracy and efficiency over conventional particle filters , especially when observations are infrequent but low-noise .
We present a verification technique for program safety that combines Iterated Specialization and Interpolating Horn Clause Solving . Our new method composes together these two techniques in a modular way by exploiting the common Horn Clause representation of the verification problem . The Iterated Specialization verifier transforms an initial set of verification conditions by using unfold/fold equivalence preserving transformation rules . During transformation , program invariants are discovered by applying widening operators . Then the output set of specialized verification conditions is analyzed by an Interpolating Horn Clause solver , hence adding the effect of interpolation to the effect of widening . The specialization and interpolation phases can be iterated , and also combined with other transformations that change the direction of propagation of the constraints ( forward from the program preconditions or backward from the error conditions ) . We have implemented our verification technique by integrating the VeriMAP verifier with the FTCLP Horn Clause solver , based on Iterated Specialization and Interpolation , respectively . Our experimental results show that the integrated verifier improves the precision of each of the individual components run separately .
In this paper , the problem of energy trading between smart grid prosumers , who can simultaneously consume and produce energy , and a grid power company is studied . The problem is formulated as a single-leader , multiple-follower Stackelberg game between the power company and multiple prosumers . In this game , the power company acts as a leader who determines the pricing strategy that maximizes its profits , while the prosumers act as followers who react by choosing the amount of energy to buy or sell so as to optimize their current and future profits . The proposed game accounts for each prosumer ' s subjective decision when faced with the uncertainty of profits , induced by the random future price . In particular , the framing effect , from the framework of prospect theory ( PT ) , is used to account for each prosumer ' s valuation of its gains and losses with respect to an individual utility reference point . The reference point changes between prosumers and stems from their past experience and future aspirations of profits . The followers ' noncooperative game is shown to admit a unique pure-strategy Nash equilibrium ( NE ) under classical game theory ( CGT ) which is obtained using a fully distributed algorithm . The results are extended to account for the case of PT using algorithmic solutions that can achieve an NE under certain conditions . Simulation results show that the total grid load varies significantly with the prosumers ' reference point and their loss-aversion level . In addition , it is shown that the power company ' s profits considerably decrease when it fails to account for the prosumers ' subjective perceptions under PT .
We put forward a model of action-based randomization mechanisms to analyse quantitative information flow ( QIF ) under generic leakage functions , and under possibly adaptive adversaries . This model subsumes many of the QIF models proposed so far . Our main contributions include the following : ( 0 ) we identify mild general conditions on the leakage function under which it is possible to derive general and significant results on adaptive QIF ; ( 0 ) we contrast the efficiency of adaptive and non-adaptive strategies , showing that the latter are as efficient as the former in terms of length up to an expansion factor bounded by the number of available actions ; ( 0 ) we show that the maximum information leakage over strategies , given a finite time horizon , can be expressed in terms of a Bellman equation . This can be used to compute an optimal finite strategy recursively , by resorting to standard methods like backward induction .
This study analyzes political interactions in the European Parliament ( EP ) by considering how the political agenda of the plenary sessions has evolved over time and the manner in which Members of the European Parliament ( MEPs ) have reacted to external and internal stimuli when making Parliamentary speeches . It does so by considering the context in which speeches are made , and the content of those speeches . To detect latent themes in legislative speeches over time , speech content is analyzed using a new dynamic topic modeling method , based on two layers of matrix factorization . This method is applied to a new corpus of all English language legislative speeches in the EP plenary from the period 0000-0000 . Our findings suggest that the political agenda of the EP has evolved significantly over time , is impacted upon by the committee structure of the Parliament , and reacts to exogenous events such as EU Treaty referenda and the emergence of the Euro-crisis have a significant impact on what is being discussed in Parliament .
We analytically investigate size and power properties of a popular family of procedures for testing linear restrictions on the coefficient vector in a linear regression model with temporally dependent errors . The tests considered are autocorrelation-corrected F-type tests based on prewhitened nonparametric covariance estimators that possibly incorporate a data-dependent bandwidth parameter , e . g . , estimators as considered in Andrews and Monahan ( 0000 ) , Newey and West ( 0000 ) , or Rho and Shao ( 0000 ) . For design matrices that are generic in a measure theoretic sense we prove that these tests either suffer from extreme size distortions or from strong power deficiencies . Despite this negative result we demonstrate that a simple adjustment procedure based on artificial regressors can often resolve this problem .
This paper provides two general classes of multiple decision functions where each member of the first class strongly controls the family-wise error rate ( FWER ) , while each member of the second class strongly controls the false discovery rate ( FDR ) . These classes offer the possibility that an optimal multiple decision function with respect to a pre-specified criterion , such as the missed discovery rate ( MDR ) , could be found within these classes . Such multiple decision functions can be utilized in multiple testing , specifically , but not limited to , the analysis of high-dimensional microarray data sets .
Positron Emission Tomography ( PET ) is a functional imaging modality widely used in neuroscience studies . To obtain meaningful quantitative results from PET images , attenuation correction is necessary during image reconstruction . For PET/MR hybrid systems , PET attenuation is challenging as Magnetic Resonance ( MR ) images do not reflect attenuation coefficients directly . To address this issue , we present deep neural network methods to derive the continuous attenuation coefficients for brain PET imaging from MR images . With only Dixon MR images as the network input , the existing U-net structure was adopted and analysis using forty patient data sets shows it is superior than other Dixon based methods . When both Dixon and zero echo time ( ZTE ) images are available , apart from stacking multiple MR images along the U-net input channels , we have proposed a new network structure to extract the features from Dixon and ZTE images independently at early layers and combine them together at later layers . Quantitative analysis based on fourteen real patient data sets demonstrates that both network approaches can perform better than the standard methods , and the proposed network structure can further reduce the PET quantification error compared to the U-net structure with multiple inputs .
A general theory for Gaussian mean estimation that automatically adapts to unknown sparsity under arbitrary norms is proposed . The theory is applied to produce adaptively minimax rate-optimal estimators in high dimensional regression and matrix estimation that involve no tuning parameters .
Lambda calculus is the basis of functional programming and higher order proof assistants . However , little is known about combinatorial properties of lambda terms , in particular , about their asymptotic distribution and random generation . This paper tries to answer questions like : How many terms of a given size are there ? What is a " typical " structure of a simply typable term ? Despite their ostensible simplicity , these questions still remain unanswered , whereas solutions to such problems are essential for testing compilers and optimizing programs whose expected efficiency depends on the size of terms . Our approach toward the afore-mentioned problems may be later extended to any language with bound variables , i . e . , with scopes and declarations . This paper presents two complementary approaches : one , theoretical , uses complex analysis and generating functions , the other , experimental , is based on a generator of lambda-terms . Thanks to de Bruijn indices , we provide three families of formulas for the number of closed lambda terms of a given size and we give four relations between these numbers which have interesting combinatorial interpretations . As a by-product of the counting formulas , we design an algorithm for generating lambda terms . Performed tests provide us with experimental data , like the average depth of bound variables and the average number of head lambdas . We also create random generators for various sorts of terms . Thereafter , we conduct experiments that answer questions like : What is the ratio of simply typable terms among all terms ? ( Very small ! ) How are simply typable lambda terms distributed among all lambda terms ? ( A typable term almost always starts with an abstraction . ) In this paper , abstractions and applications have size 0 and variables have size 0 .
This paper investigates the problem of distributed stochastic approximation in multi-agent systems . The algorithm under study consists of two steps : a local stochastic approximation step and a diffusion step which drives the network to a consensus . The diffusion step uses row-stochastic matrices to weight the network exchanges . As opposed to previous works , exchange matrices are not supposed to be doubly stochastic , and may also depend on the past estimate . We prove that non-doubly stochastic matrices generally influence the limit points of the algorithm . Nevertheless , the limit points are not affected by the choice of the matrices provided that the latter are doubly-stochastic in expectation . This conclusion legitimates the use of broadcast-like diffusion protocols , which are easier to implement . Next , by means of a central limit theorem , we prove that doubly stochastic protocols perform asymptotically as well as centralized algorithms and we quantify the degradation caused by the use of non doubly stochastic matrices . Throughout the paper , a special emphasis is put on the special case of distributed non-convex optimization as an illustration of our results .
We propose the use of probability models for ranked data as a useful alternative to a quantitative data analysis to investigate the outcome of bioassay experiments , when the preliminary choice of an appropriate normalization method for the raw numerical responses is difficult or subject to criticism . We review standard distance-based and multistage ranking models and in this last context we propose an original generalization of the Plackett-Luce model to account for the order of the ranking elicitation process . The usefulness of the novel model is illustrated with its maximum likelihood estimation for a real data set . Specifically , we address the heterogeneous nature of experimental units via model-based clustering and detail the necessary steps for a successful likelihood maximization through a hybrid version of the Expectation-Maximization algorithm . The performance of the mixture model using the new distribution as mixture components is compared with those relative to alternative mixture models for random rankings . A discussion on the interpretation of the identified clusters and a comparison with more standard quantitative approaches are finally provided .
Nearest neighbor methods are a popular class of nonparametric estimators with several desirable properties , such as adaptivity to different distance scales in different regions of space . Prior work on convergence rates for nearest neighbor classification has not fully reflected these subtle properties . We analyze the behavior of these estimators in metric spaces and provide finite-sample , distribution-dependent rates of convergence under minimal assumptions . As a by-product , we are able to establish the universal consistency of nearest neighbor in a broader range of data spaces than was previously known . We illustrate our upper and lower bounds by introducing smoothness classes that are customized for nearest neighbor classification .
We present an implementation of a recent algorithm to compute shortest-path trees in unit disk graphs in $O ( n\log n ) $ worst-case time , where $n$ is the number of disks . In the minimum-separation problem , we are given $n$ unit disks and two points $s$ and $t$ , not contained in any of the disks , and we want to compute the minimum number of disks one needs to retain so that any curve connecting $s$ to $t$ intersects some of the retained disks . We present a new algorithm solving this problem in $O ( n^0\log^0 n ) $ worst-case time and its implementation .
Constraint propagation algorithms form an important part of most of the constraint programming systems . We provide here a simple , yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way . In this framework we proceed in two steps . First , we introduce a generic iteration algorithm on partial orderings and prove its correctness in an abstract setting . Then we instantiate this algorithm with specific partial orderings and functions to obtain specific constraint propagation algorithms . In particular , using the notions commutativity and semi-commutativity , we show that the {\tt AC-0} , {\tt PC-0} , {\tt DAC} and {\tt DPC} algorithms for achieving ( directional ) arc consistency and ( directional ) path consistency are instances of a single generic algorithm . The work reported here extends and simplifies that of Apt \citeyear{Apt00b} .
Finding and fixing bugs are time-consuming activities in software development . Spectrum-based fault localization aims to identify the faulty position in source code based on the execution trace of test cases . Failing test cases and their assertions form test oracles for the failing behavior of the system under analysis . In this paper , we propose a novel concept of spectrum driven test case purification for improving fault localization . The goal of test case purification is to separate existing test cases into small fractions ( called purified test cases ) and to enhance the test oracles to further localize faults . Combining with an original fault localization technique ( e . g . , Tarantula ) , test case purification results in better ranking the program statements . Our experiments on 0000 faults in six open-source Java programs show that test case purification can effectively improve existing fault localization techniques .
We present a uniform self-stabilizing algorithm , which solves the problem of distributively finding a minimum diameter spanning tree of an arbitrary positively real-weighted graph . Our algorithm consists in two stages of stabilizing protocols . The first stage is a uniform randomized stabilizing {\em unique naming} protocol , and the second stage is a stabilizing {\em MDST} protocol , designed as a {\em fair composition} of Merlin--Segall ' s stabilizing protocol and a distributed deterministic stabilizing protocol solving the ( MDST ) problem . The resulting randomized distributed algorithm presented herein is a composition of the two stages ; it stabilizes in $O ( n\Delta+{\cal D}^0 + n \log\log n ) $ expected time , and uses $O ( n^0\log n + n \log W ) $ memory bits ( where $n$ is the order of the graph , $\Delta$ is the maximum degree of the network , $\cal D$ is the diameter in terms of hops , and $W$ is the largest edge weight ) . To our knowledge , our protocol is the very first distributed algorithm for the ( MDST ) problem . Moreover , it is fault-tolerant and works for any anonymous arbitrary network .
Nested sampling is an iterative integration procedure that shrinks the prior volume towards higher likelihoods by removing a " live " point at a time . A replacement point is drawn uniformly from the prior above an ever-increasing likelihood threshold . Thus , the problem of drawing from a space above a certain likelihood value arises naturally in nested sampling , making algorithms that solve this problem a key ingredient to the nested sampling framework . If the drawn points are distributed uniformly , the removal of a point shrinks the volume in a well-understood way , and the integration of nested sampling is unbiased . In this work , I develop a statistical test to check whether this is the case . This " Shrinkage Test " is useful to verify nested sampling algorithms in a controlled environment . I apply the shrinkage test to a test-problem , and show that some existing algorithms fail to pass it due to over-optimisation . I then demonstrate that a simple algorithm can be constructed which is robust against this type of problem . This RADFRIENDS algorithm is , however , inefficient in comparison to MULTINEST .
Social networks enable users to freely communicate with each other and share their recent news , ongoing activities or views about different topics . As a result , they can be seen as a potentially viable source of information to understand the current emerging topics/events . The ability to model emerging topics is a substantial step to monitor and summarize the information originating from social sources . Applying traditional methods for event detection which are often proposed for processing large , formal and structured documents , are less effective , due to the short length , noisiness and informality of the social posts . Recent event detection techniques address these challenges by exploiting the opportunities behind abundant information available in social networks . This article provides an overview of the state of the art in event detection from social networks .
A sup-interpretation is a tool which provides an upper bound on the size of a value computed by some symbol of a program . Sup-interpretations have shown their interest to deal with the complexity of first order functional programs . For instance , they allow to characterize all the functions bitwise computable in Alogtime . This paper is an attempt to adapt the framework of sup-interpretations to a fragment of oriented-object programs , including distinct encodings of numbers through the use of constructor symbols , loop and while constructs and non recursive methods with side effects . We give a criterion , called brotherly criterion , which ensures that each brotherly program computes objects whose size is polynomially bounded by the inputs sizes .
We study infinite two-player games where one of the players is unsure about the set of moves available to the other player . In particular , the set of moves of the other player is a strict superset of what she assumes it to be . We explore what happens to sets in various levels of the Borel hierarchy under such a situation . We show that the sets at every alternate level of the hierarchy jump to the next higher level .
Predicting user affinity to items is an important problem in applications like content optimization , computational advertising , and many more . While bilinear random effect models ( matrix factorization ) provide state-of-the-art performance when minimizing RMSE through a Gaussian response model on explicit ratings data , applying it to imbalanced binary response data presents additional challenges that we carefully study in this paper . Data in many applications usually consist of users ' implicit response that are often binary -- clicking an item or not ; the goal is to predict click rates , which is often combined with other measures to calculate utilities to rank items at runtime of the recommender systems . Because of the implicit nature , such data are usually much larger than explicit rating data and often have an imbalanced distribution with a small fraction of click events , making accurate click rate prediction difficult . In this paper , we address two problems . First , we show previous techniques to estimate bilinear random effect models with binary data are less accurate compared to our new approach based on adaptive rejection sampling , especially for imbalanced response . Second , we develop a parallel bilinear random effect model fitting framework using Map-Reduce paradigm that scales to massive datasets . Our parallel algorithm is based on a " divide and conquer " strategy coupled with an ensemble approach . Through experiments on the benchmark MovieLens data , a small Yahoo ! Front Page data set , and a large Yahoo ! Front Page data set that contains 0M users and 0B binary observations , we show that careful handling of binary response as well as identifiability issues are needed to achieve good performance for click rate prediction , and that the proposed adaptive rejection sampler and the partitioning as well as ensemble techniques significantly improve model performance .
In this article , the analysis of misspecification was extended to the recently introduced stochastic restricted biased estimators when multicollinearity exists among the explanatory variables . The Stochastic Restricted Ridge Estimator ( SRRE ) , Stochastic Restricted Almost Unbiased Ridge Estimator ( SRAURE ) , Stochastic Restricted Liu Estimator ( SRLE ) , Stochastic Restricted Almost Unbiased Liu Estimator ( SRAULE ) , Stochastic Restricted Principal Component Regression Estimator ( SRPCR ) , Stochastic Restricted r-k class estimator ( SRrk ) and Stochastic Restricted r-d class estimator ( SRrd ) were examined in the misspecified regression model due to missing relevant explanatory variables when incomplete prior information of the regression coefficients is available . Further , the superiority conditions between estimators and their respective predictors were obtained in the mean square error matrix sense ( MSEM ) . Finally , a numerical example and a Monte Carlo simulation study were done to illustrate the theoretical findings .
In this paper we discuss the estimation of a nonparametric component $f_0$ of a nonparametric additive model $Y=f_0 ( X_0 ) + . . . + f_q ( X_q ) + \varepsilon$ . We allow the number $q$ of additive components to grow to infinity and we make sparsity assumptions about the number of nonzero additive components . We compare this estimation problem with that of estimating $f_0$ in the oracle model $Z= f_0 ( X_0 ) + \varepsilon$ , for which the additive components $f_0 , \dots , f_q$ are known . We construct a two-step presmoothing-and-resmoothing estimator of $f_0$ in the additive model and state finite-sample bounds for the difference between our estimator and some smoothing estimators $\tilde f_0^{\text{oracle}}$ in the oracle model which satisfy mild conditions . In an asymptotic setting these bounds can be used to show asymptotic equivalence of our estimator and the oracle estimators ; the paper thus shows that , asymptotically , under strong enough sparsity conditions , knowledge of $f_0 , \dots , f_q$ has no effect on estimation efficiency . Our first step is to estimate all of the components in the additive model with undersmoothing using a group-Lasso estimator . We then construct pseudo responses $\hat Y$ by evaluating a desparsified modification of our undersmoothed estimator of $f_0$ at the design points . In the second step the smoothing method of the oracle estimator $\tilde f_0^{\text{oracle}}$ is applied to a nonparametric regression problem with " responses " $\hat Y$ and covariates $X_0$ . Our mathematical exposition centers primarily on establishing properties of the presmoothing estimator . We also present simulation results demonstrating close-to-oracle performance of our estimator in practical applications . The main results of the paper are also important for understanding the behavior of the presmoothing estimator when the resmoothing step is omitted .
A key goal of computer vision is to recover the underlying 0D structure from 0D observations of the world . In this paper we learn strong deep generative models of 0D structures , and recover these structures from 0D and 0D images via probabilistic inference . We demonstrate high-quality samples and report log-likelihoods on several datasets , including ShapeNet [0] , and establish the first benchmarks in the literature . We also show how these models and their inference networks can be trained end-to-end from 0D images . This demonstrates for the first time the feasibility of learning to infer 0D representations of the world in a purely unsupervised manner .
Recently , the mobile industry has experienced an extreme increment in number of its users . The GSM network with the greatest worldwide number of users succumbs to several security vulnerabilities . Although some of its security problems are addressed in its upper generations , there are still many operators using 0G systems . This paper briefly presents the most important security flaws of the GSM network and its transport channels . It also provides some practical solutions to improve the security of currently available 0G systems .
The Gordon and Betty Moore Foundation ran an Investigator Competition as part of its Data-Driven Discovery Initiative in 0000 . We received about 0 , 000 applications and each applicant had the opportunity to list up to five influential works in the general field of " Big Data " for scientific discovery . We collected nearly 0 , 000 references and 00 works were cited at least six times . This paper contains our preliminary findings .
Advanced computing and data acquisition technologies have made possible the collection of high-dimensional data streams in many fields . Efficient online monitoring tools which can correctly identify any abnormal data stream for such data are highly sought after . However , most of the existing monitoring procedures directly apply the false discover rate ( FDR ) controlling procedure to the data at each time point , and the FDR at each time point ( the point-wise FDR ) is either specified by users or determined by the in-control ( IC ) average run length ( ARL ) . If the point-wise FDR is specified by users , the resulting procedure lacks control of the global FDR and keeps users in the dark in terms of the IC-ARL . If the point-wise FDR is determined by the IC-ARL , the resulting procedure does not give users the flexibility to choose the number of false alarms ( Type-I errors ) they can tolerate when identifying abnormal data streams , which often makes the procedure too conservative . To address those limitations , we propose a two-stage monitoring procedure that can control both the IC-ARL and Type-I errors at the levels specified by users . As a result , the proposed procedure allows users to choose not only how often they expect any false alarms when all data streams are IC , but also how many false alarms they can tolerate when identifying abnormal data streams . With this extra flexibility , our proposed two-stage monitoring procedure is shown in the simulation study and real data analysis to outperform the exiting methods .
There is an increasing interest in executing complex analyses over large graphs , many of which require processing a large number of multi-hop neighborhoods or subgraphs . Examples include ego network analysis , motif counting , personalized recommendations , and others . These tasks are not well served by existing vertex-centric graph processing frameworks , where user programs are only able to directly access the state of a single vertex . This paper introduces NSCALE , a novel end-to-end graph processing framework that enables the distributed execution of complex subgraph-centric analytics over large-scale graphs in the cloud . NSCALE enables users to write programs at the level of subgraphs rather than at the level of vertices . Unlike most previous graph processing frameworks , which apply the user program to the entire graph , NSCALE allows users to declaratively specify subgraphs of interest . Our framework includes a novel graph extraction and packing ( GEP ) module that utilizes a cost-based optimizer to partition and pack the subgraphs of interest into memory on as few machines as possible . The distributed execution engine then takes over and runs the user program in parallel , while respecting the scope of the various subgraphs . Our experimental results show orders-of-magnitude improvements in performance and drastic reductions in the cost of analytics compared to vertex-centric approaches .
The idea of computing with DNA was given by Tom Head in 0000 , however in 0000 in a seminal paper , the actual successful experiment for DNA computing was performed by Adleman . The heart of the DNA computing is the DNA hybridization , however , it is also the source of errors . Thus the success of the DNA computing depends on the error control techniques . The classical coding theory techniques have provided foundation for the current information and communication technology ( ICT ) . Thus it is natural to expect that coding theory will be the foundational subject for the DNA computing paradigm . For the successful experiments with DNA computing usually we design DNA strings which are sufficiently dissimilar . This leads to the construction of a large set of DNA strings which satisfy certain combinatorial and thermodynamic constraints . Over the last 00 years , many approaches such as combinatorial , algebraic , computational have been used to construct such DNA strings . In this work , we survey this interesting area of DNA coding theory by providing key ideas of the area and current known results .
In this work , a novel subspace-based method for blind identification of multichannel finite impulse response ( FIR ) systems is presented . Here , we exploit directly the impeded Toeplitz channel structure in the signal linear model to build a quadratic form whose minimization leads to the desired channel estimation up to a scalar factor . This method can be extended to estimate any predefined linear structure , e . g . Hankel , that is usually encountered in linear systems . Simulation findings are provided to highlight the appealing advantages of the new structure-based subspace ( SSS ) method over the standard subspace ( SS ) method in certain adverse identification scenarii .
We point out some pitfalls related to the concept of an oracle property as used in Fan and Li ( 0000 , 0000 , 0000 ) which are reminiscent of the well-known pitfalls related to Hodges ' estimator . The oracle property is often a consequence of sparsity of an estimator . We show that any estimator satisfying a sparsity property has maximal risk that converges to the supremum of the loss function ; in particular , the maximal risk diverges to infinity whenever the loss function is unbounded . For ease of presentation the result is set in the framework of a linear regression model , but generalizes far beyond that setting . In a Monte Carlo study we also assess the extent of the problem in finite samples for the smoothly clipped absolute deviation ( SCAD ) estimator introduced in Fan and Li ( 0000 ) . We find that this estimator can perform rather poorly in finite samples and that its worst-case performance relative to maximum likelihood deteriorates with increasing sample size when the estimator is tuned to sparsity .
Biips is a software platform for automatic Bayesian inference with interacting particle systems . Biips allows users to define their statistical model in the probabilistic programming BUGS language , as well as to add custom functions or samplers within this language . Then it runs sequential Monte Carlo based algorithms ( particle filters , particle independent Metropolis-Hastings , particle marginal Metropolis-Hastings ) in a black-box manner so that to approximate the posterior distribution of interest as well as the marginal likelihood . The software is developed in C++ with interfaces with the softwares R , Matlab and Octave .
This paper investigates the use of bootstrap-based bias correction of semi-parametric estimators of the long memory parameter in fractionally integrated processes . The re-sampling method involves the application of the sieve bootstrap to data pre-filtered by a preliminary semi-parametric estimate of the long memory parameter . Theoretical justification for using the bootstrap techniques to bias adjust log-periodogram and semi-parametric local Whittle estimators of the memory parameter is provided . Simulation evidence comparing the performance of the bootstrap bias correction with analytical bias correction techniques is also presented . The bootstrap method is shown to produce notable bias reductions , in particular when applied to an estimator for which analytical adjustments have already been used . The empirical coverage of confidence intervals based on the bias-adjusted estimators is very close to the nominal , for a reasonably large sample size , more so than for the comparable analytically adjusted estimators . The precision of inferences ( as measured by interval length ) is also greater when the bootstrap is used to bias correct rather than analytical adjustments .
We investigate a family of regression problems in a semi-supervised setting . The task is to assign real-valued labels to a set of $n$ sample points , provided a small training subset of $N$ labeled points . A goal of semi-supervised learning is to take advantage of the ( geometric ) structure provided by the large number of unlabeled data when assigning labels . We consider random geometric graphs , with connection radius $\epsilon ( n ) $ , to represent the geometry of the data set . Functionals which model the task reward the regularity of the estimator function and impose or reward the agreement with the training data . Here we consider the discrete $p$-Laplacian regularization . We investigate asymptotic behavior when the number of unlabeled points increases , while the number of training points remains fixed . We uncover a delicate interplay between the regularizing nature of the functionals considered and the nonlocality inherent to the graph constructions . We rigorously obtain almost optimal ranges on the scaling of $\epsilon ( n ) $ for the asymptotic consistency to hold . We prove that the minimizers of the discrete functionals in random setting converge uniformly to the desired continuum limit . Furthermore we discover that for the standard model used there is a restrictive upper bound on how quickly $\epsilon ( n ) $ must converge to zero as $n \to \infty$ . We introduce a new model which is as simple as the original model , but overcomes this restriction .
With the development of next generation sequencing technology , researchers have now been able to study the microbiome composition using direct sequencing , whose output are bacterial taxa counts for each microbiome sample . One goal of microbiome study is to associate the microbiome composition with environmental covariates . We propose to model the taxa counts using a Dirichlet-multinomial ( DM ) regression model in order to account for overdispersion of observed counts . The DM regression model can be used for testing the association between taxa composition and covariates using the likelihood ratio test . However , when the number of covariates is large , multiple testing can lead to loss of power . To address the high dimensionality of the problem , we develop a penalized likelihood approach to estimate the regression parameters and to select the variables by imposing a sparse group $\ell_0$ penalty to encourage both group-level and within-group sparsity . Such a variable selection procedure can lead to selection of the relevant covariates and their associated bacterial taxa . An efficient block-coordinate descent algorithm is developed to solve the optimization problem . We present extensive simulations to demonstrate that the sparse DM regression can result in better identification of the microbiome-associated covariates than models that ignore overdispersion or only consider the proportions . We demonstrate the power of our method in an analysis of a data set evaluating the effects of nutrient intake on human gut microbiome composition . Our results have clearly shown that the nutrient intake is strongly associated with the human gut microbiome .
Distributed software engineering is widely recognized as a complex task . Among the inherent complexities is the process of obtaining a system design from its global requirement specification . This paper deals with such transformation process and suggests an approach to derive the behavior of a given system components , in the form of distributed Finite State Machines , from the global system requirements , in the form of an augmented UML Activity Diagrams notation . The process of the suggested approach is summarized in three steps : the definition of the appropriate source Meta-Model ( requirements Meta-Model ) , the definition of the target Design Meta-Model and the definition of the rules to govern the transformation during the derivation process . The derivation process transforms the global system requirements described as UML diagram activities ( extended with collaborations ) to system roles behaviors represented as UML finite state machines . The approach is implemented using Atlas Transformation Language ( ATL ) .
The three-way table problem is to decide if there exists an l x m x n table satisfying given line sums , and find a table if there is one . It is NP-complete already for l=0 and every bounded integer program can be isomorphically represented in polynomial time for some m and n as some 0 x m x n table problem . Recently , the problem was shown to be fixed-parameter tractable with parameters l , m . Here we extend this and show that the huge version of the problem , where the variable side n is a huge number encoded in binary , is also fixed-parameter tractable with parameters l , m . We also conclude that the huge multicommodity flow problem with m suppliers and a huge number n of consumers is fixed-parameter tractable parameterized by the numbers of commodities and consumer types . One of our tools is a theorem about unimodular monoids which is of interest on its own right . The monoid problem is to decide if a given integer vector is a finite nonnegative integer combination of a given set of integer vectors , and find such a decomposition if one exists . We consider sets given implicitly by an inequality system . For such sets , it was recently shown that in fixed dimension the problem is solvable in polynomial time with degree which is exponential in the dimension . Here we show that when the inequality system which defines the set is defined by a totally unimodular matrix , the monoid problem can be solved in polynomial time even in variable dimension .
Rejoinder to " Feature Matching in Time Series Modeling " by Y . Xia and H . Tong [arXiv : 0000 . 0000]
This article is devoted to nonlinear approximation and estimation via piecewise polynomials built on partitions into dyadic rectangles . The approximation rate is studied over possibly inhomogeneous and anisotropic smoothness classes that contain Besov classes . Highlighting the interest of such a result in statistics , adaptation in the minimax sense to both inhomogeneity and anisotropy of a related multivariate density estimator is proved . Besides , that estimation procedure can be implemented with a computational complexity simply linear in the sample size .
We study the rate of Bayesian consistency for hierarchical priors consisting of prior weights on a model index set and a prior on a density model for each choice of model index . Ghosal , Lember and Van der Vaart [0] have obtained general in-probability theorems on the rate of convergence of the resulting posterior distributions . We extend their results to almost sure assertions . As an application we study log spline densities with a finite number of models and obtain that the Bayes procedure achieves the optimal minimax rate $n^{-\gamma/ ( 0\gamma+0 ) }$ of convergence if the true density of the observations belongs to the H\ " {o}lder space $C^{\gamma}[0 , 0]$ . This strengthens a result in [0 ; 0] . We also study consistency of posterior distributions of the model index and give conditions ensuring that the posterior distributions concentrate their masses near the index of the best model .
We consider a distributed user association problem in the downlink of a small cell network , where small cells obtain the required energy for providing wireless services to users through ambient energy harvesting . Since energy harvesting is opportunistic in nature , the amount of harvested energy is a random variable , without a priori known characteristics . We model the network as a competitive market with uncertainty , where self-interested small cells , modeled as consumers , are willing to maximize their utility scores by selecting users , represented by commodities . The utility scores of small cells depend on the amount of harvested energy , formulated as natures ' state . Under this model , the problem is to assign users to small cells , so that the aggregate network utility is maximized . The solution is the general equilibrium under uncertainty , also called Arrow-Debreu equilibrium . We show that in our setting , such equilibrium not only exists , but also is unique and is Pareto optimal in the sense of expected aggregate network utility . We use the Walras ' tatonnement process with some modifications in order to implement the equilibrium efficiently .
Cloud computing is steadily growing and , as IaaS vendors have started to offer pay-as-you-go billing policies , it is fundamental to achieve as much elasticity as possible , avoiding over-provisioning that would imply higher costs . In this paper , we briefly analyse the orchestration characteristics of PaaSSOA , a proposed architecture already implemented for Jolie microservices , and Kubernetes , one of the various orchestration plugins for Docker ; then , we outline similarities and differences of the two approaches , with respect to their own domain of application . Furthermore , we investigate some ideas to achieve a federation of the two technologies , proposing an architectural composition of Jolie microservices on Docker Container-as-a-Service layer .
The Hammersley-Clifford theorem states that if the support of a Markov random field has a safe symbol then it is a Gibbs state with some nearest neighbour interaction . In this paper we generalise the theorem with an added condition that the underlying graph is bipartite . Taking inspiration from " Gibbs Measures and Dismantlable Graphs " by Brightwell and Winkler we introduce a notion of folding for configuration spaces called strong config-folding proving that if all Markov random fields supported on $X$ are Gibbs with some nearest neighbour interaction so are Markov random fields supported on the ' strong config-folds ' and ' strong config-unfolds ' of $X$ .
Recent advances in genomics have underscored the surprising ubiquity of DNA copy number variation ( CNV ) . Fortunately , modern genotyping platforms also detect CNVs with fairly high reliability . Hidden Markov models and algorithms have played a dominant role in the interpretation of CNV data . Here we explore CNV reconstruction via estimation with a fused-lasso penalty as suggested by Tibshirani and Wang [Biostatistics 0 ( 0000 ) 00--00] . We mount a fresh attack on this difficult optimization problem by the following : ( a ) changing the penalty terms slightly by substituting a smooth approximation to the absolute value function , ( b ) designing and implementing a new MM ( majorization--minimization ) algorithm , and ( c ) applying a fast version of Newton ' s method to jointly update all model parameters . Together these changes enable us to minimize the fused-lasso criterion in a highly effective way . We also reframe the reconstruction problem in terms of imputation via discrete optimization . This approach is easier and more accurate than parameter estimation because it relies on the fact that only a handful of possible copy number states exist at each SNP . The dynamic programming framework has the added bonus of exploiting information that the current fused-lasso approach ignores . The accuracy of our imputations is comparable to that of hidden Markov models at a substantially lower computational cost .
It is popular nowadays to bring techniques from bibliometrics and scientometrics into the world of digital libraries to analyze the collaboration patterns and explore mechanisms which underlie community development . In this paper we use the DBLP data to investigate the author ' s scientific career and provide an in-depth exploration of some of the computer science communities . We compare them in terms of productivity , population stability and collaboration trends . Besides we use these features to compare the sets of topranked conferences with their lower ranked counterparts .
In this work , we propose the marginal structured SVM ( MSSVM ) for structured prediction with hidden variables . MSSVM properly accounts for the uncertainty of hidden variables , and can significantly outperform the previously proposed latent structured SVM ( LSSVM ; Yu & Joachims ( 0000 ) ) and other state-of-art methods , especially when that uncertainty is large . Our method also results in a smoother objective function , making gradient-based optimization of MSSVMs converge significantly faster than for LSSVMs . We also show that our method consistently outperforms hidden conditional random fields ( HCRFs ; Quattoni et al . ( 0000 ) ) on both simulated and real-world datasets . Furthermore , we propose a unified framework that includes both our and several other existing methods as special cases , and provides insights into the comparison of different models in practice .
In this paper we obtain non-uniform exponential upper bounds for the rate of convergence of a version of the algorithm Context , when the underlying tree is not necessarily bounded . The algorithm Context is a well-known tool to estimate the context tree of a Variable Length Markov Chain . As a consequence of the exponential bounds we obtain a strong consistency result . We generalize in this way several previous results in the field .
This paper presents a new teleoperated spherical tensegrity robot capable of performing locomotion on steep inclined surfaces . With a novel control scheme centered around the simultaneous actuation of multiple cables , the robot demonstrates robust climbing on inclined surfaces in hardware experiments and speeds significantly faster than previous spherical tensegrity models . This robot is an improvement over other iterations in the TT-series and the first tensegrity to achieve reliable locomotion on inclined surfaces of up to 00\degree . We analyze locomotion in simulation and hardware under single and multi-cable actuation , and introduce two novel multi-cable actuation policies , suited for steep incline climbing and speed , respectively . We propose compelling justifications for the increased dynamic ability of the robot and motivate development of optimization algorithms able to take advantage of the robot ' s increased control authority .
We describe inferactive data analysis , so-named to denote an interactive approach to data analysis with an emphasis on inference after data analysis . Our approach is a compromise between Tukey ' s exploratory ( roughly speaking " model free " ) and confirmatory data analysis ( roughly speaking classical and " model based " ) , also allowing for Bayesian data analysis . We view this approach as close in spirit to current practice of applied statisticians and data scientists while allowing frequentist guarantees for results to be reported in the scientific literature , or Bayesian results where the data scientist may choose the statistical model ( and hence the prior ) after some initial exploratory analysis . While this approach to data analysis does not cover every scenario , and every possible algorithm data scientists may use , we see this as a useful step in concrete providing tools ( with frequentist statistical guarantees ) for current data scientists . The basis of inference we use is selective inference [Lee et al . , 0000 , Fithian et al . , 0000] , in particular its randomized form [Tian and Taylor , 0000a] . The randomized framework , besides providing additional power and shorter confidence intervals , also provides explicit forms for relevant reference distributions ( up to normalization ) through the {\em selective sampler} of Tian et al . [0000] . The reference distributions are constructed from a particular conditional distribution formed from what we call a DAG-DAG -- a Data Analysis Generative DAG . As sampling conditional distributions in DAGs is generally complex , the selective sampler is crucial to any practical implementation of inferactive data analysis . Our principal goal is in reviewing the recent developments in selective inference as well as describing the general philosophy of selective inference .
To achieve effective and efficient detection of Alzheimer ' s disease ( AD ) , many machine learning methods have been introduced into this realm . However , the general case of limited training samples , as well as different feature representations typically makes this problem challenging . In this work , we propose a novel multiple kernel learning framework to combine multi-modal features for AD classification , which is scalable and easy to implement . Contrary to the usual way of solving the problem in the dual space , we look at the optimization from a new perspective . By conducting Fourier transform on the Gaussian kernel , we explicitly compute the mapping function , which leads to a more straightforward solution of the problem in the primal space . Furthermore , we impose the mixed $L_{00}$ norm constraint on the kernel weights , known as the group lasso regularization , to enforce group sparsity among different feature modalities . This actually acts as a role of feature modality selection , while at the same time exploiting complementary information among different kernels . Therefore it is able to extract the most discriminative features for classification . Experiments on the ADNI data set demonstrate the effectiveness of the proposed method .
Deep learning has achieved a great success in many areas , from computer vision to natural language processing , to game playing , and much more . Yet , what deep learning is really doing is still an open question . There are a lot of works in this direction . For example , [0] tried to explain deep learning by group renormalization , and [0] tried to explain deep learning from the view of functional approximation . In order to address this very crucial question , here we see deep learning from perspective of mechanical learning and learning machine ( see [0] , [0] ) . From this particular angle , we can see deep learning much better and answer with confidence : What deep learning is really doing ? why it works well , how it works , and how much data is necessary for learning . We also will discuss advantages and disadvantages of deep learning at the end of this work .
The abundance of functional observations in scientific endeavors has led to a significant development in tools for functional data analysis ( FDA ) . This kind of data comes with several challenges : infinite-dimensionality of function spaces , observation noise , and so on . However , there is another interesting phenomena that creates problems in FDA . The functional data often comes with lateral displacements/deformations in curves , a phenomenon which is different from the height or amplitude variability and is termed phase variation . The presence of phase variability artificially often inflates data variance , blurs underlying data structures , and distorts principal components . While the separation and/or removal of phase from amplitude data is desirable , this is a difficult problem . In particular , a commonly used alignment procedure , based on minimizing the $\mathbb{L}^0$ norm between functions , does not provide satisfactory results . In this paper we motivate the importance of dealing with the phase variability and summarize several current ideas for separating phase and amplitude components . These approaches differ in the following : ( 0 ) the definition and mathematical representation of phase variability , ( 0 ) the objective functions that are used in functional data alignment , and ( 0 ) the algorithmic tools for solving estimation/optimization problems . We use simple examples to illustrate various approaches and to provide useful contrast between them .
Discovering causal relationships from data is the ultimate goal of many research areas . Constraint based causal exploration algorithms , such as PC , FCI , RFCI , PC-simple , IDA and Joint-IDA have achieved significant progress and have many applications . A common problem with these methods is the high computational complexity , which hinders their applications in real world high dimensional datasets , e . g gene expression datasets . In this paper , we present an R package , ParallelPC , that includes the parallelised versions of these causal exploration algorithms . The parallelised algorithms help speed up the procedure of experimenting big datasets and reduce the memory used when running the algorithms . The package is not only suitable for super-computers or clusters , but also convenient for researchers using personal computers with multi core CPUs . Our experiment results on real world datasets show that using the parallelised algorithms it is now practical to explore causal relationships in high dimensional datasets with thousands of variables in a single multicore computer . ParallelPC is available in CRAN repository at https : //cran . rproject . org/web/packages/ParallelPC/index . html .
Releasing Web query logs which contain valuable information for research or marketing , can breach the privacy of search engine users . Therefore rendering query logs to limit linking a query to an individual while preserving the data usefulness for analysis , is an important research problem . This survey provides an overview and discussion on the recent studies on this direction .
Protein function prediction is the important problem in modern biology . In this paper , the un-normalized , symmetric normalized , and random walk graph Laplacian based semi-supervised learning methods will be applied to the integrated network combined from multiple networks to predict the functions of all yeast proteins in these multiple networks . These multiple networks are network created from Pfam domain structure , co-participation in a protein complex , protein-protein interaction network , genetic interaction network , and network created from cell cycle gene expression measurements . Multiple networks are combined with fixed weights instead of using convex optimization to determine the combination weights due to high time complexity of convex optimization method . This simple combination method will not affect the accuracy performance measures of the three semi-supervised learning methods . Experiment results show that the un-normalized and symmetric normalized graph Laplacian based methods perform slightly better than random walk graph Laplacian based method for integrated network . Moreover , the accuracy performance measures of these three semi-supervised learning methods for integrated network are much better than the best accuracy performance measures of these three methods for the individual network .
We present the FuSSO , a functional analogue to the LASSO , that efficiently finds a sparse set of functional input covariates to regress a real-valued response against . The FuSSO does so in a semi-parametric fashion , making no parametric assumptions about the nature of input functional covariates and assuming a linear form to the mapping of functional covariates to the response . We provide a statistical backing for use of the FuSSO via proof of asymptotic sparsistency under various conditions . Furthermore , we observe good results on both synthetic and real-world data .
An algorithm is presented that efficiently solves the selection problem : finding the k-th smallest member of a set . Relevant to a divide-and-conquer strategy , the algorithm also partitions a set into small and large valued subsets . Applied recursively , this partitioning results in a sorted set . The algorithm ' s applicability is therefore much broader than just the selection problem . The presented algorithm is based upon R . W . Floyd ' s 0000 algorithm that constructs a heap from the bottom-up . Empirically , the presented algorithm ' s performance appears competitive with the popular quickselect algorithm , a variant of C . A . R . Hoare ' s 0000 quicksort algorithm . Furthermore , constructing a heap from the bottom-up is an inherently parallel process ( processors can work independently and simultaneously on subheap construction ) , suggesting a performance advantage with parallel implementations . Given the presented algorithm ' s broad applicability , simplicity , serial performance , and parallel nature , further study is warranted . Specifically , worst-case analysis is an important but still unsolved problem .
Test generation and test data selection are difficult tasks for model based testing . Tests for a program can be meld to a test suite . A lot of research is done to quantify the quality and improve a test suite . Code coverage metrics estimate the quality of a test suite . This quality is fine , if the code coverage value is high or 000% . Unfortunately it might be impossible to achieve 000% code coverage because of dead code for example . There is a gap between the feasible and theoretical maximal possible code coverage value . Our review of the research indicates , none of current research is concerned with exact gap computation . This paper presents a framework to compute such gaps exactly in an ISO-C compatible semantic and similar languages . We describe an efficient approximation of the gap in all the other cases . Thus , a tester can decide if more tests might be able or necessary to achieve better coverage .
We introduce new estimation methods for a sub-class of the Gaussian scale mixture models for wavelet trees by Wainwright , Simoncelli & Willsky that rely on modern results for composite likelihoods and approximate Bayesian inference . Our methodology is illustrated for denoising and edge detection problems in two-dimensional images .
Recent literature has shown that the control of False Discovery Rate ( FDR ) for distributed detection in wireless sensor networks ( WSNs ) can provide substantial improvement in detection performance over conventional design methodologies . In this paper , we further investigate system design issues in FDR based distributed detection . We demonstrate that improved system design may be achieved by employing the Kolmogorov-Smirnov distance metric instead of the deflection coefficient , as originally proposed in Ray&VarshneyAES00 . We also analyze the performance of FDR based distributed detection in the presence of Byzantines . Byzantines are malicious sensors which send falsified information to the Fusion Center ( FC ) to deteriorate system performance . We provide analytical and simulation results on the global detection probability as a function of the fraction of Byzantines in the network . It is observed that the detection performance degrades considerably when the fraction of Byzantines is large . Hence , we propose an adaptive algorithm at the FC which learns the Byzantines ' behavior over time and changes the FDR parameter to overcome the loss in detection performance . Detailed simulation results are provided to demonstrate the robustness of the proposed adaptive algorithm to Byzantine attacks in WSNs .
Learning of low dimensional structure in multidimensional data is a canonical problem in machine learning . One common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold . There are a rich variety of manifold learning methods available , which allow mapping of data points to the manifold . However , there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data . The best attempt is the Gaussian process latent variable model ( GP-LVM ) , but identifiability issues lead to poor performance . We solve these issues by proposing a novel Coulomb repulsive process ( Corp ) for locations of points on the manifold , inspired by physical models of electrostatic interactions among particles . Combining this process with a GP prior for the mapping function yields a novel electrostatic GP ( electroGP ) process . Focusing on the simple case of a one-dimensional manifold , we develop efficient inference algorithms , and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video .
One of the core problems in statistical models is the estimation of a posterior distribution . For topic models , the problem of posterior inference for individual texts is particularly important , especially when dealing with data streams , but is often intractable in the worst case . As a consequence , existing methods for posterior inference are approximate and do not have any guarantee on neither quality nor convergence rate . In this paper , we introduce a provably fast algorithm , namely Online Maximum a Posteriori Estimation ( OPE ) , for posterior inference in topic models . OPE has more attractive properties than existing inference approaches , including theoretical guarantees on quality and fast rate of convergence to a local maximal/stationary point of the inference problem . The discussions about OPE are very general and hence can be easily employed in a wide range of contexts . Finally , we employ OPE to design three methods for learning Latent Dirichlet Allocation from text streams or large corpora . Extensive experiments demonstrate some superior behaviors of OPE and of our new learning methods .
Sufficient dimension reduction ( SDR ) in regression , which reduces the dimension by replacing original predictors with a minimal set of their linear combinations without loss of information , is very helpful when the number of predictors is large . The standard SDR methods suffer because the estimated linear combinations usually consist of all original predictors , making it difficult to interpret . In this paper , we propose a unified method - coordinate-independent sparse estimation ( CISE ) - that can simultaneously achieve sparse sufficient dimension reduction and screen out irrelevant and redundant variables efficiently . CISE is subspace oriented in the sense that it incorporates a coordinate-independent penalty term with a broad series of model-based and model-free SDR approaches . This results in a Grassmann manifold optimization problem and a fast algorithm is suggested . Under mild conditions , based on manifold theories and techniques , it can be shown that CISE would perform asymptotically as well as if the true irrelevant predictors were known , which is referred to as the oracle property . Simulation studies and a real-data example demonstrate the effectiveness and efficiency of the proposed approach .
Phylogenetic mixture models are statistical models of character evolution allowing for heterogeneity . Each of the classes in some unknown partition of the characters may evolve by different processes , or even along different trees . The fundamental question of whether parameters of such a model are identifiable is difficult to address , due to the complexity of the parameterization . We analyze mixture models on large trees , with many mixture components , showing that both numerical and tree parameters are indeed identifiable in these models when all trees are the same . We also explore the extent to which our algebraic techniques can be employed to extend the result to mixtures on different trees .
The historical and geographical spread from older to more modern languages has long been studied by examining textual changes and in terms of changes in phonetic transcriptions . However , it is more difficult to analyze language change from an acoustic point of view , although this is usually the dominant mode of transmission . We propose a novel analysis approach for acoustic phonetic data , where the aim will be to statistically model the acoustic properties of spoken words . We explore phonetic variation and change using a time-frequency representation , namely the log-spectrograms of speech recordings . We identify time and frequency covariance functions as a feature of the language ; in contrast , mean spectrograms depend mostly on the particular word that has been uttered . We build models for the mean and covariances ( taking into account the restrictions placed on the statistical analysis of such objects ) and use these to define a phonetic transformation that models how an individual speaker would sound in a different language , allowing the exploration of phonetic differences between languages . Finally , we map back these transformations to the domain of sound recordings , allowing us to listen to the output of the statistical analysis . The proposed approach is demonstrated using recordings of the words corresponding to the numbers from " one " to " ten " as pronounced by speakers from five different Romance languages .
We present a new computational technique to detect and analyze statistically significant geographic variation in language . Our meta-analysis approach captures statistical properties of word usage across geographical regions and uses statistical methods to identify significant changes specific to regions . While previous approaches have primarily focused on lexical variation between regions , our method identifies words that demonstrate semantic and syntactic variation as well . We extend recently developed techniques for neural language models to learn word representations which capture differing semantics across geographical regions . In order to quantify this variation and ensure robust detection of true regional differences , we formulate a null model to determine whether observed changes are statistically significant . Our method is the first such approach to explicitly account for random variation due to chance while detecting regional variation in word meaning . To validate our model , we study and analyze two different massive online data sets : millions of tweets from Twitter spanning not only four different countries but also fifty states , as well as millions of phrases contained in the Google Book Ngrams . Our analysis reveals interesting facets of language change at multiple scales of geographic resolution -- from neighboring states to distant continents . Finally , using our model , we propose a measure of semantic distance between languages . Our analysis of British and American English over a period of 000 years reveals that semantic variation between these dialects is shrinking .
The demand for high performance embedded processors , for consumer electronics , is rapidly increasing for the past few years . Many of these embedded processors depend upon custom built Instruction Ser Architecture ( ISA ) such as game processor ( GPU ) , multimedia processors , DSP processors etc . Primary requirement for consumer electronic industry is low cost with high performance and low power consumption . A lot of research has been evolved to enhance the performance of embedded processors through parallel computing . But some of them focus superscalar processors i . e . single processors with more resources like Instruction Level Parallelism ( ILP ) which includes Very Long Instruction Word ( VLIW ) architecture , custom instruction set extensible processor architecture and others require more number of processing units on a single chip like Thread Level Parallelism ( TLP ) that includes Simultaneous Multithreading ( SMT ) , Chip Multithreading ( CMT ) and Chip Multiprocessing ( CMP ) . In this paper , we present a new technique , named C-slow , to enhance performance for embedded processors for consumer electronics by exploiting multithreading technique in single core processors . Without resulting into the complexity of micro controlling with Real Time Operating system ( RTOS ) , C-slowed processor can execute multiple threads in parallel using single datapath of Instruction Set processing element . This technique takes low area & approach complexity of general purpose processor running RTOS .
Stochastic process exhibiting power-law slopes in the frequency domain are frequently well modeled by fractional Brownian motion ( fBm ) . In particular , the spectral slope at high frequencies is associated with the degree of small-scale roughness or fractal dimension . However , a broad class of real-world signals have a high-frequency slope , like fBm , but a plateau in the vicinity of zero frequency . This low-frequency plateau , it is shown , implies that the temporal integral of the process exhibits diffusive behavior , dispersing from its initial location at a constant rate . Such processes are not well modeled by fBm , which has a singularity at zero frequency corresponding to an unbounded rate of dispersion . A more appropriate stochastic model is a much lesser-known random process called the Matern process , which is shown herein to be a damped version of fractional Brownian motion . This article first provides a thorough introduction to fractional Brownian motion , then examines the details of the Matern process and its relationship to fBm . An algorithm for the simulation of the Matern process in O ( N log N ) operations is given . Unlike fBm , the Matern process is found to provide an excellent match to modeling velocities from particle trajectories in an application to two-dimensional fluid turbulence .
In many studies multivariate event time data are generated from clusters having a possibly complex association pattern . Flexible models are needed to capture this dependence . Vine copulas serve this purpose . Inference methods for vine copulas are available for complete data . Event time data , however , are often subject to right-censoring . As a consequence , the existing inferential tools , e . g . likelihood estimation , need to be adapted . A two-stage estimation approach is proposed . First , the marginal distributions are modeled . Second , the dependence structure modeled by a vine copula is estimated via likelihood maximization . Due to the right-censoring single and double integrals show up in the copula likelihood expression such that numerical integration is needed for its evaluation . For the dependence modeling a sequential estimation approach that facilitates the computational challenges of the likelihood optimization is provided . A three-dimensional simulation study provides evidence for the good finite sample performance of the proposed method . Using four-dimensional mastitis data , it is shown how an appropriate vine copula model can be selected for data at hand .
In this paper , we present a novel pseudo sequence based 0-D hierarchical reference structure for light-field image compression . In the proposed scheme , we first decompose the light-field image into multiple views and organize them into a 0-D coding structure according to the spatial coordinates of the corresponding microlens . Then we mainly develop three technologies to optimize the 0-D coding structure . First , we divide all the views into four quadrants , and all the views are encoded one quadrant after another to reduce the reference buffer size as much as possible . Inside each quadrant , all the views are encoded hierarchically to fully exploit the correlations between different views . Second , we propose to use the distance between the current view and its reference views as the criteria for selecting better reference frames for each inter view . Third , we propose to use the spatial relative positions between different views to achieve more accurate motion vector scaling . The whole scheme is implemented in the reference software of High Efficiency Video Coding . The experimental results demonstrate that the proposed novel pseudo-sequence based 0-D hierarchical structure can achieve maximum 00 . 0% bit-rate savings compared with the state-of-the-art light-field image compression method .
We study systems of equations of the form X0 = f0 ( X0 , . . . , Xn ) , . . . , Xn = fn ( X0 , . . . , Xn ) , where each fi is a polynomial with nonnegative coefficients that add up to 0 . The least nonnegative solution , say mu , of such equation systems is central to problems from various areas , like physics , biology , computational linguistics and probabilistic program verification . We give a simple and strongly polynomial algorithm to decide whether mu= ( 0 , . . . , 0 ) holds . Furthermore , we present an algorithm that computes reliable sequences of lower and upper bounds on mu , converging linearly to mu . Our algorithm has these features despite using inexact arithmetic for efficiency . We report on experiments that show the performance of our algorithms .
The experimental analysis of meta-heuristic algorithm performance is usually based on comparing average performance metric values over a set of algorithm instances . When algorithms getting tight in performance gains , the additional consideration of significance of a metric improvement comes into play . However , from this moment the comparison changes from an absolute to a relative mode . Here the implications of this paradigm shift are investigated . Significance relations are formally established . Based on this , a trade-off between increasing cycle-freeness of the relation and small maximum sets can be identified , allowing for the selection of a proper significance level and resulting ranking of a set of algorithms . The procedure is exemplified on the CEC ' 00 benchmark of real parameter single objective optimization problems . The significance relation here is based on awarding ranking points for relative performance gains , similar to the Borda count voting method or the Wilcoxon signed rank test . In the particular CEC ' 00 case , five ranks for algorithm performance can be clearly identified .
We obtain the following new coloring results : * A 0-colorable graph on $n$ vertices with maximum degree~$\Delta$ can be colored , in polynomial time , using $O ( ( \Delta \log\Delta ) ^{0/0} \cdot\log{n} ) $ colors . This slightly improves an $O ( ( \Delta^{{0}/{0}} \log^{0/0}\Delta ) \cdot\log{n} ) $ bound given by Karger , Motwani and Sudan . More generally , $k$-colorable graphs with maximum degree $\Delta$ can be colored , in polynomial time , using $O ( ( \Delta^{0-{0}/{k}}\log^{0/k}\Delta ) \cdot\log{n} ) $ colors . * A 0-colorable graph on $n$ vertices can be colored , in polynomial time , using $\Ot ( n^{0/00} ) $ colors . This improves an $\Ot ( n^{0/0} ) $ bound given again by Karger , Motwani and Sudan . More generally , $k$-colorable graphs on $n$ vertices can be colored , in polynomial time , using $\Ot ( n^{\alpha_k} ) $ colors , where $\alpha_0=00/000$ , $\alpha_0=00/00$ , $\alpha_0=0000/0000$ , $\alpha_0=000/000$ , . . . The first result is obtained by a slightly more refined probabilistic analysis of the semidefinite programming based coloring algorithm of Karger , Motwani and Sudan . The second result is obtained by combining the coloring algorithm of Karger , Motwani and Sudan , the combinatorial coloring algorithms of Blum and an extension of a technique of Alon and Kahale ( which is based on the Karger , Motwani and Sudan algorithm ) for finding relatively large independent sets in graphs that are guaranteed to have very large independent sets . The extension of the Alon and Kahale result may be of independent interest .
We propose to model the acoustic space of deep neural network ( DNN ) class-conditional posterior probabilities as a union of low-dimensional subspaces . To that end , the training posteriors are used for dictionary learning and sparse coding . Sparse representation of the test posteriors using this dictionary enables projection to the space of training data . Relying on the fact that the intrinsic dimensions of the posterior subspaces are indeed very small and the matrix of all posteriors belonging to a class has a very low rank , we demonstrate how low-dimensional structures enable further enhancement of the posteriors and rectify the spurious errors due to mismatch conditions . The enhanced acoustic modeling method leads to improvements in continuous speech recognition task using hybrid DNN-HMM ( hidden Markov model ) framework in both clean and noisy conditions , where upto 00 . 0% relative reduction in word error rate ( WER ) is achieved .
We consider the $n\times n$ game of Phutball . It is shown that , given an arbitrary position of stones on the board , it is a PSPACE-hard problem to determine whether the specified player can win the game , regardless of the opponent ' s choices made during the game .
Climate change has received an extensive attention from public opinion in the last couple of years , after being considered for decades as an exclusive scientific debate . Governments and world-wide organizations such as the United Nations are working more than ever on raising and maintaining public awareness toward this global issue . In the present study , we examine and analyze Climate Change conversations in Qatar ' s Twittersphere , and sense public awareness towards this global and shared problem in general , and its various related topics in particular . Such topics include but are not limited to politics , economy , disasters , energy and sandstorms . To address this concern , we collect and analyze a large dataset of 000 million tweets posted by 00K distinct users living in Qatar -- one of the largest emitters of CO0 worldwide . We use a taxonomy of climate change topics created as part of the United Nations Pulse project to capture the climate change discourse in more than 00K tweets . We also examine which topics people refer to when they discuss climate change , and perform different analysis to understand the temporal dynamics of public interest toward these topics .
A Behavior Tree ( BT ) is a way to structure the switching between different tasks in an autonomous agent , such as a robot or a virtual entity in a computer game . BTs are a very efficient way of creating complex systems that are both modular and reactive . These properties are crucial in many applications , which has led to the spread of BT from computer game programming to many branches of AI and Robotics . In this book , we will first give an introduction to BTs , then we describe how BTs relate to , and in many cases generalize , earlier switching structures . These ideas are then used as a foundation for a set of efficient and easy to use design principles . Properties such as safety , robustness , and efficiency are important for an autonomous system , and we describe a set of tools for formally analyzing these using a state space description of BTs . With the new analysis tools , we can formalize the descriptions of how BTs generalize earlier approaches . We also show the use of BTs in automated planning and machine learning . Finally , we describe an extended set of tools to capture the behavior of Stochastic BTs , where the outcomes of actions are described by probabilities . These tools enable the computation of both success probabilities and time to completion .
Preserving user privacy is paramount when it comes to publicly disclosed datasets that contain fine-grained data about large populations . The problem is especially critical in the case of mobile traffic datasets collected by cellular operators , as they feature elevate subscriber trajectory uniqueness and they are resistant to anonymization through spatiotemporal generalization . In this work , we investigate the $k$-anonymizability of trajectories in two large-scale mobile traffic datasets , by means of a novel dedicated measure . Our results are in agreement with those of previous analyses , however they also provide additional insights on the reasons behind the poor anonimizability of mobile traffic datasets . As such , our study is a step forward in the direction of a more robust dataset anonymization .
Linear mixed-effects models have increasingly replaced mixed-model analyses of variance for statistical inference in factorial psycholinguistic experiments . Although LMMs have many advantages over ANOVA , like ANOVAs , setting them up for data analysis also requires some care . One simple option , when numerically possible , is to fit the full variance-covariance structure of random effects ( the maximal model ; Barr et al . 0000 ) , presumably to keep Type I error down to the nominal alpha in the presence of random effects . Although it is true that fitting a model with only random intercepts may lead to higher Type I error , fitting a maximal model also has a cost : it can lead to a significant loss of power . We demonstrate this with simulations and suggest that for typical psychological and psycholinguistic data , higher power is achieved without inflating Type I error rate if a model selection criterion is used to select a random effect structure that is supported by the data .
Multi-hypothesis trackers ( MHT ' s ) , which are based on the measurement-to-track association ( MTA ) concept , have long been asserted to be " Bayes-optimal . " Recently , rather bolder claims have come to the fore : " The right model of the multitarget state is that used in the multi-hypothesis tracker ( MHT ) paradigm , not the RSF [random finite set] paradigm . " Or , the RFS approach is essentially a mathematically obfuscated reinvention of MHT . In this paper it is shown that : ( a ) although MTA ' s can be given a Bayesian formulation , this formulation is not fully consistent with Bayesian statistics ; ( b ) phenomenologically , an MTA is a heuristic extrapolation of an intuitive special case to general multitarget scenarios ; ( c ) MTA ' s are , therefore , not physically real entities and thus cannot ( as with MHT ' s ) be employed as state representations of a multitarget system ; ( d ) MHT ' s are , consequently , heuristic approximations of the actual Bayes-optimal approach , the multitarget Bayes filter ; ( d ) the theoretically correct measurement modeling approach is the RSF multitarget likelihood function L_Z ( X ) = f ( Z|X ) ; ( f ) although MTA ' s do occur in f ( Z|X ) , they are the consequence of a mere change of notation during the RFS derivation of f ( Z|X ) ; and ( g ) the generalized labeled multi-Bernoulli ( GLMB ) filter of Vo and Vo is currently the only provably Bayes-optimal and computationally tractable approach for true multitarget tracking using MTA ' s .
Feature learning forms the cornerstone for tackling challenging learning problems in domains such as speech , computer vision and natural language processing . In this paper , we consider a novel class of matrix and tensor-valued features , which can be pre-trained using unlabeled samples . We present efficient algorithms for extracting discriminative information , given these pre-trained features and labeled samples for any related task . Our class of features are based on higher-order score functions , which capture local variations in the probability density function of the input . We establish a theoretical framework to characterize the nature of discriminative information that can be extracted from score-function features , when used in conjunction with labeled samples . We employ efficient spectral decomposition algorithms ( on matrices and tensors ) for extracting discriminative components . The advantage of employing tensor-valued features is that we can extract richer discriminative information in the form of an overcomplete representations . Thus , we present a novel framework for employing generative models of the input for discriminative learning .
In this paper we address the problem of estimating the ratio $\frac{q}{p}$ where $p$ is a density function and $q$ is another density , or , more generally an arbitrary function . Knowing or approximating this ratio is needed in various problems of inference and integration , in particular , when one needs to average a function with respect to one probability distribution , given a sample from another . It is often referred as {\it importance sampling} in statistical inference and is also closely related to the problem of {\it covariate shift} in transfer learning as well as to various MCMC methods . It may also be useful for separating the underlying geometry of a space , say a manifold , from the density function defined on it . Our approach is based on reformulating the problem of estimating $\frac{q}{p}$ as an inverse problem in terms of an integral operator corresponding to a kernel , and thus reducing it to an integral equation , known as the Fredholm problem of the first kind . This formulation , combined with the techniques of regularization and kernel methods , leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically . The resulting family of algorithms ( FIRE , for Fredholm Inverse Regularized Estimator ) is flexible , simple and easy to implement . We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel in the case of densities defined on $\R^d$ , compact domains in $\R^d$ and smooth $d$-dimensional sub-manifolds of the Euclidean space . We also show experimental results including applications to classification and semi-supervised learning within the covariate shift framework and demonstrate some encouraging experimental comparisons . We also show how the parameters of our algorithms can be chosen in a completely unsupervised manner .
A commonly experienced problem with population based optimisation methods is the gradual decline in population diversity that tends to occur over time . This can slow a system ' s progress or even halt it completely if the population converges on a local optimum from which it cannot escape . In this paper we present the Fitness Uniform Deletion Scheme ( FUDS ) , a simple but somewhat unconventional approach to this problem . Under FUDS the deletion operation is modified to only delete those individuals which are " common " in the sense that there exist many other individuals of similar fitness in the population . This makes it impossible for the population to collapse to a collection of highly related individuals with similar fitness . Our experimental results on a range of optimisation problems confirm this , in particular for deceptive optimisation problems the performance is significantly more robust to variation in the selection intensity .
Among the manifold takes on world literature , it is our goal to contribute to the discussion from a digital point of view by analyzing the representation of world literature in Wikipedia with its millions of articles in hundreds of languages . As a preliminary , we introduce and compare three different approaches to identify writers on Wikipedia using data from DBpedia , a community project with the goal of extracting and providing structured information from Wikipedia . Equipped with our basic set of writers , we analyze how they are represented throughout the 00 biggest Wikipedia language versions . We combine intrinsic measures ( mostly examining the connectedness of articles ) with extrinsic ones ( analyzing how often articles are frequented by readers ) and develop methods to evaluate our results . The better part of our findings seems to convey a rather conservative , old-fashioned version of world literature , but a version derived from reproducible facts revealing an implicit literary canon based on the editing and reading behavior of millions of people . While still having to solve some known issues , the introduced methods will help us build an observatory of world literature to further investigate its representativeness and biases .
Early supervised machine learning algorithms have relied on reliable expert labels to build predictive models . However , the gates of data generation have recently been opened to a wider base of users who started participating increasingly with casual labeling , rating , annotating , etc . The increased online presence and participation of humans has led not only to a democratization of unchecked inputs to algorithms , but also to a wide democratization of the " consumption " of machine learning algorithms ' outputs by general users . Hence , these algorithms , many of which are becoming essential building blocks of recommender systems and other information filters , started interacting with users at unprecedented rates . The result is machine learning algorithms that consume more and more data that is unchecked , or at the very least , not fitting conventional assumptions made by various machine learning algorithms . These include biased samples , biased labels , diverging training and testing sets , and cyclical interaction between algorithms , humans , information consumed by humans , and data consumed by algorithms . Yet , the continuous interaction between humans and algorithms is rarely taken into account in machine learning algorithm design and analysis . In this paper , we present a preliminary theoretical model and analysis of the mutual interaction between humans and algorithms , based on an iterated learning framework that is inspired from the study of human language evolution . We also define the concepts of human and algorithm blind spots and outline machine learning approaches to mend iterated bias through two novel notions : antidotes and reactive learning .
This paper presents sampling-based speech parameter generation using moment-matching networks for Deep Neural Network ( DNN ) -based speech synthesis . Although people never produce exactly the same speech even if we try to express the same linguistic and para-linguistic information , typical statistical speech synthesis produces completely the same speech , i . e . , there is no inter-utterance variation in synthetic speech . To give synthetic speech natural inter-utterance variation , this paper builds DNN acoustic models that make it possible to randomly sample speech parameters . The DNNs are trained so that they make the moments of generated speech parameters close to those of natural speech parameters . Since the variation of speech parameters is compressed into a low-dimensional simple prior noise vector , our algorithm has lower computation cost than direct sampling of speech parameters . As the first step towards generating synthetic speech that has natural inter-utterance variation , this paper investigates whether or not the proposed sampling-based generation deteriorates synthetic speech quality . In evaluation , we compare speech quality of conventional maximum likelihood-based generation and proposed sampling-based generation . The result demonstrates the proposed generation causes no degradation in speech quality .
Modeled along the truncated approach in Panigrahi ( 0000 ) , selection-adjusted inference in a Bayesian regime is based on a selective posterior . Such a posterior is determined together by a generative model imposed on data and the selection event that enforces a truncation on the assumed law . The effective difference between the selective posterior and the usual Bayesian framework is reflected in the use of a truncated likelihood . The normalizer of the truncated law in the adjusted framework is the probability of the selection event ; this is typically intractable and it leads to the computational bottleneck in sampling from such a posterior . The current work lays out a primal-dual approach of solving an approximating optimization problem to provide valid post-selective Bayesian inference . The selection procedures are posed as data-queries that solve a randomized version of a convex learning program which have the advantage of preserving more left-over information for inference . We propose a randomization scheme under which the optimization has separable constraints that result in a partially separable objective in lower dimensions for many commonly used selective queries to approximate the otherwise intractable selective posterior . We show that the approximating optimization under a Gaussian randomization gives a valid exponential rate of decay for the selection probability on a large deviation scale . We offer a primal-dual method to solve the optimization problem leading to an approximate posterior ; this allows us to exploit the usual merits of a Bayesian machinery in both low and high dimensional regimes where the underlying signal is effectively sparse . We show that the adjusted estimates empirically demonstrate better frequentist properties in comparison to the unadjusted estimates based on the usual posterior , when applied to a wide range of constrained , convex data queries .
The term ``empirical predictor ' ' refers to a two-stage predictor of a linear combination of fixed and random effects . In the first stage , a predictor is obtained but it involves unknown parameters ; thus , in the second stage , the unknown parameters are replaced by their estimators . In this paper , we consider mean squared errors ( MSE ) of empirical predictors under a general setup , where ML or REML estimators are used for the second stage . We obtain second-order approximation to the MSE as well as an estimator of the MSE correct to the same order . The general results are applied to mixed linear models to obtain a second-order approximation to the MSE of the empirical best linear unbiased predictor ( EBLUP ) of a linear mixed effect and an estimator of the MSE of EBLUP whose bias is correct to second order . The general mixed linear model includes the mixed ANOVA model and the longitudinal model as special cases .
For many companies , competitiveness in e-commerce requires a successful presence on the web . Web sites are used to establish the company ' s image , to promote and sell goods and to provide customer support . The success of a web site affects and reflects directly the success of the company in the electronic market . In this study , we propose a methodology to improve the ``success ' ' of web sites , based on the exploitation of navigation pattern discovery . In particular , we present a theory , in which success is modelled on the basis of the navigation behaviour of the site ' s users . We then exploit WUM , a navigation pattern discovery miner , to study how the success of a site is reflected in the users ' behaviour . With WUM we measure the success of a site ' s components and obtain concrete indications of how the site should be improved . We report on our first experiments with an online catalog , the success of which we have studied . Our mining analysis has shown very promising results , on the basis of which the site is currently undergoing concrete improvements .
During the past decade , with the significant progress of computational power as well as ever-rising data availability , deep learning techniques became increasingly popular due to their excellent performance on computer vision problems . The size of the Protein Data Bank has increased more than 00 fold since 0000 , which enabled the expansion of models that aim at predicting enzymatic function via their amino acid composition . Amino acid sequence however is less conserved in nature than protein structure and therefore considered a less reliable predictor of protein function . This paper presents EnzyNet , a novel 0D-convolutional neural networks classifier that predicts the Enzyme Commission number of enzymes based only on their voxel-based spatial structure . The spatial distribution of biochemical properties was also examined as complementary information . The 0-layer architecture was investigated on a large dataset of 00 , 000 enzymes from the Protein Data Bank and achieved an accuracy of 00 . 0% by exploiting only the binary representation of the protein shape . Code and datasets are available at https : //github . com/shervinea/enzynet .
We consider the wavelet transform of a finite , rooted , node-ranked , $p$-way tree , focusing on the case of binary ( $p = 0$ ) trees . We study a Haar wavelet transform on this tree . Wavelet transforms allow for multiresolution analysis through translation and dilation of a wavelet function . We explore how this works in our tree context .
The key common bottleneck in most stencil codes is data movement , and prior research has shown that improving data locality through optimisations that schedule across loops do particularly well . However , in many large PDE applications it is not possible to apply such optimisations through compilers because there are many options , execution paths and data per grid point , many dependent on run-time parameters , and the code is distributed across different compilation units . In this paper , we adapt the data locality improving optimisation called iteration space slicing for use in large OPS applications both in shared-memory and distributed-memory systems , relying on run-time analysis and delayed execution . We evaluate our approach on a number of applications , observing speedups of 0$\times$ on the Cloverleaf 0D/0D proxy application , which contain 00/000 loops respectively , $0 . 0\times$ on the linear solver TeaLeaf , and $0 . 0\times$ on the compressible Navier-Stokes solver OpenSBLI . We demonstrate strong and weak scalability up to 0000 cores of CINECA ' s Marconi supercomputer . We also evaluate our algorithms on Intel ' s Knights Landing , demonstrating maintained throughput as the problem size grows beyond 00GB , and we do scaling studies up to 0000 cores . The approach is generally applicable to any stencil DSL that provides per loop data access information .
We explore the concept of a consistent exchangeable survival process - a joint distribution of survival times in which the risk set evolves as a continuous-time Markov process with homogeneous transition rates . We show a correspondence with the de Finetti approach of constructing an exchangeable survival process by generating iid survival times conditional on a completely independent hazard measure . We describe several specific processes , showing how the number of blocks of tied failure times grows asymptotically with the number of individuals in each case . In particular , we show that the set of Markov survival processes with weakly continuous predictive distributions can be characterized by a two-dimensional family called the harmonic process . We end by applying these methods to data , showing how they can be easily extended to handle censoring .
When I first encountered PAC-Bayesian concentration inequalities they seemed to me to be rather disconnected from good old-fashioned results like Hoeffding ' s and Bernstein ' s inequalities . But , at least for one flavour of the PAC-Bayesian bounds , there is actually a very close relation , and the main innovation is a continuous version of the union bound , along with some ingenious applications . Here ' s the gist of what ' s going on , presented from a machine learning perspective .
With the advances in robotic technology , research in human-robot collaboration ( HRC ) has gained in importance . For robots to interact with humans autonomously they need active decision making that takes human partners into account . However , state-of-the-art research in HRC does often assume a leader-follower division , in which one agent leads the interaction . We believe that this is caused by the lack of a reliable representation of the human and the environment to allow autonomous decision making . This problem can be overcome by an embodied approach to HRC which is inspired by psychological studies of human-human interaction ( HHI ) . In this survey , we review neuroscientific and psychological findings of the sensorimotor patterns that govern HHI and view them in a robotics context . Additionally , we study the advances made by the robotic community into the direction of embodied HRC . We focus on the mechanisms that are required for active , physical human-robot collaboration . Finally , we discuss the similarities and differences in the two fields of study which pinpoint directions of future research .
In this paper , a methodology is presented and employed for simulating the Internet of Things ( IoT ) . The requirement for scalability , due to the possibly huge amount of involved sensors and devices , and the heterogeneous scenarios that might occur , impose resorting to sophisticated modeling and simulation techniques . In particular , multi-level simulation is regarded as a main framework that allows simulating large-scale IoT environments while keeping high levels of detail , when it is needed . We consider a use case based on the deployment of smart services in decentralized territories . A two level simulator is employed , which is based on a coarse agent-based , adaptive parallel and distributed simulation approach to model the general life of simulated entities . However , when needed a finer grained simulator ( based on OMNeT++ ) is triggered on a restricted portion of the simulated area , which allows considering all issues concerned with wireless communications . Based on this use case , it is confirmed that the ad-hoc wireless networking technologies do represent a principle tool to deploy smart services over decentralized countrysides . Moreover , the performance evaluation confirms the viability of utilizing multi-level simulation for simulating large scale IoT environments .
Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 0000 domain . One of the important parameters in the Arcade Learning Environment ( ALE ) is the frame skip rate . It decides the granularity at which agents can control game play . A frame skip value of $k$ allows the agent to repeat a selected action $k$ number of times . The current state of the art architectures like Deep Q-Network ( DQN ) and Dueling Network Architectures ( DuDQN ) consist of a framework with a static frame skip rate , where the action output from the network is repeated for a fixed number of frames regardless of the current state . In this paper , we propose a new architecture , Dynamic Frame skip Deep Q-Network ( DFDQN ) which makes the frame skip rate a dynamic learnable parameter . This allows us to choose the number of times an action is to be repeated based on the current state . We show empirically that such a setting improves the performance on relatively harder games like Seaquest .
Community recovery is a central problem that arises in a wide variety of applications such as network clustering , motion segmentation , face clustering and protein complex detection . The objective of the problem is to cluster data points into distinct communities based on a set of measurements , each of which is associated with the values of a certain number of data points . While most of the prior works focus on a setting in which the number of data points involved in a measurement is two , this work explores a generalized setting in which the number can be more than two . Motivated by applications particularly in machine learning and channel coding , we consider two types of measurements : ( 0 ) homogeneity measurement which indicates whether or not the associated data points belong to the same community ; ( 0 ) parity measurement which denotes the modulo-0 sum of the values of the data points . Such measurements are possibly corrupted by Bernoulli noise . We characterize the fundamental limits on the number of measurements required to reconstruct the communities for the considered models .
Mathematical learning environments help students in mastering mathematical knowledge . Mature environments typically offer thousands of interactive exercises . Providing feedback to students solving interactive exercises requires domain reasoners for doing the exercise-specific calculations . Since a domain reasoner has to solve an exercise in the same way a student should solve it , the structure of domain reasoners should follow the layered structure of the mathematical domains . Furthermore , learners , teachers , and environment builders have different requirements for adapting domain reasoners , such as providing more details , disallowing or enforcing certain solutions , and combining multiple mathematical domains in a new domain . In previous work we have shown how domain reasoners for solving interactive exercises can be expressed in terms of rewrite strategies , rewrite rules , and views . This paper shows how users can adapt and configure such domain reasoners to their own needs . This is achieved by enabling users to explicitly communicate the components that are used for solving an exercise .
On the one side , network simulation frameworks are important tools for research and development activities to evaluate novel approaches in a time- and cost-efficient way . On the other side , Java as a highly platform-independent programming language is ideally suited for rapid prototyping in heterogeneous scenarios . Consequently , Java simulation frameworks could be used to firstly perform functional verification of new approaches ( and protocols ) in a simulation environment and afterwards , to evaluate these approaches in real testbeds using prototype Java implementations . Finally , the simulation models can be refined using real world measurement data . Unfortunately , there is to the best of our knowledge no satisfying Java framework for network simulation , as the OMNeT++ Java support ended with OMNeT++ version 0 . 0 . Hence , our contributions are as follows : we present Java extensions for OMNeT++ 0 . 0 that enable the execution of Java simulation models and give a detailed explanation of the working principles of the OMNeT++ Java extensions that are based on Java Native Interface . We conduct several case studies to evaluate the concept of Java extensions for OMNeT++ . Most importantly , we show that the combined use of Java simulation models and C++ models ( e . g . , from the INET framework ) is possible .
We consider the following question : How many edge-disjoint plane spanning trees are contained in a complete geometric graph $GK_n$ on any set $S$ of $n$ points in general position in the plane ? We show that this number is in $\Omega ( \sqrt{n} ) $ . Further , we consider variants of this problem by bounding the diameter and the degree of the trees ( in particular considering spanning paths ) .
We prove that the single-player game clobber is solvable in linear time when played on a line or on a cycle . For this purpose , we show that this game is equivalent to an optimization problem on a set of words defined by seven classes of forbidden patterns . We also prove that , playing on the cycle , it is always possible to remove at least 0n/0 pawns , and we give a conformation for which it is not possible to do better , answering questions recently asked by Faria et al .
We apply information-based complexity analysis to support vector machine ( SVM ) algorithms , with the goal of a comprehensive continuous algorithmic analysis of such algorithms . This involves complexity measures in which some higher order operations ( e . g . , certain optimizations ) are considered primitive for the purposes of measuring complexity . We consider classes of information operators and algorithms made up of scaled families , and investigate the utility of scaling the complexities to minimize error . We look at the division of statistical learning into information and algorithmic components , at the complexities of each , and at applications to support vector machine ( SVM ) and more general machine learning algorithms . We give applications to SVM algorithms graded into linear and higher order components , and give an example in biomedical informatics .
We present an infinite series of $n$-state Eulerian automata whose reset words have length at least $ ( n^0-0 ) /0$ . This improves the current lower bound on the length of shortest reset words in Eulerian automata . We conjecture that $ ( n^0-0 ) /0$ also forms an upper bound for this class and we experimentally verify it for small automata by an exhaustive computation .
An improved image mining technique for brain tumor classification using pruned association rule with MARI algorithm is presented in this paper . The method proposed makes use of association rule mining technique to classify the CT scan brain images into three categories namely normal , benign and malign . It combines the low level features extracted from images and high level knowledge from specialists . The developed algorithm can assist the physicians for efficient classification with multiple keywords per image to improve the accuracy . The experimental result on prediagnosed database of brain images showed 00 percent and 00 percent sensitivity and accuracy respectively .
We consider the problem of fitting the parameters of a high-dimensional linear regression model . In the regime where the number of parameters $p$ is comparable to or exceeds the sample size $n$ , a successful approach uses an $\ell_0$-penalized least squares estimator , known as Lasso . Unfortunately , unlike for linear estimators ( e . g . , ordinary least squares ) , no well-established method exists to compute confidence intervals or p-values on the basis of the Lasso estimator . Very recently , a line of work \cite{javanmard0000hypothesis , confidenceJM , GBR-hypothesis} has addressed this problem by constructing a debiased version of the Lasso estimator . In this paper , we study this approach for random design model , under the assumption that a good estimator exists for the precision matrix of the design . Our analysis improves over the state of the art in that it establishes nearly optimal \emph{average} testing power if the sample size $n$ asymptotically dominates $s_0 ( \log p ) ^0$ , with $s_0$ being the sparsity level ( number of non-zero coefficients ) . Earlier work obtains provable guarantees only for much larger sample size , namely it requires $n$ to asymptotically dominate $ ( s_0 \log p ) ^0$ . In particular , for random designs with a sparse precision matrix we show that an estimator thereof having the required properties can be computed efficiently . Finally , we evaluate this approach on synthetic data and compare it with earlier proposals .
System and synthetic biology are rapidly evolving systems , but both lack tools such as those used in engineering environments to shift the their focus from the design of parts ( details ) to the design of systems ( behaviors ) ; to aggravate , there are insufficient theoretical justifications on the computational limits of biological systems . To diminish these deficiencies , we present theoretical results over the Turing-equivalence of metabolic systems , defines rules for translations of algorithms into metabolic P systems and presents a software tool to assist the task in an automatic way .
In the complex manufacturing sector a considerable amount of resources are focused on developing new skills and training workers . In that context , increasing the effectiveness of those processes and reducing the investment required is an outstanding issue . In this paper we present an experiment that shows how modern Human Computer Interaction ( HCI ) metaphors such as collaborative mixed-reality can be used to transmit procedural knowledge and could eventually replace other forms of face-to-face training . We implement a real-time Immersive Augmented Reality ( IAR ) setup with see-through cameras that allows for collaborative interactions that can simulate conventional forms of training . The obtained results indicate that people who took the IAR training achieved the same performance than people in the conventional face-to-face training condition . These results , their implications for future training and the use of HCI paradigms in this context are discussed in this paper .
Rare properties remain a challenge for statistical model checking ( SMC ) due to the quadratic scaling of variance with rarity . We address this with a variance reduction framework based on lightweight importance splitting observers . These expose the model-property automaton to allow the construction of score functions for high performance algorithms . The confidence intervals defined for importance splitting make it appealing for SMC , but optimising its performance in the standard way makes distribution inefficient . We show how it is possible to achieve equivalently good results in less time by distributing simpler algorithms . We first explore the challenges posed by importance splitting and present an algorithm optimised for distribution . We then define a specific bounded time logic that is compiled into memory-efficient observers to monitor executions . Finally , we demonstrate our framework on a number of challenging case studies .
In this paper , a novel technique for tight outer-approximation of the intersection region of a finite number of ellipses in 0-dimensional ( 0D ) space is proposed . First , the vertices of a tight polygon that contains the convex intersection of the ellipses are found in an efficient manner . To do so , the intersection points of the ellipses that fall on the boundary of the intersection region are determined , and a set of points is generated on the elliptic arcs connecting every two neighbouring intersection points . By finding the tangent lines to the ellipses at the extended set of points , a set of half-planes is obtained , whose intersection forms a polygon . To find the polygon more efficiently , the points are given an order and the intersection of the half-planes corresponding to every two neighbouring points is calculated . If the polygon is convex and bounded , these calculated points together with the initially obtained intersection points will form its vertices . If the polygon is non-convex or unbounded , we can detect this situation and then generate additional discrete points only on the elliptical arc segment causing the issue , and restart the algorithm to obtain a bounded and convex polygon . Finally , the smallest area ellipse that contains the vertices of the polygon is obtained by solving a convex optimization problem . Through numerical experiments , it is illustrated that the proposed technique returns a tighter outer-approximation of the intersection of multiple ellipses , compared to conventional techniques , with only slightly higher computational cost .
Uncertainty quantification ( UQ ) techniques are frequently used to ascertain output variability in systems with parametric uncertainty . Traditional algorithms for UQ are either system-agnostic and slow ( such as Monte Carlo ) or fast with stringent assumptions on smoothness ( such as polynomial chaos and Quasi-Monte Carlo ) . In this work , we develop a fast UQ approach for hybrid dynamical systems by extending the polynomial chaos methodology to these systems . To capture discontinuities , we use a wavelet-based Wiener-Haar expansion . We develop a boundary layer approach to propagate uncertainty through separable reset conditions . We also introduce a transport theory based approach for propagating uncertainty through hybrid dynamical systems . Here the expansion yields a set of hyperbolic equations that are solved by integrating along characteristics . The solution of the partial differential equation along the characteristics allows one to quantify uncertainty in hybrid or switching dynamical systems . The above methods are demonstrated on example problems .
This paper concerns the problem of recovering an unknown but structured signal $x \in R^n$ from $m$ quadratic measurements of the form $y_r=|<a_r , x>|^0$ for $r=0 , 0 , . . . , m$ . We focus on the under-determined setting where the number of measurements is significantly smaller than the dimension of the signal ( $m<<n$ ) . We formulate the recovery problem as a nonconvex optimization problem where prior structural information about the signal is enforced through constrains on the optimization variables . We prove that projected gradient descent , when initialized in a neighborhood of the desired signal , converges to the unknown signal at a linear rate . These results hold for any constraint set ( convex or nonconvex ) providing convergence guarantees to the global optimum even when the objective function and constraint set is nonconvex . Furthermore , these results hold with a number of measurements that is only a constant factor away from the minimal number of measurements required to uniquely identify the unknown signal . Our results provide the first provably tractable algorithm for this data-poor regime , breaking local sample complexity barriers that have emerged in recent literature . In a companion paper we demonstrate favorable properties for the optimization problem that may enable similar results to continue to hold more globally ( over the entire ambient space ) . Collectively these two papers utilize and develop powerful tools for uniform convergence of empirical processes that may have broader implications for rigorous understanding of constrained nonconvex optimization heuristics . The mathematical results in this paper also pave the way for a new generation of data-driven phase-less imaging systems that can utilize prior information to significantly reduce acquisition time and enhance image reconstruction , enabling nano-scale imaging at unprecedented speeds and resolutions .
This paper studies identifiability and convergence behaviors for parameters of multiple types in finite mixtures , and the effects of model fitting with extra mixing components . First , we present a general theory for strong identifiability , which extends from the previous work of Nguyen [0000] and Chen [0000] to address a broad range of mixture models and to handle matrix-variate parameters . These models are shown to share the same Wasserstein distance based optimal rates of convergence for the space of mixing distributions --- $n^{-0/0}$ under $W_0$ for the exact-fitted and $n^{-0/0}$ under $W_0$ for the over-fitted setting , where $n$ is the sample size . This theory , however , is not applicable to several important model classes , including location-scale multivariate Gaussian mixtures , shape-scale Gamma mixtures and location-scale-shape skew-normal mixtures . The second part of this work is devoted to demonstrating that for these " weakly identifiable " classes , algebraic structures of the density family play a fundamental role in determining convergence rates of the model parameters , which display a very rich spectrum of behaviors . For instance , the optimal rate of parameter estimation in an over-fitted location-covariance Gaussian mixture is precisely determined by the order of a solvable system of polynomial equations --- these rates deteriorate rapidly as more extra components are added to the model . The established rates for a variety of settings are illustrated by a simulation study .
Discovery of an accurate causal Bayesian network structure from observational data can be useful in many areas of science . Often the discoveries are made under uncertainty , which can be expressed as probabilities . To guide the use of such discoveries , including directing further investigation , it is important that those probabilities be well-calibrated . In this paper , we introduce a novel framework to derive calibrated probabilities of causal relationships from observational data . The framework consists of three components : ( 0 ) an approximate method for generating initial probability estimates of the edge types for each pair of variables , ( 0 ) the availability of a relatively small number of the causal relationships in the network for which the truth status is known , which we call a calibration training set , and ( 0 ) a calibration method for using the approximate probability estimates and the calibration training set to generate calibrated probabilities for the many remaining pairs of variables . We also introduce a new calibration method based on a shallow neural network . Our experiments on simulated data support that the proposed approach improves the calibration of causal edge predictions . The results also support that the approach often improves the precision and recall of predictions .
The paper is dealing with semi-classical asymptotics of a characteristic function for a stochastic process . The main technical tool is provided by the stationary phase method . The extremal range for a stochastic process is defined by limit values of the complex logarithm of the characteristic function . The paper also outlines a numerical method for calculating stochastic extrema .
Deep learning-based robotic grasping has made significant progress the past several years thanks to algorithmic improvements and increased data availability . However , state-of-the-art models are often trained on as few as hundreds or thousands of unique object instances , and as a result generalization can be a challenge . In this work , we explore a novel data generation pipeline for training a deep neural network to perform grasp planning that applies the idea of \emph{domain randomization} to object synthesis . We generate millions of unique , unrealistic procedurally generated objects , and train a deep neural network to perform grasp planning on these objects . Since the distribution of successful grasps for a given object can be highly multimodal , we propose an autoregressive grasp planning model that maps sensor inputs of a scene to a probability distribution $p_{\theta}$ , where $p_{\theta} ( g \mid I ) $ corresponds to the model ' s estimate of the normalized probability of success of the grasp $g$ conditioned on the observations $I$ . Our model allows us to sample grasps efficiently at test time ( or avoid sampling entirely ) . We evaluate our model architecture and data generation pipeline in simulation and find we can achieve $>$00\% success rate on previously unseen realistic objects at test time despite having only been trained on random objects .
The selection of an appropriate competition format is critical for both the success and credibility of any competition , both real and simulated . In this paper , the automated parallelism offered by the RoboCupSoccer 0D simulation league is leveraged to conduct a 00 , 000 game round-robin between the top 0 teams from RoboCup 0000 and 0000 . A proposed new competition format is found to reduce variation from the resultant statistically significant team performance rankings by 00% and 00% , when compared to the actual competition results from RoboCup 0000 and 0000 respectively . These results are statistically validated by generating 00 , 000 random tournaments for each of the three considered formats and comparing the respective distributions of ranking discrepancy .
We develop new methods based on graph motifs for graph clustering , allowing more efficient detection of communities within networks . We focus on triangles within graphs , but our techniques extend to other clique motifs as well . Our intuition , which has been suggested but not formalized similarly in previous works , is that triangles are a better signature of community than edges . We therefore generalize the notion of conductance for a graph to {\em triangle conductance} , where the edges are weighted according to the number of triangles containing the edge . This methodology allows us to develop variations of several existing clustering techniques , including spectral clustering , that minimize triangles split by the cluster instead of edges cut by the cluster . We provide theoretical results in a planted partition model to demonstrate the potential for triangle conductance in clustering problems . We then show experimentally the effectiveness of our methods to multiple applications in machine learning and graph mining .
The sparsity of the isotope Helium-0 , ongoing since 0000 , has initiated a new generation of neutron detectors . One particularly promising development line for detectors is the multilayer gaseous detector . In this paper , a stochastic process approach is used to determine the neutron ' s energy from the additional data afforded by the multilayer nature of these novel detectors . The data from a multi-layer detector consists of counts of the number of absorbed neutrons along the sequence of the detector ' s layers , in which the neutron absorption probability is unknown . We study the Maximum Likelihood estimator for the intensity and absorption probability , show its consistency and asymptotic normality , as the experiment time ( or the number of incoming neutrons ) goes to infinity . We combine these results with known results on the relation between the absorption probability and the wavelength to derive an estimator of the wavelength and to show consistency and asymptotic normality for the estimator .
N . L . Johnson and S . Kotz introduced in 0000 an interesting family of symmetric distributions which is based on randomly weighted average from uniform random samples . The only example that could be addressed to their work is the so-called " uniformly randomly modified tin " distribution from which two random samples have been computed . In this paper , we generalize a subfamily of their symmetric distributions and identify a concrete instance of this generalized subfamily . That instance turns out to belong to the family of Johnson and Kotz , which had not seemingly received proper attention in the literature .
Recent years have witnessed the surge of asynchronous parallel ( async-parallel ) iterative algorithms due to problems involving very large-scale data and a large number of decision variables . Because of asynchrony , the iterates are computed with outdated information , and the age of the outdated information , which we call delay , is the number of times it has been updated since its creation . Almost all recent works prove convergence under the assumption of a finite maximum delay and set their stepsize parameters accordingly . However , the maximum delay is practically unknown . This paper presents convergence analysis of an async-parallel method from a probabilistic viewpoint , and it allows for large unbounded delays . An explicit formula of stepsize that guarantees convergence is given depending on delays ' statistics . With $p+0$ identical processors , we empirically measured that delays closely follow the Poisson distribution with parameter $p$ , matching our theoretical model , and thus the stepsize can be set accordingly . Simulations on both convex and nonconvex optimization problems demonstrate the validness of our analysis and also show that the existing maximum-delay induced stepsize is too conservative , often slowing down the convergence of the algorithm .
Tree ensembles , such as random forests and boosted trees , are renowned for their high prediction performance . However , their interpretability is critically limited due to the enormous complexity . In this study , we present a method to make a complex tree ensemble interpretable by simplifying the model . Specifically , we formalize the simplification of tree ensembles as a model selection problem . Given a complex tree ensemble , we aim at obtaining the simplest representation that is essentially equivalent to the original one . To this end , we derive a Bayesian model selection algorithm that optimizes the simplified model while maintaining the prediction performance . Our numerical experiments on several datasets showed that complicated tree ensembles were reasonably approximated as interpretable .
This research paper designates the importance and usage of the case study approach effectively to educating and training software designers and software engineers both in academic and industry . Subsequently an account of the use of case studies based on software engineering in the education of professionals , there is a conversation of issues in training software designers and how a case teaching method can be used to state these issues . The paper describes a software project titled Online Tower Plotting System ( OTPS ) to develop a complete and comprehensive case study , along with supporting educational material . The case study is aimed to demonstrate a variety of software areas , modules and courses : from bachelor through masters , doctorates and even for ongoing professional development .
We give a short proof of the systolic inequality for the n-dimensional torus . The proof uses minimal hypersurfaces . It is based on the Schoen-Yau proof that an n-dimensional torus admits no metric of positive scalar curvature .
Generative Adversarial Networks ( GANs ) represent a promising class of generative networks that combine neural networks with game theory . From generating realistic images and videos to assisting musical creation , GANs are transforming many fields of arts and sciences . However , their application to healthcare has not been fully realized , more specifically in generating electronic health records ( EHR ) data . In this paper , we propose a framework for exploring the value of GANs in the context of continuous laboratory time series data . We devise an unsupervised evaluation method that measures the predictive power of synthetic laboratory test time series . Further , we show that when it comes to predicting the impact of drug exposure on laboratory test data , incorporating representation learning of the training cohorts prior to training GAN models is beneficial .
Machine Learning ( ML ) algorithms , like Convolutional Neural Networks ( CNN ) , Support Vector Machines ( SVM ) , etc . have become widespread and can achieve high statistical performance . However their accuracy decreases significantly in energy-constrained mobile and embedded systems space , where all computations need to be completed under a tight energy budget . In this work , we present a field of groves ( FoG ) implementation of random forests ( RF ) that achieves an accuracy comparable to CNNs and SVMs under tight energy budgets . Evaluation of the FoG shows that at comparable accuracy it consumes ~0 . 00x , ~00x , ~0 . 0x , and ~00 . 0x lower energy per classification compared to conventional RF , SVM_RBF , MLP , and CNN , respectively . FoG is ~0 . 0x less energy efficient than SVM_LR , but achieves 00% higher accuracy on average across all considered datasets .
The traditional approach to fault tolerant computing involves replicating computation units and applying a majority vote operation on individual result bits . This approach , however , has several limitations ; the most severe is the resource requirement . This paper presents a new method for fault tolerant computing where for a given error rate , the hamming distance between correct inputs and faulty inputs as well as the hamming distance between a correct result and a faulty result is preserved throughout processing thereby enabling correction of up to transient faults per computation cycle . The new method is compared and contrasted with current protection methods and its cost / performance is analyzed .
In join the shortest queue networks , incoming jobs are assigned to the shortest queue from among a randomly chosen subset of $D$ queues , in a system of $N$ queues ; after completion of service at its queue , a job leaves the network . We also assume that jobs arrive into the system according to a rate-$\alpha N$ Poisson process , $\alpha<0$ , with rate-0 service at each queue . When the service at queues is exponentially distributed , it was shown in Vvedenskaya et al . [Probl . Inf . Transm . 00 ( 0000 ) 00-00] that the tail of the equilibrium queue size decays doubly exponentially in the limit as $N\rightarrow\infty$ . This is a substantial improvement over the case D=0 , where the queue size decays exponentially . The reasoning in [Probl . Inf . Transm . 00 ( 0000 ) 00-00] does not easily generalize to jobs with nonexponential service time distributions . A modularized program for treating general service time distributions was introduced in Bramson et al . [In Proc . ACM SIGMETRICS ( 0000 ) 000-000] . The program relies on an ansatz that asserts , in equilibrium , any fixed number of queues become independent of one another as $N\rightarrow\infty$ . This ansatz was demonstrated in several settings in Bramson et al . [Queueing Syst . 00 ( 0000 ) 000-000] , including for networks where the service discipline is FIFO and the service time distribution has a decreasing hazard rate . In this article , we investigate the limiting behavior , as $N\rightarrow \infty$ , of the equilibrium at a queue when the service discipline is FIFO and the service time distribution has a power law with a given exponent $-\beta$ , for $\beta>0$ . We show under the above ansatz that , as $N\rightarrow\infty$ , the tail of the equilibrium queue size exhibits a wide range of behavior depending on the relationship between $\beta$ and $D$ . In particular , if $\beta>D/ ( D-0 ) $ , the tail is doubly exponential and , if $\beta<D/ ( D-0 ) $ , the tail has a power law . When $\beta=D/ ( D-0 ) $ , the tail is exponentially distributed .
At the heart of the Bitcoin is a blockchain protocol , a protocol for achieving consensus on a public ledger that records bitcoin transactions . To the extent that a blockchain protocol is used for applications such as contract signing and making certain transactions ( such as house sales ) public , we need to understand what guarantees the protocol gives us in terms of agents ' knowledge . Here , we provide a complete characterization of agent ' s knowledge when running a blockchain protocol using a variant of common knowledge that takes into account the fact that agents can enter and leave the system , it is not known which agents are in fact following the protocol ( some agents may want to deviate if they can gain by doing so ) , and the fact that the guarantees provided by blockchain protocols are probabilistic . We then consider some scenarios involving contracts and show that this level of knowledge suffices for some scenarios , but not others .
Given a planar map of $n$ segments in which we wish to efficiently locate points , we present the first randomized incremental construction of the well-known trapezoidal-map search-structure that only requires expected $O ( n \log n ) $ preprocessing time while deterministically guaranteeing worst-case linear storage space and worst-case logarithmic query time . This settles a long standing open problem ; the best previously known construction time of such a structure , which is based on a directed acyclic graph , so-called the history DAG , and with the above worst-case space and query-time guarantees , was expected $O ( n \log^0 n ) $ . The result is based on a deeper understanding of the structure of the history DAG , its depth in relation to the length of its longest search path , as well as its correspondence to the trapezoidal search tree . Our results immediately extend to planar maps induced by finite collections of pairwise interior disjoint well-behaved curves .
Recent advances in biosensors technology and mobile electroencephalographic ( EEG ) interfaces have opened new application fields for cognitive monitoring . A computable biomarker for the assessment of spontaneous aesthetic brain responses during music listening is introduced here . It derives from well-established measures of cross-frequency coupling ( CFC ) and quantifies the music-induced alterations in the dynamic relationships between brain rhythms . During a stage of exploratory analysis , and using the signals from a suitably designed experiment , we established the biomarker , which acts on brain activations recorded over the left prefrontal cortex and focuses on the functional coupling between high-beta and low-gamma oscillations . Based on data from an additional experimental paradigm , we validated the introduced biomarker and showed its relevance for expressing the subjective aesthetic appreciation of a piece of music . Our approach resulted in an affordable tool that can promote human-machine interaction and , by serving as a personalized music annotation strategy , can be potentially integrated into modern flexible music recommendation systems . Keywords : Cross-frequency coupling ; Human-computer interaction ; Brain-computer interface
In this paper , the third in a series illustrating the power of generalized linear models ( GLMs ) for the astronomical community , we elucidate the potential of the class of GLMs which handles count data . The size of a galaxy ' s globular cluster population $N_{\rm GC}$ is a prolonged puzzle in the astronomical literature . It falls in the category of count data analysis , yet it is usually modelled as if it were a continuous response variable . We have developed a Bayesian negative binomial regression model to study the connection between $N_{\rm GC}$ and the following galaxy properties : central black hole mass , dynamical bulge mass , bulge velocity dispersion , and absolute visual magnitude . The methodology introduced herein naturally accounts for heteroscedasticity , intrinsic scatter , errors in measurements in both axes ( either discrete or continuous ) , and allows modelling the population of globular clusters on their natural scale as a non-negative integer variable . Prediction intervals of 00% around the trend for expected $N_{\rm GC}$comfortably envelope the data , notably including the Milky Way , which has hitherto been considered a problematic outlier . Finally , we demonstrate how random intercept models can incorporate information of each particular galaxy morphological type . Bayesian variable selection methodology allows for automatically identifying galaxy types with different productions of GCs , suggesting that on average S0 galaxies have a GC population 00% smaller than other types with similar brightness .
This paper introduces the first deep neural network-based estimation metric for the jigsaw puzzle problem . Given two puzzle piece edges , the neural network predicts whether or not they should be adjacent in the correct assembly of the puzzle , using nothing but the pixels of each piece . The proposed metric exhibits an extremely high precision even though no manual feature extraction is performed . When incorporated into an existing puzzle solver , the solution ' s accuracy increases significantly , achieving thereby a new state-of-the-art standard .
Event-B has been actively used within the EU Deploy project to model dependable systems from various application domains . As a result , we have created a number of formal approaches to explicitly reason about dependability in the refinement process . In this paper we overview the work on formal engineering of dependable systems carried out in the Deploy project . We outline our approaches to integrating safety analysis into the development process , modelling fault tolerant systems and probabilistic dependability evaluation . We discuss achievements and challenges in development of dependable systems within the Event-B framework .
Methods for the automatic composition of services into executable workflows need detailed knowledge about the application domain , in particular about the available services and their behavior in terms of input/output data descriptions . In this paper we discuss how the EMBRACE data and methods ontology ( EDAM ) can be used as background knowledge for the composition of bioinformatics workflows . We show by means of a small example domain that the EDAM knowledge facilitates finding possible workflows , but that additional knowledge is required to guide the search towards actually adequate solutions . We illustrate how the ability to flexibly formulate domain-specific and problem-specific constraints supports the work ow development process .
The present paper introduces the open-source Java Event Tracer ( JETracer ) framework for real-time tracing of GUI events within applications based on the AWT , Swing or SWT graphical toolkits . Our framework provides a common event model for supported toolkits , the possibility of receiving GUI events in real-time , good performance in the case of complex target applications and the possibility of deployment over a network . The present paper provides the rationale for JETracer , presents related research and details its technical implementation . An empirical evaluation where JETracer is used to trace GUI events within five popular , open-source applications is also presented .
This document contains improved and updated proofs of convergence for the sampling method presented in our paper " Free-configuration Biased Sampling for Motion Planning " .
We show a principled way of deriving online learning algorithms from a minimax analysis . Various upper bounds on the minimax value , previously thought to be non-constructive , are shown to yield algorithms . This allows us to seamlessly recover known methods and to derive new ones . Our framework also captures such " unorthodox " methods as Follow the Perturbed Leader and the R^0 forecaster . We emphasize that understanding the inherent complexity of the learning problem leads to the development of algorithms . We define local sequential Rademacher complexities and associated algorithms that allow us to obtain faster rates in online learning , similarly to statistical learning theory . Based on these localized complexities we build a general adaptive method that can take advantage of the suboptimality of the observed sequence . We present a number of new algorithms , including a family of randomized methods that use the idea of a " random playout " . Several new versions of the Follow-the-Perturbed-Leader algorithms are presented , as well as methods based on the Littlestone ' s dimension , efficient methods for matrix completion with trace norm , and algorithms for the problems of transductive learning and prediction with static experts .
Given a multiset $X=\{x_0 , . . . , x_n\}$ of real numbers , the {\it floating-point set summation} problem asks for $S_n=x_0+ . . . +x_n$ . Let $E^*_n$ denote the minimum worst-case error over all possible orderings of evaluating $S_n$ . We prove that if $X$ has both positive and negative numbers , it is NP-hard to compute $S_n$ with the worst-case error equal to $E^*_n$ . We then give the first known polynomial-time approximation algorithm that has a provably small error for arbitrary $X$ . Our algorithm incurs a worst-case error at most $0 ( \mix ) E^*_n$ . \footnote{All logarithms $\log$ in this paper are base 0 . } After $X$ is sorted , it runs in O ( n ) time . For the case where $X$ is either all positive or all negative , we give another approximation algorithm with a worst-case error at most $\lceil\log\log n\rceil E^*_n$ . Even for unsorted $X$ , this algorithm runs in O ( n ) time . Previously , the best linear-time approximation algorithm had a worst-case error at most $\lceil\log n\rceil E^*_n$ , while $E^*_n$ was known to be attainable in $O ( n \log n ) $ time using Huffman coding .
Let $\cF$ be a set of $M$ classification procedures with values in $[-0 , 0]$ . Given a loss function , we want to construct a procedure which mimics at the best possible rate the best procedure in $\cF$ . This fastest rate is called optimal rate of aggregation . Considering a continuous scale of loss functions with various types of convexity , we prove that optimal rates of aggregation can be either $ ( ( \log M ) /n ) ^{0/0}$ or $ ( \log M ) /n$ . We prove that , if all the $M$ classifiers are binary , the ( penalized ) Empirical Risk Minimization procedures are suboptimal ( even under the margin/low noise condition ) when the loss function is somewhat more than convex , whereas , in that case , aggregation procedures with exponential weights achieve the optimal rate of aggregation .
A procedure for generating non-differentiable , continuously differentiable , and twice continuously differentiable classes of test functions for multiextremal multidimensional box-constrained global optimization and a corresponding package of C subroutines are presented . Each test class consists of 000 functions . Test functions are generated by defining a convex quadratic function systematically distorted by polynomials in order to introduce local minima . To determine a class , the user defines the following parameters : ( i ) problem dimension , ( ii ) number of local minima , ( iii ) value of the global minimum , ( iv ) radius of the attraction region of the global minimizer , ( v ) distance from the global minimizer to the vertex of the quadratic function . Then , all other necessary parameters are generated randomly for all 000 functions of the class . Full information about each test function including locations and values of all local minima is supplied to the user . Partial derivatives are also generated where possible .
Classifying videos according to content semantics is an important problem with a wide range of applications . In this paper , we propose a hybrid deep learning framework for video classification , which is able to model static spatial information , short-term motion , as well as long-term temporal clues in the videos . Specifically , the spatial and the short-term motion features are extracted separately by two Convolutional Neural Networks ( CNN ) . These two types of CNN-based features are then combined in a regularized feature fusion network for classification , which is able to learn and utilize feature relationships for improved performance . In addition , Long Short Term Memory ( LSTM ) networks are applied on top of the two features to further model longer-term temporal clues . The main contribution of this work is the hybrid learning framework that can model several important aspects of the video data . We also show that ( 0 ) combining the spatial and the short-term motion features in the regularized fusion network is better than direct classification and fusion using the CNN with a softmax layer , and ( 0 ) the sequence-based LSTM is highly complementary to the traditional classification strategy without considering the temporal frame orders . Extensive experiments are conducted on two popular and challenging benchmarks , the UCF-000 Human Actions and the Columbia Consumer Videos ( CCV ) . On both benchmarks , our framework achieves to-date the best reported performance : $00 . 0\%$ on the UCF-000 and $00 . 0\%$ on the CCV .
Choice decisions made by users of online applications can suffer from biases due to the users ' level of engagement . For instance , low engagement users may make random choices with no concern for the quality of items offered . This biased choice data can corrupt estimates of user preferences for items . However , one can correct for these biases if additional behavioral data is utilized . To do this we construct a new choice engagement time model which captures the impact of user engagement on choice decisions and response times associated with these choice decisions . Response times are the behavioral data we choose because they are easily measured by online applications and reveal information about user engagement . To test our model we conduct online polls with subject populations that have different levels of engagement and measure their choice decisions and response times . We have two main empirical findings . First , choice decisions and response times are correlated , with strong preferences having faster response times than weak preferences . Second , low user engagement is manifested through more random choice data and faster response times . Both of these phenomena are captured by our choice engagement time model and we find that this model fits the data better than traditional choice models . Our work has direct implications for online applications . It lets these applications remove the bias of low engagement users when estimating preferences for items . It also allows for the segmentation of users according to their level of engagement , which can be useful for targeted advertising or marketing campaigns .
A Maple package for computing Groebner bases of linear difference ideals is described . The underlying algorithm is based on Janet and Janet-like monomial divisions associated with finite difference operators . The package can be used , for example , for automatic generation of difference schemes for linear partial differential equations and for reduction of multiloop Feynman integrals . These two possible applications are illustrated by simple examples of the Laplace equation and a one-loop scalar integral of propagator type
We place ourselves in the setting of high-dimensional statistical inference where the number of variables $p$ in a dataset of interest is of the same order of magnitude as the number of observations $n$ . We consider the spectrum of certain kernel random matrices , in particular $n\times n$ matrices whose $ ( i , j ) $th entry is $f ( X_i ' X_j/p ) $ or $f ( \Vert X_i-X_j\Vert^0/p ) $ where $p$ is the dimension of the data , and $X_i$ are independent data vectors . Here $f$ is assumed to be a locally smooth function . The study is motivated by questions arising in statistics and computer science where these matrices are used to perform , among other things , nonlinear versions of principal component analysis . Surprisingly , we show that in high-dimensions , and for the models we analyze , the problem becomes essentially linear--which is at odds with heuristics sometimes used to justify the usage of these methods . The analysis also highlights certain peculiarities of models widely studied in random matrix theory and raises some questions about their relevance as tools to model high-dimensional data encountered in practice .
Multiple-choice exams are frequently used as an efficient and objective method to assess learning but they are more vulnerable to answer-copying than tests based on open questions . Several statistical tests ( known as indices in the literature ) have been proposed to detect cheating ; however , to the best of our knowledge they all lack mathematical support that guarantees optimality in any sense . We partially fill this void by deriving the uniform most powerful ( UMP ) under the assumption that the response distribution is known . In practice , however , we must estimate a behavioral model that yields a response distribution for each question . We calculate the empirical type-I and type-II error rates for several indices that assume different behavioral models using simulations based on real data from twelve nationwide multiple-choice exams taken by 0th and 0th graders in Colombia . We find that the index with the highest power among those studied , subject to the restriction of preserving the type-I error , is one based on the work of Wollack ( 0000 ) and Linden and Sotaridona ( 0000 ) and is superior to the indices studied and developed by Wesolowsky ( 0000 ) and Frary , Tideman , and Watts ( 0000 ) . We compare the results of applying this index to all 00 exams and find that examination rooms with stricter proctoring have a lower level of copying . Finally , a Bonferroni correction to control for the false positive rate is proposed to detect massive cheating .
Integrating existing heterogeneous data models for buildings , neighbourhoods and periphery devices into a common data model that can be used by all participants , such as users , services or sensors is a cumbersome task . Usually new extended standards emerge or ontologies are used to define mappings between concrete data models . Within the COOPERaTE project a neighbourhood information model ( NIM ) has been developed to address interoperability and allow for various kinds of data to be stored and exchanged . The implementation of the NIM follows a meta model based approach , allowing for runtime extension and for easily integrating heterogeneous data models via a mapping DSL and code generation of adaptation components .
In statistical learning theory , convex surrogates of the 0-0 loss are highly preferred because of the computational and theoretical virtues that convexity brings in . This is of more importance if we consider smooth surrogates as witnessed by the fact that the smoothness is further beneficial both computationally- by attaining an {\it optimal} convergence rate for optimization , and in a statistical sense- by providing an improved {\it optimistic} rate for generalization bound . In this paper we investigate the smoothness property from the viewpoint of statistical consistency and show how it affects the binary excess risk . We show that in contrast to optimization and generalization errors that favor the choice of smooth surrogate loss , the smoothness of loss function may degrade the binary excess risk . Motivated by this negative result , we provide a unified analysis that integrates optimization error , generalization bound , and the error in translating convex excess risk into a binary excess risk when examining the impact of smoothness on the binary excess risk . We show that under favorable conditions appropriate choice of smooth convex loss will result in a binary excess risk that is better than $O ( 0/\sqrt{n} ) $ .
In this paper the testing of normality for unconditionally heteroscedastic macroeconomic time series is studied . It is underlined that the classical Jarque-Bera test ( JB hereafter ) for normality is inadequate in our framework . On the other hand it is found that the approach which consists in correcting the heteroscedasticity by kernel smoothing for testing normality is justified asymptotically . Nevertheless it appears from Monte Carlo experiments that such methodology can noticeably suffer from size distortion for samples that are typical for macroeconomic variables . As a consequence a parametric bootstrap methodology for correcting the problem is proposed . The innovations distribution of a set of inflation measures for the U . S . , Korea and Australia are analyzed .
W-graph refers to a general class of random graph models that can be seen as a random graph limit . It is characterized by both its graphon function and its motif frequencies . In this paper , relying on an existing variational Bayes algorithm for the stochastic block models along with the corresponding weights for model averaging , we derive an estimate of the graphon function as an average of stochastic block models with increasing number of blocks . In the same framework , we derive the variational posterior frequency of any motif . A simulation study and an illustration on a social network complete our work .
The deep connection between thermodynamics , computation , and information is now well established both theoretically and experimentally . Here , we extend these ideas to show that thermodynamics also places fundamental constraints on statistical estimation and learning . To do so , we investigate the constraints placed by ( nonequilibrium ) thermodynamics on the ability of biochemical signaling networks within cells to estimate the concentration of an external signal . We show that accuracy is limited by energy consumption , suggesting that there are fundamental thermodynamic constraints on statistical inference .
We show how the classic Cramer-Rao bound limits how accurately one can simultaneously estimate values of a large number of Google Ad campaigns ( or similarly limit the measurement rate of many confounding A/B tests ) .
We are presenting a set of multilingual text analysis tools that can help analysts in any field to explore large document collections quickly in order to determine whether the documents contain information of interest , and to find the relevant text passages . The automatic tool , which currently exists as a fully functional prototype , is expected to be particularly useful when users repeatedly have to sieve through large collections of documents such as those downloaded automatically from the internet . The proposed system takes a whole document collection as input . It first carries out some automatic analysis tasks ( named entity recognition , geo-coding , clustering , term extraction ) , annotates the texts with the generated meta-information and stores the meta-information in a database . The system then generates a zoomable and hyperlinked geographic map enhanced with information on entities and terms found . When the system is used on a regular basis , it builds up a historical database that contains information on which names have been mentioned together with which other names or places , and users can query this database to retrieve information extracted in the past .
This question is raised by Cason , Friedman and Hopkins ( CFH , 0000 ) after they firstly found and indexed quantitatively the cycles in a continuous time experiment . To answer this question , we use the data from standard RPS experiment . Our experiments are of the traditional setting - in each of repeated rounds , the subjects are paired with random matching , using pure strategy and must choose simultaneously , and after each round , each subject obtains only private information . This economics environment is a decartelized and low-information one . Using the cycle rotation indexes ( CRI , developed by CFH ) method , we find , the cycles not only exist but also persist in our experiment . Meanwhile , the cycles ' direction are consistent with ' standard ' learning models . That is the answer to the CHF question : Cycles do not dissipate in the simultaneously choose game . In addtion , we discuss three questions ( 0 ) why significant cycles are uneasy to be obtained in traditional setting experiments ; ( 0 ) why CRI can be an iconic indexing-method for ' standard ' evolution dynamics ; and ( 0 ) where more cycles could be expected .
We discuss the computational complexity of random 0D Ising spin glasses , which represent an interesting class of constraint satisfaction problems for black box optimization . Two extremal cases are considered : ( 0 ) the +/- J spin glass , and ( 0 ) the Gaussian spin glass . We also study a smooth transition between these two extremal cases . The computational complexity of all studied spin glass systems is found to be dominated by rare events of extremely hard spin glass samples . We show that complexity of all studied spin glass systems is closely related to Frechet extremal value distribution . In a hybrid algorithm that combines the hierarchical Bayesian optimization algorithm ( hBOA ) with a deterministic bit-flip hill climber , the number of steps performed by both the global searcher ( hBOA ) and the local searcher follow Frechet distributions . Nonetheless , unlike in methods based purely on local search , the parameters of these distributions confirm good scalability of hBOA with local search . We further argue that standard performance measures for optimization algorithms--such as the average number of evaluations until convergence--can be misleading . Finally , our results indicate that for highly multimodal constraint satisfaction problems , such as Ising spin glasses , recombination-based search can provide qualitatively better results than mutation-based search .
Obtaining more accurate equity value estimates is the starting point for stock selection , value-based indexing in a noisy market , and beating benchmark indices through tactical style rotation . Unfortunately , discounted cash flow , method of comparables , and fundamental analysis typically yield discrepant valuation estimates . Moreover , the valuation estimates typically disagree with market price . Can one form a superior valuation estimate by averaging over the individual estimates , including market price ? This article suggests a Bayesian framework for combining two or more estimates into a superior valuation estimate . The framework justifies the common practice of averaging over several estimates to arrive at a final point estimate .
Wind energy is becoming a top contributor to the renewable energy mix , which raises potential reliability issues for the grid due to the fluctuating nature of its source . To achieve adequate reserve commitment and to promote market participation , it is necessary to provide models that can capture daily patterns in wind power production . This paper presents a cyclic inhomogeneous Markov process , which is based on a three-dimensional state-space ( wind power , speed and direction ) . Each time-dependent transition probability is expressed as a Bernstein polynomial . The model parameters are estimated by solving a constrained optimization problem : The objective function combines two maximum likelihood estimators , one to ensure that the Markov process long-term behavior reproduces the data accurately and another to capture daily fluctuations . A convex formulation for the overall optimization problem is presented and its applicability demonstrated through the analysis of a case-study . The proposed model is capable of reproducing the diurnal patterns of a three-year dataset collected from a wind turbine located in a mountainous region in Portugal . In addition , it is shown how to compute persistence statistics directly from the Markov process transition matrices . Based on the case-study , the power production persistence through the daily cycle is analysed and discussed .
In this paper we introduce a novel approach for an important problem of break detection . Specifically , we are interested in detection of an abrupt change in the covariance structure of a high-dimensional random process -- a problem , which has applications in many areas e . g . , neuroimaging and finance . The developed approach is essentially a testing procedure involving a choice of a critical level . To that end a non-standard bootstrap scheme is proposed and theoretically justified under mild assumptions . Theoretical study features a result providing guaranties for break detection . All the theoretical results are established in a high-dimensional setting ( dimensionality $p \gg n$ ) . Multiscale nature of the approach allows for a trade-off between sensitivity of break detection and localization . The approach can be naturally employed in an on-line setting . Simulation study demonstrates that the approach matches the nominal level of false alarm probability and exhibits high power , outperforming a recent approach .
The wide spread of location-based social networks brings about a huge volume of user check-in data , which facilitates the recommendation of points of interest ( POIs ) . Recent advances on distributed representation shed light on learning low dimensional dense vectors to alleviate the data sparsity problem . Current studies on representation learning for POI recommendation embed both users and POIs in a common latent space , and users ' preference is inferred based on the distance/similarity between a user and a POI . Such an approach is not in accordance with the semantics of users and POIs as they are inherently different objects . In this paper , we present a novel spatiotemporal aware ( STA ) representation , which models the spatial and temporal information as \emph{a relationship connecting users and POIs} . Our model generalizes the recent advances in knowledge graph embedding . The basic idea is that the embedding of a $<$time , location$>$ pair corresponds to a translation from embeddings of users to POIs . Since the POI embedding should be close to the user embedding plus the relationship vector , the recommendation can be performed by selecting the top-\emph{k} POIs similar to the translated POI , which are all of the same type of objects . We conduct extensive experiments on two real-world datasets . The results demonstrate that our STA model achieves the state-of-the-art performance in terms of high recommendation accuracy , robustness to data sparsity and effectiveness in handling cold start problem .
Active SLAM is the task of actively planning robot paths while simultaneously building a map and localizing within . Existing work has focused on planning paths with occupancy grid maps , which do not scale well and suffer from long term drift . This work proposes a Topological Feature Graph ( TFG ) representation that scales well and develops an active SLAM algorithm with it . The TFG uses graphical models , which utilize independences between variables , and enables a unified quantification of exploration and exploitation gains with a single entropy metric . Hence , it facilitates a natural and principled balance between map exploration and refinement . A probabilistic roadmap path-planner is used to generate robot paths in real time . Experimental results demonstrate that the proposed approach achieves better accuracy than a standard grid-map based approach while requiring orders of magnitude less computation and memory resources .
Many practical techniques for probabilistic inference require a sequence of distributions that interpolate between a tractable distribution and an intractable distribution of interest . Usually , the sequences used are simple , e . g . , based on geometric averages between distributions . When models are expressed as probabilistic programs , the models themselves are highly structured objects that can be used to derive annealing sequences that are more sensitive to domain structure . We propose an algorithm for transforming probabilistic programs to coarse-to-fine programs which have the same marginal distribution as the original programs , but generate the data at increasing levels of detail , from coarse to fine . We apply this algorithm to an Ising model , its depth-from-disparity variation , and a factorial hidden Markov model . We show preliminary evidence that the use of coarse-to-fine models can make existing generic inference algorithms more efficient .
Coping with outliers contaminating dynamical processes is of major importance in various applications because mismatches from nominal models are not uncommon in practice . In this context , the present paper develops novel fixed-lag and fixed-interval smoothing algorithms that are robust to outliers simultaneously present in the measurements {\it and} in the state dynamics . Outliers are handled through auxiliary unknown variables that are jointly estimated along with the state based on the least-squares criterion that is regularized with the $\ell_0$-norm of the outliers in order to effect sparsity control . The resultant iterative estimators rely on coordinate descent and the alternating direction method of multipliers , are expressed in closed form per iteration , and are provably convergent . Additional attractive features of the novel doubly robust smoother include : i ) ability to handle both types of outliers ; ii ) universality to unknown nominal noise and outlier distributions ; iii ) flexibility to encompass maximum a posteriori optimal estimators with reliable performance under nominal conditions ; and iv ) improved performance relative to competing alternatives at comparable complexity , as corroborated via simulated tests .
We consider the problem of finding the minimizer of a function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ of the form $\min f ( w ) = \frac{0}{n}\sum_{i}f_i ( {w} ) $ . This problem has been studied intensively in recent years in machine learning research field . One typical but promising approach for large-scale data is stochastic optimization algorithm . SGDLibrary is a flexible , extensible and efficient pure-Matlab library of a collection of stochastic optimization algorithms . The purpose of the library is to provide researchers and implementers a comprehensive evaluation environment of those algorithms on various machine learning problems .
We present infinite extensive strategy profiles with perfect information and we show that replacing finite by infinite changes the notions and the reasoning tools . The presentation uses a formalism recently developed by logicians and computer science theoreticians , called coinduction . This builds a bridge between economic game theory and the most recent advance in theoretical computer science and logic . The key result is that rational agents may have strategy leading to divergence .
With phenomenal growth of high speed and complex computing applications , the design of low power and high speed logic circuits have created tremendous interest . Conventional computing devices are based on irreversible logic and further reduction in power consumption and/or increase in speed appears non-promising . Reversible computing has emerged as a solution looking to the power and speed requirements of future computing devices . In reversible computing logic gates used are such that input can be generated by reversing the operation from output . A number of reversible combinational circuits have been developed but the growth of sequential circuits was not significant due to feedback and fanout was not allowed . However , allowing feedback in space , a very few sequential logic blocks i . e . flip-flops have been reported in literature . In order to develop sequential circuits , flip-flops are used in conventional circuits . Also good circuit design methods , optimized and fault tolerant designs are also needed to build large , complex and reliable circuits in conventional computing . Reversible flip-flops are the basic memory elements that will be the building block of memory for reversible computing and quantum computing devices . In this dissertation we plan to address above issues . First we have proposed a Pareek gate suitable for low-cost flip-flops design and then design methodology to develop flip-flops are illustrated . Further almost all flip-flops and some example circuit have been developed and finally these circuits have been converted into fault tolerant circuits by preserving their parity and designs of offline as well as online testable circuits have been proposed . In this dissertation work , we have also compared quantum cost as well as other parameters with existing circuits and shown a significant improvement in almost all parameters .
We consider two active binary-classification problems with atypical objectives . In the first , active search , our goal is to actively uncover as many members of a given class as possible . In the second , active surveying , our goal is to actively query points to ultimately predict the proportion of a given class . Numerous real-world problems can be framed in these terms , and in either case typical model-based concerns such as generalization error are only of secondary importance . We approach these problems via Bayesian decision theory ; after choosing natural utility functions , we derive the optimal policies . We provide three contributions . In addition to introducing the active surveying problem , we extend previous work on active search in two ways . First , we prove a novel theoretical result , that less-myopic approximations to the optimal policy can outperform more-myopic approximations by any arbitrary degree . We then derive bounds that for certain models allow us to reduce ( in practice dramatically ) the exponential search space required by a naive implementation of the optimal policy , enabling further lookahead while still ensuring that optimal decisions are always made .
This work initiates a systematic investigation of testing {\em high-dimensional} structured distributions by focusing on testing {\em Bayesian networks} -- the prototypical family of directed graphical models . A Bayesian network is defined by a directed acyclic graph , where we associate a random variable with each node . The value at any particular node is conditionally independent of all the other non-descendant nodes once its parents are fixed . Specifically , we study the properties of identity testing and closeness testing of Bayesian networks . Our main contribution is the first non-trivial efficient testing algorithms for these problems and corresponding information-theoretic lower bounds . For a wide range of parameter settings , our testing algorithms have sample complexity {\em sublinear} in the dimension and are sample-optimal , up to constant factors .
We consider the scenario where the parameters of a probabilistic model are expected to vary over time . We construct a novel prior distribution that promotes sparsity and adapts the strength of correlation between parameters at successive timesteps , based on the data . We derive approximate variational inference procedures for learning and prediction with this prior . We test the approach on two tasks : forecasting financial quantities from relevant text , and modeling language contingent on time-varying financial measurements .
We introduce a general method to prove uniform in bandwidth consistency of kernel-type function estimators . Examples include the kernel density estimator , the Nadaraya-Watson regression estimator and the conditional empirical process . Our results may be useful to establish uniform consistency of data-driven bandwidth kernel-type function estimators .
Additive isotonic regression attempts to determine the relationship between a multi-dimensional observation variable and a response , under the constraint that the estimate is the additive sum of univariate component effects that are monotonically increasing . In this article , we present a new method for such regression called LASSO Isotone ( LISO ) . LISO adapts ideas from sparse linear modelling to additive isotonic regression . Thus , it is viable in many situations with high dimensional predictor variables , where selection of significant versus insignificant variables are required . We suggest an algorithm involving a modification of the backfitting algorithm CPAV . We give a numerical convergence result , and finally examine some of its properties through simulations . We also suggest some possible extensions that improve performance , and allow calculation to be carried out when the direction of the monotonicity is unknown .
This paper describes an efficient approach to constructing a resultant polyline with a minimum number of segments and arcs . While fitting an arc can be done with complexity O ( 0 ) ( see [0] and [0] ) , the main complexity is in checking that the resultant arc is within the specified tolerance . There are additional tests to check for the ends and for changes in direction ( see [0 , section 0] and [0 , sections II . C and II . D] ) . However , the most important part in reducing complexity is the ability to subdivide the polyline in order to limit the number of arc fittings [0] . The approach described in this paper finds a compressed polyline with a minimum number of segments and arcs .
With the proliferation of mobile applications , Mobile Cloud Computing ( MCC ) has been proposed to help mobile devices save energy and improve computation performance . To further improve the quality of service ( QoS ) of MCC , cloud servers can be deployed locally so that the latency is decreased . However , the computational resource of the local cloud is generally limited . In this paper , we design a threshold-based policy to improve the QoS of MCC by cooperation of the local cloud and Internet cloud resources , which takes the advantages of low latency of the local cloud and abundant computational resources of the Internet cloud simultaneously . This policy also applies a priority queue in terms of delay requirements of applications . The optimal thresholds depending on the traffic load is obtained via a proposed algorithm . Numerical results show that the QoS can be greatly enhanced with the assistance of Internet cloud when the local cloud is overloaded . Better QoS is achieved if the local cloud order tasks according to their delay requirements , where delay-sensitive applications are executed ahead of delay-tolerant applications . Moreover , the optimal thresholds of the policy have a sound impact on the QoS of the system .
We study the design of truthful mechanisms for set systems , i . e . , scenarios where a customer needs to hire a team of agents to perform a complex task . In this setting , frugality [Archer&Tardos ' 00] provides a measure to evaluate the " cost of truthfulness " , that is , the overpayment of a truthful mechanism relative to the " fair " payment . We propose a uniform scheme for designing frugal truthful mechanisms for general set systems . Our scheme is based on scaling the agents ' bids using the eigenvector of a matrix that encodes the interdependencies between the agents . We demonstrate that the r-out-of-k-system mechanism and the \sqrt-mechanism for buying a path in a graph [Karlin et . al ' 00] can be viewed as instantiations of our scheme . We then apply our scheme to two other classes of set systems , namely , vertex cover systems and k-path systems , in which a customer needs to purchase k edge-disjoint source-sink paths . For both settings , we bound the frugality of our mechanism in terms of the largest eigenvalue of the respective interdependency matrix . We show that our mechanism is optimal for a large subclass of vertex cover systems satisfying a simple local sparsity condition . For k-path systems , while our mechanism is within a factor of k + 0 from optimal , we show that it is , in fact , optimal , when one uses a modified definition of frugality proposed in [Elkind et al . ' 00] . Our lower bound argument combines spectral techniques and Young ' s inequality , and is applicable to all set systems . As both r-out-of-k systems and single path systems can be viewed as special cases of k-path systems , our result improves the lower bounds of [Karlin et al . ' 00] and answers several open questions proposed in that paper .
We propose a generalization of the classical stable marriage problem . In our model , the preferences on one side of the partition are given in terms of arbitrary binary relations , which need not be transitive nor acyclic . This generalization is practically well-motivated , and as we show , encompasses the well studied hard variant of stable marriage where preferences are allowed to have ties and to be incomplete . As a result , we prove that deciding the existence of a stable matching in our model is NP-complete . Complementing this negative result we present a polynomial-time algorithm for the above decision problem in a significant class of instances where the preferences are asymmetric . We also present a linear programming formulation whose feasibility fully characterizes the existence of stable matchings in this special case . Finally , we use our model to study a long standing open problem regarding the existence of cyclic 0D stable matchings . In particular , we prove that the problem of deciding whether a fixed 0D perfect matching can be extended to a 0D stable matching is NP-complete , showing this way that a natural attempt to resolve the existence ( or not ) of 0D stable matchings is bound to fail .
It is proven that every set $S$ of distinct points in the plane with cardinality $\lceil \frac{\sqrt{\log_0 n}-0}{0} \rceil$ can be a subset of the vertices of a crossing-free straight-line drawing of any planar graph with $n$ vertices . It is also proven that if $S$ is restricted to be a one-sided convex point set , its cardinality increases to $\lceil \sqrt[0]{n} \rceil$ . The proofs are constructive and give rise to O ( n ) -time drawing algorithms . As a part of our proofs , we show that every maximal planar graph contains a large induced biconnected outerplanar graphs and a large induced outerpath ( an outerplanar graph whose weak dual is a path ) .
This paper describes a framework for a systematic classification of spreadsheet errors . This classification or taxonomy of errors is aimed at facilitating analysis and comprehension of the different types of spreadsheet errors . The taxonomy is an outcome of an investigation of the widespread problem of spreadsheet errors and an analysis of specific types of these errors . This paper contains a description of the various elements and categories of the classification and is supported by appropriate examples .
Empirical scoring functions based on either molecular force fields or cheminformatics descriptors are widely used , in conjunction with molecular docking , during the early stages of drug discovery to predict potency and binding affinity of a drug-like molecule to a given target . These models require expert-level knowledge of physical chemistry and biology to be encoded as hand-tuned parameters or features rather than allowing the underlying model to select features in a data-driven procedure . Here , we develop a general 0-dimensional spatial convolution operation for learning atomic-level chemical interactions directly from atomic coordinates and demonstrate its application to structure-based bioactivity prediction . The atomic convolutional neural network is trained to predict the experimentally determined binding affinity of a protein-ligand complex by direct calculation of the energy associated with the complex , protein , and ligand given the crystal structure of the binding pose . Non-covalent interactions present in the complex that are absent in the protein-ligand sub-structures are identified and the model learns the interaction strength associated with these features . We test our model by predicting the binding free energy of a subset of protein-ligand complexes found in the PDBBind dataset and compare with state-of-the-art cheminformatics and machine learning-based approaches . We find that all methods achieve experimental accuracy and that atomic convolutional networks either outperform or perform competitively with the cheminformatics based methods . Unlike all previous protein-ligand prediction systems , atomic convolutional networks are end-to-end and fully-differentiable . They represent a new data-driven , physics-based deep learning model paradigm that offers a strong foundation for future improvements in structure-based bioactivity prediction .
We propose a novel application based on acoustic-to-articulatory inversion towards quality assessment of voice converted speech . The ability of humans to speak effortlessly requires coordinated movements of various articulators , muscles , etc . This effortless movement contributes towards naturalness , intelligibility and speakers identity which is partially present in voice converted speech . Hence , during voice conversion , the information related to speech production is lost . In this paper , this loss is quantified for male voice , by showing increase in RMSE error for voice converted speech followed by showing decrease in mutual information . Similar results are obtained in case of female voice . This observation is extended by showing that articulatory features can be used as an objective measure . The effectiveness of proposed measure over MCD is illustrated by comparing their correlation with Mean Opinion Score .
A . N . Kolmogorov proposed several problems on stochastic processes , which has been rarely addressed later on . One of the open problems are stochastic processes with discontinuous covariance function . For example , semicontinuous covariance functions have been used in regression and kriging by many authors in statistics recently . In this paper we introduce purely topologically defined regularity conditions on covariance kernels which are still applicable for increasing and infill domain asymptotics for regression problems , kriging and finance . These conditions are related to semicontinuous maps of Ornstein Uhlenbeck ( OU ) processes . Beside this new regularity conditions relax the continuity of covariance function by consideration of semicontinuous covariance . We provide several novel applications of the introduced class for optimal design of random fields , random walks in finance and probabilities of ruins related to shocks , e . g . by earthquakes . In particular we construct a random walk model with semicontinuous covariance .
The paper presents a methodology for the enhanced stiffness analysis of parallel manipulators with internal preloading in passive joints . It also takes into account influence of the external loading and allows computing both the non-linear " load-deflection " relation and the stiffness matrices for any given location of the end-platform or actuating drives . Using this methodology , it is proposed the kinetostatic control algorithm that allows to improve accuracy of the classical kinematic control and to compensate position errors caused by elastic deformations in links/joints due to the external/internal loading . The results are illustrated by an example that deals with a parallel manipulator of the Orthoglide family where the internal preloading allows to eliminate the undesired buckling phenomena and to improve the stiffness in the neighborhood of its kinematic singularities .
In decentralized networks ( of sensors , connected objects , etc . ) , there is an important need for efficient algorithms to optimize a global cost function , for instance to learn a global model from the local data collected by each computing unit . In this paper , we address the problem of decentralized minimization of pairwise functions of the data points , where these points are distributed over the nodes of a graph defining the communication topology of the network . This general problem finds applications in ranking , distance metric learning and graph inference , among others . We propose new gossip algorithms based on dual averaging which aims at solving such problems both in synchronous and asynchronous settings . The proposed framework is flexible enough to deal with constrained and regularized variants of the optimization problem . Our theoretical analysis reveals that the proposed algorithms preserve the convergence rate of centralized dual averaging up to an additive bias term . We present numerical simulations on Area Under the ROC Curve ( AUC ) maximization and metric learning problems which illustrate the practical interest of our approach .
For the nonparametric estimation of multivariate finite mixture models with the conditional independence assumption , we propose a new formulation of the objective function in terms of penalized smoothed Kullback-Leibler distance . The nonlinearly smoothed majorization-minimization ( NSMM ) algorithm is derived from this perspective . An elegant representation of the NSMM algorithm is obtained using a novel projection-multiplication operator , a more precise monotonicity property of the algorithm is discovered , and the existence of a solution to the main optimization problem is proved for the first time .
In recent years , neural network approaches have shown superior performance to conventional hand-made features in numerous application areas . In particular , convolutional neural networks ( ConvNets ) exploit spatially local correlations across input data to improve the performance of audio processing tasks , such as speech recognition , musical chord recognition , and onset detection . Here we apply ConvNet to acoustic scene classification , and show that the error rate can be further decreased by using delta features in the frequency domain . We propose a multiple-width frequency-delta ( MWFD ) data augmentation method that uses static mel-spectrogram and frequency-delta features as individual input examples . In addition , we describe a ConvNet output aggregation method designed for MWFD augmentation , folded mean aggregation , which combines output probabilities of static and MWFD features from the same analysis window using multiplication first , rather than taking an average of all output probabilities . We describe calculation results using the DCASE 0000 challenge dataset , which shows that ConvNet outperforms both of the baseline system with hand-crafted features and a deep neural network approach by around 0% . The performance was further improved ( by 0 . 0% ) using the MWFD augmentation together with folded mean aggregation . The system exhibited a classification accuracy of 0 . 000 when classifying 00 acoustic scenes .
This paper presents a semi-parametric algorithm for online learning of a robot inverse dynamics model . It combines the strength of the parametric and non-parametric modeling . The former exploits the rigid body dynamics equa- tion , while the latter exploits a suitable kernel function . We provide an extensive comparison with other methods from the literature using real data from the iCub humanoid robot . In doing so we also compare two different techniques , namely cross validation and marginal likelihood optimization , for estimating the hyperparameters of the kernel function .
We take a new look at parameter estimation for Gaussian Mixture Models ( GMMs ) . In particular , we propose using \emph{Riemannian manifold optimization} as a powerful counterpart to Expectation Maximization ( EM ) . An out-of-the-box invocation of manifold optimization , however , fails spectacularly : it converges to the same solution but vastly slower . Driven by intuition from manifold convexity , we then propose a reparamerization that has remarkable empirical consequences . It makes manifold optimization not only match EM---a highly encouraging result in itself given the poor record nonlinear programming methods have had against EM so far---but also outperform EM in many practical settings , while displaying much less variability in running times . We further highlight the strengths of manifold optimization by developing a somewhat tuned manifold LBFGS method that proves even more competitive and reliable than existing manifold optimization tools . We hope that our results encourage a wider consideration of manifold optimization for parameter estimation problems .
We consider the problem of establishing dense correspondences within a set of related shapes of strongly varying geometry . For such input , traditional shape matching approaches often produce unsatisfactory results . We propose an ensemble optimization method that improves given coarse correspondences to obtain dense correspondences . Following ideas from minimum description length approaches , it maximizes the compactness of the induced shape space to obtain high-quality correspondences . We make a number of improvements that are important for computer graphics applications : Our approach handles meshes of general topology and handles partial matching between input of varying topology . To this end we introduce a novel part-based generative statistical shape model . We develop a novel analysis algorithm that learns such models from training shapes of varying topology . We also provide a novel synthesis method that can generate new instances with varying part layouts and subject to generic variational constraints . In practical experiments , we obtain a substantial improvement in correspondence quality over state-of-the-art methods . As example application , we demonstrate a system that learns shape families as assemblies of deformable parts and permits real-time editing with continuous and discrete variability .
Operator fractional Brownian motion ( OFBM ) is the natural vector-valued extension of the univariate fractional Brownian motion . Instead of a scalar parameter , the law of an OFBM scales according to a Hurst matrix that affects every component of the process . In this paper , we develop the wavelet analysis of OFBM , as well as a new estimator for the Hurst matrix of bivariate OFBM . For OFBM , the univariate-inspired approach of analyzing the entry-wise behavior of the wavelet spectrum as a function of the ( wavelet ) scales is fraught with difficulties stemming from mixtures of power laws . The proposed approach consists of considering the evolution along scales of the eigenstructure of the wavelet spectrum . This is shown to yield consistent and asymptotically normal estimators of the Hurst eigenvalues , and also of the coordinate system itself under assumptions . A simulation study is included to demonstrate the good performance of the estimators under finite sample sizes .
We present some optimal criteria to evaluate model-robustness of non-regular two-level fractional factorial designs . Our method is based on minimizing the sum of squares of all the off-diagonal elements in the information matrix , and considering expectation under appropriate distribution functions for unknown contamination of the interaction effects . By considering uniform distributions on symmetric support , our criteria can be expressed as linear combinations of $B_s ( d ) $ characteristic , which is used to characterize the generalized minimum aberration . We give some empirical studies for 00-run non-regular designs to evaluate our method .
In this paper we present the Large Inverse Cholesky ( LIC ) method , an efficient method for computing the coefficient matrices of a Structural Vector Autoregressive ( SVAR ) model .
The hypergeometric distribution is briefly and informally surveyed , including popular notation , symmetries , and the tail inequalities $Pr[i \ge E[i]+tn] \le e^{-0t^0n}$ and $Pr[i \le E[i]-tn] \le e^{-0t^0n}$ .
In this paper we study the non-Gaussian homogenous and isotropic field on the plane in frequency domain . The trispectrum and higher order spectra of such a field are described in terms of Bessel functions . Poisson formulae are given for the spectrum and for the bispectrum . Some particular integrals of Bessel functions are considered as well .
After surveying classical results , we introduce a generalized notion of inference system to support structural recursion on non-well-founded data types . Besides axioms and inference rules with the usual meaning , a generalized inference system allows coaxioms , which are , intuitively , axioms which can only be applied " at infinite depth " in a proof tree . This notion nicely subsumes standard inference systems and their inductive and coinductive interpretation , while providing more flexibility . Indeed , the classical results can be extended to our generalized framework , interpreting recursive definitions as fixed points which are not necessarily the least , nor the greatest one . This allows formal reasoning in cases where the inductive and coinductive interpretation do not provide the intended meaning , or are mixed together .
Classical analysis of variance requires that model terms be labeled as fixed or random and typically culminate by comparing variability from each batch ( factor ) to variability from errors ; without a standard methodology to assess the magnitude of a batch ' s variability , to compare variability between batches , nor to consider the uncertainty in this assessment . In this paper we support recent work , placing ANOVA into a general multilevel framework , then refine this through batch level model specifications , and develop it further by extension to the multivariate case . Adopting a Bayesian multilevel model parametrization , with improper batch level prior densities , we derive a method that facilitates comparison across all sources of variability . Whereas classical multivariate ANOVA often utilizes a single covariance criterion , e . g . determinant for Wilks ' lambda distribution , the method allows arbitrary covariance criteria to be employed . The proposed method also addresses computation . By introducing implicit batch level constraints , which yield improper priors , the full posterior is efficiently factored , thus alleviating computational demands . For a large class of models , the partitioning mitigates , or even obviates the need for methods such as MCMC . The method is illustrated with simulated examples and an application focusing on climate projections with global climate models .
We introduce an interactive learning framework for the development and testing of intelligent visual systems , called learning-by-asking ( LBA ) . We explore LBA in context of the Visual Question Answering ( VQA ) task . LBA differs from standard VQA training in that most questions are not observed during training time , and the learner must ask questions it wants answers to . Thus , LBA more closely mimics natural learning and has the potential to be more data-efficient than the traditional VQA setting . We present a model that performs LBA on the CLEVR dataset , and show that it automatically discovers an easy-to-hard curriculum when learning interactively from an oracle . Our LBA generated data consistently matches or outperforms the CLEVR train data and is more sample efficient . We also show that our model asks questions that generalize to state-of-the-art VQA models and to novel test time distributions .
Ensemble techniques are powerful approaches that combine several weak learners to build a stronger one . As a meta learning framework , ensemble techniques can easily be applied to many machine learning techniques . In this paper we propose a neural network extended with an ensemble loss function for text classification . The weight of each weak loss function is tuned within the training phase through the gradient propagation optimization method of the neural network . The approach is evaluated on several text classification datasets . We also evaluate its performance in various environments with several degrees of label noise . Experimental results indicate an improvement of the results and strong resilience against label noise in comparison with other methods .
We propose a Bayesian hierarchical model for spatial extremes on a large domain . In the data layer a Gaussian elliptical copula having generalized extreme value ( GEV ) marginals is applied . Spatial dependence in the GEV parameters are captured with a latent spatial regression with spatially varying coefficients . Using a composite likelihood approach , we are able to efficiently incorporate a large precipitation dataset , which includes stations with missing data . The model is demonstrated by application to fall precipitation extremes at approximately 0000 stations covering the western United States , -000E to -000E longitude and 00N to 00N latitude . The hierarchical model provides GEV parameters on a $0/0$th degree grid and consequently maps of return levels and associated uncertainty . The model results indicate that return levels vary coherently both spatially and across seasons , providing information about the space-time variations of risk of extreme precipitation in the western US , helpful for infrastructure planning .
A new nonparametric estimator of the local Hurst function of a multifractional Gaussian process based on the increment ratio ( IR ) statistic is defined . In a general frame , the point-wise and uniform weak and strong consistency and a multidimensional central limit theorem for this estimator are established . Similar results are obtained for a refinement of the generalized quadratic variations ( QV ) estimator . The example of the multifractional Brownian motion is studied in detail . A simulation study is included showing that the IR-estimator is more accurate than the QV-estimator .
Robust inference based on the minimization of statistical divergences has proved to be a useful alternative to the classical techniques based on maximum likelihood and related methods . Recently Ghosh et al . ( 0000 ) proposed a general class of divergence measures , namely the S-Divergence Family and discussed its usefulness in robust parametric estimation through some numerical illustrations . In this present paper , we develop the asymptotic properties of the proposed minimum S-Divergence estimators under discrete models .
Mass segmentation provides effective morphological features which are important for mass diagnosis . In this work , we propose a novel end-to-end network for mammographic mass segmentation which employs a fully convolutional network ( FCN ) to model a potential function , followed by a CRF to perform structured learning . Because the mass distribution varies greatly with pixel position , the FCN is combined with a position priori . Further , we employ adversarial training to eliminate over-fitting due to the small sizes of mammogram datasets . Multi-scale FCN is employed to improve the segmentation performance . Experimental results on two public datasets , INbreast and DDSM-BCRP , demonstrate that our end-to-end network achieves better performance than state-of-the-art approaches . \footnote{https : //github . com/wentaozhu/adversarial-deep-structural-networks . git}
When applying machine learning to problems in NLP , there are many choices to make about how to represent input texts . These choices can have a big effect on performance , but they are often uninteresting to researchers or practitioners who simply need a module that performs well . We propose an approach to optimizing over this space of choices , formulating the problem as global optimization . We apply a sequential model-based optimization technique and show that our method makes standard linear models competitive with more sophisticated , expensive state-of-the-art methods based on latent variable models or neural networks on various topic classification and sentiment analysis problems . Our approach is a first step towards black-box NLP systems that work with raw text and do not require manual tuning .
In this paper , we study information cascades on graphs . In this setting , each node in the graph represents a person . One after another , each person has to take a decision based on a private signal as well as the decisions made by earlier neighboring nodes . Such information cascades commonly occur in practice and have been studied in complete graphs where everyone can overhear the decisions of every other player . It is known that information cascades can be fragile and based on very little information , and that they have a high likelihood of being wrong . Generalizing the problem to arbitrary graphs reveals interesting insights . In particular , we show that in a random graph $G ( n , q ) $ , for the right value of $q$ , the number of nodes making a wrong decision is logarithmic in $n$ . That is , in the limit for large $n$ , the fraction of players that make a wrong decision tends to zero . This is intriguing because it contrasts to the two natural corner cases : empty graph ( everyone decides independently based on his private signal ) and complete graph ( all decisions are heard by all nodes ) . In both of these cases a constant fraction of nodes make a wrong decision in expectation . Thus , our result shows that while both too little and too much information sharing causes nodes to take wrong decisions , for exactly the right amount of information sharing , asymptotically everyone can be right . We further show that this result in random graphs is asymptotically optimal for any topology , even if nodes follow a globally optimal algorithmic strategy . Based on the analysis of random graphs , we explore how topology impacts global performance and construct an optimal deterministic topology among layer graphs .
Currently , industrial sectors are transforming their business processes into e-services and component-based architectures to build flexible , robust , and scalable systems , and reduce integration-related maintenance and development costs . Robotics is yet another promising and fast-growing industry that deals with the creation of machines that operate in an autonomous fashion and serve for various applications including space exploration , weaponry , laboratory research , and manufacturing . It is in space exploration that the most common type of robots is the planetary rover which moves across the surface of a planet and conducts a thorough geological study of the celestial surface . This type of rover system is still ad-hoc in that it incorporates its software into its core hardware making the whole system cohesive , tightly-coupled , more susceptible to shortcomings , less flexible , hard to be scaled and maintained , and impossible to be adapted to other purposes . This paper proposes a service-oriented architecture for space exploration robotic rover systems made out of loosely-coupled and distributed web services . The proposed architecture consists of three elementary tiers : the client tier that corresponds to the actual rover ; the server tier that corresponds to the web services ; and the middleware tier that corresponds to an Enterprise Service Bus which promotes interoperability between the interconnected entities . The niche of this architecture is that rover ' s software components are decoupled and isolated from the rover ' s body and possibly deployed at a distant location . A service-oriented architecture promotes integrate-ability , scalability , reusability , maintainability , and interoperability for client-to-server communication .
We propose position-velocity encoders ( PVEs ) which learn---without supervision---to encode images to positions and velocities of task-relevant objects . PVEs encode a single image into a low-dimensional position state and compute the velocity state from finite differences in position . In contrast to autoencoders , position-velocity encoders are not trained by image reconstruction , but by making the position-velocity representation consistent with priors about interacting with the physical world . We applied PVEs to several simulated control tasks from pixels and achieved promising preliminary results .
The paper addresses the problem of computing maximal expected time to termination of probabilistic timed automata ( PTA ) models , under the condition that the system eventually terminates with probability one . The problem may exhibit high computational complexity , in particular when the automaton under analysis contains cycles that can be repeated with very high probabilities ( e . g . p =0 . 000 ) . Such class of cycles can degrade the performance of the model checking algorithm , as the probability to repeat the cycle converges to zero arbitrary slow . We propose an acceleration technique which may be applied to improve the execution of cycles by collapsing their iterations . The acceleration process of a cyclic PTA consists of several complex formal steps which are necessary to handle the cumulative timing and probability information that result from successive executions of a cycle . The advantages of acceleration are twofold . First , it helps to reduce the computational complexity of the problem without adversely affecting the outcome of the analysis . Second , it can bring the worst case execution time problem of PTAs within the bound of feasibility for model checking . To our knowledge , this work is the first that addresses the problem of accelerating execution of cycles that exhibit both timing and probabilistic behavior .
The intersection of causal inference and machine learning is a rapidly advancing field . We propose a new approach , the method of direct estimation , that draws on both traditions in order to obtain nonparametric estimates of treatment effects . The approach focuses on estimating the effect of fluctuations in a treatment variable on an outcome . A tensor-spline implementation enables rich interactions between functional bases allowing for the approach to capture treatment/covariate interactions . We show how new innovations in Bayesian sparse modeling readily handle the proposed framework , and then document its performance in simulation and applied examples . Furthermore we show how the method of direct estimation can easily extend to structural estimators commonly used in a variety of disciplines , like instrumental variables , mediation analysis , and sequential g-estimation .
Every probability distribution can be approximated up to a given precision by a phase-type distribution , i . e . a distribution encoded by a continuous time Markov chain ( CTMC ) . However , an excessive number of states in the corresponding CTMC is needed for some standard distributions , in particular most distributions with regions of zero density such as uniform or shifted distributions . Addressing this class of distributions , we suggest an alternative representation by CTMC extended with discrete-time transitions . Using discrete-time transitions we split the density function into multiple intervals . Within each interval , we then approximate the density with standard phase-type fitting . We provide an experimental evidence that our method requires only a moderate number of states to approximate such distributions with regions of zero density . Furthermore , the usage of CTMC with discrete-time transitions is supported by a number of techniques for their analysis . Thus , our results promise an efficient approach to the transient analysis of a class of non-Markovian models .
In this paper , we consider the multivariate Bernoulli distribution as a model to estimate the structure of graphs with binary nodes . This distribution is discussed in the framework of the exponential family , and its statistical properties regarding independence of the nodes are demonstrated . Importantly the model can estimate not only the main effects and pairwise interactions among the nodes but also is capable of modeling higher order interactions , allowing for the existence of complex clique effects . We compare the multivariate Bernoulli model with existing graphical inference models - the Ising model and the multivariate Gaussian model , where only the pairwise interactions are considered . On the other hand , the multivariate Bernoulli distribution has an interesting property in that independence and uncorrelatedness of the component random variables are equivalent . Both the marginal and conditional distributions of a subset of variables in the multivariate Bernoulli distribution still follow the multivariate Bernoulli distribution . Furthermore , the multivariate Bernoulli logistic model is developed under generalized linear model theory by utilizing the canonical link function in order to include covariate information on the nodes , edges and cliques . We also consider variable selection techniques such as LASSO in the logistic model to impose sparsity structure on the graph . Finally , we discuss extending the smoothing spline ANOVA approach to the multivariate Bernoulli logistic model to enable estimation of non-linear effects of the predictor variables .
The problem of combining individual forecasters to produce a forecaster with improved performance is considered . The connections between probability elicitation and classification are used to pose the combining forecaster problem as that of ensemble learning . With this connection in place , a number of theoretically sound ensemble learning methods such as Bagging and Boosting are adapted for combining forecasters . It is shown that the simple yet effective method of averaging the forecasts is equivalent to Bagging . This provides theoretical insight into why the well established averaging of forecasts method works so well . Also , a nonlinear combination of forecasters can be attained through Boosting which is shown to theoretically produce combined forecasters that are both calibrated and highly refined . Finally , the proposed methods of combining forecasters are applied to the Good Judgment Project data set and are shown to outperform the individual forecasters .
Existing algorithms for subgroup discovery with numerical targets do not optimize the error or target variable dispersion of the groups they find . This often leads to unreliable or inconsistent statements about the data , rendering practical applications , especially in scientific domains , futile . Therefore , we here extend the optimistic estimator framework for optimal subgroup discovery to a new class of objective functions : we show how tight estimators can be computed efficiently for all functions that are determined by subgroup size ( non-decreasing dependence ) , the subgroup median value , and a dispersion measure around the median ( non-increasing dependence ) . In the important special case when dispersion is measured using the average absolute deviation from the median , this novel approach yields a linear time algorithm . Empirical evaluation on a wide range of datasets shows that , when used within branch-and-bound search , this approach is highly efficient and indeed discovers subgroups with much smaller errors .
Energy efficient buildings require high quality standards for all their technical equipment to enable their efficient and successful operation and management . Building simulations enable engineers to design integrated HVAC systems with complex building automation systems to control all their technical functions . Numerous studies show that especially these supposedly innovative buildings often do not reach their energy efficiency targets when in operation . Key reasons for the suboptimal performance are imprecise functional descriptions and a lack of commissioning and monitoring of the technical systems that leave suboptimal operation undetected . In the research project " Energy Navigator " we create a web-based platform that enables engineers to create a comprehensive and precise functional description for the buildings services . The system reuses this functional description - written in an appropriate domain specific language - to control the building operation , to signal malfunctions or faults , and in particular to measure energy efficiency over time . The innovative approach of the platform is the combination of design and control within one artifact linking the phases of design and operation and improving the cost effectiveness for both services . The paper will describe the concept of the platform , the technical innovation and first application examples of the research project .
The enhanced Bayesian network ( eBN ) methodology described in the companion paper facilitates the assessment of reliability and risk of engineering systems when information about the system evolves in time . We present the application of the eBN ( a ) to the assessment of the life-cycle reliability of a structural system , ( b ) to the optimization of a decision on performing measurements in that structural system , and ( c ) to the risk assessment of an infrastructure system subject to natural hazards and deterioration of constituent structures . In all applications , observations of system performances or the hazards are made at various points in time and the eBN efficiently includes these observations in the analysis to provide an updated probabilistic model of the system at all times .
Gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications . For the problem of recovering the graphical structure , information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso , which is a likelihood penalization technique . In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow . Compared to earlier work on the regression case , our treatment allows for growth in the number of non-zero parameters in the true model , which is necessary in order to cover connected graphs . We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso , and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n .
We develop a flexible framework for modeling high-dimensional imaging data observed longitudinally . The approach decomposes the observed variability of repeatedly measured high-dimensional observations into three additive components : a subject-specific imaging random intercept that quantifies the cross-sectional variability , a subject-specific imaging slope that quantifies the dynamic irreversible deformation over multiple realizations , and a subject-visit-specific imaging deviation that quantifies exchangeable effects between visits . The proposed method is very fast , scalable to studies including ultrahigh-dimensional data , and can easily be adapted to and executed on modest computing infrastructures . The method is applied to the longitudinal analysis of diffusion tensor imaging ( DTI ) data of the corpus callosum of multiple sclerosis ( MS ) subjects . The study includes $000$ subjects observed at $000$ visits . For each subject and visit the study contains a registered DTI scan of the corpus callosum at roughly 00 , 000 voxels .
In this paper , a novel method of designing a codebook for noise robust speaker identification purpose utilizing Genetic Algorithm has been proposed . Wiener filter has been used to remove the background noises from the source speech utterances . Speech features have been extracted using standard speech parameterization method such as LPC , LPCC , RCC , MFCC , ( delta ) MFCC and ( delta ) ( delta ) MFCC . For each of these techniques , the performance of the proposed system has been compared . In this codebook design method , Genetic Algorithm has the capability of getting global optimal result and hence improves the quality of the codebook . Comparing with the NOIZEOUS speech database , the experimental result shows that 00 . 00 percent accuracy has been achieved .
We prove a Fife-like characterization of the infinite binary ( 0/0 ) -power-free words , by giving a finite automaton of 00 states that encodes all such words . As a consequence , we characterize all such words that are 0-automatic .
The Memory-Centred Cognition perspective places an active association substrate at the heart of cognition , rather than as a passive adjunct . Consequently , it places prediction and priming on the basis of prior experience to be inherent and fundamental aspects of processing . Social interaction is taken here to minimally require contingent and co-adaptive behaviours from the interacting parties . In this contribution , I seek to show how the memory-centred cognition approach to cognitive architectures can provide an means of addressing these functions . A number of example implementations are briefly reviewed , particularly focusing on multi-modal alignment as a function of experience-based priming . While there is further refinement required to the theory , and implementations based thereon , this approach provides an interesting alternative perspective on the foundations of cognitive architectures to support robots engage in social interactions with humans .
In cyber-physical systems such as in-vehicle wireless sensor networks , a large number of sensor nodes continually generate measurements that should be received by other nodes such as actuators in a regular fashion . Meanwhile , energy-efficiency is also important in wireless sensor networks . Motivated by these , we develop scheduling policies which are energy efficient and simultaneously maintain " regular " deliveries of packets . A tradeoff parameter is introduced to balance these two conflicting objectives . We employ a Markov Decision Process ( MDP ) model where the state of each client is the time-since-last-delivery of its packet , and reduce it into an equivalent finite-state MDP problem . Although this equivalent problem can be solved by standard dynamic programming techniques , it suffers from a high-computational complexity . Thus we further pose the problem as a restless multi-armed bandit problem and employ the low-complexity Whittle Index policy . It is shown that this problem is indexable and the Whittle indexes are derived . Also , we prove the Whittle Index policy is asymptotically optimal and validate its optimality via extensive simulations .
This paper considers an optimal task allocation problem for human robot collaboration in human robot systems with persistent tasks . Such human robot systems consist of human operators and intelligent robots collaborating with each other to accomplish complex tasks that cannot be done by either part alone . The system objective is to maximize the probability of successfully executing persistent tasks that are formulated as linear temporal logic specifications and minimize the average cost between consecutive visits of a particular proposition . This paper proposes to model the human robot collaboration under a framework with the composition of multiple Markov Decision Process ( MDP ) with possibly unknown transition probabilities , which characterizes how human cognitive states , such as human trust and fatigue , stochastically change with the robot performance . Under the unknown MDP models , an algorithm is developed to learn the model and obtain an optimal task allocation policy that minimizes the expected average cost for each task cycle and maximizes the probability of satisfying linear temporal logic constraints . Moreover , this paper shows that the difference between the optimal policy based on the learned model and that based on the underlying ground truth model can be bounded by arbitrarily small constant and large confidence level with sufficient samples . The case study of an assembly process demonstrates the effectiveness and benefits of our proposed learning based human robot collaboration .
Bootstrapping is a popular and computationally demanding resampling method used for measuring the accuracy of sample estimates and assisting with statistical inference . R is a freely available language and environment for statistical computing popular with biostatisticians for genomic data analyses . A survey of such R users highlighted its implementation of bootstrapping as a prime candidate for parallelization to overcome computational bottlenecks . The Simple Parallel R Interface ( SPRINT ) is a package that allows R users to exploit high performance computing in multi-core desktops and supercomputers without expert knowledge of such systems . This paper describes the parallelization of bootstrapping for inclusion in the SPRINT R package . Depending on the complexity of the bootstrap statistic and the number of resamples , this implementation has close to optimal speed up on up to 00 nodes of a supercomputer and close to 000 on 000 nodes . This performance in a multi-node setting compares favourably with an existing parallelization option in the native R implementation of bootstrapping .
The Alexandria system under development at IBM Research provides an extensible framework and platform for supporting a variety of big-data analytics and visualizations . The system is currently focused on enabling rapid exploration of text-based social media data . The system provides tools to help with constructing " domain models " ( i . e . , families of keywords and extractors to enable focus on tweets and other social media documents relevant to a project ) , to rapidly extract and segment the relevant social media and its authors , to apply further analytics ( such as finding trends and anomalous terms ) , and visualizing the results . The system architecture is centered around a variety of REST-based service APIs to enable flexible orchestration of the system capabilities ; these are especially useful to support knowledge-worker driven iterative exploration of social phenomena . The architecture also enables rapid integration of Alexandria capabilities with other social media analytics system , as has been demonstrated through an integration with IBM Research ' s SystemG . This paper describes a prototypical usage scenario for Alexandria , along with the architecture and key underlying analytics .
Genome-wide association studies ( GWAS ) offer new opportunities to identify genetic risk factors for Alzheimer ' s disease ( AD ) . Recently , collaborative efforts across different institutions emerged that enhance the power of many existing techniques on individual institution data . However , a major barrier to collaborative studies of GWAS is that many institutions need to preserve individual data privacy . To address this challenge , we propose a novel distributed framework , termed Local Query Model ( LQM ) to detect risk SNPs for AD across multiple research institutions . To accelerate the learning process , we propose a Distributed Enhanced Dual Polytope Projection ( D-EDPP ) screening rule to identify irrelevant features and remove them from the optimization . To the best of our knowledge , this is the first successful run of the computationally intensive model selection procedure to learn a consistent model across different institutions without compromising their privacy while ranking the SNPs that may collectively affect AD . Empirical studies are conducted on 000 subjects with 0 . 0 million SNP features which are distributed across three individual institutions . D-EDPP achieved a 00-fold speed-up by effectively identifying irrelevant features .
Teachers have tried to teach their students by introducing text books along with verbal instructions in traditional education system . However , teaching and learning methods could be changed for developing Information and Communication Technology . It ' s time to adapt students with interactive learning system so that they can improve their learning , catching , and memorizing capabilities . It is indispensable to create high quality and realistic leaning environment for students . Visual learning can be easier to understand and deal with their learning . We developed visual learning materials in the form of video for students of primary level using different multimedia application tools . The objective of this paper is to examine the impact of students abilities to acquire new knowledge or skills through visual learning materials and blended leaning that is integration of visual learning materials with teachers instructions . We visited a primary school in Dhaka city for this study and conducted teaching with three different groups of students , ( i ) teacher taught students by traditional system on same materials and marked level of students ability to adapt by a set of questions , ( ii ) another group was taught with only visual learning material and assessment was done with 00 questionnaires , ( iii ) the third group was taught with the video of solar system combined with teachers instructions and assessed with the same questionnaires . This integration of visual materials with verbal instructions is a blended approach of learning . The interactive blended approach greatly promoted students ability of acquisition of knowledge and skills . Students response and perception were very positive towards the blended technique than the other two methods . This interactive blending leaning system may be an appropriate method especially for school children .
In this paper , we study the problem of lossless universal source coding for stationary memoryless sources on countably infinite alphabets . This task is generally not achievable without restricting the class of sources over which universality is desired . Building on our prior work , we propose natural families of sources characterized by a common dominating envelope . We particularly emphasize the notion of adaptivity , which is the ability to perform as well as an oracle knowing the envelope , without actually knowing it . This is closely related to the notion of hierarchical universal source coding , but with the important difference that families of envelope classes are not discretely indexed and not necessarily nested . Our contribution is to extend the classes of envelopes over which adaptive universal source coding is possible , namely by including max-stable ( heavy-tailed ) envelopes which are excellent models in many applications , such as natural language modeling . We derive a minimax lower bound on the redundancy of any code on such envelope classes , including an oracle that knows the envelope . We then propose a constructive code that does not use knowledge of the envelope . The code is computationally efficient and is structured to use an {E}xpanding {T}hreshold for {A}uto-{C}ensoring , and we therefore dub it the \textsc{ETAC}-code . We prove that the \textsc{ETAC}-code achieves the lower bound on the minimax redundancy within a factor logarithmic in the sequence length , and can be therefore qualified as a near-adaptive code over families of heavy-tailed envelopes . For finite and light-tailed envelopes the penalty is even less , and the same code follows closely previous results that explicitly made the light-tailed assumption . Our technical results are founded on methods from regular variation theory and concentration of measure .
We investigate the problem of counting co-authorhip in order to quantify the impact and relevance of scientific research output through normalized \textit{h-index} and \textit{g-index} . We use the papers whose authors belong to a subset of full professors of the Italian Settore Scientifico Disciplinare ( SSD ) FIS00 - Experimental Physics . In this SSD two populations , characterized by the number of co-authors of each paper , are roughly present . The total number of citations for each individuals , as well as their h-index and g-index , strongly depends on the average number of co-authors . We show that , in order to remove the dependence of the various indices on the two populations , the best way to define a fractional counting of autorship is to divide the number of citations received by each paper by the square root of the number of co-authors . This allows us to obtain some information which can be used for a better understanding of the scientific knowledge made through the process of writing and publishing papers .
We present an adaptation of the Kato-Temple inequality for bounding perturbations of eigenvalues with applications to statistical inference for random graphs , specifically hypothesis testing and change-point detection . We obtain explicit high-probability bounds for the individual distances between certain signal eigenvalues of a graph ' s adjacency matrix and the corresponding eigenvalues of the model ' s edge probability matrix , even when the latter eigenvalues have multiplicity . Our results extend more broadly to the perturbation of singular values in the presence of quite general random matrix noise .
Given an $n$-ary $k-$valued function $f$ , $gap ( f ) $ denotes the minimal number of essential variables in $f$ which become fictive when identifying any two distinct essential variables in $f$ . We particularly solve a problem concerning the explicit determination of $n$-ary $k-$valued functions $f$ with $0\leq gap ( f ) \leq n\leq k$ . Our methods yield new combinatorial results about the number of such functions .
Bovine tuberculosis ( TB ) poses a serious threat for agricultural industry in several countries , it involves potential interactions between wildlife and cattle and creates societal problems in terms of human-wildlife conflict . This study addresses connectedness network analysis , the spatial , and temporal dynamics of TB between cattle in farms and the European badger ( Meles meles ) using a large dataset generated by a calibrated agent based model . Results showed that infected network connectedness was lower in badgers than in cattle . The contribution of an infected individual to the mean distance of disease spread over time was considerably lower for badger than cattle ; badgers mainly spread the disease locally while cattle infected both locally and across longer distances . The majority of badger-induced infections occurred when individual badgers leave their home sett , and this was positively correlated with badger population growth rates . Point pattern analysis indicated aggregation in the spatial pattern of TB prevalence in badger setts across all scales . The spatial distribution of farms that were not TB free was aggregated at different scales than the spatial distribution of infected badgers and became random at larger scales . The spatial cross correlation between infected badger setts and infected farms revealed that generally infected setts and farms do not coexist except at few scales . Temporal autocorrelation detected a two year infection cycle for badgers , while there was both within the year and longer cycles for infected cattle . Temporal cross correlation indicated that infection cycles in badgers and cattle are negatively correlated . The implications of these results for understanding the dynamics of the disease are discussed .
Relations between categorical variables can be analyzed conveniently by multiple correspondence analysis ( MCA ) . %It is well suited to discover relations that may exist between categories of different variables . The graphical representation of MCA results in so-called biplots makes it easy to interpret the most important associations . However , a major drawback of MCA is that it does not have an underlying probability model for an individual selecting a category on a variable . In this paper , we propose such probability model called multinomial multiple correspondence analysis ( MMCA ) that combines the underlying low-rank representation of MCA with maximum likelihood . An efficient majorization algorithm that uses an elegant bound for the second derivative is derived to estimate the parameters . The proposed model can easily lead to overfitting causing some of the parameters to wander of to infinity . We add the nuclear norm penalty to counter this issue and discuss ways of selecting regularization parameters . The proposed approach is well suited to study and vizualise the dependences for high dimensional data .
The Simplex Tree ( ST ) is a recently introduced data structure that can represent abstract simplicial complexes of any dimension and allows efficient implementation of a large range of basic operations on simplicial complexes . In this paper , we show how to optimally compress the Simplex Tree while retaining its functionalities . In addition , we propose two new data structures called the Maximal Simplex Tree ( MxST ) and the Simplex Array List ( SAL ) . We analyze the compressed Simplex Tree , the Maximal Simplex Tree , and the Simplex Array List under various settings .
Recent experimental advances in neuroscience have opened new vistas into the immense complexity of neuronal networks . This proliferation of data challenges us on two parallel fronts . First , how can we form adequate theoretical frameworks for understanding how dynamical network processes cooperate across widely disparate spatiotemporal scales to solve important computational problems ? And second , how can we extract meaningful models of neuronal systems from high dimensional datasets ? To aid in these challenges , we give a pedagogical review of a collection of ideas and theoretical methods arising at the intersection of statistical physics , computer science and neurobiology . We introduce the interrelated replica and cavity methods , which originated in statistical physics as powerful ways to quantitatively analyze large highly heterogeneous systems of many interacting degrees of freedom . We also introduce the closely related notion of message passing in graphical models , which originated in computer science as a distributed algorithm capable of solving large inference and optimization problems involving many coupled variables . We then show how both the statistical physics and computer science perspectives can be applied in a wide diversity of contexts to problems arising in theoretical neuroscience and data analysis . Along the way we discuss spin glasses , learning theory , illusions of structure in noise , random matrices , dimensionality reduction , and compressed sensing , all within the unified formalism of the replica method . Moreover , we review recent conceptual connections between message passing in graphical models , and neural computation and learning . Overall , these ideas illustrate how statistical physics and computer science might provide a lens through which we can uncover emergent computational functions buried deep within the dynamical complexities of neuronal networks .
We study sequential programs that are instruction sequences with dynamically instantiated instructions . We define the meaning of such programs in two different ways . In either case , we give a translation by which each program with dynamically instantiated instructions is turned into a program without them that exhibits on execution the same behaviour by interaction with some service . The complexity of the translations differ considerably , whereas the services concerned are equally simple . However , the service concerned in the case of the simpler translation is far more powerful than the service concerned in the other case .
Deep learning has proven itself as a successful set of models for learning useful semantic representations of data . These , however , are mostly implicitly learned as part of a classification task . In this paper we propose the triplet network model , which aims to learn useful representations by distance comparisons . A similar model was defined by Wang et al . ( 0000 ) , tailor made for learning a ranking for image information retrieval . Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor , the Siamese network . We also discuss future possible usage as a framework for unsupervised learning .
Observability of complex systems/networks is the focus of this paper , which is shown to be closely related to the concept of contraction . Indeed , for observable network tracking it is necessary/sufficient to have one node in each contraction measured . Therefore , nodes in a contraction are equivalent to recover for loss of observability , implying that contraction size is a key factor for observability recovery . Here , using a polynomial order contraction detection algorithm , we analyze the distribution of contractions , studying its relation with key network properties . Our results show that contraction size is related to network clustering coefficient and degree heterogeneity . Particularly , in networks with power-law degree distribution , if the clustering coefficient is high there are less contractions with smaller size on average . The implication is that estimation/tracking of such systems requires less number of measurements , while their observational recovery is more restrictive in case of sensor failure . Further , in Small-World networks higher degree heterogeneity implies that there are more contractions with smaller size on average . Therefore , the estimation of representing system requires more measurements , and also the recovery of measurement failure is more limited . These results imply that one can tune the properties of synthetic networks to alleviate their estimation/observability recovery .
Knowing how many people occupy a building , and where they are located , is a key component of smart building services . Commercial , industrial and residential buildings often incorporate systems used to determine occupancy . However , relatively simple sensor technology and control algorithms limit the effectiveness of smart building services . In this paper we propose to replace sensor technology with time series models that can predict the number of occupants at a given location and time . We use Wi-Fi data sets readily available in abundance for smart building services and train Auto Regression Integrating Moving Average ( ARIMA ) models and Long Short-Term Memory ( LSTM ) time series models . As a use case scenario of smart building services , these models allow forecasting of the number of people at a given time and location in 00 , 00 and 00 minutes time intervals at building as well as Access Point ( AP ) level . For LSTM , we build our models in two ways : a separate model for every time scale , and a combined model for the three time scales . Our experiments show that LSTM combined model reduced the computational resources with respect to the number of neurons by 00 . 00 % for the AP level , and by 00 . 00 % for the building level . Further , the root mean square error ( RMSE ) was reduced by 00 . 0% - 00 . 0% for LSTM in comparison to ARIMA for the building levels models and by 00 . 0% - 00% for the AP level models .
In our project we introduce open source platform Digital Personal Assistant ( DPA ) . We talk about architecture of the platform and demonstrate potential of DPA on example interaction with Smart Home manager .
The paper considers functional linear regression , where scalar responses $Y_0 , . . . , Y_n$ are modeled in dependence of random functions $X_0 , . . . , X_n$ . We propose a smoothing splines estimator for the functional slope parameter based on a slight modification of the usual penalty . Theoretical analysis concentrates on the error in an out-of-sample prediction of the response for a new random function $X_{n+0}$ . It is shown that rates of convergence of the prediction error depend on the smoothness of the slope function and on the structure of the predictors . We then prove that these rates are optimal in the sense that they are minimax over large classes of possible slope functions and distributions of the predictive curves . For the case of models with errors-in-variables the smoothing spline estimator is modified by using a denoising correction of the covariance matrix of discretized curves . The methodology is then applied to a real case study where the aim is to predict the maximum of the concentration of ozone by using the curve of this concentration measured the preceding day .
Frequentist conditions for asymptotic suitability of Bayesian procedures focus on lower bounds for prior mass in Kullback-Leibler neighbourhoods of the data distribution . The goal of this paper is to investigate the flexibility in criteria for posterior consistency with i . i . d . data . We formulate a versatile posterior consistency theorem that applies both to well- and mis-specified models and which we use to re-derive Schwartz ' s theorem , consider Kullback-Leibler consistency and formulate consistency theorems in which priors charge metric balls . It is generalized to sieved models with Barron ' s negligible prior mass condition and to separable models with variations on Walker ' s consistency theorem . Results also apply to marginal semi-parametric consistency : support boundary estimation is considered explicitly and consistency is proved in a model for which Kullback-Leibler priors do not exist . Other examples include consistent density estimation in mixture models with Dirichlet or Gibbs-type priors of full weak support . Regarding posterior convergence at a rate , it is shown that under a mild integrability condition , the second-order Ghosal-Ghosh-van der Vaart prior mass condition can be relaxed to a lower bound to the prior mass in Schwartz ' s Kullback-Leibler neighbourhoods . The posterior rate of convergence is derived in a simple , parametric model for heavy-tailed distributions in which the Ghosal-Ghosh-van der Vaart condition cannot be satisfied by any prior .
This article studies the estimation of the causal effect of a time-varying treatment on time-to-an-event or on some other continuously distributed outcome . The paper applies to the situation where treatment is repeatedly adapted to time-dependent patient characteristics . The treatment effect cannot be estimated by simply conditioning on these time-dependent patient characteristics , as they may themselves be indications of the treatment effect . This time-dependent confounding is common in observational studies . Robins [ ( 0000 ) Biometrika 00 000--000 , ( 0000b ) Encyclopedia of Biostatistics 0 0000--0000] has proposed the so-called structural nested models to estimate treatment effects in the presence of time-dependent confounding . In this article we provide a conceptual framework and formalization for structural nested models in continuous time . We show that the resulting estimators are consistent and asymptotically normal . Moreover , as conjectured in Robins [ ( 0000b ) Encyclopedia of Biostatistics 0 0000--0000] , a test for whether treatment affects the outcome of interest can be performed without specifying a model for treatment effect . We illustrate the ideas in this article with an example .
We introduce a novel technique for verification and model synthesis of sequential programs . Our technique is based on learning a regular model of the set of feasible paths in a program , and testing whether this model contains an incorrect behavior . Exact learning algorithms require checking equivalence between the model and the program , which is a difficult problem , in general undecidable . Our learning procedure is therefore based on the framework of probably approximately correct ( PAC ) learning , which uses sampling instead and provides correctness guarantees expressed using the terms error probability and confidence . Besides the verification result , our procedure also outputs the model with the said correctness guarantees . Obtained preliminary experiments show encouraging results , in some cases even outperforming mature software verifiers .
In this work , we examine two approaches to interprocedural data-flow analysis of Sharir and Pnueli in terms of precision : the functional and the call-string approach . In doing so , not only the theoretical best , but all solutions are regarded which occur when using abstract interpretation or widening additionally . It turns out that the solutions of both approaches coincide . This property is preserved when using abstract interpretation ; in the case of widening , a comparison of the results is not always possible .
We present an investment model integrated with trust-reputation mechanisms where agents interact with each other to establish investment projects . We investigate the establishment of investment projects , the influence of the interaction between agents in the evolution of the distribution of wealth , as well as the formation of common investment networks and some of their properties . Simulation results show that the wealth distribution presents a power law in its tail . Also , it is shown that the trust and reputation mechanism presented leads to the establishment of networks among agents , which present some of the typical characteristics of real-life networks like a high clustering coefficient and short average path length .
Vine copulas constitute a flexible way for modeling of dependences using only pair copulas as building blocks . The pair-copula constructions introduced by Joe ( 0000 ) are able to encode more types of dependences in the same time since they can be expressed as a product of different types of bi-variate copulas . The Regular-vine structures ( R-vines ) , as pair copulas corresponding to a sequence of trees , have been introduced by Bedford and Cooke ( 0000 , 0000 ) and further explored by Kurowicka and Cooke ( 0000 ) . The complexity of these models strongly increases in larger dimensions . Therefore the so called truncated R-vines were introduced in Brechmann et al . ( 0000 ) . In this paper we express the Regular-vines using a special type of hypergraphs , which encodes the conditional independences .
Some astronomy projects require a blind search through a vast number of hypotheses to detect objects of interest . The number of hypotheses to test can be in the billions . A naive blind search over every single hypothesis would be far too costly computationally . We propose a hierarchical scheme for blind search , using various " resolution " levels . At lower resolution levels , " regions " of interest in the search space are singled out with a low computational cost . These regions are refined at intermediate resolution levels and only the most promising candidates are finally tested at the original fine resolution . The optimal search strategy is found by dynamic programming . We demonstrate the procedure for pulsar search from satellite gamma-ray observations and show that the power of the naive blind search can almost be matched with the hierarchical scheme while reducing the computational burden by more than three orders of magnitude .
We give an exponential lower bound for the Graver complexity of the incidence matrix of a complete bipartite graph of arbitrary size . Our result is a generalization of the result by Berstein and Onn ( 0000 ) for 0xr complete bipartite graphs , r \ge 0 .
We consider the high-dimensional linear regression model $Y = X \beta^0 + \epsilon$ with Gaussian noise $\epsilon$ and Gaussian random design $X$ . We assume that $\Sigma_0 : = {{\rm I\hskip-0 . 00em E}} X^T X / n$ is non-singular and write its inverse as $\Theta^0 : = \Sigma_0^{-0}$ . The parameter of interest is the first component $\beta_0^0$ of $\beta^0$ . We show that the asymptotic variance of a de-biased Lasso estimator can be smaller than $\Theta_{0 , 0}^0$ , under the conditions : $\beta^0$ is sparse in the sense that it has $s_0 = o ( \sqrt n / \log p ) $ non-zero entries and the first column $\Theta_0^0$ of $\Theta^0$ is not sparse . As by-product , we obtain some results for the Lasso estimator of $\beta^0$ for cases where $\beta^0$ is not sparse .
Whitening , or sphering , is a common preprocessing step in statistical analysis to transform random variables to orthogonality . However , due to rotational freedom there are infinitely many possible whitening procedures . Consequently , there is a diverse range of sphering methods in use , for example based on principal component analysis ( PCA ) , Cholesky matrix decomposition and zero-phase component analysis ( ZCA ) , among others . Here we provide an overview of the underlying theory and discuss five natural whitening procedures . Subsequently , we demonstrate that investigating the cross-covariance and the cross-correlation matrix between sphered and original variables allows to break the rotational invariance and to identify optimal whitening transformations . As a result we recommend two particular approaches : ZCA-cor whitening to produce sphered variables that are maximally similar to the original variables , and PCA-cor whitening to obtain sphered variables that maximally compress the original variables .
Many real-world phenomena can be modelled as dynamic optimization problems . In such cases , the environment problem changes dynamically and therefore , conventional methods are not capable of dealing with such problems . In this paper , a novel multi-swarm cellular particle swarm optimization algorithm is proposed by clustering and local search . In the proposed algorithm , the search space is partitioned into cells , while the particles identify changes in the search space and form clusters to create sub-swarms . Then a local search is applied to improve the solutions in the each cell . Simulation results for static standard benchmarks and dynamic environments show superiority of the proposed method over other alternative approaches .
We present a new model that describes the process of electing a group of representatives ( e . g . , a parliament ) for a group of voters . In this model , called the voting committee model , the elected group of representatives runs a number of ballots to make final decisions regarding various issues . The satisfaction of voters comes from the final decisions made by the elected committee . Our results suggest that depending on a decision system used by the committee to make these final decisions , different multi-winner election rules are most suitable for electing the committee . Furthermore , we show that if we allow not only a committee , but also an election rule used to make final decisions , to depend on the voters ' preferences , we can obtain an even better representation of the voters .
Stochastic gradient algorithms are more and more studied since they can deal efficiently and online with large samples in high dimensional spaces . In this paper , we first establish a Central Limit Theorem for these estimates as well as for their averaged version in general Hilbert spaces . Moreover , since having the asymptotic normality of estimates is often unusable without an estimation of the asymptotic variance , we introduce a new recursive algorithm for estimating this last one , and we establish its almost sure rate of convergence as well as its rate of convergence in quadratic mean . Finally , two examples consisting in estimating the parameters of the logistic regression and estimating geometric quantiles are given .
Recently , graphs have been widely used to represent many different kinds of real world data or observations such as social networks , protein-protein networks , road networks , and so on . In many cases , each node in a graph is associated with a set of its attributes and it is critical to not only consider the link structure of a graph but also use the attribute information to achieve more meaningful results in various graph mining tasks . Most previous works with attributed graphs take into ac- count attribute relationships only between individually connected nodes . However , it should be greatly valuable to find out which sets of attributes are associated with each other and whether they are statistically significant or not . Mining such significant associations , we can uncover novel relationships among the sets of attributes in the graph . We propose an algorithm that can find those attribute associations efficiently and effectively , and show experimental results that confirm the high applicability of the proposed algorithm .
Mining user opinion from Micro-Blogging has been extensively studied on the most popular social networking sites such as Twitter and Facebook in the U . S . , but few studies have been done on Micro-Blogging websites in other countries ( e . g . China ) . In this paper , we analyze the social opinion influence on Tencent , one of the largest Micro-Blogging websites in China , endeavoring to unveil the behavior patterns of Chinese Micro-Blogging users . This paper proposes a Topic-Level Opinion Influence Model ( TOIM ) that simultaneously incorporates topic factor and social direct influence in a unified probabilistic framework . Based on TOIM , two topic level opinion influence propagation and aggregation algorithms are developed to consider the indirect influence : CP ( Conservative Propagation ) and NCP ( None Conservative Propagation ) . Users ' historical social interaction records are leveraged by TOIM to construct their progressive opinions and neighbors ' opinion influence through a statistical learning process , which can be further utilized to predict users ' future opinions on some specific topics . To evaluate and test this proposed model , an experiment was designed and a sub-dataset from Tencent Micro-Blogging was used . The experimental results show that TOIM outperforms baseline methods on predicting users ' opinion . The applications of CP and NCP have no significant differences and could significantly improve recall and F0-measure of TOIM .
We overview some results on distributed learning with focus on a family of recently proposed algorithms known as non-Bayesian social learning . We consider different approaches to the distributed learning problem and its algorithmic solutions for the case of finitely many hypotheses . The original centralized problem is discussed at first , and then followed by a generalization to the distributed setting . The results on convergence and convergence rate are presented for both asymptotic and finite time regimes . Various extensions are discussed such as those dealing with directed time-varying networks , Nesterov ' s acceleration technique and a continuum sets of hypothesis .
Given some of the recent advances in Distributed Hash Table ( DHT ) based Peer-To-Peer ( P0P ) systems we ask the following questions : Are there applications where unstructured queries are still necessary ( i . e . , the underlying queries do not efficiently map onto any structured framework ) , and are there unstructured P0P systems that can deliver the high bandwidth and computing performance necessary to support such applications . Toward this end , we consider an image search application which supports queries based on image similarity metrics , such as color histogram intersection , and discuss why in this setting , standard DHT approaches are not directly applicable . We then study the feasibility of implementing such an image search system on two different unstructured P0P systems : power-law topology with percolation search , and an optimized super-node topology using structured broadcasts . We examine the average and maximum values for node bandwidth , storage and processing requirements in the percolation and super-node models , and show that current high-end computers and high-speed links have sufficient resources to enable deployments of large-scale complex image search systems .
Often , when dealing with real-world recognition problems , we do not need , and often cannot have , knowledge of the entire set of possible classes that might appear during operational testing . Moreover , sometimes some of these classes may be ill-sampled , not sampled at all or undefined . In such cases , we need to think of robust classification methods able to deal with the " unknown " and properly reject samples belonging to classes never seen during training . Notwithstanding , almost all existing classifiers to date were mostly developed for the closed-set scenario , i . e . , the classification setup in which it is assumed that all test samples belong to one of the classes with which the classifier was trained . In the open-set scenario , however , a test sample can belong to none of the known classes and the classifier must properly reject it by classifying it as unknown . In this work , we extend upon the well-known Support Vector Machines ( SVM ) classifier and introduce the Specialized Support Vector Machine ( SSVM ) , which is suitable for recognition in open-set setups . SSVM balances the empirical risk and the risk of the unknown and ensures that the region of the feature space in which a test sample would be classified as known ( one of the known classes ) is always bounded , ensuring a finite risk of the unknown . The same cannot be guaranteed by the traditional SVM formulation , even when using the Radial Basis Function ( RBF ) kernel . In this work , we also highlight the properties of the SVM classifier related to the open-set scenario , and provide necessary and sufficient conditions for an RBF SVM to have bounded open-space risk . An extensive set of experiments compares the proposed method with existing solutions in the literature for open-set recognition and the reported results show its effectiveness .
We introduce a class of logarithmic Lambert W random variables for a specific family of distributions . In particular , we characterize the log-Lambert W random variables for chi-squared distributions which naturally appear in the likelihood based inference of normal random variables .
We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout . We extend Variational Dropout to the case when dropout rates are unbounded , propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight . Interestingly , it leads to extremely sparse solutions both in fully-connected and convolutional layers . This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages . We reduce the number of parameters up to 000 times on LeNet architectures and up to 00 times on VGG-like networks with a negligible decrease of accuracy .
In this paper we study goodness-of-fit testing of single-index models . The large sample behavior of certain score-type test statistics is investigated . As a by-product , we obtain asymptotically distribution-free maximin tests for a large class of local alternatives . Furthermore , characteristic function based goodness-of-fit tests are proposed which are omnibus and able to detect peak alternatives . Simulation results indicate that the approximation through the limit distribution is acceptable already for moderate sample sizes . Applications to two real data sets are illustrated .
Recent researches on neural network have shown great advantage in computer vision over traditional algorithms based on handcrafted features and models . Neural network is now widely adopted in regions like image , speech and video recognition . But the great computation and storage complexity of neural network based algorithms poses great difficulty on its application . CPU platforms are hard to offer enough computation capacity . GPU platforms are the first choice for neural network process because of its high computation capacity and easy to use development frameworks . On the other hand , FPGA based neural network accelerator is becoming a research topic . Because specific designed hardware is the next possible solution to surpass GPU in speed and energy efficiency . Various FPGA based accelerator designs have been proposed with software and hardware optimization techniques to achieve high speed and energy efficiency . In this paper , we give an overview of previous work on neural network accelerators based on FPGA and summarize the main techniques used . Investigation from software to hardware , from circuit level to system level is carried out to complete analysis of FPGA based neural network accelerator design and serves as a guide to future work .
In this paper , we consider the Integrated Completed Likelihood ( ICL ) as a useful criterion for estimating the number of changes in the underlying distribution of data in problems where detecting the precise location of these changes is the main goal . The exact computation of the ICL requires O ( Kn0 ) operations ( with K the number of segments and n the number of data-points ) which is prohibitive in many practical situations with large sequences of data . We describe a framework to estimate the ICL with O ( Kn ) complexity . Our approach is general in the sense that it can accommodate any given model distribution . We checked the run-time and validity of our approach on simulated data and demonstrate its good performance when analyzing real Next-Generation Sequencing ( NGS ) data using a negative binomial model .
We consider perturbations of the complex quadratic map $ z \to z^0 +c$ and corresponding changes in their quasi-Mandelbrot sets . Depending on particular perturbation , visual forms of quasi-Mandelbrot set changes either sharply ( when the perturbation reaches some critical value ) or continuously . In the latter case we have a smooth transition from the classical form of the set to some forms , constructed from mostly linear structures , as it is typical for two-dimensional real number dynamics . Two examples of continuous evolution of the quasi-Mandelbrot set are described .
We tackle the problem of robust dialogue processing from the perspective of language engineering . We propose an agent-oriented architecture that allows us a flexible way of composing robust processors . Our approach is based on Shoham ' s Agent Oriented Programming ( AOP ) paradigm . We will show how the AOP agent model can be enriched with special features and components that allow us to deal with classical problems of dialogue understanding .
A fixed-mobile bigraph G is a bipartite graph such that the vertices of one partition set are given with fixed positions in the plane and the mobile vertices of the other part , together with the edges , must be added to the drawing . We assume that G is planar and study the problem of finding , for a given k >= 0 , a planar poly-line drawing of G with at most k bends per edge . In the most general case , we show NP-hardness . For k=0 and under additional constraints on the positions of the fixed or mobile vertices , we either prove that the problem is polynomial-time solvable or prove that it belongs to NP . Finally , we present a polynomial-time testing algorithm for a certain type of " layered " 0-bend drawings .
Logic Programming languages and combinational circuit synthesis tools share a common " combinatorial search over logic formulae " background . This paper attempts to reconnect the two fields with a fresh look at Prolog encodings for the combinatorial objects involved in circuit synthesis . While benefiting from Prolog ' s fast unification algorithm and built-in backtracking mechanism , efficiency of our search algorithm is ensured by using parallel bitstring operations together with logic variable equality propagation , as a mapping mechanism from primary inputs to the leaves of candidate Leaf-DAGs implementing a combinational circuit specification . After an exhaustive expressiveness comparison of various minimal libraries , a surprising first-runner , Strict Boolean Inequality " < " together with constant function " 0 " also turns out to have small transistor-count implementations , competitive to NAND-only or NOR-only libraries . As a practical outcome , a more realistic circuit synthesizer is implemented that combines rewriting-based simplification of ( < , 0 ) circuits with exhaustive Leaf-DAG circuit search . Keywords : logic programming and circuit design , combinatorial object generation , exact combinational circuit synthesis , universal boolean logic libraries , symbolic rewriting , minimal transistor-count circuit synthesis
An electroencephalography ( EEG ) based brain activity recognition is a fundamental field of study for a number of significant applications such as intention prediction , appliance control , and neurological disease diagnosis in smart home and smart healthcare domains . Existing techniques mostly focus on binary brain activity recognition for a single person , which limits their deployment in wider and complex practical scenarios . Therefore , multi-person and multi-class brain activity recognition has obtained popularity recently . Another challenge faced by brain activity recognition is the low recognition accuracy due to the massive noises and the low signal-to-noise ratio in EEG signals . Moreover , the feature engineering in EEG processing is time-consuming and highly re- lies on the expert experience . In this paper , we attempt to solve the above challenges by proposing an approach which has better EEG interpretation ability via raw Electroencephalography ( EEG ) signal analysis for multi-person and multi-class brain activity recognition . Specifically , we analyze inter-class and inter-person EEG signal characteristics , based on which to capture the discrepancy of inter-class EEG data . Then , we adopt an Autoencoder layer to automatically refine the raw EEG signals by eliminating various artifacts . We evaluate our approach on both a public and a local EEG datasets and conduct extensive experiments to explore the effect of several factors ( such as normalization methods , training data size , and Autoencoder hidden neuron size ) on the recognition results . The experimental results show that our approach achieves a high accuracy comparing to competitive state-of-the-art methods , indicating its potential in promoting future research on multi-person EEG recognition .
We present an XML-based simulation authoring environment . The proposed description language allows to describe mathematical objects such as systems of ordinary differential equations , partial differential equations in two dimensions , or simple curves and surfaces . It also allows to describe the parameters on which these objects depend . This language is independent of the target software and allows to ensure the perennity of author ' s work , as well as collaborative work and content reuse . The actual implementation of XMLlab allows to run the generated simulations within the open source mathematical software Scilab , either locally when Scilab is installed on the client machines , or on thin clients running a simple web browser , when XMLlab and Scilab are installed on a distant server running a standard HTTP server .
The effectiveness of three stop words lists for Arabic Information Retrieval---General Stoplist , Corpus-Based Stoplist , Combined Stoplist ---were investigated in this study . Three popular weighting schemes were examined : the inverse document frequency weight , probabilistic weighting , and statistical language modelling . The Idea is to combine the statistical approaches with linguistic approaches to reach an optimal performance , and compare their effect on retrieval . The LDC ( Linguistic Data Consortium ) Arabic Newswire data set was used with the Lemur Toolkit . The Best Match weighting scheme used in the Okapi retrieval system had the best overall performance of the three weighting algorithms used in the study , stoplists improved retrieval effectiveness especially when used with the BM00 weight . The overall performance of a general stoplist was better than the other two lists .
The Potts model is frequently used to describe the behavior of image classes , since it allows to incorporate contextual information linking neighboring pixels in a simple way . Its isotropic version has only one real parameter beta , known as smoothness parameter or inverse temperature , which regulates the classes map homogeneity . The classes are unavailable , and estimating them is central in important image processing procedures as , for instance , image classification . Methods for estimating the classes which stem from a Bayesian approach under the Potts model require to adequately specify a value for beta . The estimation of such parameter can be efficiently made solving the Pseudo Maximum likelihood ( PML ) equations in two different schemes , using the prior or the posterior model . Having only radiometric data available , the first scheme needs the computation of an initial segmentation , while the second uses both the segmentation and the radiometric data to make the estimation . In this paper , we compare these two PML estimators by computing the mean square error ( MSE ) , bias , and sensitivity to deviations from the hypothesis of the model . We conclude that the use of extra data does not improve the accuracy of the PML , moreover , under gross deviations from the model , this extra information introduces unpredictable distortions and bias .
This article describes high-speed multiprocessor architecture for the concurrent analyzing information represented in analytic , graph- and table forms of associative relations to search , recognize and make a decision in n-dimensional vector discrete space . Vector-logical process models of actual applications , for which the quality of solution is estimated by the proposed integral non-arithmetical metric of the interaction between Boolean vectors , are described .
Thomassen characterized some 0-plane embedding as the forbidden configuration such that a given 0-plane embedding of a graph is drawable in straight-lines if and only if it does not contain the configuration [C . Thomassen , Rectilinear drawings of graphs , J . Graph Theory , 00 ( 0 ) , 000-000 , 0000] . In this paper , we characterize some 0-plane embedding as the forbidden configuration such that a given 0-plane embedding of a graph can be re-embedded into a straight-line drawable 0-plane embedding of the same graph if and only if it does not contain the configuration . Re-embedding of a 0-plane embedding preserves the same set of pairs of crossing edges . We give a linear-time algorithm for finding a straight-line drawable 0-plane re-embedding or the forbidden configuration .
In this paper , we discuss the affordances of open-source Geoweb 0 . 0 platforms to support the participatory design of urban projects in real-world practices . We first introduce the two open-source platforms used in our study for testing purposes . Then , based on evidence from five different field studies we identify five affordances of these platforms : conversations on alternative urban projects , citizen consultation , design empowerment , design studio learning and design research . We elaborate on these in detail and identify a key set of success factors for the facilitation of better practices in the future .
Unattended Wireless Sensor Networks ( UWSNs ) are Wireless Sensor Networks characterized by sporadic sink presence and operation in hostile settings . The absence of the sink for period of time , prevents sensor nodes to offload data in real time and offer greatly increased opportunities for attacks resulting in erasure , modification , or disclosure of sensor-collected data . In this paper , we focus on UWSNs where sensor nodes collect and store data locally and try to upload all the information once the sink becomes available . One of the most relevant issues pertaining UWSNs is to guarantee a certain level of information survivability in an unreliable network and even in presence of a powerful attackers . In this paper , we first introduce an epidemic-domain inspired approach to model the information survivability in UWSN . Next , we derive a fully distributed algorithm that supports these models and give the correctness proofs .
Scheduling problems are generally NP-hard combinatorial problems , and a lot of research has been done to solve these problems heuristically . However , most of the previous approaches are problem-specific and research into the development of a general scheduling algorithm is still in its infancy . Mimicking the natural evolutionary process of the survival of the fittest , Genetic Algorithms ( GAs ) have attracted much attention in solving difficult scheduling problems in recent years . Some obstacles exist when using GAs : there is no canonical mechanism to deal with constraints , which are commonly met in most real-world scheduling problems , and small changes to a solution are difficult . To overcome both difficulties , indirect approaches have been presented ( in [0] and [0] ) for nurse scheduling and driver scheduling , where GAs are used by mapping the solution space , and separate decoding routines then build solutions to the original problem .
In this paper we study the support recovery problem for single index models $Y=f ( \boldsymbol{X}^{\intercal} \boldsymbol{\beta} , \varepsilon ) $ , where $f$ is an unknown link function , $\boldsymbol{X}\sim N_p ( 0 , \mathbb{I}_{p} ) $ and $\boldsymbol{\beta}$ is an $s$-sparse unit vector such that $\boldsymbol{\beta}_{i}\in \{\pm\frac{0}{\sqrt{s}} , 0\}$ . In particular , we look into the performance of two computationally inexpensive algorithms : ( a ) the diagonal thresholding sliced inverse regression ( DT-SIR ) introduced by Lin et al . ( 0000 ) ; and ( b ) a semi-definite programming ( SDP ) approach inspired by Amini & Wainwright ( 0000 ) . When $s=O ( p^{0-\delta} ) $ for some $\delta>0$ , we demonstrate that both procedures can succeed in recovering the support of $\boldsymbol{\beta}$ as long as the rescaled sample size $\kappa=\frac{n}{s\log ( p-s ) }$ is larger than a certain critical threshold . On the other hand , when $\kappa$ is smaller than a critical value , any algorithm fails to recover the support with probability at least $\frac{0}{0}$ asymptotically . In other words , we demonstrate that both DT-SIR and the SDP approach are optimal ( up to a scalar ) for recovering the support of $\boldsymbol{\beta}$ in terms of sample size . We provide extensive simulations , as well as a real dataset application to help verify our theoretical observations .
The Takacs--Fiksel method is a general approach to estimate the parameters of a spatial Gibbs point process . This method embraces standard procedures such as the pseudolikelihood and is defined via weight functions . In this paper we propose a general procedure to find weight functions which reduce the Godambe information and thus outperform pseudolikelihood in certain situations . The new procedure is applied to a standard dataset and to a recent neuroscience replicated point pattern dataset . Finally , the performance of the new procedure is investigated in a simulation study .
The purpose of writing this book is to suggest some improved estimators using auxiliary information in sampling schemes like simple random sampling and systematic sampling . This volume is a collection of five papers . The following problems have been discussed in the book : In chapter one an estimator in systematic sampling using auxiliary information is studied in the presence of non-response . In second chapter some improved estimators are suggested using auxiliary information . In third chapter some improved ratio-type estimators are suggested and their properties are studied under second order of approximation . In chapter four and five some estimators are proposed for estimating unknown population parameter ( s ) and their properties are studied . This book will be helpful for the researchers and students who are working in the field of finite population estimation .
A hybrid evolutionary algorithm with importance sampling method is proposed for multi-dimensional optimization problems in this paper . In order to make use of the information provided in the search process , a set of visited solutions is selected to give scores for intervals in each dimension , and they are updated as algorithm proceeds . Those intervals with higher scores are regarded as good intervals , which are used to estimate the joint distribution of optimal solutions through an interaction between the pool of good genetics , which are the individuals with smaller fitness values . And the sampling probabilities for good genetics are determined through an interaction between those estimated good intervals . It is a cross validation mechanism which determines the sampling probabilities for good intervals and genetics , and the resulted probabilities are used to design crossover , mutation and other stochastic operators with importance sampling method . As the selection of genetics and intervals is not directly dependent on the values of fitness , the resulted offsprings may avoid the trap of local optima . And a purely random EA is also combined into the proposed algorithm to maintain the diversity of population . 00 benchmark test functions are used to evaluate the performance of the proposed algorithm , and it is found that the proposed hybrid algorithm is an efficient algorithm for multi-dimensional optimization problems considered in this paper .
Datasets are often reused to perform multiple statistical analyses in an adaptive way , in which each analysis may depend on the outcomes of previous analyses on the same dataset . Standard statistical guarantees do not account for these dependencies and little is known about how to provably avoid overfitting and false discovery in the adaptive setting . We consider a natural formalization of this problem in which the goal is to design an algorithm that , given a limited number of i . i . d . ~samples from an unknown distribution , can answer adaptively-chosen queries about that distribution . We present an algorithm that estimates the expectations of $k$ arbitrary adaptively-chosen real-valued estimators using a number of samples that scales as $\sqrt{k}$ . The answers given by our algorithm are essentially as accurate as if fresh samples were used to evaluate each estimator . In contrast , prior work yields error guarantees that scale with the worst-case sensitivity of each estimator . We also give a version of our algorithm that can be used to verify answers to such queries where the sample complexity depends logarithmically on the number of queries $k$ ( as in the reusable holdout technique ) . Our algorithm is based on a simple approximate median algorithm that satisfies the strong stability guarantees of differential privacy . Our techniques provide a new approach for analyzing the generalization guarantees of differentially private algorithms .
We show that the hypercube has a face-unfolding that tiles space , and that unfolding has an edge-unfolding that tiles the plane . So the hypercube is a " dimension-descending tiler . " We also show that the hypercube cross unfolding made famous by Dali tiles space , but we leave open the question of whether or not it has an edge-unfolding that tiles the plane .
Recent work on end-to-end automatic speech recognition ( ASR ) has shown that the connectionist temporal classification ( CTC ) loss can be used to convert acoustics to phone or character sequences . Such systems are used with a dictionary and separately-trained Language Model ( LM ) to produce word sequences . However , they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation . In this paper , we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks : Switchboard and CallHome . These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity . However , due to the large number of word output units , CTC word models require orders of magnitude more data to train reliably compared to traditional systems . We present some techniques to mitigate this issue . Our CTC word model achieves a word error rate of 00 . 0%/00 . 0% on the Hub0-0000 Switchboard/CallHome test sets without any LM or decoder compared with 0 . 0%/00 . 0% for phone-based CTC with a 0-gram LM . We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM , and contrast the performance of word and phone CTC models .
We consider the problem of evaluating the cumulative distribution function ( CDF ) of the sum of order statistics , which serves to compute outage probability ( OP ) values at the output of generalized selection combining receivers . Generally , closed-form expressions of the CDF of the sum of order statistics are unavailable for many practical distributions . Moreover , the naive Monte Carlo ( MC ) method requires a substantial computational effort when the probability of interest is sufficiently small . In the region of small OP values , we propose instead two effective variance reduction techniques that yield a reliable estimate of the CDF with small computing cost . The first estimator , which can be viewed as an importance sampling estimator , has bounded relative error under a certain assumption that is shown to hold for most of the challenging distributions . An improvement of this estimator is then proposed for the Pareto and the Weibull cases . The second is a conditional MC estimator that achieves the bounded relative error property for the Generalized Gamma case and the logarithmic efficiency in the Log-normal case . Finally , the efficiency of these estimators is compared via various numerical experiments .
Adaptive optimization methods , which perform local optimization with a metric constructed from the history of iterates , are becoming increasingly popular for training deep neural networks . Examples include AdaGrad , RMSProp , and Adam . We show that for simple overparameterized problems , adaptive methods often find drastically different solutions than gradient descent ( GD ) or stochastic gradient descent ( SGD ) . We construct an illustrative binary classification problem where the data is linearly separable , GD and SGD achieve zero test error , and AdaGrad , Adam , and RMSProp attain test errors arbitrarily close to half . We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models . We observe that the solutions found by adaptive methods generalize worse ( often significantly worse ) than SGD , even when these solutions have better training performance . These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks .
Private regression has received attention from both database and security communities . Recent work by Fredrikson et al . ( USENIX Security 0000 ) analyzed the functional mechanism ( Zhang et al . VLDB 0000 ) for training linear regression models over medical data . Unfortunately , they found that model accuracy is already unacceptable with differential privacy when $\varepsilon = 0$ . We address this issue , presenting an explicit connection between differential privacy and stable learning theory through which a substantially better privacy/utility tradeoff can be obtained . Perhaps more importantly , our theory reveals that the most basic mechanism in differential privacy , output perturbation , can be used to obtain a better tradeoff for all convex-Lipschitz-bounded learning tasks . Since output perturbation is simple to implement , it means that our approach is potentially widely applicable in practice . We go on to apply it on the same medical data as used by Fredrikson et al . Encouragingly , we achieve accurate models even for $\varepsilon = 0 . 0$ . In the last part of this paper , we study the impact of our improved differentially private mechanisms on model inversion attacks , a privacy attack introduced by Fredrikson et al . We observe that the improved tradeoff makes the resulting differentially private model more susceptible to inversion attacks . We analyze this phenomenon formally .
Sending compressed video data in error-prone environments ( like the Internet and wireless networks ) might cause data degradation . Error concealment techniques try to conceal the received data in the decoder side . In this paper , an adaptive boundary matching algorithm is presented for recovering the damaged motion vectors ( MVs ) . This algorithm uses an outer boundary matching or directional temporal boundary matching method to compare every boundary of candidate macroblocks ( MBs ) , adaptively . It gives a specific weight according to the accuracy of each boundary of the damaged MB . Moreover , if each of the adjacent MBs is already concealed , different weights are given to the boundaries . Finally , the MV with minimum adaptive boundary distortion is selected as the MV of the damaged MB . Experimental results show that the proposed algorithm can improve both objective and subjective quality of reconstructed frames without any considerable computational complexity . The average PSNR in some frames of test sequences increases about 0 . 00 , 0 . 00 , 0 . 00 , 0 . 00 , 0 . 00 , and 0 . 00 dB compared to average MV , classic boundary matching , directional boundary matching , directional temporal boundary matching , outer boundary matching , and dynamical temporal error concealment algorithm , respectively .
Execution of concurrent programs implies frequent switching between different thread contexts . This property perplexes analyzing and reasoning about concurrent programs . Trace simplification is a technique that aims at alleviating this problem via transforming a concurrent program trace ( execution ) into a semantically equivalent one . The resulted trace typically includes less number of context switches than that in the original trace . This paper presents a new static approach for trace simplification . This approach is based on a connectivity analysis that calculates for each trace-point connectivity and context-switching information . The paper also presents a novel operational semantics for concurrent programs . The semantics is used to prove the correctness and efficiency of the proposed techniques for connectivity analysis and trace simplification . The results of experiments testing the proposed technique on problems treated by previous work for trace simplification are also shown in the paper . The results prove the efficiency and effectiveness of the proposed method .
Motivated by the resurgence of neural networks in being able to solve complex learning tasks we undertake a study of high depth networks using ReLU gates which implement the function $x \mapsto \max\{0 , x\}$ . We try to understand the role of depth in such neural networks by showing size lowerbounds against such network architectures in parameter regimes hitherto unexplored . In particular we show the following two main results about neural nets computing Boolean functions of input dimension $n$ , 0 . We use the method of random restrictions to show almost linear , $\Omega ( \epsilon^{0 ( 0-\delta ) }n^{0-\delta} ) $ , lower bound for completely weight unrestricted LTF-of-ReLU circuits to match the Andreev function on at least $\frac{0}{0} +\epsilon$ fraction of the inputs for $\epsilon > \sqrt{0\frac{\log^{\frac {0}{0-\delta}} ( n ) }{n}}$ for any $\delta \in ( 0 , \frac 0 0 ) $ 0 . We use the method of sign-rank to show exponential in dimension lower bounds for ReLU circuits ending in a LTF gate and of depths upto $O ( n^{\xi} ) $ with $\xi < \frac{0}{0}$ with some restrictions on the weights in the bottom most layer . All other weights in these circuits are kept unrestricted . This in turns also implies the same lowerbounds for LTF circuits with the same architecture and the same weight restrictions on their bottom most layer . Along the way we also show that there exists a $\mathbb{R}^ n\rightarrow \mathbb{R}$ Sum-of-ReLU-of-ReLU function which Sum-of-ReLU neural nets can never represent no matter how large they are allowed to be .
Humans represent the objects in the same category using their properties , in order to well discriminate and understand them , and an intelligent robot should be able to do the same . In this work , we propose a robot system that can automatically perceive the object properties through touch . We work on the common object category of clothing . The robot moves under the guidance of an external Kinect sensor , and squeezes the clothes with a GelSight tactile sensor , then it recognizes the 00 properties of the clothing according to the tactile data . The target properties are the physical properties , like thickness , fuzziness , softness and endurance , and semantic properties like wearing season and preferred washing methods . We collect a dataset of 000 varied pieces of clothes , and make 0000 exploring iterations on them . To learn the useful information from the high-dimensional sensory output , we applied Convolutional Neural Networks ( CNN ) on the tactile data for recognizing the clothing properties , and on the Kinect depth images for selecting exploration locations . Experiments show that using the trained neural networks , the robot can automatically explore the unknown clothes and learn their properties . This work proposes a new architecture for active tactile perception system with vision-touch system , and has potential to enable robots to help humans with varied clothing related housework .
This article describes lossless compression algorithms for multisets of sequences , taking advantage of the multiset ' s unordered structure . Multisets are a generalisation of sets where members are allowed to occur multiple times . A multiset can be encoded na\ " ively by simply storing its elements in some sequential order , but then information is wasted on the ordering . We propose a technique that transforms the multiset into an order-invariant tree representation , and derive an arithmetic code that optimally compresses the tree . Our method achieves compression even if the sequences in the multiset are individually incompressible ( such as cryptographic hash sums ) . The algorithm is demonstrated practically by compressing collections of SHA-0 hash sums , and multisets of arbitrary , individually encodable objects .
The ever growing demand for remote sensing data products by user community has resulted in many Indian and foreign remote sensing satellites being launched . The diversity in the remote sensing sensors has resulted in heterogeneous software and hardware environments for generating geospatial data products . The workflow automation software knows as information management system is in place at National Remote Sensing Centre ( NRSC ) catering to the needs of the data processing and data dissemination . The software components of workflow are interfaced in different heterogeneous environments that get executed at data processing software in automated and semi automated modes . For every new satellite being launched , the software is modified or upgraded if new business processes are introduced . In this study , we propose a software architecture that gives more flexible automation with very less manageable code . The study also addresses utilization and extraction of useful information from historic production and customer details . A comparison of the current workflow software architecture with existing practices in industry like Service Oriented Architecture ( SOA ) , Extensible Markup Languages ( XML ) , and Event based architectures has been made . A new hybrid approach based on the industry practices is proposed to improve the existing workflow .
Likelihood-free methods , such as approximate Bayesian computation , are powerful tools for practical inference problems with intractable likelihood functions . Markov chain Monte Carlo and sequential Monte Carlo variants of approximate Bayesian computation can be effective techniques for sampling posterior distributions in an approximate Bayesian computation setting . However , without careful consideration of convergence criteria and selection of proposal kernels , such methods can lead to very biased inference or computationally inefficient sampling . In contrast , rejection sampling for approximate Bayesian computation , despite being computationally intensive , results in independent , identically distributed samples from the approximated posterior . We propose an alternative method for the acceleration of likelihood-free Bayesian inference that applies multilevel Monte Carlo variance reduction techniques directly to rejection sampling . The resulting method retains the accuracy advantages of rejection sampling while significantly improving the computational efficiency
Dynamic linear models with Gaussian observations and Gaussian states lead to closed-form formulas for posterior simulation . However , these closed-form formulas break down when the response or state evolution ceases to be Gaussian . Dynamic , generalized linear models exemplify a class of models for which this is the case , and include , amongst other models , dynamic binomial logistic regression and dynamic negative binomial regression . Finding and appraising posterior simulation techniques for these models is important since modeling temporally correlated categories or counts is useful in a variety of disciplines , including ecology , economics , epidemiology , medicine , and neuroscience . In this paper , we present one such technique , P\ ' olya-Gamma data augmentation , and compare it against two competing methods . We find that the P\ ' olya-Gamma approach works well for dynamic logistic regression and for dynamic negative binomial regression when the count sizes are small . Supplementary files are provided for replicating the benchmarks .
Designing experiments for generalized linear models is difficult because optimal designs depend on unknown parameters . Here we investigate local optimality . We propose to study for a given design its region of optimality in parameter space . Often these regions are semi-algebraic and feature interesting symmetries . We demonstrate this with the Rasch Poisson counts model . For any given interaction order between the explanatory variables we give a characterization of the regions of optimality of a special saturated design . This extends known results from the case of no interaction . We also give an algebraic and geometric perspective on optimality of experimental designs for the Rasch Poisson counts model using polyhedral and spectrahedral geometry .
As robots aspire for long-term autonomous operations in complex dynamic environments , the ability to reliably take mission-critical decisions in ambiguous situations becomes critical . This motivates the need to build systems that have situational awareness to assess how qualified they are at that moment to make a decision . We call this self-evaluating capability as introspection . In this paper , we take a small step in this direction and propose a generic framework for introspective behavior in perception systems . Our goal is to learn a model to reliably predict failures in a given system , with respect to a task , directly from input sensor data . We present this in the context of vision-based autonomous MAV flight in outdoor natural environments , and show that it effectively handles uncertain situations .
This paper formulates a novel problem on graphs : find the minimal subset of edges in a fully connected graph , such that the resulting graph contains all spanning trees for a set of specifed sub-graphs . This formulation is motivated by an un-supervised grammar induction problem from computational linguistics . We present a reduction to some known problems and algorithms from graph theory , provide computational complexity results , and describe an approximation algorithm .
State of the art analyzers in the Logic Programming ( LP ) paradigm are nowadays mature and sophisticated . They allow inferring a wide variety of global properties including termination , bounds on resource consumption , etc . The aim of this work is to automatically transfer the power of such analysis tools for LP to the analysis and verification of Java bytecode ( JVML ) . In order to achieve our goal , we rely on well-known techniques for meta-programming and program specialization . More precisely , we propose to partially evaluate a JVML interpreter implemented in LP together with ( an LP representation of ) a JVML program and then analyze the residual program . Interestingly , at least for the examples we have studied , our approach produces very simple LP representations of the original JVML programs . This can be seen as a decompilation from JVML to high-level LP source . By reasoning about such residual programs , we can automatically prove in the CiaoPP system some non-trivial properties of JVML programs such as termination , run-time error freeness and infer bounds on its resource consumption . We are not aware of any other system which is able to verify such advanced properties of Java bytecode .
An explicit formula for the chaotic representation of the powers of increments , ( X_{t+t_0}-X_{t_0} ) ^n , of a Levy process is presented . There are two different chaos expansions of a square integrable functional of a Levy process : one with respect to the compensated Poisson random measure and the other with respect to the orthogonal compensated powers of the jumps of the Levy process . Computationally explicit formulae for both of these chaos expansions of ( X_{t+t_0}-X_{t_0} ) ^n are given in this paper . Simulation results verify that the representation is satisfactory . The CRP of a number of financial derivatives can be found by expressing them in terms of ( X_{t+t_0}-X_{t_0} ) ^n using Taylor ' s expansion .
Model composition plays a central role in many software engineering activities such as evolving models to add new features and reconciling conflicting design models developed in parallel by different development teams . As model composition is usually an error-prone and effort-consuming task , its potential benefits , such as gains in productivity can be compromised . However , there is no empirical knowledge nowadays about the effort required to compose design models . Only feedbacks of model composition evangelists are available , and they often diverge . Consequently , developers are unable to conduct any cost-effectiveness analysis as well as identify , predict , or reduce composition effort . The inability of evaluating composition effort is due to three key problems . First , the current evaluation frameworks do not consider fundamental concepts in model composition such as conflicts and inconsistencies . Second , researchers and developers do not know what factors can influence the composition effort in practice . Third , practical knowledge about how such influential factors may affect the developers ' effort is severely lacking . In this context , the contributions of this thesis are threefold : ( i ) a quality model for supporting the evaluation of model composition effort , ( ii ) practical knowledge , derived from a family of quantitative and qualitative empirical studies , about model composition effort and its influential factors , and ( iii ) insight about how to evaluate model composition efforts and tame the side effects of such influential factors .
Stochastic differential equations ( SDEs ) are established tools to model physical phenomena whose dynamics are affected by random noise . By estimating parameters of an SDE intrinsic randomness of a system around its drift can be identified and separated from the drift itself . When it is of interest to model dynamics within a given population , i . e . to model simultaneously the performance of several experiments or subjects , mixed-effects modelling allows for the distinction of between and within experiment variability . A framework to model dynamics within a population using SDEs is proposed , representing simultaneously several sources of variation : variability between experiments using a mixed-effects approach and stochasticity in the individual dynamics using SDEs . These " stochastic differential mixed-effects models " have applications in e . g . pharmacokinetics/pharmacodynamics and biomedical modelling . A parameter estimation method is proposed and computational guidelines for an efficient implementation are given . Finally the method is evaluated using simulations from standard models like the two-dimensional Ornstein-Uhlenbeck ( OU ) and the square root models .
This paper develops meshless methods for probabilistically describing discretisation error in the numerical solution of partial differential equations . This construction enables the solution of Bayesian inverse problems while accounting for the impact of the discretisation of the forward problem . In particular , this drives statistical inferences to be more conservative in the presence of significant solver error . Theoretical results are presented describing rates of convergence for the posteriors in both the forward and inverse problems . This method is tested on a challenging inverse problem with a nonlinear forward model .
Machine learning is increasingly used to make sense of the physical world yet may suffer from adversarial manipulation . We examine the Viola-Jones 0D face detection algorithm to study whether images can be created that humans do not notice as faces yet the algorithm detects as faces . We show that it is possible to construct images that Viola-Jones recognizes as containing faces yet no human would consider a face . Moreover , we show that it is possible to construct images that fool facial detection even when they are printed and then photographed .
This paper identifies and estimates the coefficients in a multivariate errors-in-variables linear model when the unobserved arbitrarily dependent regressors are not jointly normal and independent of errors . To identify the coefficients , we use variation in the second-order partial derivatives of the log characteristic function of the unobserved regressors ; a property of only not jointly normal distributions . A root-n consistent and asymptotically normal extremum estimator performs well in simulations relative to third and fourth order moment estimators .
The log-determinant of a kernel matrix appears in a variety of machine learning problems , ranging from determinantal point processes and generalized Markov random fields , through to the training of Gaussian processes . Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand . In the spirit of probabilistic numerics , we reinterpret the problem of computing the log-determinant as a Bayesian inference problem . In particular , we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget . Beyond its novelty and theoretic appeal , the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant , while also quantifying the uncertainty due to budget-constrained evidence .
The marginal maximum a posteriori probability ( MAP ) estimation problem , which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized , is an important inference problem in many models , such as those with hidden variables or uncertain parameters . Unfortunately , marginal MAP can be NP-hard even on trees , and has attracted less attention in the literature compared to the joint MAP ( maximization ) and marginalization problems . We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem , making it possible to easily extend most or all variational-based algorithms to marginal MAP . In particular , we derive a set of " mixed-product " message passing algorithms for marginal MAP , whose form is a hybrid of max-product , sum-product and a novel " argmax-product " message updates . We also derive a class of convergent algorithms based on proximal point methods , including one that transforms the marginal MAP problem into a sequence of standard marginalization problems . Theoretically , we provide guarantees under which our algorithms give globally or locally optimal solutions , and provide novel upper bounds on the optimal objectives . Empirically , we demonstrate that our algorithms significantly outperform the existing approaches , including a state-of-the-art algorithm based on local search methods .
How to achieve differential privacy in the distributed setting , where the dataset is distributed among the distrustful parties , is an important problem . We consider in what condition can a protocol inherit the differential privacy property of a function it computes . The heart of the problem is the secure multiparty computation of randomized function . A notion \emph{obliviousness} is introduced , which captures the key security problems when computing a randomized function from a deterministic one in the distributed setting . By this observation , a sufficient and necessary condition about computing a randomized function from a deterministic one is given . The above result can not only be used to determine whether a protocol computing differentially private function is secure , but also be used to construct secure one . Then we prove that the differential privacy property of a function can be inherited by the protocol computing it if the protocol privately computes it . A composition theorem of differentially private protocols is also presented . We also construct some protocols to generate random variate in the distributed setting , such as the uniform random variates and the inversion method . By using these fundamental protocols , we construct protocols of the Gaussian mechanism , the Laplace mechanism and the Exponential mechanism . Importantly , all these protocols satisfy obliviousness and so can be proved to be secure in a simulation based manner . We also provide a complexity bound of computing randomized function in the distribute setting . Finally , to show that our results are fundamental and powerful to multiparty differential privacy , we construct a differentially private empirical risk minimization protocol .
In this article we investigate high-dimensional banded sample covariance matrices under the regime that the sample size $n$ , the dimension $p$ and the bandwidth $d$ tend simultaneously to infinity such that $$n/p\to 0 \ \ \text{and} \ \ 0d/n\to y>0 . $$ It is shown that the empirical spectral distribution of those matrices almost surely converges weakly to some deterministic probability measure which is characterized by its moments . Certain restricted compositions of natural numbers play a crucial role in the evaluation of the expected moments of the empirical spectral distribution .
We consider the design of prediction market mechanisms known as automated market makers . We show that we can design these mechanisms via the mold of \emph{exponential family distributions} , a popular and well-studied probability distribution template used in statistics . We give a full development of this relationship and explore a range of benefits . We draw connections between the information aggregation of market prices and the belief aggregation of learning agents that rely on exponential family distributions . We develop a very natural analysis of the market behavior as well as the price equilibrium under the assumption that the traders exhibit risk aversion according to exponential utility . We also consider similar aspects under alternative models , such as when traders are budget constrained .
We summarize the potential impact that the European Union ' s new General Data Protection Regulation will have on the routine use of machine learning algorithms . Slated to take effect as law across the EU in 0000 , it will restrict automated individual decision-making ( that is , algorithms that make decisions based on user-level predictors ) which " significantly affect " users . The law will also effectively create a " right to explanation , " whereby a user can ask for an explanation of an algorithmic decision that was made about them . We argue that while this law will pose large challenges for industry , it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation .
The indoor climate conditions of monumental buildings are very important for the conservation of these objects . Simplified models with physical meaning are desired that are capable of simulating temperature and relative humidity . In this paper we research state-space models as methodology for the inverse modeling of climate responses of unheated monumental buildings . It is concluded that this approach is very promising for obtaining physical models and parameters of indoor climate responses . Furthermore state space models can be simulated very efficiently : the simulation duration time of a 000 year hourly based period take less than a second on an ordinary computer .
We address the problem of surface inpainting , which aims to fill in holes or missing regions on a Riemann surface based on its surface geometry . In practical situation , surfaces obtained from range scanners often have holes where the 0D models are incomplete . In order to analyze the 0D shapes effectively , restoring the incomplete shape by filling in the surface holes is necessary . In this paper , we propose a novel conformal approach to inpaint surface holes on a Riemann surface based on its surface geometry . The basic idea is to represent the Riemann surface using its conformal factor and mean curvature . According to Riemann surface theory , a Riemann surface can be uniquely determined by its conformal factor and mean curvature up to a rigid motion . Given a Riemann surface $S$ , its mean curvature $H$ and conformal factor $\lambda$ can be computed easily through its conformal parameterization . Conversely , given $\lambda$ and $H$ , a Riemann surface can be uniquely reconstructed by solving the Gauss-Codazzi equation on the conformal parameter domain . Hence , the conformal factor and the mean curvature are two geometric quantities fully describing the surface . With this $\lambda$-$H$ representation of the surface , the problem of surface inpainting can be reduced to the problem of image inpainting of $\lambda$ and $H$ on the conformal parameter domain . Once $\lambda$ and $H$ are inpainted , a Riemann surface can be reconstructed which effectively restores the 0D surface with missing holes . Since the inpainting model is based on the geometric quantities $\lambda$ and $H$ , the restored surface follows the surface geometric pattern . We test the proposed algorithm on synthetic data as well as real surface data . Experimental results show that our proposed method is an effective surface inpainting algorithm to fill in surface holes on an incomplete 0D models based their surface geometry .
Sensing on smartphones is known to be power-hungry . It has been shown that this problem can be solved by adding an ultra low-power processor to execute simple , frequent sensor data processing . While very effective in saving energy , this resulting heterogeneous , distributed architecture poses a significant challenge to application development . We present Reflex , a suite of runtime and compilation techniques to conceal the heterogeneous , distributed nature from developers . The Reflex automatically transforms the developer ' s code for distributed execution with the help of the Reflex runtime . To create a unified system illusion , Reflex features a novel software distributed shared memory ( DSM ) design that leverages the extreme architectural asymmetry between the low-power processor and the powerful central processor to achieve both energy efficiency and performance . We report a complete realization of Reflex for heterogeneous smartphones with Maemo/Linux as the central kernel . Using a tri-processor hardware prototype and sensing applications reported in recent literature , we evaluate the Reflex realization for programming transparency , energy efficiency , and performance . We show that Reflex supports a programming style that is very close to contemporary smartphone programming . It allows existing sensing applications to be ported with minor source code changes . Reflex reduces the system power in sensing by up to 00% , and its runtime system only consumes 00% local memory on a typical ultra-low power processor .
We design a sequential Monte Carlo scheme for the dual purpose of Bayesian inference and model selection . We consider the application context of urban mobility , where several modalities of transport and different measurement devices can be employed . Therefore , we address the joint problem of online tracking and detection of the current modality . For this purpose , we use interacting parallel particle filters , each one addressing a different model . They cooperate for providing a global estimator of the variable of interest and , at the same time , an approximation of the posterior density of each model given the data . The interaction occurs by a parsimonious distribution of the computational effort , with online adaptation for the number of particles of each filter according to the posterior probability of the corresponding model . The resulting scheme is simple and flexible . We have tested the novel technique in different numerical experiments with artificial and real data , which confirm the robustness of the proposed scheme .
Activity diagrams ( ADs ) have recently become widely used in the modeling of workflows , business processes , and web-services , where they serve various purposes , from documentation , requirement definitions , and test case specifications , to simulation and code generation . As models , programs , and systems evolve over time , understanding changes and their impact is an important challenge , which has attracted much research efforts in recent years . In this paper we present addiff , a semantic differencing operator for ADs . Unlike most existing approaches to model comparison , which compare the concrete or the abstract syntax of two given diagrams and output a list of syntactical changes or edit operations , addiff considers the Semantics of the diagrams at hand and outputs a set of diff witnesses , each of which is an execution trace that is possible in the first AD and is not possible in the second . We motivate the use of addiff , formally define it , and show two algorithms to compute it , a concrete forward-search algorithm and a symbolic xpoint algorithm , implemented using BDDs and integrated into the Eclipse IDE . Empirical results and examples demonstrate the feasibility and unique contribution of addiff to the state-of-the-art in version comparison and evolution analysis .
We propose a new approach , along with refinements , based on $L_0$ penalties and aimed at jointly estimating several related regression models . Its main interest is that it can be rewritten as a weighted lasso on a simple transformation of the original data set . In particular , it does not need new dedicated algorithms and is ready to implement under a variety of regression models , {\em e . g . } , using standard R packages . Moreover , asymptotic oracle properties are derived along with preliminary non-asymptotic results , suggesting good theoretical properties . Our approach is further compared with state-of-the-art competitors under various settings on synthetic data : these empirical results confirm that our approach performs at least similarly to its competitors . As a final illustration , an analysis of road safety data is provided .
A counterparty credit limit ( CCL ) is a limit imposed by a financial institution to cap its maximum possible exposure to a specified counterparty . Although CCLs are designed to help institutions mitigate counterparty risk by selective diversification of their exposures , their implementation restricts the liquidity that institutions can access in an otherwise centralized pool . We address the question of how this mechanism impacts trade prices and volatility , both empirically and via a new model of trading with CCLs . We find empirically that CCLs cause little impact on trade . However , our model highlights that in extreme situations , CCLs could serve to destabilize prices and thereby influence systemic risk .
Clustering is a very popular network structuring technique which mainly addresses the issue of scalability in large scale Wireless Sensor Networks . Additionally , it has been shown to improve the energy efficiency and prolong the life of the network . The suggested protocols mostly base their clustering criteria on some grouping attribute ( s ) of the nodes . One important attribute that is largely ignored by most of the existing multi-hop clustering protocols is the reliability of the communication links between the nodes . In this paper , we suggest an adaptive and completely distributed multi-hop clustering protocol that incorporates different notions of reliability of the communication links , among other things , into a composite metric and uses it in all phases of the clustering process . The joining criteria for the nodes , which lie at one hop from the elected cluster heads , to a particular cluster not only consider the reliability of their communication link with their cluster head but also other important attributes . The nodes that lie outside the communication range of cluster heads become cluster members transitively through existing cluster members utilizing the end-to-end notion of link reliability , between the nodes and the cluster heads , along with other important attributes . Similarly , inter-cluster communication paths are selected using a set of criteria that includes the end-to-end communication link reliability with the sink node along with other important node and network attributes . We believe that incorporating link reliability in all phases of clustering process results in an efficient multi-hop communication hierarchy that has the potential of bringing down the total communication costs in the network .
Scientists increasingly rely on simulation runs of complex models in lieu of cost-prohibitive or infeasible experimentation . The data output of many controlled simulation runs , the ensemble , is used to verify correctness and quantify uncertainty . However , due to their size and complexity , ensembles are difficult to visually analyze because the working set often exceeds strict memory limitations . We present a navigable ensemble analysis tool , NEA , for interactive exploration of ensembles . NEA ' s pre-processing component takes advantage of the data similarity characteristics of ensembles to represent the data in a new , spatially-efficient data structure which does not require fully reconstructing the original data at visualization time . This data structure allows a fine degree of control in working set management , which enables interactive ensemble exploration while fitting within memory limitations . Scientists can also gain new insights from the data-similarity analysis in the pre-processing component .
An autonomous variational inference algorithm for arbitrary graphical models requires the ability to optimize variational approximations over the space of model parameters as well as over the choice of tractable families used for the variational approximation . In this paper , we present a novel combination of graph partitioning algorithms with a generalized mean field ( GMF ) inference algorithm . This combination optimizes over disjoint clustering of variables and performs inference using those clusters . We provide a formal analysis of the relationship between the graph cut and the GMF approximation , and explore several graph partition strategies empirically . Our empirical results provide rather clear support for a weighted version of MinCut as a useful clustering algorithm for GMF inference , which is consistent with the implications from the formal analysis .
Undirected graphical models are applied in genomics , protein structure prediction , and neuroscience to identify sparse interactions that underlie discrete data . Although Bayesian methods for inference would be favorable in these contexts , they are rarely used because they require doubly intractable Monte Carlo sampling . Here , we develop a framework for scalable Bayesian inference of discrete undirected models based on two new methods . The first is Persistent VI , an algorithm for variational inference of discrete undirected models that avoids doubly intractable MCMC and approximations of the partition function . The second is Fadeout , a reparameterization approach for variational inference under sparsity-inducing priors that captures a posteriori correlations between parameters and hyperparameters with noncentered parameterizations . We find that , together , these methods for variational inference substantially improve learning of sparse undirected graphical models in simulated and real problems from physics and biology .
This paper aims to present the current position of the ongoing research into developing the requirements and acceptance of a virtual therapeutic community in Second Life , specifically for people with Borderline Personality Disorder ( BPD ) . The research has identified this particular user group given that people with BPD often require high levels of support , which can result in emergency hospital admissions , in addition to the significant economic cost of treating people BPD in relation to other mental illnesses ( NCCMH , 0000 ) . The research is also intended to be used as framework for other mental health conditions . This work is a continuation of the research carried out in exploring the potential of virtual therapeutic communities based on existing models of therapeutic hospitals as well as virtual treatments and support , in treating people with BPD . An interdisciplinary approach to this research features collaboration from areas in HCI , forensic psychology and psychotherapy .
In this paper , we first provide a spanning tree ( ST ) -based centralized group key agreement protocol for unbalanced mobile Ad Hoc networks ( MANETs ) . Based on the centralized solution , a local spanning tree ( LST ) -based distributed protocol for general MANETs is subsequently presented . Both protocols follow the basic features of the HSK scheme : 0 ) H means that a hybrid approach , which is the combination of key agreement and key distribution via symmetric encryption , is exploited ; 0 ) S indicates that a ST or LSTs are adopted to form a connected network topology ; and 0 ) K implies that the extended Kruskal algorithm is employed to handle dynamic events . It is shown that the HSK scheme is a uniform approach to handle the initial key establishment process as well as all kinds of dynamic events in group key agreement protocol for MANETs . Additionally , the extended Kruskal algorithm enables to realize the reusability of the precomputed secure links to reduce the overhead . Moreover , some other aspects , such as the network topology connectivity and security , are well analyzed .
In this paper the filtering of partially observed diffusions , with discrete-time observations , is considered . It is assumed that only biased approximations of the diffusion can be obtained , for choice of an accuracy parameter indexed by $l$ . A multilevel estimator is proposed , consisting of a telescopic sum of increment estimators associated to the successive levels . The work associated to $\mathcal{O} ( \varepsilon^0 ) $ mean-square error between the multilevel estimator and average with respect to the filtering distribution is shown to scale optimally , for example as $\mathcal{O} ( \varepsilon^{-0} ) $ for optimal rates of convergence of the underlying diffusion approximation . The method is illustrated on some toy examples as well as estimation of interest rate based on real S&P 000 stock price data .
Bayesian analysis of functions and curves is considered , where warping and other geometrical transformations are often required for meaningful comparisons . We focus on two applications involving the classification of mouse vertebrae shape outlines and the alignment of mass spectrometry data in proteomics . The functions and curves of interest are represented using the recently introduced square root velocity function , which enables a warping invariant elastic distance to be calculated in a straightforward manner . We distinguish between various spaces of interest : the original space , the ambient space after standardizing , and the quotient space after removing a group of transformations . Using Gaussian process models in the ambient space and Dirichlet priors for the warping functions , we explore Bayesian inference for curves and functions . Markov chain Monte Carlo algorithms are introduced for simulating from the posterior , including simulated tempering for multimodal posteriors . We also compare ambient and quotient space estimators for mean shape , and explain their frequent similarity in many practical problems using a Laplace approximation . A simulation study is carried out , as well as shape classification of the mouse vertebra outlines and practical alignment of the mass spectrometry functions .
We employ a parameter-free distribution estimation framework where estimators are random distributions and utilize the Kullback-Leibler ( KL ) divergence as a loss function . Wu and Vos [J . Statist . Plann . Inference 000 ( 0000 ) 0000-0000] show that when an estimator obtained from an i . i . d . sample is viewed as a random distribution , the KL risk of the estimator decomposes in a fashion parallel to the mean squared error decomposition when the estimator is a real-valued random variable . In this paper , we explore how conditional versions of distribution expectation ( $E^{\dagger}$ ) can be defined so that a distribution version of the Rao-Blackwell theorem holds . We define distributional expectation and variance ( $V^{\dagger}$ ) that also provide a decomposition of KL risk in exponential and mixture families . For exponential families , we show that the maximum likelihood estimator ( viewed as a random distribution ) is distribution unbiased and is the unique uniformly minimum distribution variance unbiased ( UMV$^{\dagger}$U ) estimator . Furthermore , we show that the MLE is robust against model specification in that if the true distribution does not belong to the exponential family , the MLE is UMV$^{\dagger}$U for the KL projection of the true distribution onto the exponential families provided these two distribution have the same expectation for the canonical statistic . To allow for estimators taking values outside of the exponential family , we include results for KL projection and define an extended projection to accommodate the non-existence of the MLE for families having discrete sample space . Illustrative examples are provided .
The aim of this work is to address issues where formal specifications cannot be realized on a given dynamical system subjected to a changing environment . Such failures occur whenever the dynamics of the system restrict the robot in such a way that the environment may prevent the robot from progressing safely to its goals . We provide a framework that automatically synthesizes revisions to such specifications that restrict the assumed behaviors of the environment and the behaviors of the system . We provide a means for explaining such modifications to the user in a concise , easy-to-understand manner . Integral to the framework is a new algorithm for synthesizing controllers for reactive specifications that include a discrete representation of the robot ' s dynamics . The new approach is demonstrated with a complex task implemented using a unicycle model .
In observational studies , treatment may be adapted to covariates at several times without a fixed protocol , in continuous time . Treatment influences covariates , which influence treatment , which influences covariates , and so on . Then even time-dependent Cox-models cannot be used to estimate the net treatment effect . Structural nested models have been applied in this setting . Structural nested models are based on counterfactuals : the outcome a person would have had had treatment been withheld after a certain time . Previous work on continuous-time structural nested models assumes that counterfactuals depend deterministically on observed data , while conjecturing that this assumption can be relaxed . This article proves that one can mimic counterfactuals by constructing random variables , solutions to a differential equation , that have the same distribution as the counterfactuals , even given past observed data . These " mimicking " variables can be used to estimate the parameters of structural nested models without assuming the treatment effect to be deterministic .
Calculating and categorizing the similarity of curves is a fundamental problem which has generated much recent interest . However , to date there are no implementations of these algorithms for curves on surfaces with provable guarantees on the quality of the measure . In this paper , we present a similarity measure for any two cycles that are homologous , where we calculate the minimum area of any homology ( or connected bounding chain ) between the two cycles . The minimum area homology exists for broader classes of cycles than previous measures which are based on homotopy . It is also much easier to compute than previously defined measures , yielding an efficient implementation that is based on linear algebra tools . We demonstrate our algorithm on a range of inputs , showing examples which highlight the feasibility of this similarity measure .
We propose a hybrid model of differential privacy that considers a combination of regular and opt-in users who desire the differential privacy guarantees of the local privacy model and the trusted curator model , respectively . We demonstrate that within this model , it is possible to design a new type of blended algorithm for the task of privately computing the head of a search log . This blended approach provides significant improvements in the utility of obtained data compared to related work while providing users with their desired privacy guarantees . Specifically , on two large search click data sets , comprising 0 . 00 and 00 GB respectively , our approach attains NDCG values exceeding 00% across a range of privacy budget values .
In this manuscript we present exponential inequalities for spatial lattice processes which take values in a separable Hilbert space and satisfy certain dependence conditions . We consider two types of dependence : spatial data under $\alpha$-mixing conditions and spatial data which satisfies a weak dependence condition introduced by Dedecker and Prieur [0000] . We demonstrate their usefulness in the functional kernel regression model of Ferraty and Vieu [0000] where we study uniform consistency properties of the estimated regression operator on increasing subsets of the underlying function space .
Joint models for longitudinal and time-to-event data constitute an attractive modeling framework that has received a lot of interest in the recent years . This paper presents the capabilities of the R package JMbayes for fitting these models under a Bayesian approach using Markon chain Monte Carlo algorithms . JMbayes can fit a wide range of joint models , including among others joint models for continuous and categorical longitudinal responses , and provides several options for modeling the association structure between the two outcomes . In addition , this package can be used to derive dynamic predictions for both outcomes , and offers several tools to validate these predictions in terms of discrimination and calibration . All these features are illustrated using a real data example on patients with primary biliary cirrhosis .
This paper is aimed at deriving the universality of the largest eigenvalue of a class of high-dimensional real or complex sample covariance matrices of the form $\mathcal{W}_N=\Sigma^{0/0}XX^*\Sigma ^{0/0}$ . Here , $X= ( x_{ij} ) _{M , N}$ is an $M\times N$ random matrix with independent entries $x_{ij} , 0\leq i\leq M , 0\leq j\leq N$ such that $\mathbb{E}x_{ij}=0$ , $\mathbb{E}|x_{ij}|^0=0/N$ . On dimensionality , we assume that $M=M ( N ) $ and $N/M\rightarrow d\in ( 0 , \infty ) $ as $N\rightarrow\infty$ . For a class of general deterministic positive-definite $M\times M$ matrices $\Sigma$ , under some additional assumptions on the distribution of $x_{ij}$ ' s , we show that the limiting behavior of the largest eigenvalue of $\mathcal{W}_N$ is universal , via pursuing a Green function comparison strategy raised in [Probab . Theory Related Fields 000 ( 0000 ) 000-000 , Adv . Math . 000 ( 0000 ) 0000-0000] by Erd\H{o}s , Yau and Yin for Wigner matrices and extended by Pillai and Yin [Ann . Appl . Probab . 00 ( 0000 ) 000-0000] to sample covariance matrices in the null case ( $\Sigma=I$ ) . Consequently , in the standard complex case ( $\mathbb{E}x_{ij}^0=0$ ) , combing this universality property and the results known for Gaussian matrices obtained by El Karoui in [Ann . Probab . 00 ( 0000 ) 000-000] ( nonsingular case ) and Onatski in [Ann . Appl . Probab . 00 ( 0000 ) 000-000] ( singular case ) , we show that after an appropriate normalization the largest eigenvalue of $\mathcal{W}_N$ converges weakly to the type 0 Tracy-Widom distribution $\mathrm{TW}_0$ . Moreover , in the real case , we show that when $\Sigma$ is spiked with a fixed number of subcritical spikes , the type 0 Tracy-Widom limit $\mathrm{TW}_0$ holds for the normalized largest eigenvalue of $\mathcal {W}_N$ , which extends a result of F\ ' {e}ral and P\ ' {e}ch\ ' {e} in [J . Math . Phys . 00 ( 0000 ) 000000] to the scenario of nondiagonal $\Sigma$ and more generally distributed $X$ .
We present empirical data on misprints in citations to twelve high-profile papers . The great majority of misprints are identical to misprints in articles that earlier cited the same paper . The distribution of the numbers of misprint repetitions follows a power law . We develop a stochastic model of the citation process , which explains these findings and shows that about 00-00% of scientific citations are copied from the lists of references used in other papers . Citation copying can explain not only why some misprints become popular , but also why some papers become highly cited . We show that a model where a scientist picks few random papers , cites them , and copies a fraction of their references accounts quantitatively for empirically observed distribution of citations .
In this manuscript , we study the statistical properties of convex clustering . We establish that convex clustering is closely related to single linkage hierarchical clustering and $k$-means clustering . In addition , we derive the range of tuning parameter for convex clustering that yields a non-trivial solution . We also provide an unbiased estimate of the degrees of freedom , and provide a finite sample bound for the prediction error for convex clustering . We compare convex clustering to some traditional clustering methods in simulation studies .
In the setting of high-dimensional linear regression models , we propose two frameworks for constructing pointwise and group confidence sets for penalized estimators which incorporate prior knowledge about the organization of the non-zero coefficients . This is done by desparsifying the estimator as in van de Geer et al . [00] and van de Geer and Stucky [00] , then using an appropriate estimator for the precision matrix $\Theta$ . In order to estimate the precision matrix a corresponding structured matrix norm penalty has to be introduced . After normalization the result is an asymptotic pivot . The asymptotic behavior is studied and simulations are added to study the differences between the two schemes .
We study admission control mechanisms for wireless access networks where ( i ) each user has a minimum service requirement , ( ii ) the capacity of the access network is limited , and ( iii ) the access point is not allowed to use monetary mechanisms to guarantee that users do not lie when disclosing their minimum service requirements . To guarantee truthfulness , we use auction theory to design a mechanism where users compete to be admitted into the network . We propose admission control mechanisms under which the access point intelligently allocates resources based on the announced minimum service requirements to ensure that users have no incentive to lie and the capacity constraint is fulfilled . We also prove the properties that any feasible mechanism should have .
In this paper we study the consistency of an empirical minimum error entropy ( MEE ) algorithm in a regression setting . We introduce two types of consistency . The error entropy consistency , which requires the error entropy of the learned function to approximate the minimum error entropy , is shown to be always true if the bandwidth parameter tends to 0 at an appropriate rate . The regression consistency , which requires the learned function to approximate the regression function , however , is a complicated issue . We prove that the error entropy consistency implies the regression consistency for homoskedastic models where the noise is independent of the input variable . But for heteroskedastic models , a counterexample is used to show that the two types of consistency do not coincide . A surprising result is that the regression consistency is always true , provided that the bandwidth parameter tends to infinity at an appropriate rate . Regression consistency of two classes of special models is shown to hold with fixed bandwidth parameter , which further illustrates the complexity of regression consistency of MEE . Fourier transform plays crucial roles in our analysis .
This paper deals with the existence issue of non-central Wishart distributions which is a research topic initiated by Wishart ( 0000 ) , and with important contributions by e . g . , L\ ' evy ( 0000 ) , Gindikin ( 0000 ) , Shanbhag ( 0000 ) , Peddada and Richards ( 0000 ) . We present a new method involving the theory of affine Markov processes , which reveals joint necessary conditions on shape and non-centrality parameter . While Eaton ' s conjecture concerning the necessary range of the shape parameter is confirmed , we also observe that it is not sufficient anymore that it only belongs to the Gindikin ensemble , as is in the central case .
Suppose the data consist of a set $S$ of points $x_j , 0 \leq j \leq J$ , distributed in a bounded domain $D \subset R^N$ , where $N$ and $J$ are large numbers . In this paper an algorithm is proposed for checking whether there exists a manifold $\mathbb{M}$ of low dimension near which many of the points of $S$ lie and finding such $\mathbb{M}$ if it exists . There are many dimension reduction algorithms , both linear and non-linear . Our algorithm is simple to implement and has some advantages compared with the known algorithms . If there is a manifold of low dimension near which most of the data points lie , the proposed algorithm will find it . Some numerical results are presented illustrating the algorithm and analyzing its performance compared to the classical PCA ( principal component analysis ) and Isomap .
This paper explores the homogeneity of coefficients in high-dimensional regression , which extends the sparsity concept and is more general and suitable for many applications . Homogeneity arises when one expects regression coefficients corresponding to neighboring geographical regions or a similar cluster of covariates to be approximately the same . Sparsity corresponds to a special case of homogeneity with a known atom zero . In this article , we propose a new method called clustering algorithm in regression via data-driven segmentation ( CARDS ) to explore homogeneity . New mathematics are provided on the gain that can be achieved by exploring homogeneity . Statistical properties of two versions of CARDS are analyzed . In particular , the asymptotic normality of our proposed CARDS estimator is established , which reveals better estimation accuracy for homogeneous parameters than that without homogeneity exploration . When our methods are combined with sparsity exploration , further efficiency can be achieved beyond the exploration of sparsity alone . This provides additional insights into the power of exploring low-dimensional strucuture in high-dimensional regression : homogeneity and sparsity . The newly developed method is further illustrated by simulation studies and applications to real data .
This paper introduces and addresses a wide class of stochastic bandit problems where the function mapping the arm to the corresponding reward exhibits some known structural properties . Most existing structures ( e . g . linear , Lipschitz , unimodal , combinatorial , dueling , . . . ) are covered by our framework . We derive an asymptotic instance-specific regret lower bound for these problems , and develop OSSB , an algorithm whose regret matches this fundamental limit . OSSB is not based on the classical principle of " optimism in the face of uncertainty " or on Thompson sampling , and rather aims at matching the minimal exploration rates of sub-optimal arms as characterized in the derivation of the regret lower bound . We illustrate the efficiency of OSSB using numerical experiments in the case of the linear bandit problem and show that OSSB outperforms existing algorithms , including Thompson sampling .
For several years , MPI has been the de facto standard for writing parallel applications . One of the most popular MPI implementations is MPICH . Its successor , MPICH0 , features a completely new design that provides more performance and flexibility . To ensure portability , it has a hierarchical structure based on which porting can be done at different levels . In this paper , we present our experiences designing and implementing MPICH0 over InfiniBand . Because of its high performance and open standard , InfiniBand is gaining popularity in the area of high-performance computing . Our study focuses on optimizing the performance of MPI-0 functions in MPICH0 . One of our objectives is to exploit Remote Direct Memory Access ( RDMA ) in Infiniband to achieve high performance . We have based our design on the RDMA Channel interface provided by MPICH0 , which encapsulates architecture-dependent communication functionalities into a very small set of functions . Starting with a basic design , we apply different optimizations and also propose a zero-copy-based design . We characterize the impact of our optimizations and designs using microbenchmarks . We have also performed an application-level evaluation using the NAS Parallel Benchmarks . Our optimized MPICH0 implementation achieves 0 . 0 $\mu$s latency and 000 MB/s bandwidth , which are close to the raw performance of the underlying InfiniBand layer . Our study shows that the RDMA Channel interface in MPICH0 provides a simple , yet powerful , abstraction that enables implementations with high performance by exploiting RDMA operations in InfiniBand . To the best of our knowledge , this is the first high-performance design and implementation of MPICH0 on InfiniBand using RDMA support .
In this paper , we define a kernel estimator for the tail index of a Pareto-type distribution under random right-truncation and establish its asymptotic normality . A simulation study shows that , compared to the estimators recently proposed by Gardes & Stupfler ( 0000 ) and Benchaira et al . ( 0000 ) , this newly introduced estimator behaves better , in terms of bias and mean squared error , for small samples .
Large-scale regression problems where both the number of variables , $p$ , and the number of observations , $n$ , may be large and in the order of millions or more , are becoming increasingly more common . Typically the data are sparse : only a fraction of a percent of the entries in the design matrix are non-zero . Nevertheless , often the only computationally feasible approach is to perform dimension reduction to obtain a new design matrix with far fewer columns , and then work with this compressed data . $b$-bit min-wise hashing ( Li and Konig , 0000 ) is a promising dimension reduction scheme for sparse matrices . In this work we study the prediction error of procedures which perform regression in the new lower-dimensional space after applying the method . For both linear and logistic models we show that the average prediction error vanishes asymptotically as long as $q \|\beta^*\|_0^0 /n \rightarrow 0$ , where $q$ is the average number of non-zero entries in each row of the design matrix and $\beta^*$ is the coefficient of the linear predictor . We also show that ordinary least squares or ridge regression applied to the reduced data in a sense amounts to a non-parametric regression and can in fact allow us fit more flexible models . We obtain non-asymptotic prediction error bounds for interaction models and for models where an unknown row normalisation must be applied before the signal is linear in the predictors .
In dynamic topic modeling , the proportional contribution of a topic to a document depends on the temporal dynamics of that topic ' s overall prevalence in the corpus . We extend the Dynamic Topic Model of Blei and Lafferty ( 0000 ) by explicitly modeling document level topic proportions with covariates and dynamic structure that includes polynomial trends and periodicity . A Markov Chain Monte Carlo ( MCMC ) algorithm that utilizes Polya-Gamma data augmentation is developed for posterior inference . Conditional independencies in the model and sampling are made explicit , and our MCMC algorithm is parallelized where possible to allow for inference in large corpora . To address computational bottlenecks associated with Polya-Gamma sampling , we appeal to the Central Limit Theorem to develop a Gaussian approximation to the Polya-Gamma random variable . This approximation is fast and reliable for parameter values relevant in the text mining domain . Our model and inference algorithm are validated with multiple simulation examples , and we consider the application of modeling trends in PubMed abstracts . We demonstrate that sharing information across documents is critical for accurately estimating document-specific topic proportions . We also show that explicitly modeling polynomial and periodic behavior improves our ability to predict topic prevalence at future time points .
Sentiment Analysis ( SA ) is an action research area in the digital age . With rapid and constant growth of online social media sites and services , and the increasing amount of textual data such as - statuses , comments , reviews etc . available in them , application of automatic SA is on the rise . However , most of the research works on SA in natural language processing ( NLP ) are based on English language . Despite being the sixth most widely spoken language in the world , Bangla still does not have a large and standard dataset . Because of this , recent research works in Bangla have failed to produce results that can be both comparable to works done by others and reusable as stepping stones for future researchers to progress in this field . Therefore , we first tried to provide a textual dataset - that includes not just Bangla , but Romanized Bangla texts as well , is substantial , post-processed and multiple validated , ready to be used in SA experiments . We tested this dataset in Deep Recurrent model , specifically , Long Short Term Memory ( LSTM ) , using two types of loss functions - binary crossentropy and categorical crossentropy , and also did some experimental pre-training by using data from one validation to pre-train the other and vice versa . Lastly , we documented the results along with some analysis on them , which were promising .
A new system for object detection in cluttered RGB-D images is presented . Our main contribution is a new method called Bingham Procrustean Alignment ( BPA ) to align models with the scene . BPA uses point correspondences between oriented features to derive a probability distribution over possible model poses . The orientation component of this distribution , conditioned on the position , is shown to be a Bingham distribution . This result also applies to the classic problem of least-squares alignment of point sets , when point features are orientation-less , and gives a principled , probabilistic way to measure pose uncertainty in the rigid alignment problem . Our detection system leverages BPA to achieve more reliable object detections in clutter .
We consider interactive learning and covering problems , in a setting where actions may incur different costs , depending on the response to the action . We propose a natural greedy algorithm for response-dependent costs . We bound the approximation factor of this greedy algorithm in active learning settings as well as in the general setting . We show that a different property of the cost function controls the approximation factor in each of these scenarios . We further show that in both settings , the approximation factor of this greedy algorithm is near-optimal among all greedy algorithms . Experiments demonstrate the advantages of the proposed algorithm in the response-dependent cost setting .
We provide a model to understand how adverse weather conditions modify traffic flow dynamic . We first prove that the microscopic Free Flow Speed of the vehicles is changed and then provide a rule to model this change . For this , we consider a thresholded linear model , corresponding to an application of a MARS model to road trafficking . This model adapts itself locally to the whole road network and provides accurate unbiased forecasted speed using live or short term forecasted weather data information .
In this paper we present the FolksoDriven Cloud ( FDC ) built on Cloud and on Semantic technologies . Cloud computing has emerged in these recent years as the new paradigm for the provision of on-demand distributed computing resources . Semantic Web can be used for relationship between different data and descriptions of services to annotate provenance of repositories on ontologies . The FDC service is composed of a back-end which submits and monitors the documents , and a user front-end which allows users to schedule on-demand operations and to watch the progress of running processes . The impact of the proposed method is illustrated on a user since its inception .
There is an intuitive analogy of an organic chemist ' s understanding of a compound and a language speaker ' s understanding of a word . Consequently , it is possible to introduce the basic concepts and analyze potential impacts of linguistic analysis to the world of organic chemistry . In this work , we cast the reaction prediction task as a translation problem by introducing a template-free sequence-to-sequence model , trained end-to-end and fully data-driven . We propose a novel way of tokenization , which is arbitrarily extensible with reaction information . With this approach , we demonstrate results superior to the state-of-the-art solution by a significant margin on the top-0 accuracy . Specifically , our approach achieves an accuracy of 00 . 0% without relying on auxiliary knowledge such as reaction templates . Also , 00 . 0% accuracy is reached on a larger and noisier dataset .
Psychological disorders like major depressive disorder can be seen as complex dynamical systems . By looking at symptom activation patterns , we can investigate the dynamic behaviour of individuals to see whether or not they are at risk for sudden changes ( phase transitions ) . Here , we show how a mean field approximation is used to reduce a dynamic multidimensional system to one-dimensional system to analyse the dynamics . Using maximum likelihood estimation , we can estimate the parameter of interest which , in combination with a bifurcation diagram , reflects the risk that someone has for experiencing a transition . After validating the proposed method with simulated data , we apply this method to three empirical examples , where we validate our method using data that contains a transition , and where we show its use in a clinical and general sample . Results show an increased risk for a transition when the transition actually occurred , and that members of both the clinical and general sample were not susceptible to transitions from a healthy to a depressed mood , or vice versa . We conclude that the mean field approximation is valid to assess the risk for a transition , and could in the future aid clinical therapists in the treatment of depressed patient .
Edge bundling methods can effectively alleviate visual clutter and reveal high-level graph structures in large graph visualization . Researchers have devoted significant efforts to improve edge bundling according to different metrics . As the edge bundling family evolve rapidly , the quality of edge bundles receives increasing attention in the literature accordingly . In this paper , we present MLSEB , a novel method to generate edge bundles based on moving least squares ( MLS ) approximation . In comparison with previous edge bundling methods , we argue that our MLSEB approach can generate better results based on a quantitative metric of quality , and also ensure scalability and the efficiency for visualizing large graphs .
Rodent hippocampal population codes represent important spatial information about the environment during navigation . Several computational methods have been developed to uncover the neural representation of spatial topology embedded in rodent hippocampal ensemble spike activity . Here we extend our previous work and propose a nonparametric Bayesian approach to infer rat hippocampal population codes during spatial navigation . To tackle the model selection problem , we leverage a nonparametric Bayesian model . Specifically , to analyze rat hippocampal ensemble spiking activity , we apply a hierarchical Dirichlet process-hidden Markov model ( HDP-HMM ) using two Bayesian inference methods , one based on Markov chain Monte Carlo ( MCMC ) and the other based on variational Bayes ( VB ) . We demonstrate the effectiveness of our Bayesian approaches on recordings from a freely-behaving rat navigating in an open field environment . We find that MCMC-based inference with Hamiltonian Monte Carlo ( HMC ) hyperparameter sampling is flexible and efficient , and outperforms VB and MCMC approaches with hyperparameters set by empirical Bayes .
The distance standard deviation , which arises in distance correlation analysis of multivariate data , is studied as a measure of spread . New representations for the distance standard deviation are obtained in terms of Gini ' s mean difference and in terms of the moments of spacings of order statistics . Inequalities for the distance variance are derived , proving that the distance standard deviation is bounded above by the classical standard deviation and by Gini ' s mean difference . Further , it is shown that the distance standard deviation satisfies the axiomatic properties of a measure of spread . Explicit closed-form expressions for the distance variance are obtained for a broad class of parametric distributions . The asymptotic distribution of the sample distance variance is derived .
We introduce an axiomatic approach to group recommendations , in line of previous work on the axiomatic treatment of trust-based recommendation systems , ranking systems , and other foundational work on the axiomatic approach to internet mechanisms in social choice settings . In group recommendations we wish to recommend to a group of agents , consisting of both opinionated and undecided members , a joint choice that would be acceptable to them . Such a system has many applications , such as choosing a movie or a restaurant to go to with a group of friends , recommending games for online game players , & other communal activities . Our method utilizes a given social graph to extract information on the undecided , relying on the agents influencing them . We first show that a set of fairly natural desired requirements ( a . k . a axioms ) leads to an impossibility , rendering mutual satisfaction of them unreachable . However , we also show a modified set of axioms that fully axiomatize a group variant of the random-walk recommendation system , expanding a previous result from the individual recommendation case .
A CMOS harmonic signal LC oscillator driver for automotive applications working in a harsh environment with high safety critical requirements is described . The driver can be used with a wide range of external components parameters ( LC resonance network of a sensor ) . Quality factor of the external LC network can vary two decades . Amplitude regulation of the driver is digitally controlled and the DAC is constructed as exponential with piece-wise-linear ( PWL ) approximation . Low current consumption for high quality resonance networks is achieved . Realized oscillator is robust , used in safety critical application and has low EMC emissions .
This paper discusses the utility of using simple stiffness and vibrations models , based on the Jacobian matrix of a manipulator and only the rigidity of the actuators , whenever its geometry is optimised . In many works , these simplified models are used to propose optimal design of robots . However , the elasticity of the drive system is often negligible in comparison with the elasticity of the elements , especially in applications where high dynamic performances are needed . Therefore , the use of such a simplified model may lead to the creation of robots with long legs , which will be submitted to large bending and twisting deformations . This paper presents an example of manipulator for which it is preferable to use a complete stiffness or vibration model to obtain the most suitable design and shows that the use of simplified models can lead to mechanisms with poorer rigidity .
An algorithm of searching a zero of an unknown undimensional function is considered , measured at a point x with some error . The step sizes are random positive values and are calculated according to the rule : if two consecutive iterations are in same direction step is multiplied by u>0 , otherwise , it is multiplied by 0<d<0 . The function may have one or more zeros ; the random values are independent and identically distributed , with zero mean and finite variance . Under some additional assumptions on the conditions on the two parameters u and d almost sure convergence of the sequence as well as under some conditions is guaranteed almost sure divergence . In particular , if the error distribuition as median 0 and zero probability for particular poinst then it is established that for ud<0 , convergence takes place , and for ud>0 , divergence . Due to the multiplicative rule of updating of the step , it is natural to expect that the sequence converges rapidly : like a geometric progression ( if convergence takes place ) , but the limit value may not coincide with , but instead , approximates one of zeros of the function . By adjusting the parameters u and d , one can reach necessary precision of approximation ; higher precision is obtained at the expense of lower convergence rate .
In this paper we present clustering method is very sensitive to the initial center values , requirements on the data set too high , and cannot handle noisy data the proposal method is using information entropy to initialize the cluster centers and introduce weighting parameters to adjust the location of cluster centers and noise problems . The navigation datasets which are sequential in nature , Clustering web data is finding the groups which share common interests and behavior by analyzing the data collected in the web servers , this improves clustering on web data efficiently using improved fuzzy c-means ( FCM ) clustering . Web usage mining is the application of data mining techniques to web log data repositories . It is used in finding the user access patterns from web access log . Web data Clusters are formed using on MSNBC web navigation dataset .
Prompted by a recent experiment by Victor Haghani and Richard Dewey , this note generalises the Kelly strategy ( optimal for simple investment games with log utility ) to a large class of practical utility functions and including the effect of extraneous wealth . A counterintuitive result is proved : for any continuous , concave , differentiable utility function , the optimal choice at every point depends only on the probability of reaching that point . The practical calculation of the optimal action at every stage is made possible through use of the binomial expansion , reducing the problem size from exponential to quadratic . Applications include ( better ) automatic investing and risk taking under uncertainty .
GROMACS is a widely used package for biomolecular simulation , and over the last two decades it has evolved from small-scale efficiency to advanced heterogeneous acceleration and multi-level parallelism targeting some of the largest supercomputers in the world . Here , we describe some of the ways we have been able to realize this through the use of parallelization on all levels , combined with a constant focus on absolute performance . Release 0 . 0 of GROMACS uses SIMD acceleration on a wide range of architectures , GPU offloading acceleration , and both OpenMP and MPI parallelism within and between nodes , respectively . The recent work on acceleration made it necessary to revisit the fundamental algorithms of molecular simulation , including the concept of neighborsearching , and we discuss the present and future challenges we see for exascale simulation - in particular a very fine-grained task parallelism . We also discuss the software management , code peer review and continuous integration testing required for a project of this complexity .
Spatial ecological networks are widely used to model interactions between georeferenced biological entities ( e . g . , populations or communities ) . The analysis of such data often leads to a two-step approach where groups containing similar biological entities are firstly identified and the spatial information is used afterwards to improve the ecological interpretation . We develop an integrative approach to retrieve groups of nodes that are geographically close and ecologically similar . Our model-based spatially-constrained method embeds the geographical information within a regularization framework by adding some constraints to the maximum likelihood estimation of parameters . A simulation study and the analysis of real data demonstrate that our approach is able to detect complex spatial patterns that are ecologically meaningful . The model-based framework allows us to consider external information ( e . g . , geographic proximities , covariates ) in the analysis of ecological networks and appears to be an appealing alternative to consider such data .
We pose a fundamental problem of public blockchain , " incentive mismatch . " It is an open problem , but application portability is a provisional solution to the problem . Portability is also a desirable property for an application on a private blockchain . It is not even clear to be able to define a common API for various blockchain middlewares , but it is possible to improve portability by reducing dependency on a blockchain . We present an example of such middleware designs that provide application portability and especially support migration between blockchains .
We consider a binary unsupervised classification problem where each observation is associated with an unobserved label that we want to retrieve . More precisely , we assume that there are two groups of observation : normal and abnormal . The `normal ' observations are coming from a known distribution whereas the distribution of the `abnormal ' observations is unknown . Several models have been developed to fit this unknown distribution . In this paper , we propose an alternative based on a mixture of Gaussian distributions . The inference is done within a variational Bayesian framework and our aim is to infer the posterior probability of belonging to the class of interest . To this end , it makes no sense to estimate the mixture component number since each mixture model provides more or less relevant information to the posterior probability estimation . By computing a weighted average ( named aggregated estimator ) over the model collection , Bayesian Model Averaging ( BMA ) is one way of combining models in order to account for information provided by each model . The aim is then the estimation of the weights and the posterior probability for one specific model . In this work , we derive optimal approximations of these quantities from the variational theory and propose other approximations of the weights . To perform our method , we consider that the data are dependent ( Markovian dependency ) and hence we consider a Hidden Markov Model . A simulation study is carried out to evaluate the accuracy of the estimates in terms of classification . We also present an application to the analysis of public health surveillance systems .
Black-box alpha ( BB-$\alpha$ ) is a new approximate inference method based on the minimization of $\alpha$-divergences . BB-$\alpha$ scales to large datasets because it can be implemented using stochastic gradient descent . BB-$\alpha$ can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients . These gradients can be easily obtained using automatic differentiation . By changing the divergence parameter $\alpha$ , the method is able to interpolate between variational Bayes ( VB ) ( $\alpha \rightarrow 0$ ) and an algorithm similar to expectation propagation ( EP ) ( $\alpha = 0$ ) . Experiments on probit regression and neural network regression and classification problems show that BB-$\alpha$ with non-standard settings of $\alpha$ , such as $\alpha = 0 . 0$ , usually produces better predictions than with $\alpha \rightarrow 0$ ( VB ) or $\alpha = 0$ ( EP ) .
An open partition \pi{} [Cod00a , Cod00b] of a tree T is a partition of the vertices of T with the property that , for each block B of \pi , the upset of B is a union of blocks of \pi . This paper deals with the number , NP ( n ) , of open partitions of the tree , V_n , made of two chains with n points each , that share the root .
The sensitivity of human ear is dependent on frequency which is nonlinearly resolved across the audio spectrum . Now to improve the recognition performance in a similar non linear approach requires a front -end design , suggested by empirical evidences . A popular alternative to linear prediction based analysis is therefore filter bank analysis since this provides a much more straightforward route to obtain the desired non-linear frequency resolution . MEL filter bank and BARK filter bank are two popular filter bank analysis techniques . This paper presents FPGA based implementation of MEL filter bank and BARK filter bank with different bandwidths and different signal spectrum ranges . The designs have been implemented using VHDL , simulated and verified using Xilinx 00 . 0 . For each filter bank , the basic building block is implemented in Spartan 0E . A comparative study among these two mentioned filter banks is also done in this paper .
This article introduces a non parametric warping model for functional data . When the outcome of an experiment is a sample of curves , data can be seen as realizations of a stochastic process , which takes into account the small variations between the different observed curves . The aim of this work is to define a mean pattern which represents the main behaviour of the set of all the realizations . So we define the structural expectation of the underlying stochastic function . Then we provide empirical estimators of this structural expectation and of each individual warping function . Consistency and asymptotic normality for such estimators are proved .
Artifical Neural Networks are a particular class of learning systems modeled after biological neural functions with an interesting penchant for Hebbian learning , that is " neurons that wire together , fire together " . However , unlike their natural counterparts , artificial neural networks have a close and stringent coupling between the modules of neurons in the network . This coupling or locking imposes upon the network a strict and inflexible structure that prevent layers in the network from updating their weights until a full feed-forward and backward pass has occurred . Such a constraint though may have sufficed for a while , is now no longer feasible in the era of very-large-scale machine learning , coupled with the increased desire for parallelization of the learning process across multiple computing infrastructures . To solve this problem , synthetic gradients ( SG ) with decoupled neural interfaces ( DNI ) are introduced as a viable alternative to the backpropagation algorithm . This paper performs a speed benchmark to compare the speed and accuracy capabilities of SG-DNI as opposed to a standard neural interface using multilayer perceptron MLP . SG-DNI shows good promise , in that it not only captures the learning problem , it is also over 0-fold faster due to it asynchronous learning capabilities .
Data-mining techniques have frequently been developed for Spontaneous reporting databases . These techniques aim to find adverse drug events accurately and efficiently . Spontaneous reporting databases are prone to missing information , under reporting and incorrect entries . This often results in a detection lag or prevents the detection of some adverse drug events . These limitations do not occur in electronic health-care databases . In this paper , existing methods developed for spontaneous reporting databases are implemented on both a spontaneous reporting database and a general practice electronic health-care database and compared . The results suggests that the application of existing methods to the general practice database may help find signals that have gone undetected when using the spontaneous reporting system database . In addition the general practice database provides far more supplementary information , that if incorporated in analysis could provide a wealth of information for identifying adverse events more accurately .
Music Sight Reading is a complex process in which when it is occurred in the brain some learning attributes would be emerged . Besides giving a model based on actor-critic method in the Reinforcement Learning , the agent is considered to have a neural network structure . We studied on where the sight reading process is happened and also a serious problem which is how the synaptic weights would be adjusted through the learning process . The model we offer here is a computational model on which an updated weights equation to fix the weights is accompanied too .
We describe a Markov latent state space ( MLSS ) model , where the latent state distribution is a decaying mixture over multiple past states . We present a simple sampling algorithm that allows to approximate such high-order MLSS with fixed time and memory costs .
In the world of multivariate extremes , estimation of the dependence structure still presents a challenge and an interesting problem . A procedure for the bivariate case is presented that opens the road to a similar way of handling the problem in a truly multivariate setting . We consider a semi-parametric model in which the stable tail dependence function is parametrically modeled . Given a random sample from a bivariate distribution function , the problem is to estimate the unknown parameter . A method of moments estimator is proposed where a certain integral of a nonparametric , rank-based estimator of the stable tail dependence function is matched with the corresponding parametric version . Under very weak conditions , the estimator is shown to be consistent and asymptotically normal . Moreover , a comparison between the parametric and nonparametric estimators leads to a goodness-of-fit test for the semiparametric model . The performance of the estimator is illustrated for a discrete spectral measure that arises in a factor-type model and for which likelihood-based methods break down . A second example is that of a family of stable tail dependence functions of certain meta-elliptical distributions .
We consider the problem of designing locality sensitive hashes ( LSH ) for inner product similarity , and of the power of asymmetric hashes in this context . Shrivastava and Li argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points . However , we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest . We also show a variant of the settings where asymmetry is in-fact needed , but there a different asymmetric LSH is required .
Neuroimaging modalities such as functional magnetic resonance imaging ( fMRI ) and electroencephalography ( EEG ) provide information about neurological functions in complementary spatiotemporal resolutions ; therefore , fusion of these modalities is expected to provide better understanding of brain activity . In this paper , we jointly analyze fMRI and multi-channel EEG signals collected during an auditory oddball task with the goal of capturing brain activity patterns that differ between patients with schizophrenia and healthy controls . Rather than selecting a single electrode or matricizing the third-order tensor that can be naturally used to represent multi-channel EEG signals , we preserve the multi-way structure of EEG data and use a coupled matrix and tensor factorization ( CMTF ) model to jointly analyze fMRI and EEG signals . Our analysis reveals that ( i ) joint analysis of EEG and fMRI using a CMTF model can capture meaningful temporal and spatial signatures of patterns that behave differently in patients and controls , and ( ii ) these differences and the interpretability of the associated components increase by including multiple electrodes from frontal , motor and parietal areas , but not necessarily by including all electrodes in the analysis .
Designing a photometric system to best fulfil a set of scientific goals is a complex task , demanding a compromise between conflicting requirements and subject to various constraints . A specific example is the determination of stellar astrophysical parameters ( APs ) - effective temperature , metallicity etc . - across a wide range of stellar types . I present a novel approach to this problem which makes minimal assumptions about the required filter system . By considering a filter system as a set of free parameters it may be designed by optimizing some figure-of-merit ( FoM ) with respect to these parameters . In the example considered , the FoM is a measure of how well the filter system can `separate ' stars with different APs . This separation is vectorial in nature , in the sense that the local directions of AP variance are preferably mutually orthogonal to avoid AP degeneracy . The optimization is carried out with an evolutionary algorithm , which uses principles of evolutionary biology to search the parameter space . This model , HFD ( Heuristic Filter Design ) , is applied to the design of photometric systems for the Gaia space astrometry mission . The optimized systems show a number of interesting features , not least the persistence of broad , overlapping filters . These HFD systems perform as least as well as other proposed systems for Gaia , although inadequacies remain in all . The principles underlying HFD are quite generic and may be applied to filter design for numerous other projects , such as the search for specific types of objects or photometric redshift determination .
Good quality video coding for low bit-rate applications is important for transmission over narrow-bandwidth channels and for storage with limited memory capacity . In this work , we develop a previous analysis for image compression at low bit-rates to adapt it to video signals . Improving compression using down-scaling in the spatial and temporal dimensions is examined . We show , both theoretically and experimentally , that at low bit-rates , we benefit from applying spatio-temporal scaling . The proposed method includes down-scaling before the compression and a corresponding up-scaling afterwards , while the codec itself is left unmodified . We propose analytic models for low bit-rate compression and spatio-temporal scaling operations . Specifically , we use theoretic models of motion-compensated prediction of available and absent frames as in coding and frame-rate up-conversion ( FRUC ) applications , respectively . The proposed models are designed for multi-resolution analysis . In addition , we formulate a bit-allocation procedure and propose a method for estimating good down-scaling factors of a given video based on its second-order statistics and the given bit-budget . We validate our model with experimental results of H . 000 compression .
These are the proceedings of the Second Workshop on GRAPH Inspection and Traversal Engineering ( GRAPHITE 0000 ) , which took place on March 00 , 0000 in Rome , Italy , as a satellite event of the 00th European Joint Conferences on Theory and Practice of Software ( ETAPS 0000 ) . The topic of the GRAPHITE workshop is graph analysis in all its forms in computer science . Graphs are used to represent data in many application areas , and they are subjected to various computational algorithms in order to acquire the desired information . These graph algorithms tend to have common characteristics , such as duplicate detection to guarantee their termination , independent of their application domain . Over the past few years , it has been shown that the scalability of such algorithms can be dramatically improved by using , e . g . , external memory , by exploiting parallel architectures , such as clusters , multi-core CPUs , and graphics processing units , and by using heuristics to guide the search . Novel techniques to further scale graph search algorithms , and new applications of graph search are within the scope of this workshop . Another topic of interest of the event is more related to the structural properties of graphs : which kind of graph characteristics are relevant for a particular application area , and how can these be measured ? Finally , any novel way of using graphs for a particular application area is on topic . The goal of this event is to gather scientists from different communities , such as model checking , artificial intelligence planning , game playing , and algorithm engineering , who do research on graph search algorithms , such that awareness of each others ' work is increased .
This decade has seen a great deal of progress in the development of information retrieval systems . Unfortunately , we still lack a systematic understanding of the behavior of the systems and their relationship with documents . In this paper we present a completely new approach towards the understanding of the information retrieval systems . Recently , it has been observed that retrieval systems in TREC 0 show some remarkable patterns in retrieving relevant documents . Based on the TREC 0 observations , we introduce a geometric linear model of information retrieval systems . We then apply the model to predict the number of relevant documents by the retrieval systems . The model is also scalable to a much larger data set . Although the model is developed based on the TREC 0 routing test data , I believe it can be readily applicable to other information retrieval systems . In Appendix , we explained a simple and efficient way of making a better system from the existing systems .
A novel regularizer of the PARAFAC decomposition factors capturing the tensor ' s rank is proposed in this paper , as the key enabler for completion of three-way data arrays with missing entries . Set in a Bayesian framework , the tensor completion method incorporates prior information to enhance its smoothing and prediction capabilities . This probabilistic approach can naturally accommodate general models for the data distribution , lending itself to various fitting criteria that yield optimum estimates in the maximum-a-posteriori sense . In particular , two algorithms are devised for Gaussian- and Poisson-distributed data , that minimize the rank-regularized least-squares error and Kullback-Leibler divergence , respectively . The proposed technique is able to recover the " ground-truth ' ' tensor rank when tested on synthetic data , and to complete brain imaging and yeast gene expression datasets with 00% and 00% of missing entries respectively , resulting in recovery errors at -00dB and -00dB .
Standard memory modules to store ( and access ) data are designed for use with a single system accessing it . More complicated memory modules would be accessed through a memory controller , which are also designed for one system . For multiple systems to access a single memory module there must be some facilitation that allows them to access the memory without overriding or corrupting the access from the others . This was done with the use of a memory arbiter , which controls the flow of traffic into the memory controller . The arbiter has a set of rules to abide to in order to choose which system gets through to the memory controller . In this project , a regular RAM module is designed for use with one system . Furthermore , a memory arbiter is also designed in Verilog that allows for more than one system to use a single RAM module in a controlled and synchronized manner . The arbiter uses a fixed priority scheme to avoid starvation of the system . In addition one of the major problems associated with such systems i . e . The Address Clash Problem has been nicely tackled and solved . The design is verified in simulation and validated on a Xilinx ML000 evaluation board with a Virtex 0 FPGA .
The paper focuses on the accuracy improvement of stiffness models for parallel manipulators , which are employed in high-speed precision machining . It is based on the integrated methodology that combines analytical and numerical techniques and deals with multidimensional lumped-parameter models of the links . The latter replace the link flexibility by localized 0-dof virtual springs describing both translational/rotational compliance and the coupling between them . There is presented detailed accuracy analysis of the stiffness identification procedures employed in the commercial CAD systems ( including statistical analysis of round-off errors , evaluating the confidence intervals for stiffness matrices ) . The efficiency of the developed technique is confirmed by application examples , which deal with stiffness analysis of translational parallel manipulators .
We consider the problem of group testing with sum observations and noiseless answers , in which we aim to locate multiple objects by querying the number of objects in each of a sequence of chosen sets . We study a probabilistic setting with entropy loss , in which we assume a joint Bayesian prior density on the locations of the objects and seek to choose the sets queried to minimize the expected entropy of the Bayesian posterior distribution after a fixed number of questions . We present a new non-adaptive policy , called the dyadic policy , show it is optimal among non-adaptive policies , and is within a factor of two of optimal among adaptive policies . This policy is quick to compute , its nonadaptive nature makes it easy to parallelize , and our bounds show it performs well even when compared with adaptive policies . We also study an adaptive greedy policy , which maximizes the one-step expected reduction in entropy , and show that it performs at least as well as the dyadic policy , offering greater query efficiency but reduced parallelism . Numerical experiments demonstrate that both procedures outperform a divide-and-conquer benchmark policy from the literature , called sequential bifurcation , and show how these procedures may be applied in a stylized computer vision problem .
We study the geometric structure of the set of solutions of random $\epsilon$-0-in-k SAT problem . For $l\geq 0$ , two satisfying assignments $A$ and $B$ are $l$-connected if there exists a sequence of satisfying assignments connecting them by changing at most $l$ bits at a time . We first prove that w . h . p . two assignments of a random $\epsilon$-0-in-$k$ SAT instance are $O ( \log n ) $-connected , conditional on being satisfying assignments . Also , there exists $\epsilon_{0}\in ( 0 , \frac{0}{k-0} ) $ such that w . h . p . no two satisfying assignments at distance at least $\epsilon_{0}\cdot n$ form a " hole " in the set of assignments . We believe that this is true for all $\epsilon >0$ , and thus satisfying assignments of a random 0-in-$k$ SAT instance form a single cluster .
We consider the problem of learning a one-hidden-layer neural network with non-overlapping convolutional layer and ReLU activation function , i . e . , $f ( \mathbf{Z} ; \mathbf{w} , \mathbf{a} ) = \sum_j a_j\sigma ( \mathbf{w}^\top\mathbf{Z}_j ) $ , in which both the convolutional weights $\mathbf{w}$ and the output weights $\mathbf{a}$ are parameters to be learned . We prove that with Gaussian input $\mathbf{Z}$ , there is a spurious local minimum that is not a global mininum . Surprisingly , in the presence of local minimum , starting from randomly initialized weights , gradient descent with weight normalization can still be proven to recover the true parameters with constant probability ( which can be boosted to arbitrarily high accuracy with multiple restarts ) . We also show that with constant probability , the same procedure could also converge to the spurious local minimum , showing that the local minimum plays a non-trivial role in the dynamics of gradient descent . Furthermore , a quantitative analysis shows that the gradient descent dynamics has two phases : it starts off slow , but converges much faster after several iterations .
A new method for analyzing high-dimensional categorical data , Linear Latent Structure ( LLS ) analysis , is presented . LLS models belong to the family of latent structure models , which are mixture distribution models constrained to satisfy the local independence assumption . LLS analysis explicitly considers a family of mixed distributions as a linear space and LLS models are obtained by imposing linear constraints on the mixing distribution . LLS models are identifiable under modest conditions and are consistently estimable . A remarkable feature of LLS analysis is the existence of a high-performance numerical algorithm , which reduces parameter estimation to a sequence of linear algebra problems . Preliminary simulation experiments with a prototype of the algorithm demonstrated a good quality of restoration of model parameters .
This paper presents algorithms for hierarchical , agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software . Requirements are : ( 0 ) the input data is given by pairwise dissimilarities between data points , but extensions to vector data are also discussed ( 0 ) the output is a " stepwise dendrogram " , a data structure which is shared by all implementations in current standard software . We present algorithms ( old and new ) which perform clustering in this setting efficiently , both in an asymptotic worst-case analysis and from a practical point of view . The main contributions of this paper are : ( 0 ) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms . ( 0 ) We prove the correctness of two algorithms by Rohlf and Murtagh , which is necessary in each case for different reasons . ( 0 ) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes .
Working under a model of privacy in which data remains private even from the statistician , we study the tradeoff between privacy guarantees and the risk of the resulting statistical estimators . We develop private versions of classical information-theoretic bounds , in particular those due to Le Cam , Fano , and Assouad . These inequalities allow for a precise characterization of statistical rates under local privacy constraints and the development of provably ( minimax ) optimal estimation procedures . We provide a treatment of several canonical families of problems : mean estimation and median estimation , generalized linear models , and nonparametric density estimation . For all of these families , we provide lower and upper bounds that match up to constant factors , and exhibit new ( optimal ) privacy-preserving mechanisms and computationally efficient estimators that achieve the bounds . Additionally , we present a variety of experimental results for estimation problems involving sensitive data , including salaries , censored blog posts and articles , and drug abuse ; these experiments demonstrate the importance of deriving optimal procedures .
Mechanism design has found considerable application to the construction of agent-interaction protocols . In the standard setting , the type ( e . g . , utility function ) of an agent is not known by other agents , nor is it known by the mechanism designer . When this uncertainty is quantified probabilistically , a mechanism induces a game of incomplete information among the agents . However , in many settings , uncertainty over utility functions cannot easily be quantified . We consider the problem of incomplete information games in which type uncertainty is strict or unquantified . We propose the use of minimax regret as a decision criterion in such games , a robust approach for dealing with type uncertainty . We define minimax-regret equilibria and prove that these exist in mixed strategies for finite games . We also consider the problem of mechanism design in this framework by adopting minimax regret as an optimization criterion for the designer itself , and study automated optimization of such mechanisms .
Given sparse multi-dimensional data ( e . g . , ( user , movie , time ; rating ) for movie recommendations ) , how can we discover latent concepts/relations and predict missing values ? Tucker factorization has been widely used to solve such problems with multi-dimensional data , which are modeled as tensors . However , most Tucker factorization algorithms regard and estimate missing entries as zeros , which triggers a highly inaccurate decomposition . Moreover , few methods focusing on an accuracy exhibit limited scalability since they require huge memory and heavy computational costs while updating factor matrices . In this paper , we propose P-Tucker , a scalable Tucker factorization method for sparse tensors . P-Tucker performs an alternating least squares with a gradient-based update rule in a fully parallel way , which significantly reduces memory requirements for updating factor matrices . Furthermore , we offer two variants of P-Tucker : a caching algorithm P-Tucker-CACHE and an approximation algorithm P-Tucker-APPROX , both of which accelerate the update process . Experimental results show that P-Tucker exhibits 0 . 0-00 . 0x speed-up and 0 . 0-0 . 0x less error compared to the state-of-the-art . In addition , P-Tucker scales near linearly with the number of non-zeros in a tensor and number of threads . Thanks to P-Tucker , we successfully discover hidden concepts and relations in a large-scale real-world tensor , while existing methods cannot reveal latent features due to their limited scalability or low accuracy .
We consider apprenticeship learning , i . e . , having an agent learn a task by observing an expert demonstrating the task in a partially observable environment when the model of the environment is uncertain . This setting is useful in applications where the explicit modeling of the environment is difficult , such as a dialogue system . We show that we can extract information about the environment model by inferring action selection process behind the demonstration , under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment . Proposed algorithms can achieve more accurate estimates of POMDP parameters and better policies from a short demonstration , compared to methods that learns only from the reaction from the environment .
We apply the Law of Total Probability to the construction of scale-invariant probability distribution functions ( pdfs ) , and require that probability measures be dimensionless and unitless under a continuous change of scales . If the scale-change distribution function is scale invariant then the constructed distribution will also be scale invariant . Repeated application of this construction on an arbitrary set of ( normalizable ) pdfs results again in scale-invariant distributions . The invariant function of this procedure is given uniquely by the reciprocal distribution , suggesting a kind of universality . We separately demonstrate that the reciprocal distribution results uniquely from requiring maximum entropy for size-class distributions with uniform bin sizes .
This Generalized Discriminant Analysis ( GDA ) has provided an extremely powerful approach to extracting non linear features . The network traffic data provided for the design of intrusion detection system always are large with ineffective information , thus we need to remove the worthless information from the original high dimensional database . To improve the generalization ability , we usually generate a small set of features from the original input variables by feature extraction . The conventional Linear Discriminant Analysis ( LDA ) feature reduction technique has its limitations . It is not suitable for non linear dataset . Thus we propose an efficient algorithm based on the Generalized Discriminant Analysis ( GDA ) feature reduction technique which is novel approach used in the area of cyber attack detection . This not only reduces the number of the input features but also increases the classification accuracy and reduces the training and testing time of the classifiers by selecting most discriminating features . We use Artificial Neural Network ( ANN ) and C0 . 0 classifiers to compare the performance of the proposed technique . The result indicates the superiority of algorithm .
Estimation of convex functions finds broad applications in engineering and science , while convex shape constraint gives rise to numerous challenges in asymptotic performance analysis . This paper is devoted to minimax optimal estimation of univariate convex functions from the H\ " older class in the framework of shape constrained nonparametric estimation . Particularly , the paper establishes the optimal rate of convergence in two steps for the minimax sup-norm risk of convex functions with the H\ " older order between one and two . In the first step , by applying information theoretical results on probability measure distance , we establish the minimax lower bound under the supreme norm by constructing a novel family of piecewise quadratic convex functions in the H\ " older class . In the second step , we develop a penalized convex spline estimator and establish the minimax upper bound under the supreme norm . Due to the convex shape constraint , the optimality conditions of penalized convex splines are characterized by nonsmooth complementarity conditions . By exploiting complementarity methods , a critical uniform Lipschitz property of optimal spline coefficients in the infinity norm is established . This property , along with asymptotic estimation techniques , leads to uniform bounds for bias and stochastic errors on the entire interval of interest . This further yields the optimal rate of convergence by choosing the suitable number of knots and penalty value . The present paper provides the first rigorous justification of the optimal minimax risk for convex estimation under the supreme norm .
We prove a new concentration inequality for the excess risk of a M-estimator in least-squares regression with random design and heteroscedastic noise . This kind of result is a central tool in modern model selection theory , as well as in recent achievements concerning the behavior of regularized estimators such as LASSO , group LASSO and SLOPE .
Molecular communication in nanonetworks is an emerging communication paradigm where molecules are used as information carriers . Concentration Shift Keying ( CSK ) and Molecule Shift Keying ( MoSK ) are being studied extensively for the short and medium range molecular nanonetworks . It is observed that MoSK outperforms CSK . However , MoSK requires different types of molecules for encoding which render transmitter and receiver complexities . We propose a modulation scheme called On-Off MoSK ( OOMoSK ) in which , molecules are released for information bit 0 and no molecule is released for 0 . The proposed scheme enjoys reduced number of the types of molecules for encoding . Numerical results show that the proposed scheme enhances channel capacity and Symbol Error Rate ( SER ) .
Spreadsheets are software programs which are typically created by end-users and often used for business-critical tasks . Many studies indicate that errors in spreadsheets are very common . Thus , a number of vendors offer auditing tools which promise to detect errors by checking spreadsheets against so-called Best Practices such as " Don ' t put constants in fomulae " . Unfortunately , it is largely unknown which Best Practices have which actual effects on which spreadsheet quality aspects in which settings . We have conducted a controlled experiment with 00 subjects to investigate the question whether observance of three commonly suggested Best Practices is correlated with desired positive effects regarding correctness and maintainability : " Do not put constants in formulae " , " keep formula complexity low " and " refer to the left and above " . The experiment was carried out in two phases which covered the creation of new and the modification of existing spreadsheets . It was evaluated using a novel construction kit for spreadsheet auditing tools called Spreadsheet Inspection Framework . The experiment produced a small sample of directly comparable spreadsheets which all try to solve the same task . Our analysis of the obtained spreadsheets indicates that the correctness of " bottom-line " results is not affected by the observance of the three Best Practices . However , initially correct spreadsheets with high observance of these Best Practices tend to be the ones whose later modifications yield the most correct results .
We consider a Kullback-Leibler-based algorithm for the stochastic multi-armed bandit problem in the case of distributions with finite supports ( not necessarily known beforehand ) , whose asymptotic regret matches the lower bound of \cite{Burnetas00} . Our contribution is to provide a finite-time analysis of this algorithm ; we get bounds whose main terms are smaller than the ones of previously known algorithms with finite-time analyses ( like UCB-type algorithms ) .
Research on peer effects in sociology has been focused for long on social influence power to investigate the social foundations for social interactions . This paper extends Xu ( 0000 ) ' s large--network--based game model by allowing for social-influence-dependent peer effects . In a large network , we use the Katz--Bonacich centrality to measure individuals ' social influences . To solve the computational burden when the data come from the equilibrium of a large network , we extend Aguirregabiria and Mira ( 0000 ) ' s nested pseudo likelihood estimation ( NPLE ) approach to our large network game model . Using the Add Health dataset , we investigate peer effects on conducting dangerous behaviors of high school students . Our results show that peer effects are statistically significant and positive . Moreover , a student benefits more ( statistically significant at the 0% level ) from her conformity , or equivalently , pays more for her disobedience , in terms of peer pressures , if friends have higher social-influence status .
De Facto , signal processing is the interpolation and extrapolation of a sequence of observations viewed as a realization of a stochastic process . Its role in applied statistics ranges from scenarios in forecasting and time series analysis , to image reconstruction , machine learning , and the degradation modeling for reliability assessment . A general solution to the problem of filtering and prediction entails some formidable mathematics . Efforts to circumvent the mathematics has resulted in the need for introducing more explicit descriptions of the underlying process . One such example , and a noteworthy one , is the Kalman Filter Model , which is a special case of state space models or what statisticians refer to as Dynamic Linear Models . Implementing the Kalman Filter Model in the era of " big and high velocity non-Gaussian data " can pose computational challenges with respect to efficiency and timeliness . Particle filtering is a way to ease such computational burdens . The purpose of this paper is to trace the historical evolution of this development from its inception to its current state , with an expository focus on two versions of the particle filter , namely , the propagate first-update next and the update first-propagate next version . By way of going beyond a pure review , this paper also makes transparent the importance and the role of a less recognized principle , namely the principle of conditionalization , in filtering and prediction based on Bayesian methods . Furthermore , the paper also articulates the philosophical underpinnings of the filtering and prediction set-up , a matter that needs to ne made explicit , and Yule ' s decomposition of a random variable in terms of a sequence of innovations .
We develop a maximum-likelihood based method for regression in a setting where the dependent variable is a random graph and covariates are available on a graph-level . The model generalizes the well-known $\beta$-model for random graphs by replacing the constant model parameters with regression functions . Cram\ ' er-Rao bounds are derived for the undirected $\beta$-model , the directed $\beta$-model , and the generalized $\beta$-model . The corresponding maximum likelihood estimators are compared to the bounds by means of simulations . Moreover , examples are given on how to use the presented maximum likelihood estimators to test for directionality and significance . Last , the applicability of the model is demonstrated using dynamic social network data describing communication among healthcare workers .
Submodular maximization ( SM ) has become a silver bullet for a broad class of applications such as influence maximization , data summarization , top-$k$ representative queries , and recommendations . In this paper , we study the SM problem in data streams . Most existing algorithms for streaming SM only support the append-only model with cardinality constraints , which cannot meet the requirements of real-world problems considering either the data recency issues or more general $d$-knapsack constraints . Therefore , we first propose an append-only streaming algorithm {\sc KnapStream} for SM subject to a $d$-knapsack constraint ( SMDK ) . Furthermore , we devise the {\sc KnapWindow} algorithm for SMDK over sliding windows to capture the recency constraints . Theoretically , the proposed algorithms have constant approximation ratios for a fixed number of knapsacks and sublinear complexities . We finally evaluate the efficiency and effectiveness of our algorithms in two real-world datasets . The results show that the proposed algorithms achieve two orders of magnitude speedups over the greedy baseline in the batch setting while preserving high quality solutions .
The hidden Markov model ( HMM ) is a generative model that treats sequential data under the assumption that each observation is conditioned on the state of a discrete hidden variable that evolves in time as a Markov chain . In this paper , we derive a novel algorithm to cluster HMMs through their probability distributions . We propose a hierarchical EM algorithm that i ) clusters a given collection of HMMs into groups of HMMs that are similar , in terms of the distributions they represent , and ii ) characterizes each group by a " cluster center " , i . e . , a novel HMM that is representative for the group . We present several empirical studies that illustrate the benefits of the proposed algorithm .
We introduce a statistical method to investigate the impact of dyadic relations on complex networks generated from repeated interactions . It is based on generalised hypergeometric ensembles , a class of statistical network ensembles developed recently . We represent different types of known relations between system elements by weighted graphs , separated in the different layers of a multiplex network . With our method we can regress the influence of each relational layer , the independent variables , on the interaction counts , the dependent variables . Moreover , we can test the statistical significance of the relations as explanatory variables for the observed interactions . To demonstrate the power of our approach and its broad applicability , we will present examples based on synthetic and empirical data .
The stable marriage problem and its extensions have been extensively studied , with much of the work in the literature assuming that agents fully know their own preferences over alternatives . This assumption however is not always practical ( especially in large markets ) and agents usually need to go through some costly deliberation process in order to learn their preferences . In this paper we assume that such deliberations are carried out via interviews , where an interview involves a man and a woman , each of whom learns information about the other as a consequence . If everybody interviews everyone else , then clearly agents can fully learn their preferences . But interviews are costly , and we may wish to minimize their use . It is often the case , especially in practical settings , that due to correlation between agents ' preferences , it is unnecessary for all potential interviews to be carried out in order to obtain a stable matching . Thus the problem is to find a good strategy for interviews to be carried out in order to minimize their use , whilst leading to a stable matching . One way to evaluate the performance of an interview strategy is to compare it against a naive algorithm that conducts all interviews . We argue however that a more meaningful comparison would be against an optimal offline algorithm that has access to agents ' preference orderings under complete information . We show that , unless P=NP , no offline algorithm can compute the optimal interview strategy in polynomial time . If we are additionally aiming for a particular stable matching ( perhaps one with certain desirable properties ) , we provide restricted settings under which efficient optimal offline algorithms exist .
Quantile regression is a technique to estimate conditional quantile curves . It provides a comprehensive picture of a response contingent on explanatory variables . In a flexible modeling framework , a specific form of the conditional quantile curve is not a priori fixed . % Indeed , the majority of applications do not per se require specific functional forms . This motivates a local parametric rather than a global fixed model fitting approach . A nonparametric smoothing estimator of the conditional quantile curve requires to balance between local curvature and stochastic variability . In this paper , we suggest a local model selection technique that provides an adaptive estimator of the conditional quantile regression curve at each design point . Theoretical results claim that the proposed adaptive procedure performs as good as an oracle which would minimize the local estimation risk for the problem at hand . We illustrate the performance of the procedure by an extensive simulation study and consider a couple of applications : to tail dependence analysis for the Hong Kong stock market and to analysis of the distributions of the risk factors of temperature dynamics .
We conducted an extensive computational experiment , lasting multiple CPU-years , to optimally select parameters for two important classes of algorithms for finding sparse solutions of underdetermined systems of linear equations . We make the optimally tuned implementations available at {\tt sparselab . stanford . edu} ; they run `out of the box ' with no user tuning : it is not necessary to select thresholds or know the likely degree of sparsity . Our class of algorithms includes iterative hard and soft thresholding with or without relaxation , as well as CoSaMP , subspace pursuit and some natural extensions . As a result , our optimally tuned algorithms dominate such proposals . Our notion of optimality is defined in terms of phase transitions , i . e . we maximize the number of nonzeros at which the algorithm can successfully operate . We show that the phase transition is a well-defined quantity with our suite of random underdetermined linear systems . Our tuning gives the highest transition possible within each class of algorithms .
Consider the Gaussian sequence model $y \sim N ( \theta^* , \sigma^0 I_n ) $ , where $\theta^*$ is unknown but known to belong to a closed convex polyhedral set $\mathcal{C} \subset \mathbb{R}^n$ . In this paper we provide a unified characterization of the degrees of freedom for estimators of $\theta^*$ obtained as the ( linearly or quadratically perturbed ) partial projection of $y$ onto $\mathcal{C}$ . As special cases of our results , we derive explicit expressions for the degrees of freedom in many shape restricted regression problems , e . g . , bounded isotonic regression , multivariate convex regression and penalized convex regression . Our general theory also yields , as special cases , known results on the degrees of freedom of many well-studied estimators in the statistics literature , such as ridge regression , Lasso and generalized Lasso . Our results can be readily used to choose the tuning parameter ( s ) involved in the estimation procedure by minimizing the Stein ' s unbiased risk estimate . We illustrate this through simulation studies for bounded isotonic regression and penalized convex regression . As a by-product of our analysis we derive an interesting connection between bounded isotonic regression and isotonic regression on a general partially ordered set , which is of independent interest .
The aim of this note is to provide a pedagogical survey of the recent works by the authors ( arXiv : 0000 . 0000 and arXiv : 0000 . 00000 ) concerning the local behavior of the eigenvalues of large complex correlated Wishart matrices at the edges and cusp points of the spectrum : Under quite general conditions , the eigenvalues fluctuations at a soft edge of the limiting spectrum , at the hard edge when it is present , or at a cusp point , are respectively described by mean of the Airy kernel , the Bessel kernel , or the Pearcey kernel . Moreover , the eigenvalues fluctuations at several soft edges are asymptotically independent . In particular , the asymptotic fluctuations of the matrix condition number can be described . Finally , the next order term of the hard edge asymptotics is provided .
Drawing intuition from a ( physical ) hydraulic system , we present a novel framework , constructively showing the existence of a strong Nash equilibrium in resource selection games ( i . e . , asymmetric singleton congestion games ) with nonatomic players , the coincidence of strong equilibria and Nash equilibria in such games , and the uniqueness of the cost of each given resource across all Nash equilibria . Our proofs allow for explicit calculation of Nash equilibrium and for explicit and direct calculation of the resulting ( unique ) costs of resources , and do not hinge on any fixed-point theorem , on the Minimax theorem or any equivalent result , on linear programming , or on the existence of a potential ( though our analysis does provide powerful insights into the potential , via a natural concrete physical interpretation ) . A generalization of resource selection games , called resource selection games with I . D . -dependent weighting , is defined , and the results are extended to this family , showing the existence of strong equilibria , and showing that while resource costs are no longer unique across Nash equilibria in games of this family , they are nonetheless unique across all strong Nash equilibria , drawing a novel fundamental connection between group deviation and I . D . -congestion . A natural application of the resulting machinery to a large class of constraint-satisfaction problems is also described .
In the framework of prediction with expert advice , we consider a recently introduced kind of regret bounds : the bounds that depend on the effective instead of nominal number of experts . In contrast to the Normal- Hedge bound , which mainly depends on the effective number of experts but also weakly depends on the nominal one , we obtain a bound that does not contain the nominal number of experts at all . We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales .
Semantic matching is of central importance to many natural language tasks \cite{bordes0000semantic , RetrievalQA} . A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them . As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech . The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling , but also capture the rich matching patterns at different levels . Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages . The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models .
Tensor decompositions are promising tools for big data analytics as they bring multiple modes and aspects of data to a unified framework , which allows us to discover complex internal structures and correlations of data . Unfortunately most existing approaches are not designed to meet the major challenges posed by big data analytics . This paper attempts to improve the scalability of tensor decompositions and provides two contributions : A flexible and fast algorithm for the CP decomposition ( FFCP ) of tensors based on their Tucker compression ; A distributed randomized Tucker decomposition approach for arbitrarily big tensors but with relatively low multilinear rank . These two algorithms can deal with huge tensors , even if they are dense . Extensive simulations provide empirical evidence of the validity and efficiency of the proposed algorithms .
Modeling spike firing assumes that spiking statistics are Poisson , but real data violates this assumption . To capture non-Poissonian features , in order to fix the inevitable inherent irregularity , researchers rescale the time axis with tedious computational overhead instead of searching for another distribution . Spikes or action potentials are precisely-timed changes in the ionic transport through synapses adjusting the synaptic weight , successfully modeled and developed as a memristor . Memristance value is multiples of initial resistance . This reminds us with the foundations of quantum mechanics . We try to quantize potential and resistance , as done with energy . After reviewing Planck curve for blackbody radiation , we propose the quantization equations . We introduce and prove a theorem that quantizes the resistance . Then we define the tyke showing its basic characteristics . Finally we give the basic transformations to model spiking and link an energy quantum to a tyke . Investigation shows how this perfectly models the neuron spiking , with over 00% match .
In the multivariate one-sample location model , we propose a class of flexible robust , affine-equivariant L-estimators of location , for distributions invoking affine-invariance of Mahalanobis distances of individual observations . An involved iteration process for their computation is numerically illustrated .
Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks . We train a set of state-of-the-art neural networks ( Maxout networks ) on three benchmark datasets : MNIST , CIFAR-00 and SVHN . They are trained with three distinct formats : floating point , fixed point and dynamic fixed point . For each of those datasets and for each of those formats , we assess the impact of the precision of the multiplications on the final error after training . We find that very low precision is sufficient not just for running trained networks but also for training them . For example , it is possible to train Maxout networks with 00 bits multiplications .
We describe a strategy-based approach to teaching natural deduction using a notation that emphasises the order in which deductions are constructed , together with a {\LaTeX} package and Java app to aid in the production of teaching resources and classroom demonstrations . Our approach is aimed at students with little exposure to mathematical method and has been developed while teaching undergraduate classes for philosophy students over the last ten years .
In this short report , we discuss an approach to estimating causal graphs in which indicators of causal influence between variables are treated as labels in a machine learning formulation . Available data on the variables of interest are used as " inputs " to estimate the labels . We frame the problem as one of semi-supervised learning : available interventional data or background knowledge provide labels on some edges in the graph and the remaining edges are treated as unlabelled objects . To illustrate the key ideas , we consider a simple approach to feature construction ( rooted in bivariate kernel density estimation ) and embed this within a semi-supervised manifold framework . Results on yeast knockout data demonstrate that the proposed approach can identify causal relationships as validated by unseen interventional experiments . An advantage of the formulation we propose is that by reframing causal discovery as semi-supervised learning , it allows a range of data-driven approaches to be brought to bear on causal discovery , without demanding specification of full probability models or explicit models of underlying mechanisms .
By far , the fractional derivative model is mainly related to the modelling of complicated solid viscoelastic material . In this study , we try to build the fractional derivative PDE model for broadband ultrasound propagation through human tissues .
We provide theoretical and empirical evidence for a type of asymmetry between causes and effects that is present when these are related via linear models contaminated with additive non-Gaussian noise . Assuming that the causes and the effects have the same distribution , we show that the distribution of the residuals of a linear fit in the anti-causal direction is closer to a Gaussian than the distribution of the residuals in the causal direction . This Gaussianization effect is characterized by reduction of the magnitude of the high-order cumulants and by an increment of the differential entropy of the residuals . The problem of non-linear causal inference is addressed by performing an embedding in an expanded feature space , in which the relation between causes and effects can be assumed to be linear . The effectiveness of a method to discriminate between causes and effects based on this type of asymmetry is illustrated in a variety of experiments using different measures of Gaussianity . The proposed method is shown to be competitive with state-of-the-art techniques for causal inference .
Previous models for learning entity and relationship embeddings of knowledge graphs such as TransE , TransH , and TransR aim to explore new links based on learned representations . However , these models interpret relationships as simple translations on entity embeddings . In this paper , we try to learn more complex connections between entities and relationships . In particular , we use a Convolutional Neural Network ( CNN ) to learn entity and relationship representations in knowledge graphs . In our model , we treat entities and relationships as one-dimensional numerical sequences with the same length . After that , we combine each triplet of head , relationship , and tail together as a matrix with height 0 . CNN is applied to the triplets to get confidence scores . Positive and manually corrupted negative triplets are used to train the embeddings and the CNN model simultaneously . Experimental results on public benchmark datasets show that the proposed model outperforms state-of-the-art models on exploring unseen relationships , which proves that CNN is effective to learn complex interactive patterns between entities and relationships .
Advances in remote sensing technologies have made it possible to use high-resolution visual data for weather observation and forecasting tasks . We propose the use of multi-layer neural networks for understanding complex atmospheric dynamics based on multichannel satellite images . The capability of our model was evaluated by using a linear regression task for single typhoon coordinates prediction . A specific combination of models and different activation policies enabled us to obtain an interesting prediction result in the northeastern hemisphere ( ENH ) .
In classical computation , a " write-only memory " ( WOM ) is little more than an oxymoron , and the addition of WOM to a ( deterministic or probabilistic ) classical computer brings no advantage . We prove that quantum computers that are augmented with WOM can solve problems that neither a classical computer with WOM nor a quantum computer without WOM can solve , when all other resource bounds are equal . We focus on realtime quantum finite automata , and examine the increase in their power effected by the addition of WOMs with different access modes and capacities . Some problems that are unsolvable by two-way probabilistic Turing machines using sublogarithmic amounts of read/write memory are shown to be solvable by these enhanced automata .
Surface electrical potential and observational growth recordings were made of a protoplasmic tube of the slime mould Physarum polycephalum in response to a multitude of stimuli with regards to sensory fusion or multisensory integration . Each stimulus was tested alone and in combination in order to evaluate for the first time the effect that multiple stimuli have on the frequency of streaming oscillation . White light caused a decrease in frequency whilst increasing the temperature and applying a food source in the form of oat flakes both increased the frequency . Simultaneously stimulating P . polycephalum with light and oat flake produced no net change in frequency , while combined light and heat stimuli showed an increase in frequency smaller than that observed for heat alone . When the two positive stimuli , oat flakes and heat , were combined , there was a net increase in frequency similar to the cumulative increases caused by the individual stimuli . Boolean logic gates were derived from the measured frequency change .
In this paper , we explore salient questions about user interests , conversations and friendships in the Facebook social network , using a novel latent space model that integrates several data types . A key challenge of studying Facebook ' s data is the wide range of data modalities such as text , network links , and categorical labels . Our latent space model seamlessly combines all three data modalities over millions of users , allowing us to study the interplay between user friendships , interests , and higher-order network-wide social trends on Facebook . The recovered insights not only answer our initial questions , but also reveal surprising facts about user interests in the context of Facebook ' s ecosystem . We also confirm that our results are significant with respect to evidential information from the study subjects .
In the context of robust covariance matrix estimation , this paper proposes a new approach to understanding the behavior of scatter matrix M-estimators introduced during the 00 ' s in the statistics community . During the last decade , a renewed interest for robust estimators appeared in the signal processing community , mainly due to their flexibility to the statistical model and their robustness to outliers and/or missing data . However , the behavior of these estimators still remains unclear and not understood well enough . A major disadvantage is that they are described by fixed-point equations that make their statistical analysis very difficult . To fill this gap , the main contribution of this work is to compare these estimators to the well-known sample covariance matrix ( SCM ) in order to infer more accurately their statistical behaviors . Indeed , under the Gaussian assumption , the SCM follows a Wishart distribution . However , when observations turn to be non-Gaussian , the SCM performance can be seriously degraded . In this article , we propose to use robust estimators , more adapted to the case of non-Gaussian models and presence of outliers , and to rely on the SCM properties in a Gaussian framework for establishing their theoretical analysis . To confirm our claims we also present results for a widely used function of $M$-estimators , the Mahalanobis distance . Finally , Monte Carlo simulations for various robust scatter matrix estimators are presented to validate theoretical results .
We propose a novel method to implement an optical see-through head mounted display which renders real aerial images with a wide viewing angle , called an Air Mounted Eyepiece ( AME ) . To achieve the AMD design , we employ an off-the-shelf head mounted display and Transmissive Mirror Device ( TMD ) which is usually used in aerial real imaging systems . In the proposed method , we replicate the function of the head mounted display ( HMD ) itself , which is used in the air by using the TMD and presenting a real image of eyepiece in front of the eye . Moreover , it can realize a wide viewing angle 0D display by placing a virtual lens in front of the eye without wearing an HMD . In addition to enhancing the experience of mixed reality and augmented reality , our proposed method can be used as a 0D imaging method for use in other applications such as in automobiles and desktop work . We aim to contribute to the field of human-computer interaction and the research on eyepiece interfaces by discussing the advantages and the limitations of this near-eye optical system .
Differential equations arise in mathematics , physics , medicine , pharmacology , communications , image processing and animation , etc . An Ordinary Differential Equation ( ODE ) is a differential equation if it involves derivatives with respect to only one independent variable which can be studied from different perspectives , such as : analytical methods , graphical methods and numerical methods . This research paper therefore revises the standard Runge - Kutta fourth order algorithm by using compiler techniques to dynamically evaluate the inputs and implement the algorithm for both first and second order derivatives of the ODE . We have been able to develop and implement the software that can be used to evaluate inputs and compute solutions ( approximately and analytically ) for the ODE function at a more efficient rate than the traditional method .
Auto-Encoders are unsupervised models that aim to learn patterns from observed data by minimizing a reconstruction cost . The useful representations learned are often found to be sparse and distributed . On the other hand , compressed sensing and sparse coding assume a data generating process , where the observed data is generated from some true latent signal source , and try to recover the corresponding signal from measurements . Looking at auto-encoders from this \textit{signal recovery perspective} enables us to have a more coherent view of these techniques . In this paper , in particular , we show that the \textit{true} hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \ell^{0} $ row length and the bias vectors takes the value ( approximately ) equal to the negative of the data mean . The recovery also becomes more and more accurate as the sparsity in hidden signals increases . Additionally , we empirically demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given .
Swarm dynamics is the study of collections of agents that interact with one another without central control . In natural systems , insects , birds , fish and other large mammals function in larger units to increase the overall fitness of the individuals . Their behavior is coordinated through local interactions to enhance mate selection , predator detection , migratory route identification and so forth [Andersson and Wallander 0000 ; Buhl et al . 0000 ; Nagy et al . 0000 ; Partridge 0000 ; Sumpter et al . 0000] . In artificial systems , swarms of autonomous agents can augment human activities such as search and rescue , and environmental monitoring by covering large areas with multiple nodes [Alami et al . 0000 ; Caruso et al . 0000 ; Ogren et al . 0000 ; Paley et al . 0000 ; Sibley et al . 0000] . In this paper , we explore the interplay between swarm dynamics , covert leadership and theoretical information transfer . A leader is a member of the swarm that acts upon information in addition to what is provided by local interactions . Depending upon the leadership model , leaders can use their external information either all the time or in response to local conditions [Couzin et al . 0000 ; Sun et al . 0000] . A covert leader is a leader that is treated no differently than others in the swarm , so leaders and followers participate equally in whatever interaction model is used [Rossi et al . 0000] . In this study , we use theoretical information transfer as a means of analyzing swarm interactions to explore whether or not it is possible to distinguish between followers and leaders based on interactions within the swarm . We find that covert leaders can be distinguished from followers in a swarm because they receive less transfer entropy than followers .
Users in social networks whose posts stay at the top of their followers ' {} feeds the longest time are more likely to be noticed . Can we design an online algorithm to help them decide when to post to stay at the top ? In this paper , we address this question as a novel optimal control problem for jump stochastic differential equations . For a wide variety of feed dynamics , we show that the optimal broadcasting intensity for any user is surprisingly simple -- it is given by the position of her most recent post on each of her follower ' s feeds . As a consequence , we are able to develop a simple and highly efficient online algorithm , RedQueen , to sample the optimal times for the user to post . Experiments on both synthetic and real data gathered from Twitter show that our algorithm is able to consistently make a user ' s posts more visible over time , is robust to volume changes on her followers ' feeds , and significantly outperforms the state of the art .
One of the most controversial problems in neural decoding is quantifying the information loss caused by ignoring noise correlations during optimal brain computations . For more than a decade , the measure here called $ \Delta I^{DL} $ has been believed exact . However , we have recently shown that it can exceed the information loss $ \Delta I^{B} $ caused by optimal decoders constructed ignoring noise correlations . Unfortunately , the different information notions underlying $ \Delta I^{DL} $ and $ \Delta I^{B} $ , and the putative rigorous information-theoretical derivation of $ \Delta I^{DL} $ , both render unclear whether those findings indicate either flaws in $ \Delta I^{DL} $ or major departures from traditional relations between information and decoding . Here we resolve this paradox and prove that , under certain conditions , observing $ \Delta I^{DL} {>}\Delta I^{B} $ implies that $ \Delta I^{DL} $ is flawed . Motivated by this analysis , we test both measures using neural populations that transmit independent information . Our results show that $ \Delta I^{DL} $ may deem noise correlations more important when decoding the populations together than when decoding them in parallel , whereas the opposite may occur for $ \Delta I^{B} $ . We trace these phenomena back , for $ \Delta I^{B} $ , to the choice of tie-breaking rules , and for $ \Delta I^{DL} $ , to unforeseen limitations within its information-theoretical foundations . Our study contributes with better estimates that potentially improve theoretical and experimental inferences currently drawn from $ \Delta I^{DL} $ without noticing that it may constitute an upper bound . On the practical side , our results promote the design of optimal decoding algorithms and neuroprosthetics without recording noise correlations , thereby saving experimental and computational resources .
Variational inference ( VI ) provides fast approximations of a Bayesian posterior in part because it formulates posterior approximation as an optimization problem : to find the closest distribution to the exact posterior over some family of distributions . For practical reasons , the family of distributions in VI is usually constrained so that it does not include the exact posterior , even as a limit point . Thus , no matter how long VI is run , the resulting approximation will not approach the exact posterior . We propose to instead consider a more flexible approximating family consisting of all possible finite mixtures of a parametric base distribution ( e . g . , Gaussian ) . For efficient inference , we borrow ideas from gradient boosting to develop an algorithm we call boosting variational inference ( BVI ) . BVI iteratively improves the current approximation by mixing it with a new component from the base distribution family and thereby yields progressively more accurate posterior approximations as more computing time is spent . Unlike a number of common VI variants including mean-field VI , BVI is able to capture multimodality , general posterior covariance , and nonstandard posterior shapes .
Cloud Computing has emerged as a key technology to deliver and manage computing , platform , and software services over the Internet . Task scheduling algorithms play an important role in the efficiency of cloud computing services as they aim to reduce the turnaround time of tasks and improve resource utilization . Several task scheduling algorithms have been proposed in the literature for cloud computing systems , the majority relying on the computational complexity of tasks and the distribution of resources . However , several tasks scheduled following these algorithms still fail because of unforeseen changes in the cloud environments . In this paper , using tasks execution and resource utilization data extracted from the execution traces of real world applications at Google , we explore the possibility of predicting the scheduling outcome of a task using statistical models . If we can successfully predict tasks failures , we may be able to reduce the execution time of jobs by rescheduling failed tasks earlier ( i . e . , before their actual failing time ) . Our results show that statistical models can predict task failures with a precision up to 00 . 0% , and a recall up to 00 . 0% . We simulate the potential benefits of such predictions using the tool kit GloudSim and found that they can improve the number of finished tasks by up to 00% . We also perform a case study using the Hadoop framework of Amazon Elastic MapReduce ( EMR ) and the jobs of a gene expression correlations analysis study from breast cancer research . We find that when extending the scheduler of Hadoop with our predictive models , the percentage of failed jobs can be reduced by up to 00% , with an overhead of less than 0 minutes .
Deep convolutional neural networks ( CNNs ) obtain outstanding results in tasks that require human-level understanding of data , like image or speech recognition . However , their computational load is significant , motivating the development of CNN-specialized accelerators . This work presents NEURAghe , a flexible and efficient hardware/software solution for the acceleration of CNNs on Zynq SoCs . NEURAghe leverages the synergistic usage of Zynq ARM cores and of a powerful and flexible Convolution-Specific Processor deployed on the reconfigurable logic . The Convolution-Specific Processor embeds both a convolution engine and a programmable soft core , releasing the ARM processors from most of the supervision duties and allowing the accelerator to be controlled by software at an ultra-fine granularity . This methodology opens the way for cooperative heterogeneous computing : while the accelerator takes care of the bulk of the CNN workload , the ARM cores can seamlessly execute hard-to-accelerate parts of the computational graph , taking advantage of the NEON vector engines to further speed up computation . Through the companion NeuDNN SW stack , NEURAghe supports end-to-end CNN-based classification with a peak performance of 000 Gops/s , and an energy efficiency of 00 Gops/W . Thanks to our heterogeneous computing model , our platform improves upon the state-of-the-art , achieving a frame rate of 0 . 0 fps on the end-to-end execution of VGG-00 , and 0 . 0 fps on ResNet-00 .
We study the envy-free cake-cutting problem for $d+0$ players with $d$ cuts , for both the oracle function model and the polynomial time function model . For the former , we derive a $\theta ( ( {0\over\epsilon} ) ^{d-0} ) $ time matching bound for the query complexity of $d+0$ player cake cutting with Lipschitz utilities for any $d> 0$ . When the utility functions are given by a polynomial time algorithm , we prove the problem to be PPAD-complete . For measurable utility functions , we find a fully polynomial-time algorithm for finding an approximate envy-free allocation of a cake among three people using two cuts .
There are many familiar situations in which a manager seeks to design a system in which users share a resource , but outcomes depend on the information held and actions taken by users . If communication is possible , the manager can ask users to report their private information and then , using this information , instruct them on what actions they should take . If the users are compliant , this reduces the manager ' s optimization problem to a well-studied problem of optimal control . However , if the users are self-interested and not compliant , the problem is much more complicated : when asked to report their private information , the users might lie ; upon receiving instructions , the users might disobey . Here we ask whether the manager can design the system to get around both of these difficulties . To do so , the manager must provide for the users the incentives to report truthfully and to follow the instructions , despite the fact that the users are self-interested . For a class of environments that includes many resource allocation games in communication networks , we provide tools for the manager to design an efficient system . In addition to reports and recommendations , the design we employ allows the manager to intervene in the system after the users take actions . In an abstracted environment , we find conditions under which the manager can achieve the same outcome it could if users were compliant , and conditions under which it does not . We then apply our framework and results to design a flow control management system .
Interactive multi-view video streaming ( IMVS ) services permit to remotely immerse within a 0D scene . This is possible by transmitting a set of reference camera views ( anchor views ) , which are used by the clients to freely navigate in the scene and possibly synthesize additional viewpoints of interest . From a networking perspective , the big challenge in IMVS systems is to deliver to each client the best set of anchor views that maximizes the navigation quality , minimizes the view-switching delay and yet satisfies the network constraints . Integrating adaptive streaming solutions in free-viewpoint systems offers a promising solution to deploy IMVS in large and heterogeneous scenarios , as long as the multi-view video representations on the server are properly selected . We therefore propose to optimize the multi-view data at the server by minimizing the overall resource requirements , yet offering a good navigation quality to the different users . We propose a video representation set optimization for multiview adaptive streaming systems and we show that it is NP-hard . We therefore introduce the concept of multi-view navigation segment that permits to cast the video representation set selection as an integer linear programming problem with a bounded computational complexity . We then show that the proposed solution reduces the computational complexity while preserving optimality in most of the 0D scenes . We then provide simulation results for different classes of users and show the gain offered by an optimal multi-view video representation selection compared to recommended representation sets ( e . g . , Netflix and Apple ones ) or to a baseline representation selection algorithm where the encoding parameters are decided a priori for all the views .
Multi-task/Multi-output learning seeks to exploit correlation among tasks to enhance performance over learning or solving each task independently . In this paper , we investigate this problem in the context of Gaussian Processes ( GPs ) and propose a new model which learns a mixture of latent processes by decomposing the covariance matrix into a sum of structured hidden components each of which is controlled by a latent GP over input features and a " weight " over tasks . From this sum structure , we propose a parallelizable parameter learning algorithm with a predetermined initialization for the " weights " . We also notice that an ensemble parameter learning approach using mini-batches of training data not only reduces the computation complexity of learning but also improves the regression performance . We evaluate our model on two datasets , the smaller Swiss Jura dataset and another relatively larger ATMS dataset from NOAA . Substantial improvements are observed compared with established alternatives .
We propose a new general version of Stein ' s method for univariate distributions . In particular we propose a canonical definition of the Stein operator of a probability distribution {which is based on a linear difference or differential-type operator} . The resulting Stein identity highlights the unifying theme behind the literature on Stein ' s method ( both for continuous and discrete distributions ) . Viewing the Stein operator as an operator acting on pairs of functions , we provide an extensive toolkit for distributional comparisons . Several abstract approximation theorems are provided . Our approach is illustrated for comparison of several pairs of distributions : normal vs normal , sums of independent Rademacher vs normal , normal vs Student , and maximum of random variables vs exponential , Frechet and Gumbel .
Animal movement exhibits complex behavior which can be influenced by unobserved environmental conditions . We propose a model which allows for a spatially-varying movement rate and spatially-varying drift through a semiparametric potential surface and a separate motility surface . These surfaces are embedded in a stochastic differential equation framework which allows for complex animal movement patterns in space . The resulting model is used to analyze the spatially-varying behavior of ants to provide insight into the spatial structure of ant movement in the nest .
Detecting anomalies of a cyber physical system ( CPS ) , which is a complex system consisting of both physical and software parts , is important because a CPS often operates autonomously in an unpredictable environment . However , because of the ever-changing nature and lack of a precise model for a CPS , detecting anomalies is still a challenging task . To address this problem , we propose applying an outlier detection method to a CPS log . By using a log obtained from an actual aquarium management system , we evaluated the effectiveness of our proposed method by analyzing outliers that it detected . By investigating the outliers with the developer of the system , we confirmed that some outliers indicate actual faults in the system . For example , our method detected failures of mutual exclusion in the control system that were unknown to the developer . Our method also detected transient losses of functionalities and unexpected reboots . On the other hand , our method did not detect anomalies that were too many and similar . In addition , our method reported rare but unproblematic concurrent combinations of operations as anomalies . Thus , our approach is effective at finding anomalies , but there is still room for improvement .
In this paper , a novel framework is proposed for optimizing the operation and performance of a large-scale , multi-hop millimeter wave ( mmW ) backhaul within a wireless small cell network ( SCN ) that encompasses multiple mobile network operators ( MNOs ) . The proposed framework enables the small base stations ( SBSs ) to jointly decide on forming the multi-hop , mmW links over backhaul infrastructure that belongs to multiple , independent MNOs , while properly allocating resources across those links . In this regard , the problem is addressed using a novel framework based on matching theory that is composed to two , highly inter-related stages : a multi-hop network formation stage and a resource management stage . One unique feature of this framework is that it jointly accounts for both wireless channel characteristics and economic factors during both network formation and resource management . The multi-hop network formation stage is formulated as a one-to-many matching game which is solved using a novel algorithm , that builds on the so-called deferred acceptance algorithm and is shown to yield a stable and Pareto optimal multi-hop mmW backhaul network . Then , a one-to-many matching game is formulated to enable proper resource allocation across the formed multi-hop network . This game is then shown to exhibit peer effects and , as such , a novel algorithm is developed to find a stable and optimal resource management solution that can properly cope with these peer effects . Simulation results show that the proposed framework yields substantial gains , in terms of the average sum rate , reaching up to 00% and 00% , respectively , compared to a non-cooperative scheme in which inter-operator sharing is not allowed and a random allocation approach . The results also show that our framework provides insights on how to manage pricing and the cost of the cooperative mmW backhaul network for the MNOs .
Scientists and engineers commonly use simulation models , or computer experiments , to study real systems for which actual experimentation is costly , difficult , or impossible . Many computer experiments are \emph{stochastic} in the sense that repeated runs with the same input configuration will result in slightly or drastically different outputs . For expensive or time-consuming simulations , stochastic kriging \citep{ankenman} is commonly used to generate predictions for simulation model outputs subject to uncertainty due to both function approximation and stochastic variation . Here , we decompose error in stochastic kriging predictions into nominal , numeric , parameter estimation and parameter estimation numeric components and provide means to control each in terms of properties of the underlying experimental design . The design properties implied for each source of error are weakly conflicting and several broad principles are proposed . In brief , the space-filling properties " small fill distance " and " large separation distance " should balance with replication at unique input configurations , with number of replications depending on the relative magnitudes of the stochastic and process variability . Non-stationarity implies higher input density in more active regions , while regression functions imply a balance with traditional design properties . A few examples are presented to illustrate the results .
Inverse Reinforcement Learning ( IRL ) describes the problem of learning an unknown reward function of a Markov Decision Process ( MDP ) from observed behavior of an agent . Since the agent ' s behavior originates in its policy and MDP policies depend on both the stochastic system dynamics as well as the reward function , the solution of the inverse problem is significantly influenced by both . Current IRL approaches assume that if the transition model is unknown , additional samples from the system ' s dynamics are accessible , or the observed behavior provides enough samples of the system ' s dynamics to solve the inverse problem accurately . These assumptions are often not satisfied . To overcome this , we present a gradient-based IRL approach that simultaneously estimates the system ' s dynamics . By solving the combined optimization problem , our approach takes into account the bias of the demonstrations , which stems from the generating policy . The evaluation on a synthetic MDP and a transfer learning task shows improvements regarding the sample efficiency as well as the accuracy of the estimated reward functions and transition models .
Bayesian optimization has recently attracted the attention of the automatic machine learning community for its excellent results in hyperparameter tuning . BO is characterized by the sample efficiency with which it can optimize expensive black-box functions . The efficiency is achieved in a similar fashion to the learning to learn methods : surrogate models ( typically in the form of Gaussian processes ) learn the target function and perform intelligent sampling . This surrogate model can be applied even in the presence of noise ; however , as with most regression methods , it is very sensitive to outlier data . This can result in erroneous predictions and , in the case of BO , biased and inefficient exploration . In this work , we present a GP model that is robust to outliers which uses a Student-t likelihood to segregate outliers and robustly conduct Bayesian optimization . We present numerical results evaluating the proposed method in both artificial functions and real problems .
The main results of this paper are ( I ) a simulation algorithm which , under quite general constraints , transforms algorithms running on the Congested Clique into algorithms running in the MapReduce model , and ( II ) a distributed $O ( \Delta ) $-coloring algorithm running on the Congested Clique which has an expected running time of ( i ) $O ( 0 ) $ rounds , if $\Delta \geq \Theta ( \log^0 n ) $ ; and ( ii ) $O ( \log \log n ) $ rounds otherwise . Applying the simulation theorem to the Congested-Clique $O ( \Delta ) $-coloring algorithm yields an $O ( 0 ) $-round $O ( \Delta ) $-coloring algorithm in the MapReduce model . Our simulation algorithm illustrates a natural correspondence between per-node bandwidth in the Congested Clique model and memory per machine in the MapReduce model . In the Congested Clique ( and more generally , any network in the $\mathcal{CONGEST}$ model ) , the major impediment to constructing fast algorithms is the $O ( \log n ) $ restriction on message sizes . Similarly , in the MapReduce model , the combined restrictions on memory per machine and total system memory have a dominant effect on algorithm design . In showing a fairly general simulation algorithm , we highlight the similarities and differences between these models .
We give a simple statistical proof of a binomial identity , by evaluating the Laplace transform of the maximum of n independent exponential random variables in two different ways . As a by product , we obtain a simple proof of an interesting result concerning the exponential distribution . The connections between a probabilistic approach and the statistical approach are discussed , which explains why certain binomial identities admit probabilistic interpretations . In the process , several new binomial identities are also obtained and discussed .
The greedy spanner is a high-quality spanner : its total weight , edge count and maximal degree are asymptotically optimal and in practice significantly better than for any other spanner with reasonable construction time . Unfortunately , all known algorithms that compute the greedy spanner of n points use Omega ( n^0 ) space , which is impractical on large instances . To the best of our knowledge , the largest instance for which the greedy spanner was computed so far has about 00 , 000 vertices . We present a O ( n ) -space algorithm that computes the same spanner for points in R^d running in O ( n^0 log^0 n ) time for any fixed stretch factor and dimension . We discuss and evaluate a number of optimizations to its running time , which allowed us to compute the greedy spanner on a graph with a million vertices . To our knowledge , this is also the first algorithm for the greedy spanner with a near-quadratic running time guarantee that has actually been implemented .
Conditional independence testing is a fundamental problem underlying causal discovery and a particularly challenging task in the presence of nonlinear and high-dimensional dependencies . Here a fully non-parametric test for continuous data based on conditional mutual information combined with a local permutation scheme is presented . Through a nearest neighbor approach , the test efficiently adapts also to non-smooth distributions due to strongly nonlinear dependencies . Numerical experiments demonstrate that the test reliably simulates the null distribution even for small sample sizes and with high-dimensional conditioning sets . The test is better calibrated than kernel-based tests utilizing an analytical approximation of the null distribution , especially for non-smooth densities , and reaches the same or higher power levels . Combining the local permutation scheme with the kernel tests leads to better calibration , but suffers in power . For smaller sample sizes and lower dimensions , the test is faster than random fourier feature-based kernel tests if the permutation scheme is ( embarrassingly ) parallelized , but the runtime increases more sharply with sample size and dimensionality . Thus , more theoretical research to analytically approximate the null distribution and speed up the estimation for larger sample sizes is desirable .
This paper studies robust regression in the settings of Huber ' s $\epsilon$-contamination models . We consider estimators that are maximizers of multivariate regression depth functions . These estimators are shown to achieve minimax rates in the settings of $\epsilon$-contamination models for various regression problems including nonparametric regression , sparse linear regression , reduced rank regression , etc . We also discuss a general notion of depth function for linear operators that has potential applications in robust functional linear regression .
In this paper we consider Tyler ' s robust covariance M-estimator under group symmetry constraints . We assume that the covariance matrix is invariant to the conjugation action of a unitary matrix group , referred to as group symmetry . Examples of group symmetric structures include circulant , perHermitian and proper quaternion matrices . We introduce a group symmetric version of Tyler ' s estimator ( STyler ) and provide an iterative fixed point algorithm to compute it . The classical results claim that at least n=p+0 sample points in general position are necessary to ensure the existence and uniqueness of Tyler ' s estimator , where p is the ambient dimension . We show that the STyler requires significantly less samples . In some groups even two samples are enough to guarantee its existence and uniqueness . In addition , in the case of elliptical populations , we provide high probability bounds on the error of the STyler . These too , quantify the advantage of exploiting the symmetry structure . Finally , these theoretical results are supported by numerical simulations . ted by numerical simulations .
Today ' s forging die manufacturing process must be adapted to several evolutions in machining process generation : CAD/CAM models , CAM software solutions and High Speed Machining ( HSM ) . In this context , the adequacy between die shape and HSM process is in the core of machining preparation and process planning approaches . This paper deals with an original approach of machining preparation integrating this adequacy in the main tasks carried out . In this approach , the design of the machining process is based on two levels of decomposition of the geometrical model of a given die with respect to HSM cutting conditions ( cutting speed and feed rate ) and technological constrains ( tool selection , features accessibility ) . This decomposition assists machining assistant to generate an HSM process . The result of this decomposition is the identification of machining features .
This paper develops a Bayesian control chart for the percentiles of the Weibull distribution , when both its in-control and out-of-control parameters are unknown . The Bayesian approach enhances parameter estimates for small sample sizes that occur when monitoring rare events as in high-reliability applications or genetic mutations . The chart monitors the parameters of the Weibull distribution directly , instead of transforming the data as most Weibull-based charts do in order to comply with their normality assumption . The chart uses the whole accumulated knowledge resulting from the likelihood of the current sample combined with the information given by both the initial prior knowledge and all the past samples . The chart is adapting since its control limits change ( e . g . narrow ) during the Phase I . An example is presented and good Average Run Length properties are demonstrated . In addition , the paper gives insights into the nature of monitoring Weibull processes by highlighting the relationship between distribution and process parameters .
Intention-oriented process mining is based on the belief that the fundamental nature of processes is mostly intentional ( unlike activity-oriented process ) and aims at discovering strategy and intentional process models from event-logs recorded during the process enactment . In this paper , we present an application of intention-oriented process mining for the domain of incident management of an Information Technology Infrastructure Library ( ITIL ) process . We apply the Map Miner Method ( MMM ) on a large real-world dataset for discovering hidden and unobservable user behavior , strategies and intentions . We first discover user strategies from the given activity sequence data by applying Hidden Markov Model ( HMM ) based unsupervised learning technique . We then process the emission and transition matrices of the discovered HMM to generate a coarse-grained Map Process Model . We present the first application or study of the new and emerging field of Intention-oriented process mining on an incident management event-log dataset and discuss its applicability , effectiveness and challenges .
Many theoretical results in the machine learning domain stand only for functions that are Lipschitz continuous . Lipschitz continuity is a strong form of continuity that linearly bounds the variations of a function . In this paper , we derive tight Lipschitz constants for two families of metrics : Mahalanobis distances and bounded-space bilinear forms . To our knowledge , this is the first time the Mahalanobis distance is formally proved to be Lipschitz continuous and that such tight Lipschitz constants are derived .
Characterization of long-term disease dynamics , from disease-free to end-stage , is integral to understanding the course of neurodegenerative diseases such as Parkinson ' s and Alzheimer ' s ; and ultimately , how best to intervene . Natural history studies typically recruit multiple cohorts at different stages of disease and follow them longitudinally for a relatively short period of time . We propose a latent time joint mixed effects model to characterize long-term disease dynamics using this short-term data . Markov chain Monte Carlo methods are proposed for estimation , model selection , and inference . We apply the model to detailed simulation studies and data from the Alzheimer ' s Disease Neuroimaging Initiative .
With the large-scale penetration of the internet , for the first time , humanity has become linked by a single , open , communications platform . Harnessing this fact , we report insights arising from a unified internet activity and location dataset of an unparalleled scope and accuracy drawn from over a trillion ( 0 . 0$\times 00^{00}$ ) observations of end-user internet connections , with temporal resolution of just 00min over 0000-0000 . We first apply this dataset to the expansion of the internet itself over 0 , 000 urban agglomerations globally . We find that unique IP per capita counts reach saturation at approximately one IP per three people , and take , on average , 00 . 0 years to achieve ; eclipsing the estimated 000- and 00- year saturation times for steam-power and electrification respectively . Next , we use intra-diurnal internet activity features to up-scale traditional over-night sleep observations , producing the first global estimate of over-night sleep duration in 000 cities over 0 years . We find statistically significant variation between continental , national and regional sleep durations including some evidence of global sleep duration convergence . Finally , we estimate the relationship between internet concentration and economic outcomes in 000 OECD regions and find that the internet ' s expansion is associated with negative or positive productivity gains , depending strongly on sectoral considerations . To our knowledge , our study is the first of its kind to use online/offline activity of the entire internet to infer social science insights , demonstrating the unparalleled potential of the internet as a social data-science platform .
Cognitive Social Structure ( CSS ) network studies collect relational data on respondents ' direct ties and their perception of ties among all other individuals in the network . When reporting their perception networks , respondents commit two types of errors , namely , omission ( false negatives ) and commission ( false positives ) errors . We first assess the relationship between these two error types , and their contributions on the overall respondent accuracy . Next we propose a method for estimating networks based on perceptions of a random sample of respondents from a bounded social network , which utilizes the Receiving Operator Characteristic ( ROC ) curve for balancing the tradeoffs between omission and commission errors . A comparative numerical study shows that the proposed estimation method performs well . This new method can be easily integrated to organization studies that use randomized surveys to study multiple organizations . The burgeoning field of multilevel analysis of inter-organizational networks can also immensely benefit from this approach .
A mesh is a graph that divides physical space into regularly-shaped regions . Meshes computations form the basis of many applications , e . g . finite-element methods , image rendering , and collision detection . In one important mesh primitive , called a mesh update , each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices . The performance of a mesh update depends on the layout of the mesh in memory . This paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters . Such a memory layout is called cache-oblivious . Formally , for a $d$-dimensional mesh $G$ , block size $B$ , and cache size $M$ ( where $M=\Omega ( B^d ) $ ) , the mesh update of $G$ uses $O ( 0+|G|/B ) $ memory transfers . The paper also shows how the mesh-update performance degrades for smaller caches , where $M=o ( B^d ) $ . The paper then gives two algorithms for finding cache-oblivious mesh layouts . The first layout algorithm runs in time $O ( |G|\log^0|G| ) $ both in expectation and with high probability on a RAM . It uses $O ( 0+|G|\log^0 ( |G|/M ) /B ) $ memory transfers in expectation and $O ( 0+ ( |G|/B ) ( \log^0 ( |G|/M ) + \log|G| ) ) $ memory transfers with high probability in the cache-oblivious and disk-access machine ( DAM ) models . The layout is obtained by finding a fully balanced decomposition tree of $G$ and then performing an in-order traversal of the leaves of the tree . The second algorithm runs faster by almost a $\log|G|/\log\log|G|$ factor in all three memory models , both in expectation and with high probability . The layout obtained by finding a relax-balanced decomposition tree of $G$ and then performing an in-order traversal of the leaves of the tree .
The bound to factor large integers is dominated by the computational effort to discover numbers that are smooth , typically performed by sieving a polynomial sequence . On a von Neumann architecture , sieving has log-log amortized time complexity to check each value for smoothness . This work presents a neuromorphic sieve that achieves a constant time check for smoothness by exploiting two characteristic properties of neuromorphic architectures : constant time synaptic integration and massively parallel computation . The approach is validated by modifying msieve , one of the fastest publicly available integer factorization implementations , to use the IBM Neurosynaptic System ( NS0e ) as a coprocessor for the sieving stage .
Delay/Disruption-Tolerant Networks ( DTNs ) have been around for more than a decade and have especially been proposed to be used in scenarios where communication infrastructure is unavailable . In such scenarios , DTNs can offer a best-effort communication service by exploiting user mobility . Natural disasters are an important application scenario for DTNs when the cellular network is destroyed by natural forces . To assess the performance of such networks before deployment , we require appropriate knowledge of human mobility . In this paper , we address this problem by designing , implementing , and evaluating a novel mobility model for large-scale natural disasters . Due to the lack of GPS traces , we reverse-engineer human mobility of past natural disasters ( focusing on 0000 Haiti earthquake and 0000 Typhoon Haiyan ) by leveraging knowledge of 000 experts from 00 Disaster Response Organizations ( DROs ) . By means of simulation-based experiments , we compare and contrast our mobility model to other well-known models , and evaluate their impact on DTN performance . Finally , we make our source code available to the public .
Many machine learning classifiers are vulnerable to adversarial perturbations . An adversarial perturbation modifies an input to change a classifier ' s prediction without causing the input to seem substantially different to human perception . We deploy three methods to detect adversarial images . Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying . Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA . Other detectors and a colorful saliency map are in an appendix .
The experimental design problem concerns the selection of k points from a potentially large design pool of p-dimensional vectors , so as to maximize the statistical efficiency regressed on the selected k design points . Statistical efficiency is measured by optimality criteria , including A ( verage ) , D ( eterminant ) , T ( race ) , E ( igen ) , V ( ariance ) and G-optimality . Except for the T-optimality , exact optimization is NP-hard . We propose a polynomial-time regret minimization framework to achieve a $ ( 0+\varepsilon ) $ approximation with only $O ( p/\varepsilon^0 ) $ design points , for all the optimality criteria above . In contrast , to the best of our knowledge , before our work , no polynomial-time algorithm achieves $ ( 0+\varepsilon ) $ approximations for D/E/G-optimality , and the best poly-time algorithm achieving $ ( 0+\varepsilon ) $-approximation for A/V-optimality requires $k = \Omega ( p^0/\varepsilon ) $ design points .
It is estimated that 000 million people globally are visually impaired . A majority of these people live in developing countries and are among the elderly population . One of the most difficult tasks faced by the visually impaired is identification of people . While naturally , voice recognition is a common method of identification , it is an intuitive and difficult process . The rise of computation capability of mobile devices gives motivation to develop applications that can assist visually impaired persons . With the availability of mobile devices , these people can be assisted by an additional method of identification through intelligent software based on computer vision techniques . In this paper , we present the design and implementation of a face detection and recognition system for the visually impaired through the use of mobile computing . This mobile system is assisted by a server-based support system . The system was tested on a custom video database . Experiment results show high face detection accuracy and promising face recognition accuracy in suitable conditions . The challenges of the system lie in better recognition techniques for difficult situations in terms of lighting and weather .
In this letter we focus on designing self-organizing diffusion mobile adaptive networks where the individual agents are allowed to move in pursuit of an objective ( target ) . The well-known Adapt-then-Combine ( ATC ) algorithm is already available in the literature as a useful distributed diffusion-based adaptive learning network . However , in the ATC diffusion algorithm , fixed step sizes are used in the update equations for velocity vectors and location vectors . When the nodes are too far away from the target , such strategies may require large number of iterations to reach the target . To address this issue , in this paper we suggest two modifications on the ATC mobile adaptive network to improve its performance . The proposed modifications include ( i ) distance-based variable step size adjustment at diffusion algorithms to update velocity vectors and location vectors ( ii ) to use a selective cooperation , by choosing the best nodes at each iteration to reduce the number of communications . The performance of the proposed algorithm is evaluated by simulation tests where the obtained results show the superior performance of the proposed algorithm in comparison with the available ATC mobile adaptive network .
Currently there are many clinical trials using paper case report forms as the primary data collection tool . Cloud Computing platforms provide big potential for increasing efficiency through a web-based data collection interface , especially for large-scale multi-center trials . Traditionally , clinical and biological data for multi-center trials are stored in one dedicated , centralized database system running at a data coordinating center ( DCC ) . This paper presents C-PASS-PC , a cloud-driven prototype of multi-center proactive surveillance system for prostate cancer . The prototype is developed in PHP , JQuery and CSS with an Oracle backend in a local Web server and database server and deployed on Google App Engine ( GAE ) and Google Cloud SQL-MySQL . The deploying process is fast and easy to follow . The C-PASS-PC prototype can be accessed through an SSL-enabled web browser . Our approach proves the concept that cloud computing platforms such as GAE is a suitable and flexible solution in the near future for multi-center clinical trials .
We consider the computational complexity of pure Nash equilibria in graphical games . It is known that the problem is NP-complete in general , but tractable ( i . e . , in P ) for special classes of graphs such as those with bounded treewidth . It is then natural to ask : is it possible to characterize all tractable classes of graphs for this problem ? In this work , we provide such a characterization for the case of bounded in-degree graphs , thereby resolving the gap between existing hardness and tractability results . In particular , we analyze the complexity of PUREGG ( C , - ) , the problem of deciding the existence of pure Nash equilibria in graphical games whose underlying graphs are restricted to class C . We prove that , under reasonable complexity theoretic assumptions , for every recursively enumerable class C of directed graphs with bounded in-degree , PUREGG ( C , - ) is in polynomial time if and only if the reduced graphs ( the graphs resulting from iterated removal of sinks ) of C have bounded treewidth . We also give a characterization for PURECHG ( C , - ) , the problem of deciding the existence of pure Nash equilibria in colored hypergraphical games , a game representation that can express the additional structure that some of the players have identical local utility functions . We show that the tractable classes of bounded-arity colored hypergraphical games are precisely those whose reduced graphs have bounded treewidth modulo homomorphic equivalence . Our proofs make novel use of Grohe ' s characterization of the complexity of homomorphism problems .
Data depth proves successful in the analysis of multivariate data sets , in particular deriving an overall center and assigning ranks to the observed units . Two key features are : the directions of the ordering , from the center towards the outside , and the recognition of a unique center irrespective of the distribution being unimodal or multimodal . This behaviour is a consequence of the monotonicity of the ranks that decrease along any ray from the deepest point . Recently , a wider framework allowing identification of partial centers was suggested in Agostinelli and Romanazzi [0000] . The corresponding generalized depth functions , called local depth functions are able to record local fluctuations and can be used in mode detection , identification of components in mixture models and in cluster analysis . Functional data [Ramsay and Silverman , 0000] are become common nowadays . Recently , Lopez-Pintado and Romo [0000] has proposed the half-region depth suited for functional data and for high dimensional data . Here , following their work we propose a local version of this data depth , we study its theoretical properties and illustrate its behaviour with examples based on real data sets .
The major issue for the wheeled mobile robot is the low level controller gains tuning up especially in the robot competition . The floor surface can be damaged by the robot wheels during the competition , therefore the surface coefficient can be changed over time . PI gains have to be tuned before every match along the competition . In this research , the torque controller is defined and implemented in order to solve this problem . Torque controller consists of a PI controller for the robot wheel ' s angular velocity and a dynamic equation of brushless motor . The motor dynamics can be derived from the energy conservation law . Three different carpets , which have the different friction coefficients , are used in the experiments . The robot wheel ' s angular velocity profiles are generated from the robot kinematics with different initial conditions . The output paths of the robot with the torque controller are compared with the output paths of the robot with regular PI controller when the same wheel angular velocity profiles are applied . The results show that the torque controller can provide a better robot path than the normal PI controller .
We propose a rank-$k$ variant of the classical Frank-Wolfe algorithm to solve convex optimization over a trace-norm ball . Our algorithm replaces the top singular-vector computation ( $0$-SVD ) in Frank-Wolfe with a top-$k$ singular-vector computation ( $k$-SVD ) , which can be done by repeatedly applying $0$-SVD $k$ times . Alternatively , our algorithm can be viewed as a rank-$k$ restricted version of projected gradient descent . We show that our algorithm has a linear convergence rate when the objective function is smooth and strongly convex , and the optimal solution has rank at most $k$ . This improves the convergence rate and the total time complexity of the Frank-Wolfe method and its variants .
In this paper we present a generic framework for the asymptotic performance analysis of subspace-based parameter estimation schemes . It is based on earlier results on an explicit first-order expansion of the estimation error in the signal subspace obtained via an SVD of the noisy observation matrix . We extend these results in a number of aspects . Firstly , we derive an explicit first-order expansion of the Higher- Order SVD ( HOSVD ) -based subspace estimate . Secondly , we show how to obtain explicit first-order expansions of the estimation error of ESPRIT-type algorithms and provide the expressions for matrix-based and tensor-based Standard ESPRIT and Unitary ESPRIT . Thirdly , we derive closed-form expressions for the mean square error ( MSE ) and show that they only depend on the second-order moments of the noise . Hence , we only need the noise to be zero mean and possess finite second order moments . Fourthly , we investigate the effect of using Structured Least Squares ( SLS ) to solve the overdetermined shift invariance equations in ESPRIT and provide an explicit first-order expansion as well as a closed-form MSE expression . Finally , we simplify the MSE for the special case of a single source and compute the asymptotic efficiency of the investigated ESPRIT-type algorithms in compact closed-form expressions which only depend on the array size and the effective SNR . Our results are more general than existing results on the performance analysis of ESPRIT-type algorithms since ( a ) we do not need any assumptions about the noise except for the mean to be zero and the second-order moments to be finite ( in contrast to earlier results that require Gaussianity or second-order circular symmetry ) ; ( b ) our results are asymptotic in the effective SNR , i . e . , we do not require the number of samples to be large ; ( c ) we present a framework that incorporates various ESPRIT-type algorithms in one unified manner .
Development organizations and International Non-Governmental Organizations have been emphasizing the high potential of Free and Open Source Software for the Less Developed Countries . Cost reduction , less vendor dependency and increased potential for local capacity development have been their main arguments . In spite of its advantages , Free and Open Source Software is not widely adopted at the African continent . In this book the authors will explore the grounds on with these expectations are based . Where do they come from and is there evidence to support these expectations ? Over the past years several projects have been initiated and some good results have been achieved , but at the same time many challenges were encountered . What lessons can be drawn from these experiences and do these experiences contain enough evidence to support the high expectations ? Several projects and their achievements will be considered . In the final part of the book the future of Free and Open Source Software for Development will be explored . Special attention is given to the African continent since here challenges are highest . What is the role of Free and open Source Software for Development and how do we need to position and explore the potential ? What are the threats ? The book aims at professionals that are engaged in the design and implementation of ICT for Development ( ICT0D ) projects and want to improve their understanding of the role Free and Open Source Software can play .
We consider fragments of first-order logic and as models we allow finite and infinite words simultaneously . The only binary relations apart from equality are order comparison < and the successor predicate +0 . We give characterizations of the fragments Sigma0 = Sigma0[< , +0] and FO0 = FO0[< , +0] in terms of algebraic and topological properties . To this end we introduce the factor topology over infinite words . It turns out that a language L is in the intersection of FO0 and Sigma0 if and only if L is the interior of an FO0 language . Symmetrically , a language is in the intersection of FO0 and Pi0 if and only if it is the topological closure of an FO0 language . The fragment Delta0 , which by definition is the intersection of Sigma0 and Pi0 contains exactly the clopen languages in FO0 . In particular , over infinite words Delta0 is a strict subclass of FO0 . Our characterizations yield decidability of the membership problem for all these fragments over finite and infinite words ; and as a corollary we also obtain decidability for infinite words . Moreover , we give a new decidable algebraic characterization of dot-depth 0/0 over finite words . Decidability of dot-depth 0/0 over finite words was first shown by Gla{\ss}er and Schmitz in STACS 0000 , and decidability of the membership problem for FO0 over infinite words was shown 0000 by Wilke in his habilitation thesis whereas decidability of Sigma0 over infinite words was not known before .
In applications such as clinical safety analysis , the data of the experiments usually consists of frequency counts . In the analysis of such data , researchers often face the problem of multiple testing based on discrete test statistics , aimed at controlling family-wise error rate ( FWER ) . Most existing FWER controlling procedures are developed for continuous data , which are often conservative when analyzing discrete data . By using minimal attainable p-values , several FWER controlling procedures have been developed for discrete data in the literature . In this paper , by utilizing known marginal distributions of true null p-values , three more powerful stepwise procedures are developed , which are modified versions of the conventional Bonferroni , Holm and Hochberg procedures , respectively . It is proved that the first two procedures strongly control the FWER under arbitrary dependence and are more powerful than the existing Tarone-type procedures , while the last one only ensures control of the FWER in special scenarios . Through extensive simulation studies , we provide numerical evidence of superior performance of the proposed procedures in terms of the FWER control and minimal power . A real clinical safety data is used to demonstrate applications of our proposed procedures . An R package " MHTdiscrete " and a web application are developed for implementing the proposed procedures .
Python is a popular dynamic language with a large part of its appeal coming from powerful libraries and extension modules . These augment the language and make it a productive environment for a wide variety of tasks , ranging from web development ( Django ) to numerical analysis ( NumPy ) . Unfortunately , Python ' s performance is quite poor when compared to modern implementations of languages such as Lua and JavaScript . Why does Python lag so far behind these other languages ? As we show , the very same API and extension libraries that make Python a powerful language also make it very difficult to efficiently execute . Given that we want to retain access to the great extension libraries that already exist for Python , how fast can we make it ? To evaluate this , we designed and implemented Falcon , a high-performance bytecode interpreter fully compatible with the standard CPython interpreter . Falcon applies a number of well known optimizations and introduces several new techniques to speed up execution of Python bytecode . In our evaluation , we found Falcon an average of 00% faster than the standard Python interpreter on most benchmarks and in some cases about 0 . 0X faster .
A method has been developed for the analysis of images of sentinel lymph nodes generated by a spectral scanning device . The aim is to classify the nodes , excised during surgery for breast cancer , as normal or metastatic . The data from one node constitute spectra at 00 wavelengths for each pixel of a 00*00 grid . For the analysis , the spectra are reduced to scores on two factors , one derived externally from a linear discriminant analysis using spectra taken manually from known normal and metastatic tissue , and one derived from the node under investigation to capture variability orthogonal to the external factor . Then a three-group mixture model ( normal , metastatic , non-nodal background ) using multivariate t distributions is fitted to the scores , with external data being used to specify informative prior distributions for the parameters of the three distributions . A Markov random field prior imposes smoothness on the image generated by the model . Finally , the node is classified as metastatic if any one pixel in this smoothed image is classified as metastatic . The model parameters were tuned on a training set of nodes , and then the tuned model was tested on a separate validation set of nodes , achieving satisfactory sensitivity and specificity . The aim in developing the analysis was to allow flexibility in the way each node is modelled whilst still using external information . The Bayesian framework employed is ideal for this .
Various adaptive randomization procedures ( adaptive designs ) have been proposed to clinical trials . This paper discusses several broad families of procedures , such as the play-the-winner rule and Markov chain model , randomized play-the-winner rule and urn models , drop-the-loser rule , doubly biased coin adaptive design . Asymptotic theories are presented with several pivotal proofs . The effect of delayed responses , the power and variability comparison of these designs are also discussed .
Approximate counting via correlation decay is the core algorithmic technique used in the sharp delineation of the computational phase transition that arises in the approximation of the partition function of anti-ferromagnetic two-spin models . Previous analyses of correlation-decay algorithms implicitly depended on the occurrence of strong spatial mixing ( SSM ) . This means that one uses worst-case analysis of the recursive procedure that creates the sub-instances . We develop a new analysis method that is more refined than the worst-case analysis . We take the shape of instances in the computation tree into consideration and amortise against certain " bad " instances that are created as the recursion proceeds . This enables us to show correlation decay and to obtain an FPTAS even when SSM fails . We apply our technique to the problem of approximately counting independent sets in hypergraphs with degree upper-bound Delta and with a lower bound k on the arity of hyperedges . Liu and Lin gave an FPTAS for k>=0 and Delta<=0 ( lack of SSM was the obstacle preventing this algorithm from being generalised to Delta=0 ) . Our technique gives a tight result for Delta=0 , showing that there is an FPTAS for k>=0 and Delta<=0 . The best previously-known approximation scheme for Delta=0 is the Markov-chain simulation based FPRAS of Bordewich , Dyer and Karpinski , which only works for k>=0 . Our technique also applies for larger values of k , giving an FPTAS for k>=Delta . This bound is not substantially stronger than existing randomised results in the literature . Nevertheless , it gives the first deterministic approximation scheme in this regime . Moreover , unlike existing results , it leads to an FPTAS for counting dominating sets in regular graphs with sufficiently large degree . We further demonstrate that approximately counting independent sets in hypergraphs is NP-hard even within the uniqueness regime .
We consider the problem where we have a multi-way table of means , indexed by several factors , where each factor can have a large number of levels . The entry in each cell is the mean of some response , averaged over the observations falling into that cell . Some cells may be very sparsely populated , and in extreme cases , not populated at all . We might still like to estimate an expected response in such cells . We propose here a novel hierarchical ANOVA ( HANOVA ) representation for such data . Sparse cells will lean more on the lower-order interaction model for the data . These in turn could have components that are poorly represented in the data , in which case they rely on yet lower-order models . Our approach leads to a simple hierarchical algorithm , requiring repeated calculations of sub-table means of modified counts . The algorithm has shown superiority over the unshrinked methods in both simulations and real data sets .
Combining and summarizing meta-data from various kinds of data sources is one possible solution to the data fragmentation we are suffering from . Multiple projects have addressed this issue already . This paper presents a new approach named Memacs . It automatically generates a detailed linked diary of our digital artifacts scattered across local files of multiple formats as well as data silos of the internet . Being elegantly simple and open , Memacs uses already existing visualization features of GNU Emacs and Org-mode to provide a promising platform for life-logging , Quantified Self movement , and people looking for advanced Personal Information Management ( PIM ) in general .
We consider the problem of performing matrix completion with side information on row-by-row and column-by-column similarities . We build upon recent proposals for matrix estimation with smoothness constraints with respect to row and column graphs . We present a novel iterative procedure for directly minimizing an information criterion in order to select an appropriate amount row and column smoothing , namely perform model selection . We also discuss how to exploit the special structure of the problem to scale up the estimation and model selection procedure via the Hutchinson estimator . We present simulation results and an application to predicting associations in imaging-genomics studies .
We present here a formal foundation for an iterative and incremental approach to constructing and evaluating preference queries . Our main focus is on query modification : a query transformation approach which works by revising the preference relation in the query . We provide a detailed analysis of the cases where the order-theoretic properties of the preference relation are preserved by the revision . We consider a number of different revision operators : union , prioritized and Pareto composition . We also formulate algebraic laws that enable incremental evaluation of preference queries . Finally , we consider two variations of the basic framework : finite restrictions of preference relations and weak-order extensions of strict partial order preference relations .
Non-technical losses ( NTL ) in electric power grids arise through electricity theft , broken electric meters or billing errors . They can harm the power supplier as well as the whole economy of a country through losses of up to 00% of the total power distribution . For NTL detection , researchers use artificial intelligence to analyse data . This work is about improving the extraction of more meaningful features from a data set . With these features , the prediction quality will increase .
In this paper we provide a preliminary analysis of Google+ privacy . We identified that Google+ shares photo metadata with users who can access the photograph and discuss its potential impact on privacy . We also identified that Google+ encourages the provision of other names including maiden name , which may help criminals performing identity theft . We show that Facebook lists are a superset of Google+ circles , both functionally and logically , even though Google+ provides a better user interface . Finally we compare the use of encryption and depth of privacy control in Google+ versus in Facebook .
We study parameter inference in large-scale latent variable models . We first propose an unified treatment of online inference for latent variable models from a non-canonical exponential family , and draw explicit links between several previously proposed frequentist or Bayesian methods . We then propose a novel inference method for the frequentist estimation of parameters , that adapts MCMC methods to online inference of latent variable models with the proper use of local Gibbs sampling . Then , for latent Dirich-let allocation , we provide an extensive set of experiments and comparisons with existing work , where our new approach outperforms all previously proposed methods . In particular , using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods . Moreover , Bayesian inference through variational methods perform poorly , sometimes leading to worse fits with latent variables of higher dimensionality .
Most professional golfers and analysts think that winning on the PGA Tour peaks when golfers are in their thirties . Rather than relying on educated guesses , we can actually use available statistical data to determine the actual ages at which golfers peak their golf game . We can also test the hypothesis that age affects winning professional golf tournaments . Using data available from the Golf Channel , the PGA Tour , and LPGA Tour , I calculated and provided the mean , the median , and the mode ages at which professional golfers on the PGA , European PGA , Champions , and LPGA Tours had won over a five-year period . More specifically , the ages at which golfers on the PGA , European PGA , Champions Tour , and LPGA Tours peak their wins are 00 , 00 , 00 , and 00 , respectively . The regression analyses I conducted seem to support my hypothesis that age affects winning professional golf tournaments .
Authenticating websites is an ongoing problem for users . Recent proposals have suggested strengthening current server authentication methods by incorporating website location as an additional authentication factor . In this work , we explore how location information affects users ' decision-making for security and privacy . We conducted a series of qualitative interviews to learn how location can be integrated into users ' decision-making for security , and we designed a security indicator to alert the user to changes in website locations . We evaluated our tool in a 00-participant user study and found that users were less likely to perform security-sensitive tasks when alerted to location changes . Our results suggest that website location can be used as an effective indicator for users ' security assessment .
Due to rapid expansion of urban areas in recent years , management of curbside parking has become increasingly important . To mitigate congestion , while meeting a city ' s diverse needs , performance-based pricing schemes have received a significant amount of attention . However , several recent studies suggest location , time-of-day , and awareness of policies are the primary factors that drive parking decisions . In light of this , we provide an extensive data-driven study of the spatio-temporal characteristics of curbside parking . This work advances the understanding of where and when to set pricing policies , as well as where to target information and incentives to drivers looking to park . Harnessing data provided by the Seattle Department of Transportation , we develop a Gaussian mixture model based technique to identify zones with similar spatial parking demand as quantified by spatial autocorrelation . In support of this technique , we introduce a metric based on the repeatability of our Gaussian mixture model to investigate temporal consistency .
Parallel finite element algorithms based on object-oriented concepts are presented . Moreover , the design and implementation of a data structure proposed are utilized in realizing a parallel geometric multigrid method . The ParFEMapper and the ParFECommunicator are the key components of the data structure in the proposed parallel scheme . These classes are constructed based on the type of finite elements ( continuous or nonconforming or discontinuous ) used . The proposed solver is compared with the open source direct solvers , MUMPS and PasTiX . Further , the performance of the parallel multigrid solver is analyzed up to 0000 processors . The solver shows a very good speedup up to 000 processors and the problem size has to be increased in order to maintain the good speedup when the number of processors are increased further . As a result , the parallel solver is able to handle large scale problems on massively parallel supercomputers . The proposed parallel finite element algorithms and multigrid solver are implemented in our in-house package ParMooN .
Editing faces in videos is a popular yet challenging aspect of computer vision and graphics , which encompasses several applications including facial attractiveness enhancement , makeup transfer , face replacement , and expression manipulation . Simply applying image-based warping algorithms to video-based face editing produces temporal incoherence in the synthesized videos because it is impossible to consistently localize facial features in two frames representing two different faces in two different videos ( or even two consecutive frames representing the same face in one video ) . Therefore , high performance face editing usually requires significant manual manipulation . In this paper we propose a novel temporal-spatial-smooth warping ( TSSW ) algorithm to effectively exploit the temporal information in two consecutive frames , as well as the spatial smoothness within each frame . TSSW precisely estimates two control lattices in the horizontal and vertical directions respectively from the corresponding control lattices in the previous frame , by minimizing a novel energy function that unifies a data-driven term , a smoothness term , and feature point constraints . Corresponding warping surfaces then precisely map source frames to the target frames . Experimental testing on facial attractiveness enhancement , makeup transfer , face replacement , and expression manipulation demonstrates that the proposed approaches can effectively preserve spatial smoothness and temporal coherence in editing facial geometry , skin detail , identity , and expression , which outperform the existing face editing methods . In particular , TSSW is robust to subtly inaccurate localization of feature points and is a vast improvement over image-based warping methods .
The concept of femtocell access points underlaying existing communication infrastructure has recently emerged as a key technology that can significantly improve the coverage and performance of next-generation wireless networks . In this paper , we propose a framework for macrocell-femtocell cooperation under a closed access policy , in which a femtocell user may act as a relay for macrocell users . In return , each cooperative macrocell user grants the femtocell user a fraction of its superframe . We formulate a coalitional game with macrocell and femtocell users being the players , which can take individual and distributed decisions on whether to cooperate or not , while maximizing a utility function that captures the cooperative gains , in terms of throughput and delay . We show that the network can selforganize into a partition composed of disjoint coalitions which constitutes the recursive core of the game representing a key solution concept for coalition formation games in partition form . Simulation results show that the proposed coalition formation algorithm yields significant gains in terms of average rate per macrocell user , reaching up to 000% , relative to the non-cooperative case . Moreover , the proposed approach shows an improvement in terms of femtocell users ' rate of up to 00% when compared to the traditional closed access policy .
Data Mining is a way of extracting data or uncovering hidden patterns of information from databases . So , there is a need to prevent the inference rules from being disclosed such that the more secure data sets cannot be identified from non sensitive attributes . This can be done through removing or adding certain item sets in the transactions Sanitization . The purpose is to hide the Inference rules , so that the user may not be able to discover any valuable information from other non sensitive data and any organisation can release all samples of their data without the fear of Knowledge Discovery In Databases which can be achieved by investigating frequently occurring item sets , rules that can be mined from them with the objective of hiding them . Another way is to release only limited samples in the new database so that there is no information loss and it also satisfies the legitimate needs of the users . The major problem is uncovering hidden patterns , which causes a threat to the database security . Sensitive data are inferred from non-sensitive data based on the semantics of the application the user has , commonly known as the inference problem . Two fundamental approaches to protect sensitive rules from disclosure are that , preventing rules from being generated by hiding the frequent sets of data items and reducing the importance of the rules by setting their confidence below a user-specified threshold .
We propose a Bayesian modeling framework for jointly analyzing multiple functional responses of different types ( e . g . binary and continuous data ) . Our approach is based on a multivariate latent Gaussian process and models the dependence among the functional responses through the dependence of the latent process . Our framework easily accommodates additional covariates . We offer a way to estimate the multivariate latent covariance , allowing for implementation of multivariate functional principal components analysis ( FPCA ) to specify basis expansions and simplify computation . We demonstrate our method through both simulation studies and an application to real data from a periodontal study .
A class of examples concerning the relationship of linear regression and maximal correlation is provided . More precisely , these examples show that if two random variables have ( strictly ) linear regression on each other , then their maximal correlation is not necessarily equal to their ( absolute ) correlation .
The valuation of real estates ( e . g . , house , land , among others ) is of extreme importance for decision making . Their singular characteristics make valuation through hedonic pricing methods dificult since the theory does not specify the correct regression functional form nor which explanatory variables should be included in the hedonic equation . In this article we perform real estate appraisal using a class of regression models proposed by Rigby & Stasinopoulos ( 0000 ) : generalized additive models for location , scale and shape ( GAMLSS ) . Our empirical analysis shows that these models seem to be more appropriate for estimation of the hedonic prices function than the regression models currently used to that end .
Langevin diffusion is a commonly used tool for sampling from a given distribution . In this work , we establish that when the target density $p^*$ is such that $\log p^*$ is $L$ smooth and $m$ strongly convex , discrete Langevin diffusion produces a distribution $p$ with $KL ( p||p^* ) \leq \epsilon$ in $\tilde{O} ( \frac{d}{\epsilon} ) $ steps , where $d$ is the dimension of the sample space . We also study the convergence rate when the strong-convexity assumption is absent . By considering the Langevin diffusion as a gradient flow in the space of probability distributions , we obtain an elegant analysis that applies to the stronger property of convergence in KL-divergence and gives a conceptually simpler proof of the best-known convergence results in weaker metrics .
We propose a generalized partially linear functional single index risk score model for repeatedly measured outcomes where the index itself is a function of time . We fuse the nonparametric kernel method and regression spline method , and modify the generalized estimating equation to facilitate estimation and inference . We use local smoothing kernel to estimate the unspecified coefficient functions of time , and use B-splines to estimate the unspecified function of the single index component . The covariance structure is taken into account via a working model , which provides valid estimation and inference procedure whether or not it captures the true covariance . The estimation method is applicable to both continuous and discrete outcomes . We derive large sample properties of the estimation procedure and show a different convergence rate for each component of the model . The asymptotic properties when the kernel and regression spline methods are combined in a nested fashion has not been studied prior to this work , even in the independent data case .
It is well known that the SOM algorithm achieves a clustering of data which can be interpreted as an extension of Principal Component Analysis , because of its topology-preserving property . But the SOM algorithm can only process real-valued data . In previous papers , we have proposed several methods based on the SOM algorithm to analyze categorical data , which is the case in survey data . In this paper , we present these methods in a unified manner . The first one ( Kohonen Multiple Correspondence Analysis , KMCA ) deals only with the modalities , while the two others ( Kohonen Multiple Correspondence Analysis with individuals , KMCA\_ind , Kohonen algorithm on DISJonctive table , KDISJ ) can take into account the individuals , and the modalities simultaneously .
Semi-supervised wrapper methods are concerned with building effective supervised classifiers from partially labeled data . Though previous works have succeeded in some fields , it is still difficult to apply semi-supervised wrapper methods to practice because the assumptions those methods rely on tend to be unrealistic in practice . For practical use , this paper proposes a novel semi-supervised wrapper method , Dual Teaching , whose assumptions are easy to set up . Dual Teaching adopts two external classifiers to estimate the false positives and false negatives of the base learner . Only if the recall of every external classifier is greater than zero and the sum of the precision is greater than one , Dual Teaching will train a base learner from partially labeled data as effectively as the fully-labeled-data-trained classifier . The effectiveness of Dual Teaching is proved in both theory and practice .
We present an outline of the theory of certain L\ ' evy-driven , multivariate stochastic processes , where the processes are represented by rational transfer functions ( Continuous-time AutoRegressive Moving Average or CARMA models ) and their applications in non-Gaussian time series modelling . We discuss in detail their definition , their spectral representation , the equivalence to linear state space models and further properties like the second order structure and the tail behaviour under a heavy-tailed input . Furthermore , we study the estimation of the parameters using quasi-maximum likelihood estimates for the auto-regressive and moving average parameters , as well as how to estimate the driving L\ ' evy process .
Exclusive social groups are ones in which the group members decide whether or not to admit a candidate to the group . Examples of exclusive social groups include academic departments and fraternal organizations . In the present paper we introduce an analytic framework for studying the dynamics of exclusive social groups . In our model , every group member is characterized by his opinion , which is represented as a point on the real line . The group evolves in discrete time steps through a voting process carried out by the group ' s members . Due to homophily , each member votes for the candidate who is more similar to him ( i . e . , closer to him on the line ) . An admission rule is then applied to determine which candidate , if any , is admitted . We consider several natural admission rules including majority and consensus . We ask : how do different admission rules affect the composition of the group in the long term ? We study both growing groups ( where new members join old ones ) and fixed-size groups ( where new members replace those who quit ) . Our analysis reveals intriguing phenomena and phase transitions , some of which are quite counterintuitive .
Despite the recent developments that allowed neural networks to achieve impressive performance on a variety of applications , these models are intrinsically affected by the problem of overgeneralization , due to their partitioning of the full input space into the fixed set of target classes used during training . Thus it is possible for novel inputs belonging to categories unknown during training or even completely unrecognizable to humans to fool the system into classifying them as one of the known classes , even with a high degree of confidence . Solving this problem may help improve the security of such systems in critical applications , and may further lead to applications in the context of open set recognition and 0-class recognition . This paper presents a novel way to compute a confidence score using denoising autoencoders and shows that such confidence score can correctly identify the regions of the input space close to the training distribution by approximately identifying its local maxima .
One of the advantages that decision trees have over many other models is their ability to natively handle categorical predictors without having to first transform them ( e . g . , by using one-hot encoding ) . However , in this paper , we show how this capability can also lead to an inherent " absent levels " problem for decision tree based algorithms that , to the best of our knowledge , has never been thoroughly discussed , and whose consequences have never been carefully explored . This predicament occurs whenever there is indeterminacy in how to handle an observation that has reached a categorical split which was determined when the observation ' s level was absent during training . Although these incidents may appear to be innocuous , by using Leo Breiman and Adele Cutler ' s random forests FORTRAN code and the randomForest R package as motivating case studies , we show how overlooking the absent levels problem can systematically bias a model . Afterwards , we discuss some heuristics that can possibly be used to help mitigate the absent levels problem and , using three real data examples taken from public repositories , we demonstrate the superior performance and reliability of these heuristics over some of the existing approaches that are currently being employed in practice due to oversights in the software implementations of decision tree based algorithms . Given how extensively these algorithms have been used , it is conceivable that a sizable number of these models have been unknowingly and seriously affected by this issue---further emphasizing the need for the development of both theory and software that accounts for the absent levels problem .
Spatial Independent Components Analysis ( ICA ) is increasingly used in the context of functional Magnetic Resonance Imaging ( fMRI ) to study cognition and brain pathologies . Salient features present in some of the extracted Independent Components ( ICs ) can be interpreted as brain networks , but the segmentation of the corresponding regions from ICs is still ill-controlled . Here we propose a new ICA-based procedure for extraction of sparse features from fMRI datasets . Specifically , we introduce a new thresholding procedure that controls the deviation from isotropy in the ICA mixing model . Unlike current heuristics , our procedure guarantees an exact , possibly conservative , level of specificity in feature detection . We evaluate the sensitivity and specificity of the method on synthetic and fMRI data and show that it outperforms state-of-the-art approaches .
In this paper , we propose a new system design framework for large vocabulary automatic chord estimation . Our approach is based on an integration of traditional sequence segmentation processes and deep learning chord classification techniques . We systematically explore the design space of the proposed framework for a range of parameters , namely deep neural nets , network configurations , input feature representations , segment tiling schemes , and training data sizes . Experimental results show that among the three proposed deep neural nets and a baseline model , the recurrent neural network based system has the best average chord quality accuracy that significantly outperforms the other considered models . Furthermore , our bias-variance analysis has identified a glass ceiling as a potential hindrance to future improvements of large vocabulary automatic chord estimation systems .
The propensity score analysis is one of the most widely used methods for studying the causal treatment effect in observational studies . This paper studies treatment effect estimation with the method of matching weights . This method resembles propensity score matching but offers a number of new features including efficient estimation , rigorous variance calculation , simple asymptotics , statistical tests of balance , clearly identified target population with optimal sampling property , and no need for choosing matching algorithm and caliper size . In addition , we propose the mirror histogram as a useful tool for graphically displaying balance . The method also shares some features of the inverse probability weighting methods , but the computation remains stable when the propensity scores approach 0 or 0 . An augmented version of the matching weight estimator is developed that has the double robust property , i . e . , the estimator is consistent if either the outcome model or the propensity score model is correct . In the numerical studies , the proposed methods demonstrated better performance than many widely used propensity score analysis methods such as stratification by quintiles , matching with propensity scores , and inverse probability weighting .
Cloud computing provides an effective business model for the deployment of IT infrastructure , platform , and software services . Often , facilities are outsourced to cloud providers and this offers the service consumer virtualization technologies without the added cost burden of development . However , virtualization introduces serious threats to service delivery such as Denial of Service ( DoS ) attacks , Cross-VM Cache Side Channel attacks , Hypervisor Escape and Hyper-jacking . One of the most sophisticated forms of attack is the cross-VM cache side channel attack that exploits shared cache memory between VMs . A cache side channel attack results in side channel data leakage , such as cryptographic keys . Various techniques used by the attackers to launch cache side channel attack are presented , as is a critical analysis of countermeasures against cache side channel attacks .
This paper develops theoretical results regarding noisy 0-bit compressed sensing and sparse binomial regression . We show that a single convex program gives an accurate estimate of the signal , or coefficient vector , for both of these models . We demonstrate that an s-sparse signal in R^n can be accurately estimated from m = O ( slog ( n/s ) ) single-bit measurements using a simple convex program . This remains true even if each measurement bit is flipped with probability nearly 0/0 . Worst-case ( adversarial ) noise can also be accounted for , and uniform results that hold for all sparse inputs are derived as well . In the terminology of sparse logistic regression , we show that O ( slog ( n/s ) ) Bernoulli trials are sufficient to estimate a coefficient vector in R^n which is approximately s-sparse . Moreover , the same convex program works for virtually all generalized linear models , in which the link function may be unknown . To our knowledge , these are the first results that tie together the theory of sparse logistic regression to 0-bit compressed sensing . Our results apply to general signal structures aside from sparsity ; one only needs to know the size of the set K where signals reside . The size is given by the mean width of K , a computable quantity whose square serves as a robust extension of the dimension .
Inverse classification is the process of manipulating an instance such that it is more likely to conform to a specific class . Past methods that address such a problem have shortcomings . Greedy methods make changes that are overly radical , often relying on data that is strictly discrete . Other methods rely on certain data points , the presence of which cannot be guaranteed . In this paper we propose a general framework and method that overcomes these and other limitations . The formulation of our method can use any differentiable classification function . We demonstrate the method by using logistic regression and Gaussian kernel SVMs . We constrain the inverse classification to occur on features that can actually be changed , each of which incurs an individual cost . We further subject such changes to fall within a certain level of cumulative change ( budget ) . Our framework can also accommodate the estimation of ( indirectly changeable ) features whose values change as a consequence of actions taken . Furthermore , we propose two methods for specifying feature-value ranges that result in different algorithmic behavior . We apply our method , and a proposed sensitivity analysis-based benchmark method , to two freely available datasets : Student Performance from the UCI Machine Learning Repository and a real world cardiovascular disease dataset . The results obtained demonstrate the validity and benefits of our framework and method .
This is the preprint version of our paper on IEEE Virtual Reality Conference 0000 . A touch-less interaction technology on vision based wearable device is designed and evaluated . Users interact with the application with dynamic hands/feet gestures in front of the camera . Several proof-of-concept prototypes with eleven dynamic gestures are developed based on the touch-less interaction . At last , a comparing user study evaluation is proposed to demonstrate the usability of the touch-less approach , as well as the impact on user ' s emotion , running on a wearable framework or Google Glass .
We study maximum likelihood estimation in log-linear models under conditional Poisson sampling schemes . We derive necessary and sufficient conditions for existence of the maximum likelihood estimator ( MLE ) of the model parameters and investigate estimability of the natural and mean-value parameters under a nonexistent MLE . Our conditions focus on the role of sampling zeros in the observed table . We situate our results within the framework of extended exponential families , and we exploit the geometric properties of log-linear models . We propose algorithms for extended maximum likelihood estimation that improve and correct the existing algorithms for log-linear model analysis .
Alloy is formal modeling language based on first-order relational logic , with no specific support for specifying reactive systems . We propose the usage of temporal logic to specify such systems , and show how bounded model checking can be performed with the Alloy Analyzer .
We consider a coloring problem on dynamic , one-dimensional point sets : points appearing and disappearing on a line at given times . We wish to color them with k colors so that at any time , any sequence of p ( k ) consecutive points , for some function p , contains at least one point of each color . We prove that no such function p ( k ) exists in general . However , in the restricted case in which points appear gradually , but never disappear , we give a coloring algorithm guaranteeing the property at any time with p ( k ) =0k-0 . This can be interpreted as coloring point sets in R^0 with k colors such that any bottomless rectangle containing at least 0k-0 points contains at least one point of each color . Here a bottomless rectangle is an axis-aligned rectangle whose bottom edge is below the lowest point of the set . For this problem , we also prove a lower bound p ( k ) >ck , where c>0 . 00 . Hence for every k there exists a point set , every k-coloring of which is such that there exists a bottomless rectangle containing ck points and missing at least one of the k colors . Chen et al . ( 0000 ) proved that no such function $p ( k ) $ exists in the case of general axis-aligned rectangles . Our result also complements recent results from Keszegh and Palvolgyi on cover-decomposability of octants ( 0000 , 0000 ) .
Watermarking algorithms needs properties of robustness and perceptibility . But these properties are affected by different -0 types of attacks performed on watermarked images . The goal of performing attacks is destroy the information of watermark hidden in the watermarked image . So every Algorithms should be previously tested by developers so that it would not affected by attacks .
Routing games are one of the most successful domains of application of game theory . It is well understood that simple dynamics converge to equilibria , whose performance is nearly optimal regardless of the size of the network or the number of agents . These strong theoretical assertions prompt a natural question : How well do these pen-and-paper calculations agree with the reality of everyday traffic routing ? We focus on a semantically rich dataset from Singapore ' s National Science Experiment that captures detailed information about the daily behavior of thousands of Singaporean students . Using this dataset , we can identify the routes as well as the modes of transportation used by the students , e . g . car ( driving or being driven to school ) versus bus or metro , estimate source and sink destinations ( home-school ) and trip duration , as well as their mode-dependent available routes . We quantify both the system and individual optimality . Our estimate of the Empirical Price of Anarchy lies between 0 . 00 and 0 . 00 . Individually , the typical behavior is consistent from day to day and nearly optimal , with low regret for not deviating to alternative paths .
We propose a variable decomposition algorithm -greedy block coordinate descent ( GBCD ) - in order to make dense Gaussian process regression practical for large scale problems . GBCD breaks a large scale optimization into a series of small sub-problems . The challenge in variable decomposition algorithms is the identification of a subproblem ( the active set of variables ) that yields the largest improvement . We analyze the limitations of existing methods and cast the active set selection into a zero-norm constrained optimization problem that we solve using greedy methods . By directly estimating the decrease in the objective function , we obtain not only efficient approximate solutions for GBCD , but we are also able to demonstrate that the method is globally convergent . Empirical comparisons against competing dense methods like Conjugate Gradient or SMO show that GBCD is an order of magnitude faster . Comparisons against sparse GP methods show that GBCD is both accurate and capable of handling datasets of 000 , 000 samples or more .
Deep neural networks currently demonstrate state-of-the-art performance in several domains . At the same time , models of this class are very demanding in terms of computational resources . In particular , a large amount of memory is required by commonly used fully-connected layers , making it hard to use the models on low-end devices and stopping the further increase of the model size . In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved . In particular , for the Very Deep VGG networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 000000 times leading to the compression factor of the whole network up to 0 times .
In this paper , we compare two analytical models for evaluation of cache coherence overhead of a shared bus multiprocessor with private caches . The models are based on a closed queuing network with different service disciplines . We find that the priority discipline can be used as a lower-level bound . Some numerical results are shown graphically .
Smart devices with built-in sensors , computational capabilities , and network connectivity have become increasingly pervasive . The crowds of smart devices offer opportunities to collectively sense and perform computing tasks in an unprecedented scale . This paper presents Crowd-ML , a privacy-preserving machine learning framework for a crowd of smart devices , which can solve a wide range of learning problems for crowdsensing data with differential privacy guarantees . Crowd-ML endows a crowdsensing system with an ability to learn classifiers or predictors online from crowdsensing data privately with minimal computational overheads on devices and servers , suitable for a practical and large-scale employment of the framework . We analyze the performance and the scalability of Crowd-ML , and implement the system with off-the-shelf smartphones as a proof of concept . We demonstrate the advantages of Crowd-ML with real and simulated experiments under various conditions .
This paper introduces the combinatorial Boolean model ( CBM ) , which is defined as the class of linear combinations of conjunctions of Boolean attributes . This paper addresses the issue of learning CBM from labeled data . CBM is of high knowledge interpretability but na\ " {i}ve learning of it requires exponentially large computation time with respect to data dimension and sample size . To overcome this computational difficulty , we propose an algorithm GRAB ( GRAfting for Boolean datasets ) , which efficiently learns CBM within the $L_0$-regularized loss minimization framework . The key idea of GRAB is to reduce the loss minimization problem to the weighted frequent itemset mining , in which frequent patterns are efficiently computable . We employ benchmark datasets to empirically demonstrate that GRAB is effective in terms of computational efficiency , prediction accuracy and knowledge discovery .
We derive explicit expressions for a family of radially symmetric , non-differentiable , Spartan covariance functions in $\mathbb{R}^0$ that involve the modified Bessel function of the second kind . In addition to the characteristic length and the amplitude coefficient , the Spartan covariance parameters include the rigidity coefficient $\eta_{0}$ which determines the shape of the covariance function . If $ \eta_{0} >> 0$ Spartan covariance functions exhibit multiscaling . We also derive a family of radially symmetric , infinitely differentiable Bessel-Lommel covariance functions valid in $\mathbb{R}^{d} , d\ge 0$ . We investigate the parametric dependence of the integral range for Spartan and Bessel-Lommel covariance functions using explicit relations and numerical simulations . Finally , we define a generalized spectrum of correlation scales $\lambda^{ ( \alpha ) }_{c}$ in terms of the fractional Laplacian of the covariance function ; for $0 \le \alpha \le0$ the $\lambda^{ ( \alpha ) }_{c}$ extend from the smoothness microscale $ ( \alpha=0 ) $ to the integral range $ ( \alpha=0 ) $ . The smoothness scale of mean-square continuous but non-differentiable random fields vanishes ; such fields , however , can be discriminated by means of $\lambda^{ ( \alpha ) }_{c}$ scales obtained for $\alpha <0$ .
Efficient Maximum Inner Product Search ( MIPS ) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes . Solutions based on locality-sensitive hashing ( LSH ) as well as tree-based solutions have been investigated in the recent literature , to perform approximate MIPS in sublinear time . In this paper , we compare these to another extremely simple approach for solving approximate MIPS , based on variants of the k-means clustering algorithm . Specifically , we propose to train a spherical k-means , after having reduced the MIPS problem to a Maximum Cosine Similarity Search ( MCSS ) . Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings , show that this simple approach yields much higher speedups , for the same retrieval precision , than current state-of-the-art hashing-based and tree-based methods . This simple method also yields more robust retrievals when the query is corrupted by noise .
We present a second iteration of a machine learning approach to static code analysis and fingerprinting for weaknesses related to security , software engineering , and others using the open-source MARF framework and the MARFCAT application based on it for the NIST ' s SATE IV static analysis tool exposition workshop ' s data sets that include additional test cases , including new large synthetic cases . To aid detection of weak or vulnerable code , including source or binary on different platforms the machine learning approach proved to be fast and accurate to for such tasks where other tools are either much slower or have much smaller recall of known vulnerabilities . We use signal and NLP processing techniques in our approach to accomplish the identification and classification tasks . MARFCAT ' s design from the beginning in 0000 made is independent of the language being analyzed , source code , bytecode , or binary . In this follow up work with explore some preliminary results in this area . We evaluated also additional algorithms that were used to process the data .
We derive a general multiple state model for critical illness insurances . In contrast to the classical model , we take into account that the probability of death for a dread disease sufferer may depend on the duration of the disease , and the payment of benefits associated with a severe disease depends not only on the diagnosis but also on the disease stage . We apply the introduced model to the analysis of a critical illness insurance against the risk of lung cancer . Based on the real data for the Lower Silesian Voivodship in Poland , we estimate the transition matrix , related to the discrete-time Markov model . The obtained probabilistic structure of the model can be directly used to cost not only critical illness insurances and life insurances with accelerated death benefits option , but also to viatical settlement contracts .
Consider an important meeting to be held in a team-based organization . Taking availability constraints into account , an online scheduling poll is being used in order to decide upon the exact time of the meeting . Decisions are to be taken during the meeting , therefore each team would like to maximize its relative attendance in the meeting ( i . e . , the proportional number of its participating team members ) . We introduce a corresponding game , where each team can declare ( in the scheduling poll ) a lower total availability , in order to improve its relative attendance---the pay-off . We are especially interested in situations where teams can form coalitions . We provide an efficient algorithm that , given a coalition , finds an optimal way for each team in a coalition to improve its pay-off . In contrast , we show that deciding whether such a coalition exists is NP-hard . We also study the existence of Nash equilibria : Finding Nash equilibria for various small sizes of teams and coalitions can be done in polynomial time while it is coNP-hard if the coalition size is unbounded .
Existing models of Multi-Hop Wireless Networks ( MHWNs ) assume that interference estimators of link quality such as observed busy time predict the capacity of the links . We show that these estimators do not capture the intricate interactions that occur at the scheduling level , which have a large impact on effective link capacity under contention based MAC protocols . We observe that scheduling problems arise only among those interfering sources whose concurrent transmissions cannot be prevented by the MAC protocol ' s collision management mechanisms ; other interfering sources can arbitrate the medium and coexist successfully . Based on this observation , we propose a methodology for rating links and show that it achieves high correlation with observed behavior in simulation . We then use this rating as part of a branch-and-bound framework based on a linear programming formulation for traffic engineering in static MHWNs and show that it achieves considerable improvement in performance relative to interference based models .
This paper presents the kinematic analysis of the 0-PPPS parallel robot with an equilateral mobile platform and a U-shape base . The proposed design and appropriate selection of parameters allow to formulate simpler direct and inverse kinematics for the manipulator under study . The parallel singularities associated with the manipulator depend only on the orientation of the end-effector , and thus depend only on the orientation of the end effector . The quaternion parameters are used to represent the aspects , i . e . the singularity free regions of the workspace . A cylindrical algebraic decomposition is used to characterize the workspace and joint space with a low number of cells . The dis-criminant variety is obtained to describe the boundaries of each cell . With these simplifications , the 0-PPPS parallel robot with proposed design can be claimed as the simplest 0 DOF robot , which further makes it useful for the industrial applications .
In this article , the estimation of reliability of a system is discussed $p ( y<x ) $ when strength , $X$ , and stress , $Y$ , are two independent exponential distribution with different scale parameters when the available data are type II Censored sample . Different methods for estimating the reliability are applied . The point estimators obtained are maximum likelihood estimator , uniformly minimum variance unbiased estimator , and Bayesian estimators based on conjugate and non informative prior distributions . A comparison of the estimates obtained is performed . Interval estimators of the reliability are also discussed .
Agile software development methodologies focus on software projects which are behind schedule or highly likely to have a problematic development phase . In the last decade , Agile methods have transformed from cult techniques to mainstream methodologies . Scrum , an Agile software development method , has been widely adopted due to its adaptive nature . This paper presents a metric that measures the quality of the testing process in a Scrum process . As product quality and process quality correlate , improved test quality can ensure high quality products . Also , gaining experience from eight years of successful Scrum implementation at SoftwarePeople , we describe the Scrum process emphasizing the testing process . We propose a metric Product Backlog Rating ( PBR ) to assess the testing process in Scrum . PBR considers the complexity of the features to be developed in an iteration of Scrum , assesses test ratings and offers a numerical score of the testing process . This metric is able to provide a comprehensive overview of the testing process over the development cycle of a product . We present a case study which shows how the metric is used at SoftwarePeople . The case study explains some features that have been developed in a Sprint in terms of feature complexity and potential test assessment difficulties and shows how PBR is calculated during the Sprint . We propose a test process assessment metric that provides insights into the Scrum testing process . However , the metric needs further evaluation considering associated resources ( e . g . , quality assurance engineers , the length of the Scrum cycle ) .
The inferential model ( IM ) approach , like fiducial and its generalizations , depends on a representation of the data-generating process . Here , a particular variation on the IM construction is considered , one based on generalized associations . The resulting generalized IM is more flexible than the basic IM in that it does not require a complete specification of the data-generating process and is provably valid under mild conditions . Computation and marginalization strategies are discussed , and two applications of this generalized IM approach are presented .
The margin of victory of an election is a useful measure to capture the robustness of an election outcome . It also plays a crucial role in determining the sample size of various algorithms in post election audit , polling etc . In this work , we present efficient sampling based algorithms for estimating the margin of victory of elections . More formally , we introduce the \textsc{$ ( c , \epsilon , \delta ) $--Margin of Victory} problem , where given an election $\mathcal{E}$ on $n$ voters , the goal is to estimate the margin of victory $M ( \mathcal{E} ) $ of $\mathcal{E}$ within an additive factor of $c MoV ( \mathcal{E} ) +\epsilon n$ . We study the \textsc{$ ( c , \epsilon , \delta ) $--Margin of Victory} problem for many commonly used voting rules including scoring rules , approval , Bucklin , maximin , and Copeland$^{\alpha} . $ We observe that even for the voting rules for which computing the margin of victory is NP-Hard , there may exist efficient sampling based algorithms , as observed in the cases of maximin and Copeland$^{\alpha}$ voting rules .
Recently a lot of multimedia applications are emerging on portable appliances . They require both the flexibility of upgradeable devices ( traditionally software based ) and a powerful computing engine ( typically hardware ) . In this context , programmable HW and dynamic reconfiguration allow novel approaches to the migration of algorithms from SW to HW . Thus , in the frame of the Symbad project , we propose an industrial design flow for reconfigurable SoC ' s . The goal of Symbad consists of developing a system level design platform for hardware and software SoC systems including formal and semi-formal verification techniques .
Discussion of " Estimating the historical and future probabilities of large terrorist events " by Aaron Clauset and Ryan Woodard [arXiv : 0000 . 0000] .
We have investigated the product life-cycles of almost 00 000 hit singles performed on the 00 biggest national phonographic markets in Europe including : Austria , Belgium , France , Germany , Ireland , Italy , Netherlands , Norway , Spain , Sweden , Switzerland and the United Kingdom . We have considered weekly singles charts from the last 00 years ( 0000-0000 ) in each country . We analyze the spread of hit singles popularity ( chart topping ) as an epidemiological process performed on various European countries . Popular hit singles are contagious from one country to another . Thus , we consider time delays between the initial hit single release and reaching the highest position on consecutive national singles charts . We create directed network of countries representing transitions of hit singles popularity between countries . It is obtained by simulating the most likely paths and picking up the most frequent links . A country of initial hit single release is considered as a source of infection . Our algorithm builds up spanning trees by attaching new nodes . The probability of attachment depends on : 0 ) new nodes immunity 0 ) infectivity of previous nodes from the tree . Thus we obtain network of popularity spread with a hub the UK , a bridge the Netherlands and outliers Italy and Spain . We have found a characteristic topology of hit singles popularity spread . The positive correlation between this network and geographic or cultural grid-map of Europe is also observed . However , the network of popularity spread has some typical properties of complex networks .
Representation learning is the foundation for the recent success of neural network models . However , the distributed representations generated by neural networks are far from ideal . Due to their highly entangled nature , they are di cult to reuse and interpret , and they do a poor job of capturing the sparsity which is present in real- world transformations . In this paper , I describe methods for learning disentangled representations in the two domains of graphics and computation . These methods allow neural methods to learn representations which are easy to interpret and reuse , yet they incur little or no penalty to performance . In the Graphics section , I demonstrate the ability of these methods to infer the generating parameters of images and rerender those images under novel conditions . In the Computation section , I describe a model which is able to factorize a multitask learning problem into subtasks and which experiences no catastrophic forgetting . Together these techniques provide the tools to design a wide range of models that learn disentangled representations and better model the factors of variation in the real world .
Background : The quality of a software product depends on the quality of the software process followed in developing the product . Therefore , many higher education institutions ( HEI ) and software organizations have implemented software process improvement ( SPI ) training courses to improve the software quality . Objective : Because the duration of a course is a concern for HEI and software organizations , we investigate whether the quality of software projects will be improved by reorganizing the activities of the ten assignments of the original personal software process ( PSP ) course into a modified PSP having fewer assignments ( i . e . , seven assignments ) . Method : The assignments were developed by following a modified PSP with fewer assignments but including the phases , forms , standards , and logs suggested in the original PSP . The measurement of the quality of the software assignments was based on defect density . Results : When the activities in the original PSP were reordered into fewer assignments , as practitioners progress through the PSP training , the defect density improved with statistical significance . Conclusions : Our modified PSP could be applied in academy and industrial environments which are concerned in the sense of reducing the PSP training time
For many years , we have observed industry struggling in defining a high quality requirements engineering ( RE ) and researchers trying to understand industrial expectations and problems . Although we are investigating the discipline with a plethora of empirical studies , they still do not allow for empirical generalisations . To lay an empirical and externally valid foundation about the state of the practice in RE , we aim at a series of open and reproducible surveys that allow us to steer future research in a problem-driven manner . We designed a globally distributed family of surveys in joint collaborations with different researchers and completed the first run in Germany . The instrument is based on a theory in the form of a set of hypotheses inferred from our experiences and available studies . We test each hypothesis in our theory and identify further candidates to extend the theory by correlation and Grounded Theory analysis . In this article , we report on the design of the family of surveys , its underlying theory , and the full results obtained from Germany with participants from 00 companies . The results reveal , for example , a tendency to improve RE via internally defined qualitative methods rather than relying on normative approaches like CMMI . We also discovered various RE problems that are statistically significant in practice . For instance , we could corroborate communication flaws or moving targets as problems in practice . Our results are not yet fully representative but already give first insights into current practices and problems in RE , and they allow us to draw lessons learnt for future replications . Our results obtained from this first run in Germany make us confident that the survey design and instrument are well-suited to be replicated and , thereby , to create a generalisable empirical basis of RE in practice .
The massive parallelism and resource sharing embodying today ' s cloud business model not only exacerbate the security challenge of timing channels , but also undermine the viability of defenses based on resource partitioning . We propose hypervisor-enforced timing mitigation to control timing channels in cloud environments . This approach closes " reference clocks " internal to the cloud by imposing a deterministic view of time on guest code , and uses timing mitigators to pace I/O and rate-limit potential information leakage to external observers . Our prototype hypervisor is the first system to mitigate timing-channel leakage across full-scale existing operating systems such as Linux and applications in arbitrary languages . Mitigation incurs a varying performance cost , depending on workload and tunable leakage-limiting parameters , but this cost may be justified for security-critical cloud applications and data .
Using Bernstein polynomial approximations , we prove the central limit theorem for linear spectral statistics of sample covariance matrices , indexed by a set of functions with continuous fourth order derivatives on an open interval including $[ ( 0-\sqrt{y} ) ^0 , ( 0+\sqrt{y} ) ^0]$ , the support of the Mar\u{c}enko--Pastur law . We also derive the explicit expressions for asymptotic mean and covariance functions .
In duty-cycled wireless sensor networks , deployed sensor nodes are usually put to sleep for energy efficiency according to sleep scheduling approaches . Any sleep scheduling scheme with its supporting protocols ensures that data can always be routed from source to sink . In this paper , we investigate a problem of multi-hop broadcast and routing in random sleep scheduling scheme , and propose a novel protocol , called DQSB , by quasi-synchronization mechanism to achieve reliable broadcast and less latency routing . DQSB neither assumes time synchronization which requires all neighboring nodes wake up at the same time , nor assumes duty-cycled awareness which makes it difficult to use in asynchronous WSNs . Furthermore , the benefit of quasi-synchronized mechanism for broadcast from sink to other nodes is the less latency routing paths for reverse data collection to sink because of no or less sleep waiting time . Simulation results show that DQSB outperforms the existing protocols in broadcast times performance and keeps relative tolerant broadcast latency performance , even in the case of unreliable links . The proposed DQSB protocol , in this paper , can be recognized as a tradeoff between broadcast times and broadcast latency . We also explore the impact of parameters in the assumption and the approach to get proper values for supporting DQSB .
The short text has been the prevalent format for information of Internet in recent decades , especially with the development of online social media , whose millions of users generate a vast number of short messages everyday . Although sophisticated signals delivered by the short text make it a promising source for topic modeling , its extreme sparsity and imbalance brings unprecedented challenges to conventional topic models like LDA and its variants . Aiming at presenting a simple but general solution for topic modeling in short texts , we present a word co-occurrence network based model named WNTM to tackle the sparsity and imbalance simultaneously . Different from previous approaches , WNTM models the distribution over topics for each word instead of learning topics for each document , which successfully enhance the semantic density of data space without importing too much time or space complexity . Meanwhile , the rich contextual information preserved in the word-word space also guarantees its sensitivity in identifying rare topics with convincing quality . Furthermore , employing the same Gibbs sampling with LDA makes WNTM easily to be extended to various application scenarios . Extensive validations on both short and normal texts testify the outperformance of WNTM as compared to baseline methods . And finally we also demonstrate its potential in precisely discovering newly emerging topics or unexpected events in Weibo at pretty early stages .
